@article{rayyan-364534539,
  title={AltCanvas: A Tile-Based Editor for Visual Content Creation with Generative AI for Blind or Visually Impaired People - Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  year={2024},
  author={Lee, Seonghee and Kohga, Maho and Landau, Steve and O'Modhrain, Sile and Subramonyam, Hari},
  url={https://doi.org/10.1145/3663548.3675600},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ASSETS '24},
  abstract={People with visual impairments often struggle to create content that relies heavily on visual elements, particularly when conveying spatial and structural information. Existing accessible drawing tools, which construct images line by line, are suitable for simple tasks like math but not for more expressive artwork. On the other hand, emerging generative AI-based text-to-image tools can produce expressive illustrations from descriptions in natural language, but they lack precise control over image composition and properties. To address this gap, our work integrates generative AI with a constructive approach that provides users with enhanced control and editing capabilities. Our system, AltCanvas, features a tile-based interface enabling users to construct visual scenes incrementally, with each tile representing an object within the scene. Users can add, edit, move, and arrange objects while receiving speech and audio feedback. Once completed, the scene can be rendered as a color illustration or as a vector for tactile graphic generation. Involving 14 blind or low-vision users in design and evaluation, we found that participants effectively used the AltCanvas’s workflow to create illustrations.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3663548.3675600},
  booktitle={Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  chapter={0}
}

@article{rayyan-364534544,
  title={I-Scratch: Independent Slide Creation With Auditory Comment and Haptic Interface for the Blind and Visually Impaired - Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  year={2025},
  author={Kim, Gyeongdeok and Lim, Chungman and Park, Gunhyuk},
  url={https://doi.org/10.1145/3706598.3713553},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI '25},
  keywords={The Blind and Visually Impaired, Accessibility, Slide Editing, Graphical Tactile Display, Multimodal Interfaces, AI Assistance},
  abstract={Presentation software still holds barriers to independent creation for blind and visually impaired users (BVIs) due to its visual-centric interface. To address this gap, we introduce I-Scratch, a multimodal system which empowers BVIs to independently create, explore, and edit PowerPoint slides. We initially designed I-Scratch to tackle the practical challenges faced by BVIs and refined I-Scratch to improve its usability and accessibility through iterative participatory sessions involving a blind user. I-Scratch integrates a graphical tactile display with auditory guidance for multimodal feedback, simplifies the user interface, and leverages AI technologies for visual assistance in image generation and content interpretation. A user study with ten BVIs demonstrated that I-Scratch enables them to produce visually coherent and aesthetically pleasing slides independently, achieving 91.25% of full and partial successes with a CSI score of 85.07. We present five guidelines and future directions to support the creative work of BVIs using presentation software.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3706598.3713553},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534545,
  title={In-Page Navigation Aids for Screen-Reader Users with Automatic Topicalisation and Labelling},
  year={2024},
  month={7},
  journal={ACM Trans. Access. Comput.},
  issn={1936-7228},
  volume={17},
  number={2},
  author={Silva, Jorge Sassaki Resende and Cardoso, Paula Christina Figueira and De Bettio, Raphael Winckler and Tavares, Daniela Cardoso and Silva, Carlos Alberto and Watanabe, Willian Massami and Freire, Andr\'{E} Pimenta},
  url={https://doi.org/10.1145/3649223},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  keywords={Accessibility, natural language processing, screen readers, topic segmentation and labelling, large language models, assistive technologies},
  abstract={Navigation aids such as headers and internal links provide vital support for screen-reader users on web documents to grasp a document’s structure. However, when such navigation aids are unavailable or not appropriately marked up, this situation can cause serious difficulties. This article presents the design and evaluation of a tool for automatically generating navigation aids with headers and internal links for screen readers with topicalisation and labelling algorithms. The proposed tool uses natural language processing techniques to divide a web document into topic segments and label each segment in two cycles based on its content. We conducted an initial user study in the first cycle with eight blind and partially-sighted screen reader users. The evaluation involved tasks with questions answered by participants with information from texts with and without automatically generated headers. The results in the first cycle provided preliminary indicators of performance improvement and cognitive load reduction. The second cycle involved co-designing an improved version with two blind experts in web accessibility, resulting in a browser extension which injects automatically generated headers and in-page navigation with internal links, along with improvements in the generation of labels using OpenAI’s ChatGPT. The browser extension was evaluated by seven blind participants using the same four texts used to evaluate the preliminary prototype developed in the first cycle. With the two development cycles, the study provided important insights into the design of navigation aids for screen-reader users using natural language processing techniques, including the potential use of generative artificial intelligence for assistive technologies and limitations that need to be explored in future research.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3649223},
  chapter={0}
}

@article{rayyan-364534551,
  title={Speaking with My Screen Reader: Using Audio Fictions to Explore Conversational Access to Interfaces - Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
  year={2023},
  author={Phutane, Mahika and Jung, Crescentia and Chen, Niu and Azenkot, Shiri},
  url={https://doi.org/10.1145/3597638.3608404},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ASSETS '23},
  keywords={blind and low vision, conversational agents, design fiction, interviews, screen readers, voice assistants},
  abstract={Conversational assistants, an inherently accessible mode of interaction for blind and low vision (BLV) individuals, offer opportunities to support nonvisual access in new ways. In this paper, we explore whether and how human-like conversations can support access to interfaces, advancing the impersonal linear access provided by screen readers today. We first interviewed 10 BLV participants about this approach, but found it difficult to situate conversations around a future technology. So we turned to a speculative design approach and created four audio fictions: pre-recorded dialogues between users and their hypothetical screen reader assistants wherein assistants assumed distinct roles: a friend, butler, expert, and caregiver. We presented the audio fictions to 14 BLV participants and found that personable conversations can meaningfully extend the screen reader experience. We observed a tension between AI adaptation and screen reader customization. Participants further expressed a need to maintain control at three distinct levels: granular cursor movement, screen representation, and task assistance. Through the lens of assistant roles, we address fundamental questions about anthropomorphizing CAs and assistive technology broadly.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3597638.3608404},
  booktitle={Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
  chapter={0}
}

@article{rayyan-364534554,
  title={Audio Description Automatons: Exploring Perspectives on Personas for Generative AI Description Writing Assistants - Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  year={2025},
  author={Jiang, Lucy and Zhu, Amanda and Oppegaard, Brett},
  url={https://doi.org/10.1145/3706599.3719966},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI EA '25},
  keywords={audio description, image description, generative AI, personas, blind, low vision, AI assistant},
  abstract={Visual media is often made accessible to blind and low vision (BLV) people through audio description (AD), typically written by experts. Prior efforts to increase the scale of description output have involved sighted novices as describers or used generative AI (GenAI) to automatically convert images to text; however, description quality remains a concern. To support novice describers in writing high quality descriptions, we designed and developed a GenAI-powered online tool, “Guidedogs,” featuring five dogs with unique names, images, and voices that provided immediate and varied feedback on draft descriptions. We piloted the tool during a large hackathon-style description workshop in 2024. Through 17 semi-structured interviews, we explored the efficacy of using metaphors as personas for AI assistants and gathered insights on participants’ perceptions on using AI for accessibility purposes. We contribute preliminary insights on generative AI assistant personas in an accessibility context and share design considerations to guide future work.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3706599.3719966},
  booktitle={Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534555,
  title={"I look at it as the king of knowledge": How Blind People Use and Understand Generative AI Tools - Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  year={2024},
  author={Adnin, Rudaiba and Das, Maitraye},
  url={https://doi.org/10.1145/3663548.3675631},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ASSETS '24},
  keywords={Accessibility, ChatGPT, Generative AI, blind, visual impairment},
  abstract={The proliferation of Generative Artificial Intelligence (GenAI) tools has brought a critical shift in how people approach information retrieval and content creation in diverse contexts. Yet, we have limited understanding of how blind people use and make sense of GenAI systems. To bridge this gap, we report findings from interviews with 19 blind individuals who incorporate mainstream GenAI tools like ChatGPT and Be My AI in their everyday practices. Our findings reveal how blind users navigate accessibility issues, inaccuracies, hallucinations, and idiosyncracies associated with GenAI and develop interesting (but often flawed) mental models of how these tools work. We discuss key considerations for rethinking access and information verification in GenAI tools, unpacking erroneous mental models among blind users, and reconciling harms and benefits of GenAI from an accessibility perspective.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"} | RAYYAN-LABELS: Unsure if scope of HCI applies},
  doi={10.1145/3663548.3675631},
  booktitle={Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  chapter={0}
}

@article{rayyan-364534583,
  title={Adapting Online Customer Reviews for Blind Users: A Case Study of Restaurant Reviews - Proceedings of the 22nd International Web for All Conference},
  year={2025},
  pages={135–146},
  author={Sunkara, Mohan and Kolgar Nayak, Akshay and Kalari, Sandeep and Prakash, Yash and Jayarathna, Sampath and Lee, Hae-Na and Ashok, Vikas},
  url={https://doi.org/10.1145/3744257.3744276},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={W4A '25},
  keywords={blind, screen reader, visual impairment, assistive technology, online discussion forum, large language model},
  abstract={Online reviews have become an integral aspect of consumer decision-making on e-commerce websites, especially in the restaurant industry. Unlike sighted users who can visually skim through the reviews, perusing reviews remains challenging for blind users, who rely on screen reader assistive technology that supports predominantly one-dimensional narration of content via keyboard shortcuts. In an interview study, we uncovered numerous pain points of blind screen reader users with online restaurant reviews, notably, the listening fatigue and frustration after going through only the first few reviews. To address these issues, we developed QuickCue assistive tool that performs aspect-focused sentiment-driven summarization to reorganize the information in the reviews into an alternative, thematically-organized presentation that is conveniently perusable with a screen reader. At its core, QuickCue utilizes a large language model to perform aspect-based joint classification for grouping reviews, followed by focused summarizations within the groups to generate concise representations of reviewers’ opinions, which are then presented to the screen reader users via an accessible interface. Evaluation of QuickCue in a user study with 10 participants showed significant improvements in overall usability and task workload compared to the status quo screen reader.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3744257.3744276},
  booktitle={Proceedings of the 22nd International Web for All Conference},
  chapter={0}
}

@article{rayyan-364534585,
  title={Everyday Uncertainty: How Blind People Use GenAI Tools for Information Access - Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  year={2025},
  author={Tang, Xinru and Abdolrahmani, Ali and Gergle, Darren and Piper, Anne Marie},
  url={https://doi.org/10.1145/3706598.3713433},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI '25},
  keywords={uncertainty, generative artificial intelligence, accessibility, blind, screen reader users},
  abstract={Generative AI (GenAI) tools promise to advance non-visual information access but introduce new challenges due to output errors, hallucinations, biases, and constantly changing capabilities. Through interviews with 20 blind screen reader users who use various GenAI applications for diverse tasks, we show how they approached information access with everyday uncertainty, or a mindset of skepticism and criticality towards both AI- and human-mediated assistance as well as information itself. Instead of expecting information to be ‘correct’ and ‘complete’, participants extracted cues from error-prone information sources; treated all information as tentative; acknowledged and explored information subjectivity; and constantly adjusted their expectations and strategies considering the politics around access. The concept of everyday uncertainty situates GenAI tools among the interconnected assistive applications, humans, and sociomaterial conditions that both enable and hinder the ongoing production of access. We discuss the implications of everyday uncertainty for future design and research.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3706598.3713433},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534586,
  title={MAIDR Meets AI: Exploring Multimodal LLM-Based Data Visualization Interpretation by and with Blind and Low-Vision Users - Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  year={2024},
  author={Seo, JooYoung and Kamath, Sanchita S. and Zeidieh, Aziz and Venkatesh, Saairam and McCurry, Sean},
  url={https://doi.org/10.1145/3663548.3675660},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ASSETS '24},
  keywords={Accessibility, Blind, Data Visualization, Generative AI, Large Language Models, Low Vision, Multimodality, Screen Readers},
  abstract={This paper investigates how blind and low-vision (BLV) users interact with multimodal large language models (LLMs) to interpret data visualizations. Building upon our previous work on the multimodal access and interactive data representation (MAIDR) framework, our mixed-visual-ability team co-designed maidrAI, an LLM extension providing multiple AI responses to users’ visual queries. To explore generative AI-based data representation, we conducted user studies with 8 BLV participants, tasking them with interpreting box plots using our system. We examined how participants personalize LLMs through prompt engineering, their preferences for data visualization descriptions, and strategies for verifying LLM responses. Our findings highlight three dimensions affecting BLV users’ decision-making process: modal preference, LLM customization, and multimodal data representation. This research contributes to designing more accessible data visualization tools for BLV users and advances the understanding of inclusive generative AI applications.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3663548.3675660},
  booktitle={Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  chapter={0}
}

@article{rayyan-364534596,
  title={Defining Patterns for a Conversational Web - Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  year={2023},
  author={Pucci, Emanuele and Possaghi, Isabella and Cutrupi, Claudia Maria and Baez, Marcos and Cappiello, Cinzia and Matera, Maristella},
  url={https://doi.org/10.1145/3544548.3581145},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI '23},
  keywords={Conversational Patterns, Conversational UIs, Conversational Web Browsing},
  abstract={Conversational agents are emerging as channels for a natural and accessible interaction with digital services. Their benefits span across a wide range of usage scenarios and address visual impairments and any situational impairments that may take advantage of voice-based interactions. A few works highlighted the potential and the feasibility of adopting conversational agents for making the Web truly accessible for everyone. Yet, there is still a lack of concrete guidance in designing conversational experiences for browsing the Web. This paper illustrates a human-centered process that involved 26 blind and visually impaired people to investigate their difficulties when using assistive technology for accessing the Web, and their attitudes and preferences on adopting conversational agents. In response to the identified challenges, the paper introduces patterns for conversational Web browsing. It also discusses design implications that can promote Conversational AI as a technology to enhance Web accessibility.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3544548.3581145},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534605,
  title={ImageExplorer Deployment: Understanding Text-Based and Touch-Based Image Exploration in the Wild - Proceedings of the 21st International Web for All Conference},
  year={2024},
  pages={59–69},
  author={Xu, Andi and Cai, Minyu and Hou, Dier and Chang, Ruei-Che and Guo, Anhong},
  url={https://doi.org/10.1145/3677846.3677861},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={W4A '24},
  keywords={Accessibility, Alt Text, Blind, Deployment, Image Caption, ImageExplorer, Screen Reader, Touch Exploration, Visual Impairment},
  abstract={Blind and visually-impaired (BVI) users often rely on alt-texts to understand images. AI-generated alt-texts can be scalable and efficient but may lack details and are prone to errors. Multi-layered touch interfaces, on the other hand, can provide rich details and spatial information, but may take longer to explore and cause higher mental load. To understand how BVI users leverage these two methods, we deployed ImageExplorer, an iOS app on the Apple App Store that provides multi-layered image information via both text-based and touch-based interfaces with customizable levels of granularity. Across 12 months, 371 users uploaded 651 images and explored 694 times. Their activities were logged to help us understand how BVI users consume image captions in the wild. This work informs a holistic understanding of BVI users’ image exploration behavior and influential factors. We provide design implications for future models of image captioning and visual access tools.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3677846.3677861},
  booktitle={Proceedings of the 21st International Web for All Conference},
  chapter={0}
}

@article{rayyan-364534614,
  title={Designing Accessible Obfuscation Support for Blind Individuals’ Visual Privacy Management - Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  year={2024},
  author={Zhang, Lotus and Stangl, Abigale and Sharma, Tanusree and Tseng, Yu-Yun and Xu, Inan and Gurari, Danna and Wang, Yang and Findlater, Leah},
  url={https://doi.org/10.1145/3613904.3642713},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI '24},
  keywords={accessibility, blind photography, privacy-preservation technology},
  abstract={Blind individuals commonly share photos in everyday life. Despite substantial interest from the blind community in being able to independently obfuscate private information in photos, existing tools are designed without their inputs. In this study, we prototyped a preliminary screen reader-accessible obfuscation interface to probe for feedback and design insights. We implemented a version of the prototype through off-the-shelf AI models (e.g., SAM, BLIP2, ChatGPT) and a Wizard-of-Oz version that provides human-authored guidance. Through a user study with 12 blind participants who obfuscated diverse private photos using the prototype, we uncovered how they understood and approached visual private content manipulation, how they reacted to frictions such as inaccuracy with existing AI models and cognitive load, and how they envisioned such tools to be better designed to support their needs (e.g., guidelines for describing visual obfuscation effects, co-creative interaction design that respects blind users’ agency).},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3613904.3642713},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534619,
  title={TableNarrator: Making Image Tables Accessible to Blind and Low Vision People - Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  year={2025},
  author={Mo, Ye and Huang, Gang and li, liangcheng and Deng, Dazhen and Yu, Zhi and Xu, Yilun and Ye, Kai and Zhou, Sheng and Bu, Jiajun},
  url={https://doi.org/10.1145/3706598.3714329},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI '25},
  keywords={Accessibility, Assistive Technology, Screen Reader, Image Tables, Computer Vision, Large Language Model},
  abstract={The widespread use of image tables presents significant accessibility challenges for blind and low vision (BLV) people, limiting their access to critical data. Despite advancements in artificial intelligence (AI) for interpreting image tables, current solutions often fail to consider the specific needs of BLV users, leading to a poor user experience. To address these issues, we introduce TableNarrator, an innovative system designed to enhance the accessibility of image tables. Informed by accessibility standards and user feedback, TableNarrator leverages AI to generate alternative text tailored to the cognitive and reading preferences of BLV users. It streamlines access through a simple interaction mode and offers personalized options. Our evaluations, from both technical and user perspectives, demonstrate that TableNarrator not only provides accurate and comprehensive table information but also significantly enhances the user experience for BLV people.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3706598.3714329},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534660,
  title={VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People - Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
  year={2025},
  author={Killough, Daniel and Feng, Justin and Ching, Zheng Xue and Wang, Daniel and Dyava, Rithvik and Tian, Yapeng and Zhao, Yuhang},
  url={https://doi.org/10.1145/3746059.3747641},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={UIST '25},
  keywords={Virtual reality, accessibility, computer vision, artificial intelligence, blindness, spatial audio, screen reader},
  abstract={Virtual Reality (VR) is inaccessible to blind people. While research has investigated many techniques to enhance VR accessibility, they require additional developer effort to integrate. As such, most mainstream VR apps remain inaccessible as the industry de-prioritizes accessibility. We present VRSight, an end-to-end system that recognizes VR scenes post hoc through a set of AI models (e.g., object detection, depth estimation, LLM-based atmosphere interpretation) and generates tone-based, spatial audio feedback, empowering blind users to interact in VR without developer intervention. To enable virtual element detection, we further contribute DISCOVR, a VR dataset consisting of 30 virtual object classes from 17 social VR apps, substituting real-world datasets that remain not applicable to VR contexts. Nine participants used VRSight to explore an off-the-shelf VR app (Rec Room), demonstrating its effectiveness in facilitating social tasks like avatar awareness and available seat identification.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3746059.3747641},
  booktitle={Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
  chapter={0}
}

@article{rayyan-364534727,
  title={VideoA11y: Method and Dataset for Accessible Video Description - Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  year={2025},
  author={Li, Chaoyu and Padmanabhuni, Sid and Cheema, Maryam S and Seifi, Hasti and Fazli, Pooyan},
  url={https://doi.org/10.1145/3706598.3714096},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI '25},
  keywords={Video Accessibility, Video Description, Video Understanding, Blind and Low Vision Users, Multimodal Large Language Models},
  abstract={Video descriptions are crucial for blind and low vision (BLV) users to access visual content. However, current artificial intelligence models for generating descriptions often fall short due to limitations in the quality of human annotations within training datasets, resulting in descriptions that do not fully meet BLV users’ needs. To address this gap, we introduce VideoA11y, an approach that leverages multimodal large language models (MLLMs) and video accessibility guidelines to generate descriptions tailored for BLV individuals. Using this method, we have curated VideoA11y-40K, the largest and most comprehensive dataset of 40,000 videos described for BLV users. Rigorous experiments across 15 video categories, involving 347 sighted participants, 40 BLV participants, and seven professional describers, showed that VideoA11y descriptions outperform novice human annotations and are comparable to trained human annotations in clarity, accuracy, objectivity, descriptiveness, and user satisfaction. We evaluated models on VideoA11y-40K using both standard and custom metrics, demonstrating that MLLMs fine-tuned on this dataset produce high-quality accessible descriptions. Code and dataset are available at https://people-robots.github.io/VideoA11y/.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3706598.3714096},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534734,
  title={Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People - Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  year={2024},
  author={Gonzalez Penuela, Ricardo E and Collins, Jazmin and Bennett, Cynthia and Azenkot, Shiri},
  url={https://doi.org/10.1145/3613904.3642211},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI '24},
  keywords={AI, Assistive Technology, Blind and Low Vision People, Computer Vision, Diary Study, Scene Description, Use cases},
  abstract={"Scene description" applications that describe visual content in a photo are useful daily tools for blind and low vision (BLV) people. Researchers have studied their use, but they have only explored those that leverage remote sighted assistants; little is known about applications that use AI to generate their descriptions. Thus, to investigate their use cases, we conducted a two-week diary study where 16 BLV participants used an AI-powered scene description application we designed. Through their diary entries and follow-up interviews, users shared their information goals and assessments of the visual descriptions they received. We analyzed the entries and found frequent use cases, such as identifying visual features of known objects, and surprising ones, such as avoiding contact with dangerous objects. We also found users scored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for satisfaction and 2.43 out of 4 (SD=1.16) for trust, showing that descriptions still need significant improvements to deliver satisfying and trustworthy experiences. We discuss future opportunities for AI as it becomes a more powerful accessibility tool for BLV users.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3613904.3642211},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534785,
  title={StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI - Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
  year={2025},
  author={Froehlich, Jon E. and Fiannaca, Alexander J. and Jaber, Nimer M and Tsaran, Victor and Kane, Shaun K.},
  url={https://doi.org/10.1145/3746059.3747756},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={UIST '25},
  keywords={Accessible maps, Multimodal LLMs, AI Chat, Street View},
  abstract={Interactive streetscape mapping tools such as Google Street View (GSV) and Meta Mapillary enable users to virtually navigate and experience real-world environments via immersive 360° imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal AI, accessible navigation controls, and conversational speech. With StreetViewAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetViewAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3746059.3747756},
  booktitle={Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
  chapter={0}
}

@article{rayyan-364534795,
  title={Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals - Proceedings of the 2025 ACM Designing Interactive Systems Conference},
  year={2025},
  pages={458–474},
  author={Cheema, Maryam and Seifi, Hasti and Fazli, Pooyan},
  url={https://doi.org/10.1145/3715336.3735685},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={DIS '25},
  keywords={audio description, online videos, accessibility, multimodal large language models, blind and low vision},
  abstract={Audio descriptions (AD) make videos accessible for blind and low vision (BLV) users by describing visual elements that cannot be understood from the main audio track. AD created by professionals or novice describers is time-consuming and offers little customization or control to BLV viewers on description length and content and when they receive it. To address this gap, we explore user-driven AI-generated descriptions, enabling BLV viewers to control both the timing and level of detail of the descriptions they receive. In a study, 20 BLV participants activated audio descriptions for seven different video genres with two levels of detail: concise and detailed. Our findings reveal differences in the preferred frequency and level of detail of ADs for different videos, participants’ sense of control with this style of AD delivery, and its limitations. We discuss the implications of these findings for the development of future AD tools for BLV users.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3715336.3735685},
  booktitle={Proceedings of the 2025 ACM Designing Interactive Systems Conference},
  chapter={0}
}

@article{rayyan-364534821,
  title={Accessible Nonverbal Cues to Support Conversations in VR for Blind and Low Vision People - Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  year={2024},
  author={Jung, Crescentia and Collins, Jazmin and Gonzalez Penuela, Ricardo E. and Segal, Jonathan Isaac and Won, Andrea Stevenson and Azenkot, Shiri},
  url={https://doi.org/10.1145/3663548.3675663},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ASSETS '24},
  keywords={VR, accessibility, blind, low vision},
  abstract={Social VR has increased in popularity due to its affordances for rich, embodied, and nonverbal communication. However, nonverbal communication remains inaccessible for blind and low vision people in social VR. We designed accessible cues with audio and haptics to represent three nonverbal behaviors: eye contact, head shaking, and head nodding. We evaluated these cues in real-time conversation tasks where 16 blind and low vision participants conversed with two other users in VR. We found that the cues were effective in supporting conversations in VR. Participants had statistically significantly higher scores for accuracy and confidence in detecting attention during conversations with the cues than without. We also found that participants had a range of preferences and uses for the cues, such as learning social norms. We present design implications for handling additional cues in the future, such as the challenges of incorporating AI. Through this work, we take a step towards making interpersonal embodied interactions in VR fully accessible for blind and low vision people.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3663548.3675663},
  booktitle={Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  chapter={0}
}

@article{rayyan-364534855,
  title={SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers - Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  year={2024},
  author={Ning, Zheng and Wimer, Brianna L and Jiang, Kaiwen and Chen, Keyi and Ban, Jerrick and Tian, Yapeng and Zhao, Yuhang and Li, Toby Jia-Jun},
  url={https://doi.org/10.1145/3613904.3642632},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI '24},
  keywords={accessibility, audio description, video consumption},
  abstract={Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to access video content. However, conventional static ADs can leave out detailed information in videos, impose a high mental load, neglect the diverse needs and preferences of BLV users, and lack immersion. To tackle these challenges, we introduce Spica, an AI-powered system that enables BLV users to interactively explore video content. Informed by prior empirical studies on BLV video consumption, Spica offers interactive mechanisms for supporting temporal navigation of frame captions and spatial exploration of objects within key frames. Leveraging an audio-visual machine learning pipeline, Spica augments existing ADs by adding interactivity, spatial sound effects, and individual object descriptions without requiring additional human annotation. Through a user study with 14 BLV participants, we evaluated the usability and usefulness of Spica and explored user behaviors, preferences, and mental models when interacting with augmented ADs.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3613904.3642632},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534859,
  title={ImageAlly: a human-AI hybrid approach to support blind people in detecting and redacting private image content - Proceedings of the Nineteenth USENIX Conference on Usable Privacy and Security},
  year={2023},
  author={Zhang, Zhuohao (Jerry) and Kaushik, Smirity and Seo, JooYoung and Yuan, Haolin and Das, Sauvik and Findlater, Leah and Gurari, Danna and Stangl, Abigale and Wang, Yang},
  publisher={USENIX Association},
  address={USA},
  series={SOUPS '23},
  abstract={Many people who are blind take and post photos to share about their lives and connect with others. Yet, current technology does not provide blind people with accessible ways to handle when private information is unintentionally captured in their images. To explore the technology design in supporting them with this task, we developed a design probe for blind people-- ImageAlly--that employs a human-AI hybrid approach to detect and redact private image content. ImageAlly notifies users when potential private information is detected in their images, using computer vision, and enables them to transfer those images to trusted sighted allies to edit the private content. In an exploratory study with pairs of blind participants and their sighted allies, we found that blind people felt empowered by ImageAlly to prevent privacy leakage in sharing images on social media. They also found other benefits from using ImageAlly, such as potentially improving their relationship with allies and giving allies the awareness of the accessibility challenges they face.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  booktitle={Proceedings of the Nineteenth USENIX Conference on Usable Privacy and Security},
  chapter={0}
}

@article{rayyan-364534863,
  title={VQAsk: a multimodal Android GPT-based application to help blind users visualize pictures - Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
  year={2024},
  author={De Marsico, Maria and Giacanelli, Chiara and Manganaro, Clizia Giorgia and Palma, Alessio and Santoro, Davide},
  url={https://doi.org/10.1145/3656650.3656677},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={AVI '24},
  keywords={Visual Question Answering, natural language processing and computer vision for scene interpretation, visually impaired users},
  abstract={VQAsk is an Android application that helps visually impaired users to get information about images framed by their smartphones. It enables to interact with one’s photographs or the surrounding visual environment through a question-and-answer interface integrating three modalities: speech interaction, haptic feedback that facilitates navigation and interaction, and sight. VQAsk is primarily designed to help visually impaired users mentally visualize what they cannot see, but it can also accommodate users with varying levels of visual ability. To this aim, it embeds advanced NLP and Computer Vision techniques to answer all user questions about the image on the cell screen. Image processing is enhanced by background removal through advanced segmentation models that identify important image elements. The outcomes of a testing phase confirmed the importance of this project as a first attempt at using AI-supported multimodality to enhance visually impaired users’ experience.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3656650.3656677},
  booktitle={Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
  chapter={0}
}

@article{rayyan-364534872,
  title={ImageExplorer: Multi-Layered Touch Exploration to Encourage Skepticism Towards Imperfect AI-Generated Image Captions - Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  year={2022},
  author={Lee, Jaewook and Herskovitz, Jaylin and Peng, Yi-Hao and Guo, Anhong},
  url={https://doi.org/10.1145/3491102.3501966},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI '22},
  keywords={Automatic image captioning, Blind, accessibility, alt text, alternative text, encourage skepticism, imperfect AI, screen reader, touch exploration, visual impairment},
  abstract={Blind users rely on alternative text (alt-text) to understand an image; however, alt-text is often missing. AI-generated captions are a more scalable alternative, but they often miss crucial details or are completely incorrect, which users may still falsely trust. In this work, we sought to determine how additional information could help users better judge the correctness of AI-generated captions. We developed&nbsp;ImageExplorer, a touch-based multi-layered image exploration system that allows users to explore the spatial layout and information hierarchies of images, and compared it with popular text-based (Facebook) and touch-based (Seeing AI) image exploration systems in a study with 12 blind participants. We found that exploration was generally successful in encouraging skepticism towards imperfect captions. Moreover, many participants preferred&nbsp;ImageExplorer for its multi-layered and spatial information presentation, and Facebook for its summary and ease of use. Finally, we identify design improvements for effective and explainable image exploration systems for blind users.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3491102.3501966},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534887,
  title={SceneGenA11y: How can Runtime Generative tools improve the Accessibility of a Virtual 3D Scene? - Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  year={2025},
  author={Cao, Xinyun and Ju, Kexin Phyllis and Li, Chenglin and Jain, Dhruv},
  url={https://doi.org/10.1145/3706599.3720265},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={CHI EA '25},
  keywords={BLV, DHH, accessibility, generative AI, virtual 3D scenes},
  abstract={With the popularity of virtual 3D applications, from video games to educational content and virtual reality scenarios, the accessibility of 3D scene information is vital to ensure inclusive and equitable experiences for all. Previous work include information substitutions like audio description and captions, as well as personalized modifications, but they could only provide predefined accommodations. In this work, we propose SceneGenA11y, a system that responds to the user's natural language prompts to improve accessibility of a 3D virtual scene in runtime. The system primes LLM agents with accessibility-related knowledge, allowing users to explore the scene and perform verifiable modifications to improve accessibility. We conducted a preliminary evaluation of our system with three blind and low-vision people and three deaf and hard-of-hearing people. The results show that our system is intuitive to use and can successfully improve accessibility. We discussed usage patterns of the system, potential improvements, and integration into apps. We ended with highlighting plans for future work.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3706599.3720265},
  booktitle={Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  chapter={0}
}

@article{rayyan-364534895,
  title={EditScribe: Non-Visual Image Editing with Natural Language Verification Loops - Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  year={2024},
  author={Chang, Ruei-Che and Liu, Yuxuan and Zhang, Lotus and Guo, Anhong},
  url={https://doi.org/10.1145/3663548.3675599},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ASSETS '24},
  keywords={Accessibility, assistive technology, blind, creativity support tools, generative AI, image editing, low vision, visual authoring},
  abstract={Image editing is an iterative process that requires precise visual evaluation and manipulation for the output to match the editing intent. However, current image editing tools do not provide accessible interaction nor sufficient feedback for blind and low vision individuals to achieve this level of control. To address this, we developed EditScribe, a prototype system that makes object-level image editing actions accessible using natural language verification loops powered by large multimodal models. Using EditScribe, the user first comprehends the image content through initial general and object descriptions, then specifies edit actions using open-ended natural language prompts. EditScribe performs the image edit, and provides four types of verification feedback for the user to verify the performed edit, including a summary of visual changes, AI judgement, and updated general and object descriptions. The user can ask follow-up questions to clarify and probe into the edits or verification feedback, before performing another edit. In a study with ten blind or low-vision users, we found that EditScribe supported participants to perform and verify image edit actions non-visually. We observed different prompting strategies from participants, and their perceptions on the various types of verification feedback. Finally, we discuss the implications of leveraging natural language verification loops to make visual authoring non-visually accessible.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3663548.3675599},
  booktitle={Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  chapter={0}
}

@article{rayyan-364534897,
  title={The Potential of a Visual Dialogue Agent In a Tandem Automated Audio Description System for Videos - Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
  year={2023},
  author={Stangl, Abigale and Ihorn, Shasta and Siu, Yue-Ting and Bodi, Aditya and Castanon, Mar and Narins, Lothar D and Yoon, Ilmi},
  url={https://doi.org/10.1145/3597638.3608402},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ASSETS '23},
  keywords={AI, Audio Description, Blind and Low Vision, Minimum Viable Description, Virtual Agents, Virtual Volunteer, Visual Assistance, Visual Dialogue, Visual Question Answering},
  abstract={The relentless pace of video production exacerbates the digital accessibility gap that individuals who are blind or low vision (BLV) face on a daily basis, resulting in disproportionate exclusion from community opportunities and risk management. Whereas previous automated audio description (AD) systems provide single-tool approaches for delivering minimum viable description (MVD) or delivering on-demand visual question answering (VQA), we present a tandem AI-based AD tool that combines MVD and on-demand VQA. A user study with 26 BLV individuals explored how the tandem system may be used under the conditions of delivering MVD and/or on-demand VQA with AI-only or human-in-the-loop support. When each tool was used in isolation, AI-only conditions scored significantly lower in both user enjoyment and comprehension. When used in tandem, AI-only conditions matched outcomes delivered with human-in-the-loop, which suggests that AI-only AD tools may be most effective when both types of tools are used in tandem. A multimodal analysis of interactions with the tandem system revealed areas for system improvement in terms of the timing of AD delivery and accurate content delivery. We discuss how the use of both types of tools in a tandem system can mitigate some of the digital frictions that have plagued efforts in machine learning and automated tools for accessibility.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3597638.3608402},
  booktitle={Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
  chapter={0}
}

@article{rayyan-364534910,
  title={Utilizing a Dense Video Captioning Technique for Generating Image Descriptions of Comics for People with Visual Impairments - Proceedings of the 29th International Conference on Intelligent User Interfaces},
  year={2024},
  pages={750–760},
  author={Kim, Suhyun and Lee, Semin and Kim, Kyungok and Oh, Uran},
  url={https://doi.org/10.1145/3640543.3645154},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={IUI '24},
  keywords={comics, dense video captioning, image description, people with visual impairment},
  abstract={To improve the accessibility of visual figures, auto-generation of text description of individual images has been studied. However, it cannot be directly applied to comics as the descriptions can be redundant as similar scenes appear in a row. To address this issue, we propose generating the descriptions per group of related images and demonstrate how an dense captioning technique for videos can be utilized for this purpose and ways to improve its performance. To assess the effectiveness of our approach and to identify factors affecting the quality of text descriptions of comics, we conducted a preliminary study with 3 sighted evaluators and a main user study with 12 participants with visual impairments. The results show that text descriptions generated per group of images are perceived to be better than those generated per image in terms of accuracy, clarity, understandability, length, informativeness and preference for sighted groups, when annotator is human. In the same conditions, when the annotator is AI, it exhibited better performance in terms of length. Also, people with visual impairments prefer group descriptions because of conciseness, smooth connectivity of sentences, and non-repetitive features. Based on the findings, we provide design recommendations for generating accessible comic descriptions at a scale for blind users.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3640543.3645154},
  booktitle={Proceedings of the 29th International Conference on Intelligent User Interfaces},
  chapter={0}
}

@article{rayyan-364535006,
  title={Semi-Automatic BVI Human-Centered Image Conversational Descriptions: Leveraging LLMs and Expert Refinements for Inclusive Visual Accessibility},
  year={2025},
  journal={IEEE Access},
  issn={2169-3536},
  volume={13},
  pages={156072-156090},
  author={Salous, Mazen and Lange, Daniel and von Reeken, Timo and Wolters, Maria K. and Heuten, Wilko and Boll, Susanne and Abdenebaoui, Larbi},
  keywords={Visualization;Oral communication;Videos;Large language models;Iterative methods;Guidelines;Traini...},
  abstract={Ensuring that blind and visually impaired (BVI) individuals can fully participate in today’s image-rich digital world remains a significant challenge. Current visual assistants often rely on annotations from sighted contributors, which may fail to capture BVI individuals’ preferences and expectations. Addressing this challenge, we explore how to develop optimal, conversational image descriptions that resonate with BVI individuals, and how to effectively scale their creation. We propose a semi-automatic approach that couples large language models (LLMs) with iterative, BVI-driven refinements. Starting with initial LLM-generated image descriptions, a small set of BVI experts refine them to better meet BVI individuals’ needs. These enhanced examples guide subsequent LLM outputs, which are then evaluated by BVI end-users to confirm improved quality and satisfaction. We contribute to constructing large-scale, BVI-centered training data, thereby advancing inclusive and conversational visual accessibility. Throughout our studies, the results show a significant improvement in BVI end-users’ satisfaction with image conversational descriptions when edited by BVI experts. Additionally, the results indicate promising improvements when the LLM re-generates descriptions using those edits as few-shot examples. Moreover, we got valuable insights from BVI participants’ focused mainly on optimizing clarity, relevance, and interaction patterns for image descriptions and conversational exchanges between AI and BVI individuals.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1109/ACCESS.2025.3605490},
  chapter={0}
}

@article{rayyan-364535022,
  title={Improving Web Accessibility With an LLM-Based Tool: A Preliminary Evaluation for STEM Images},
  year={2025},
  journal={IEEE Access},
  issn={2169-3536},
  volume={13},
  pages={107566-107582},
  author={Pedemonte, Giacomo and Leotta, Maurizio and Ribaudo, Marina},
  keywords={Data visualization;Visualization;Dolphins;Guidelines;Browsers;STEM;Dogs;Anatomy;Transformers;Soci...},
  abstract={Ensuring equitable access to web-based visual content in Science, Technology, Engineering, and Mathematics (STEM) disciplines remains a significant challenge for visually impaired users. This preliminary study explores the use of Large Language Models (LLMs) to automatically generate high-quality alternative texts for complex web images in these domains, contributing to the development of an accessibility tool. First, we analyzed the outputs of various LLM-based image-captioning systems, selected the most suitable one (Gemini), and developed a browser extension, AlternAtIve, capable of generating alternative descriptions at varying verbosity levels. To evaluate AlternAtIve, we assessed its perceived usefulness in a study involving 35 participants, including a blind user. Additionally, we manually compared the quality of the outputs generated by AlternAtIve with those provided by two state-of-the-practice tools from the Google Web Store, using a custom metric that computes the quality of the descriptions considering their correctness, usefulness, and completeness. The results show that the descriptions generated with AlternAtIve achieved high quality scores, almost always better than those of the other two tools. Although conveying the meaning of complex images to visually impaired users through descriptions remains challenging, the findings suggest that AI-based tools, such as AlternAtIve, can significantly improve the web navigation experience for screen reader users.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"} | RAYYAN-LABELS: low participant count},
  doi={10.1109/ACCESS.2025.3577519},
  chapter={0}
}

@article{rayyan-364562582,
  title={From Cluttered to Clear: Improving the Web Accessibility Design for Screen Reader Users in E-commerce With Generative {AI}},
  year={2025},
  journal={CoRR},
  author={Yu, Yaman and Ryskeldiev, Bektur and Tsutsui, Ayaka and Gillingham, Matthew and Wang, Yang},
  url={https://doi.org/10.48550/arXiv.2502.18701},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.48550/ARXIV.2502.18701},
  chapter={0}
}

@article{rayyan-364562583,
  title={The Sky is the Limit: Understanding How Generative {AI} can Enhance Screen Reader Users' Experience with Productivity Applications - Proceedings of the 2025 {CHI} Conference on Human Factors in Computing Systems, {CHI} 2025, YokohamaJapan, 26 April 2025- 1 May 2025},
  year={2025},
  pages={1165:1--1165:17},
  author={Perera, Minoli and Ananthanarayan, Swamy and Goncu, Cagatay and Marriott, Kim},
  url={https://doi.org/10.1145/3706598.3713634},
  publisher={{ACM}},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3706598.3713634},
  editor={Yamashita, Naomi and Evers, Vanessa and Yatani, Koji and Ding, Sharon Xianghua and Lee, Bongshin and Chetty, Marshini and Dugas, Phoebe O. Toups},
  booktitle={Proceedings of the 2025 {CHI} Conference on Human Factors in Computing Systems, {CHI} 2025, YokohamaJapan, 26 April 2025- 1 May 2025},
  chapter={0}
}

@article{rayyan-364562673,
  title={VizAbility: Enhancing Chart Accessibility with LLM-based Conversational Interaction - Proceedings of the 37th Annual {ACM} Symposium on User Interface Software and Technology, {UIST} 2024, Pittsburgh, PA, USA, October 13-16, 2024},
  year={2024},
  pages={89:1--89:19},
  author={Gorniak, Joshua and Kim, Yoon and Wei, Donglai and Kim, Nam Wook},
  url={https://doi.org/10.1145/3654777.3676414},
  publisher={{ACM}},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3654777.3676414},
  editor={Yao, Lining and Goel, Mayank and Ion, Alexandra and Lopes, Pedro},
  booktitle={Proceedings of the 37th Annual {ACM} Symposium on User Interface Software and Technology, {UIST} 2024, Pittsburgh, PA, USA, October 13-16, 2024},
  chapter={0}
}

@article{rayyan-364610847,
  title={{Defining patterns for a conversational web} - Conference on Human Factors in Computing Systems - Proceedings},
  year={2023},
  pages={1--17},
  author={Pucci, E. and Possaghi, I. and Cutrupi, C.M. and Baez, M. and Cappiello, C. and Matera, M.},
  address={Hamburg, Germany},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3544548.3581145},
  booktitle={Conference on Human Factors in Computing Systems - Proceedings},
  chapter={0}
}

@article{rayyan-369531893,
  title={Enabling Uniform Computer Interaction Experience for Blind Users through Large Language Models - Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  year={2024},
  author={Kodandaram, Satwik Ram and Uckun, Utku and Bi, Xiaojun and Ramakrishnan, IV and Ashok, Vikas},
  url={https://doi.org/10.1145/3663548.3675605},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ASSETS '24},
  keywords={Accessibility, Assistive technology, Blind users, Computer Interaction, Large language models (LLMs), Uniform interaction},
  abstract={Blind individuals, who by necessity depend on screen readers to interact with computers, face considerable challenges in navigating the diverse and complex graphical user interfaces of different computer applications. The heterogeneity of various application interfaces often requires blind users to remember different keyboard combinations and navigation methods to use each application effectively. To alleviate this significant interaction burden imposed by heterogeneous application interfaces, we present&nbsp;Savant, a novel assistive technology powered by large language models (LLMs) that allows blind screen reader users to interact uniformly with any application interface through natural language. Novelly, Savant can automate a series of tedious screen reader actions on the control elements of the application when prompted by a natural language command from the user. These commands can be flexible in the sense that the user is not strictly required to specify the exact names of the control elements in the command. A user study evaluation of&nbsp;Savant with 11 blind participants demonstrated significant improvements in interaction efficiency and usability compared to current practices.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3663548.3675605},
  booktitle={Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  chapter={0}
}

@article{rayyan-384060119,
  title={ProgramAlly: Creating Custom Visual Access Programs via Multi-Modal End-User Programming - Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
  year={2024},
  author={Herskovitz, Jaylin and Xu, Andi and Alharbi, Rahaf and Guo, Anhong},
  url={https://doi.org/10.1145/3654777.3676391},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={UIST '24},
  keywords={Accessibility, Assistive technology, Blind, Design, Do-it-yourself, End-user programming, Visual impairment},
  abstract={Existing visual assistive technologies are built for simple and common use cases, and have few avenues for blind people to customize their functionalities. Drawing from prior work on DIY assistive technology, this paper investigates end-user programming as a means for users to create and customize visual access programs to meet their unique needs. We introduce ProgramAlly, a system for creating custom filters for visual information, e.g., ‘find NUMBER on BUS’, leveraging three end-user programming approaches: block programming, natural language, and programming by example. To implement ProgramAlly, we designed a representation of visual filtering tasks based on scenarios encountered by blind people, and integrated a set of on-device and cloud models for generating and running these programs. In user studies with 12 blind adults, we found that participants preferred different programming modalities depending on the task, and envisioned using visual access programs to address unique accessibility challenges that are otherwise difficult with existing applications. Through ProgramAlly, we present an exploration of how blind end-users can create visual access programs to customize and control their experiences.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3654777.3676391},
  booktitle={Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
  chapter={0}
}

@article{rayyan-384771385,
  title={VizXpress: Towards Expressive Visual Content by Blind Creators Through AI Support - Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
  year={2025},
  author={Zhang, Lotus and Zhang, Zhuohao (Jerry) and Clepper, Gina and Li, Franklin Mingzhe and Carrington, Patrick and Wobbrock, Jacob O. and Findlater, Leah},
  url={https://doi.org/10.1145/3663547.3746345},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ASSETS '25},
  keywords={accessibility, creativity support},
  abstract={From curating the layout of a resume to selecting filters for social media, creating and configuring visual content allows individuals to express identity, communicate intent, and engage socially, yet blind individuals often face significant barriers to such expressive practices. Prior accessibility research primarily addresses functional content configuration, leaving little understanding of blind individuals’ expressive visual creation needs. To better understand and support these needs, we conducted a two-stage study: first, we interviewed 10 blind participants to understand their motivations, current practices, and barriers in visual expression, and to ideate on potential visual editing support; second, based on interview insights, we developed an interactive prototype (VizXpress) that provides real-time feedback on visual aesthetics using a vision-language model and supports automated and manual visual editing controls. We used VizXpress as a design probe to further explore accessible design opportunities for visual expression. Our findings highlight many blind users’ strong interest in creating visually expressive content, nuanced informational requirements for subjective aesthetics (e.g., color, mood, lighting), and ongoing accessibility challenges with visual creative tools. Grounded in these insights, we propose design implications including richer aesthetic feedback, controlled intelligent editing, and accessible manual editing mechanisms.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3663547.3746345},
  booktitle={Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
  chapter={0}
}

@article{rayyan-384781321,
  title={"Before, I Asked My Mom, Now I Ask ChatGPT": Visual Privacy Management with Generative AI for Blind and Low-Vision People},
  year={2025},
  author={Sharma, Tanusree and Tseng, Yu-Yun and Zhang, Lotus and Ide, Ayae and Mack, Kelly Avery and Findlater, Leah and Gurari, Danna and Wang, Yang},
  url={https://arxiv.org/abs/2507.00286},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  chapter={0}
}

@article{rayyan-389655721,
  title={GestureVoice: Enabling Multimodal Text Editing for Blind Users Using Gestures and Voice - Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
  year={2025},
  author={Khanna, Prerna and Reddy, Monalika Padma and Ramakrishnan, IV and Bi, Xiaojun and Balasubramanian, Aruna},
  url={https://doi.org/10.1145/3663547.3746388},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ASSETS '25},
  keywords={Text editing, Accessibility, Blind users, Wearables, Gestures, Voice},
  abstract={Text editing on smartphones presents substantial difficulties for blind users, particularly in mobile situations where using the smartphone touch screen is challenging. While voice input allows for hands-free text creation, editing the text typically requires physical interaction with the touchscreen, negating the benefits of the hands-free input mechanism. This paper introduces GestureVoice, a novel multimodal approach that enables screen-free text editing for blind users. By leveraging smartwatch-based hand gestures for navigation and voice commands for correction, GestureVoice allows users to edit text without any contact with their smartphones. &nbsp;GestureVoice replaces cumbersome screen-based interaction for choosing the navigation granularity with an intuitive mid-air hand gesture. It also introduces an adaptive crown cursor (rotating the physical dial of the watch) to smoothly navigate to the edit location. A preliminary study highlighted the significant time spent by blind users correcting text errors using traditional methods. In contrast, our evaluation with 8 blind users demonstrates that GestureVoice achieves a 53.80\% reduction in text editing time, offering a more efficient, intuitive, and screen-free solution for blind users.},
  note={RAYYAN-INCLUSION: {"Leif Arthur"=>"Included"}},
  doi={10.1145/3663547.3746388},
  booktitle={Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
  chapter={0}
}

