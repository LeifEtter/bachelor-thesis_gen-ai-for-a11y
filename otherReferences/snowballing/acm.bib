@inproceedings{10.1145/3529836.3529856,
author = {Shrestha, Raju},
title = {A transformer-based deep learning model for evaluation of accessibility of image descriptions},
year = {2022},
isbn = {9781450395700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529836.3529856},
doi = {10.1145/3529836.3529856},
abstract = {Images have become an integral part of digital and online media and they are used for creative expression and dissemination of knowledge. To address image accessibility challenges to the visually impaired community, adequate textual image descriptions or captions are provided, which can be read through screen readers. These descriptions could be either human-authored or software-generated. It is found that most of the image descriptions provided tend to be generic, inadequate, and often unreliable making them inaccessible. There are tools, methods, and metrics used to evaluate the quality of the generated text, but almost all of them are word-similarity-based and generic. There are standard guidelines such as NCAM image accessibility guidelines to help write accessible image descriptions. However, web content developers and authors do not seem to use them much, possibly due to the lack of knowledge, undermining the importance of accessibility coupled with complexity and difficulty understanding the guidelines. To our knowledge, none of the quality evaluation techniques take into account accessibility aspects. To address this, a deep learning model based on the transformer, a most recent and most effective architecture used in natural language processing, which measures compliance of the given image description to ten NCAM guidelines, is proposed. The experimental results confirm the effectiveness of the proposed model. This work could contribute to the growing research towards accessible images not only on the web but also on all digital devices.},
booktitle = {Proceedings of the 2022 14th International Conference on Machine Learning and Computing},
pages = {28â€“33},
numpages = {6},
keywords = {Transformer, Neural networks, NCAM guidelines, Image description, Image accessibility, Evaluation, Deep learning},
location = {Guangzhou, China},
series = {ICMLC '22}
}
@misc{wu2023visualchatgpttalkingdrawing,
      title={Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models}, 
      author={Chenfei Wu and Shengming Yin and Weizhen Qi and Xiaodong Wang and Zecheng Tang and Nan Duan},
      year={2023},
      eprint={2303.04671},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.04671}, 
}
@misc{zhu2023chatgptasksblip2answers,
      title={ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions}, 
      author={Deyao Zhu and Jun Chen and Kilichbek Haydarov and Xiaoqian Shen and Wenxuan Zhang and Mohamed Elhoseiny},
      year={2023},
      eprint={2303.06594},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.06594}, 
}