@inproceedings{10.1145/3736251.3754331,
author = {Clear, Alison and Clear, Tony and Impagliazzo, John and Morakanyane, Resego and Odom-Bartel, Rebecca and Zhang, Ming},
title = {Digital Sobriety: Sustainable Use of Gen AI in Higher Computing Education},
year = {2025},
isbn = {9798400719424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3736251.3754331},
doi = {10.1145/3736251.3754331},
abstract = { ''Digital Sobriety'' advocates a more conscious and measured use of Gen AI in our teaching. This fashionable but profligate new technology, on its current trajectory, threatens the future of our planet. As computing educators and members of ACM as a professional society, what obligations do these aspects of the ''AI Revolution'' impose on us? To whom do we disclose the danger to the environment of an enthusiastic and uninformed adoption of GenAI in our teaching, by our students, colleagues and institutions? Instead of lemming-like rushing to adopt the newest shiny thing in the AI Revolution, what hard questions do we need to ask ourselves? Or should we simply ban the use of this fashionable but profligate new technology? We argue that Gen AI and its unconscious and enthusiastic adoption expose us as educators to accusations of profligacy in our actions and blind ignorance oof the environmental costs of our actions. In the ACM codes of ethics, we see obligations to act to ensure that computing technology contributes the social good ''In addition to a safe social environment, human well-being requires a safe natural environment. Therefore, computing professionals should promote environmental sustainability both locally and globally'' As computing educators we need to consider what obligations do these aspects of the ''AI Revolution'' impose on us.},
booktitle = {Proceedings of the ACM Global on Computing Education Conference 2025 Vol 2},
pages = {350–352},
numpages = {3},
keywords = {computing education, digital sobriety, engineering education, gen ai, green it},
location = {Gaborone, Botswana},
series = {CompEd 2025}
}

@inproceedings{10.1007/978-3-031-98459-4_36,
author = {Ding, Shi and Smith, Jason Brent and Magerko, Brian},
title = {Considering Large Language Model Integration in&nbsp;Expressive Computer Science Learning Environments for&nbsp;Blind and&nbsp;Visually Impaired Learners Through Co-design},
year = {2025},
isbn = {978-3-031-98458-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-98459-4_36},
doi = {10.1007/978-3-031-98459-4_36},
abstract = {Although large language models (LLMs) have transformed the way people can interact with information and receive personalized help in learning spaces, we have a limited understanding of how blind and visually impaired (BVI) learners use these LLMs-powered technologies. This paper explores the potential of LLMs to improve the BVI learners’ experience in EarSketch, a music coding platform designed to foster computational thinking through creative expression. Through co-design sessions with BVI learners and students, we identify key challenges such as intuitive search, navigation, and feedback, and propose LLM-based solutions to address these barriers. We synthesize key design implications with broader relevance for expressive computer science learning environments.},
booktitle = {Artificial Intelligence in Education: 26th International Conference, AIED 2025, Palermo, Italy, July 22–26, 2025, Proceedings, Part IV},
pages = {472–480},
numpages = {9},
keywords = {Human Centered Computing, Large Language Models, Learning System, Accessibility},
location = {Palermo, Italy}
}

@inproceedings{10.1145/3663548.3675600,
author = {Lee, Seonghee and Kohga, Maho and Landau, Steve and O'Modhrain, Sile and Subramonyam, Hari},
title = {AltCanvas: A Tile-Based Editor for Visual Content Creation with Generative AI for Blind or Visually Impaired People},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675600},
doi = {10.1145/3663548.3675600},
abstract = {People with visual impairments often struggle to create content that relies heavily on visual elements, particularly when conveying spatial and structural information. Existing accessible drawing tools, which construct images line by line, are suitable for simple tasks like math but not for more expressive artwork. On the other hand, emerging generative AI-based text-to-image tools can produce expressive illustrations from descriptions in natural language, but they lack precise control over image composition and properties. To address this gap, our work integrates generative AI with a constructive approach that provides users with enhanced control and editing capabilities. Our system, AltCanvas, features a tile-based interface enabling users to construct visual scenes incrementally, with each tile representing an object within the scene. Users can add, edit, move, and arrange objects while receiving speech and audio feedback. Once completed, the scene can be rendered as a color illustration or as a vector for tactile graphic generation. Involving 14 blind or low-vision users in design and evaluation, we found that participants effectively used the AltCanvas’s workflow to create illustrations.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {70},
numpages = {22},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3733155.3737910,
author = {Leporini, Barbara and Buzzi, Marina and Della Penna, Giuseppe},
title = {A Preliminary Evaluation of Generative AI Tools for Blind Users: Usability and Screen Reader Interaction},
year = {2025},
isbn = {9798400714023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733155.3737910},
doi = {10.1145/3733155.3737910},
abstract = {The increasing use of Generative Artificial Intelligence (GAI) tools such as ChatGPT, Copilot, Perplexity and Gemini opens up new possible scenarios for supporting work and everyday activities. For people who are blind, the usability of such tools through screen readers is crucial to ensure their use of such AI-based technologies. In this study, we explore the accessibility and usability of the interfaces of four popular AI-based tools via screen readers through a combination of semi-automated evaluations and inspections conducted by both sighted and blind accessibility experts and screen readers with more than 20 years of experience. Navigation, labeling of control elements, feedback mechanisms, and prompt handling were considered in the study. The results point to usability difficulties in all tools, particularly in navigation structure, clarity of feedback and interactive elements. Although this work empirically explores the accessibility of AI-based tools it brings out the first critical issues that deserve further investigation. However, they are based on a small group of experts and thus should be considered preliminary and useful for future studies.},
booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {562–568},
numpages = {7},
keywords = {Accessibility, Blind users, ChatGPT, Copilot, Gemini, Generative AI, Perplexity, screen reader interaction},
location = {
},
series = {PETRA '25}
}

@article{10.1145/3729371,
author = {He, Ziyao and Huq, Syed Fatiul and Malek, Sam},
title = {Enhancing Web Accessibility: Automated Detection of Issues with Generative AI},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729371},
doi = {10.1145/3729371},
abstract = {Websites are integral to people’s daily lives, with billions in use today. However, due to limited awareness of accessibility and its guidelines, developers often release web apps that are inaccessible to people with disabilities, who make up around 16% of the global population. To ensure a baseline of accessibility, software engineers rely on automated checkers that assess a webpage’s compliance based on predefined rules. Unfortunately, these tools typically cover only a small subset of accessibility guidelines and often overlook violations that require a semantic understanding of the webpage. The advent of generative AI, known for its ability to comprehend textual and visual content, has created new possibilities for detecting accessibility violations. We began by studying the most widely used guideline, WCAG, to determine the testable success criteria that generative AI could address. This led to the development of an automated tool called GenA11y, which extracts elements from a page related to each success criterion and inputs them into an LLM prompted to detect accessibility issues on the web. Evaluations of GenA11y showed its effectiveness, with a precision of 94.5% and a recall of 87.61%. Additionally, when tested on real websites, GenA11y identified an average of eight more types of accessibility violations than the combination of existing tools.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE101},
numpages = {24},
keywords = {Accessibility, Generative AI, LLM, WCAG}
}

@inproceedings{10.1145/3706598.3713634,
author = {Perera, Minoli and Ananthanarayan, Swamy and Goncu, Cagatay and Marriott, Kim},
title = {The Sky is the Limit: Understanding How Generative AI can Enhance Screen Reader Users' Experience with Productivity Applications},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713634},
doi = {10.1145/3706598.3713634},
abstract = {Productivity applications including word processors, spreadsheets, and presentation tools are crucial in work, education, and personal settings. Blind users typically access these tools via screen readers (SRs) and face significant accessibility and usability challenges. Recent advancements in Generative AI (GenAI) may address these challenges by enabling natural language interactions and contextual task understanding. However, there is limited understanding of SR users’ needs and attitudes toward GenAI assistance in these applications. We surveyed 99 SR users to gain a holistic understanding of the challenges they face when using productivity applications, the impact of these challenges on their productivity and independence, and their initial perceptions of AI assistance. Driven by their enthusiasm, we conducted interviews with 16 SR users to explore their attitudes toward GenAI and its potential usefulness in productivity applications. Our findings highlight its need to support existing SR workflows and the importance of enabling customization and task verification.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1165},
numpages = {17},
keywords = {blind, accessibility, productivity applications, assistive technology, screen readers, AI assistants, Generative AI, virtual assistants},
location = {
},
series = {CHI '25}
}

@article{10.1002/pra2.1046,
author = {Chen, Huitong and Yan, Hui and Wu, Zhaotong and Zhao, Xuefeng},
title = {Silicon‐based Life or Carbon‐based Life? An Exploratory Study on Visual Information Source Selection of Blind or Visually Impaired Persons},
year = {2024},
issue_date = {October 2024},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {61},
number = {1},
url = {https://doi.org/10.1002/pra2.1046},
doi = {10.1002/pra2.1046},
abstract = {The development of generative AI and Large Visual‐Language Models, has the potential to overcome the reliance of blind or visually impaired (BVI) persons on human sources for visual information. Action research and semi‐structured interviews were conducted with 19 BVI persons to explore their visual information needs and source selection. This study categorizes their needs into description, navigation, and manipulation. Accessibility, credibility, and interactivity of information sources are the primary criteria for BVI persons in information source selection. When visual task demand high accurate information, BVI persons are more likely to select human information sources due to their accessibility and credibility. When tasks are less urgent and requiring less accuracy information, the use of AI information sources increases due to the special accessibility. The purpose of the study is to understand the impact of AI on BVI persons, and provide theoretical and practical insights for the development of human‐centered AI.},
journal = {Proceedings of the Association for Information Science and Technology},
month = oct,
pages = {493–498},
numpages = {6},
keywords = {Blind or visually impaired persons, Generative AI, information source selection, visual information need}
}

@inproceedings{10.1145/3706598.3713553,
author = {Kim, Gyeongdeok and Lim, Chungman and Park, Gunhyuk},
title = {I-Scratch: Independent Slide Creation With Auditory Comment and Haptic Interface for the Blind and Visually Impaired},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713553},
doi = {10.1145/3706598.3713553},
abstract = {Presentation software still holds barriers to independent creation for blind and visually impaired users (BVIs) due to its visual-centric interface. To address this gap, we introduce I-Scratch, a multimodal system which empowers BVIs to independently create, explore, and edit PowerPoint slides. We initially designed I-Scratch to tackle the practical challenges faced by BVIs and refined I-Scratch to improve its usability and accessibility through iterative participatory sessions involving a blind user. I-Scratch integrates a graphical tactile display with auditory guidance for multimodal feedback, simplifies the user interface, and leverages AI technologies for visual assistance in image generation and content interpretation. A user study with ten BVIs demonstrated that I-Scratch enables them to produce visually coherent and aesthetically pleasing slides independently, achieving 91.25% of full and partial successes with a CSI score of 85.07. We present five guidelines and future directions to support the creative work of BVIs using presentation software.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1161},
numpages = {23},
keywords = {The Blind and Visually Impaired, Accessibility, Slide Editing, Graphical Tactile Display, Multimodal Interfaces, AI Assistance},
location = {
},
series = {CHI '25}
}

@article{10.1145/3649223,
author = {Silva, Jorge Sassaki Resende and Cardoso, Paula Christina Figueira and De Bettio, Raphael Winckler and Tavares, Daniela Cardoso and Silva, Carlos Alberto and Watanabe, Willian Massami and Freire, Andr\'{E} Pimenta},
title = {In-Page Navigation Aids for Screen-Reader Users with Automatic Topicalisation and Labelling},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3649223},
doi = {10.1145/3649223},
abstract = {Navigation aids such as headers and internal links provide vital support for screen-reader users on web documents to grasp a document’s structure. However, when such navigation aids are unavailable or not appropriately marked up, this situation can cause serious difficulties. This article presents the design and evaluation of a tool for automatically generating navigation aids with headers and internal links for screen readers with topicalisation and labelling algorithms. The proposed tool uses natural language processing techniques to divide a web document into topic segments and label each segment in two cycles based on its content. We conducted an initial user study in the first cycle with eight blind and partially-sighted screen reader users. The evaluation involved tasks with questions answered by participants with information from texts with and without automatically generated headers. The results in the first cycle provided preliminary indicators of performance improvement and cognitive load reduction. The second cycle involved co-designing an improved version with two blind experts in web accessibility, resulting in a browser extension which injects automatically generated headers and in-page navigation with internal links, along with improvements in the generation of labels using OpenAI’s ChatGPT. The browser extension was evaluated by seven blind participants using the same four texts used to evaluate the preliminary prototype developed in the first cycle. With the two development cycles, the study provided important insights into the design of navigation aids for screen-reader users using natural language processing techniques, including the potential use of generative artificial intelligence for assistive technologies and limitations that need to be explored in future research.},
journal = {ACM Trans. Access. Comput.},
month = jul,
articleno = {12},
numpages = {45},
keywords = {Accessibility, natural language processing, screen readers, topic segmentation and labelling, large language models, assistive technologies}
}

@inproceedings{10.1145/3744257.3744279,
author = {Sunkara, Mohan and Kolgar Nayak, Akshay and Kalari, Sandeep and Prakash, Yash and Jayarathna, Sampath and Lee, Hae-Na and Ashok, Vikas},
title = {QuickQue: Enabling Quick Access to Information in User Reviews for Screen Reader Users},
year = {2025},
isbn = {9798400718823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744257.3744279},
doi = {10.1145/3744257.3744279},
abstract = {Efficiently perusing online customer reviews is presently challenging for blind users, who rely on a screen reader that supports predominantly one-dimensional narration of content via keyboard shortcuts. To address this, with restaurant reviews as seminal case study, we developed QuickCue, a browser extension prototype that enables screen reader users to quickly obtain the positives and negatives regarding different aspects of a restaurant (e.g., food quality, hygiene, ambiance), without having to sift through numerous reviews containing redundant information. At its core, QuickCue utilizes a large language model to perform aspect and sentiment-based joint classification of reviews to group them based on the types of contained information, followed by focused summarizations within the groups to generate concise representations of reviewers’ opinions, which are then presented to users via an accessible interface. Evaluation of QuickCue in a user study with 10 participants showed significant improvements in overall usability and task workload compared to the status quo screen reader.},
booktitle = {Proceedings of the 22nd International Web for All Conference},
pages = {22–24},
numpages = {3},
keywords = {blind, screen reader, online reviews, usability},
location = {
},
series = {W4A '25}
}

@inproceedings{10.1145/3491101.3519832,
author = {Baez, Marcos and Cutrupi, Claudia Maria and Matera, Maristella and Possaghi, Isabella and Pucci, Emanuele and Spadone, Gianluca and Cappiello, Cinzia and Pasquale, Antonella},
title = {Exploring challenges for Conversational Web Browsing with Blind and Visually Impaired Users},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519832},
doi = {10.1145/3491101.3519832},
abstract = {Conversational AI is changing the way we interact with digital services. However, there is still a lack of conversational paradigms facilitating access to the Web. This paper discusses a new approach for Conversational Web Browsing, and introduces a design space identified through a user-centered process that involved 26 blind and visually impaired users. The paper also illustrates the conceptual architecture of a software framework that can automatically generate conversational agents for the Conversational Web.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {234},
numpages = {7},
keywords = {Conversational Patterns, Conversational UIs, Conversational Web Browsing},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3706598.3713797,
author = {Zhong, Mingyuan and Chen, Ruolin and Chen, Xia and Fogarty, James and Wobbrock, Jacob O.},
title = {ScreenAudit: Detecting Screen Reader Accessibility Errors in Mobile Apps Using Large Language Models},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713797},
doi = {10.1145/3706598.3713797},
abstract = {Many mobile apps are inaccessible, thereby excluding people from their potential benefits. Existing rule-based accessibility checkers aim to mitigate these failures by identifying errors early during development but are constrained in the types of errors they can detect. We present ScreenAudit, an LLM-powered system designed to traverse mobile app screens, extract metadata and transcripts, and identify screen reader accessibility errors overlooked by existing checkers. We recruited six accessibility experts including one screen reader user to evaluate ScreenAudit’s reports across 14 unique app screens. Our findings indicate that ScreenAudit achieves an average coverage of 69.2%, compared to only 31.3% with a widely-used accessibility checker. Expert feedback indicated that ScreenAudit delivered higher-quality feedback and addressed more aspects of screen reader accessibility compared to existing checkers, and that ScreenAudit would benefit app developers in real-world settings.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1163},
numpages = {19},
keywords = {Mobile accessibility, large language models, accessibility audit.},
location = {
},
series = {CHI '25}
}

@article{10.1016/j.infsof.2025.107821,
author = {Vera-Amaro, Guillermo and Rojano-C\'{a}ceres, Jos\'{e} Rafael},
title = {Towards accessible website design through artificial intelligence: A systematic literature review},
year = {2025},
issue_date = {Oct 2025},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {186},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2025.107821},
doi = {10.1016/j.infsof.2025.107821},
journal = {Inf. Softw. Technol.},
month = oct,
numpages = {31},
keywords = {Web accessibility, Systematic literature review, Artificial intelligence, Wcag, Machine learning, Large language models}
}

@inproceedings{10.1145/3750069.3750310,
author = {Patern\`{o}, Fabio and Vinci, Manuela and Manca, Marco and Iannuzzi, Nicola},
title = {How an LLM Can Improve Automatic Web Accessibility Validation ?},
year = {2025},
isbn = {9798400721021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3750069.3750310},
doi = {10.1145/3750069.3750310},
abstract = {Digital accessibility is important since it allows all people, including those with disabilities, to interact and access the desired information available on the Web. The W3C WCAG guidelines provide a rich set of indications about how to obtain it. Over time, they have become very extensive in order to consider the many possible cases. Manual checking of all the corresponding techniques is impossible; thus, interest in the support provided by automatic tools is increasing. However, current validation tools sometimes have several limitations in their analysis, which still require considerable manual intervention to validate several accessibility techniques. Large Language Models (LLMs) present an opportunity to address such cases. In this paper, we report on an investigation that focused on exploiting the functionality made available by the GPT 4o APIs to address such cases. We report on the types of prompting techniques used for this purpose, how they have been exploited, for which accessibility techniques, and how they have been validated. The results provide useful indications for understanding the role of large language models for accessibility validation.},
booktitle = {Proceedings of the 16th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {23},
numpages = {8},
keywords = {Accessibility validation, Automatic Tools, Large Language Models},
location = {
},
series = {CHItaly '25}
}

@inproceedings{10.1145/3597638.3608404,
author = {Phutane, Mahika and Jung, Crescentia and Chen, Niu and Azenkot, Shiri},
title = {Speaking with My Screen Reader: Using Audio Fictions to Explore Conversational Access to Interfaces},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608404},
doi = {10.1145/3597638.3608404},
abstract = {Conversational assistants, an inherently accessible mode of interaction for blind and low vision (BLV) individuals, offer opportunities to support nonvisual access in new ways. In this paper, we explore whether and how human-like conversations can support access to interfaces, advancing the impersonal linear access provided by screen readers today. We first interviewed 10 BLV participants about this approach, but found it difficult to situate conversations around a future technology. So we turned to a speculative design approach and created four audio fictions: pre-recorded dialogues between users and their hypothetical screen reader assistants wherein assistants assumed distinct roles: a friend, butler, expert, and caregiver. We presented the audio fictions to 14 BLV participants and found that personable conversations can meaningfully extend the screen reader experience. We observed a tension between AI adaptation and screen reader customization. Participants further expressed a need to maintain control at three distinct levels: granular cursor movement, screen representation, and task assistance. Through the lens of assistant roles, we address fundamental questions about anthropomorphizing CAs and assistive technology broadly.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {53},
numpages = {18},
keywords = {blind and low vision, conversational agents, design fiction, interviews, screen readers, voice assistants},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3706599.3721278,
author = {Faruqi, Faraz and Perroni-Scharf, Maxine and Walia, Jaskaran Singh and Zhu, Yunyi and Feng, Shuyue and Degraen, Donald and Mueller, Stefanie},
title = {Demonstration of TactStyle: Generating Tactile Textures with Generative AI for Digital Fabrication},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3721278},
doi = {10.1145/3706599.3721278},
abstract = {Recent work in Generative AI enables the stylization of 3D models based on image prompts. However, these methods do not incorporate tactile information, leading to designs that lack the expected tactile properties. We present TactStyle, a system that allows creators to stylize 3D models with images while incorporating the expected tactile properties. TactStyle accomplishes this using a modified image-generation model fine-tuned to generate heightfields for given surface textures. By optimizing 3D model surfaces to embody a generated texture, TactStyle creates models that match the desired style and replicate the tactile experience. We present TactStyle’s user interface to create stylized 3D models with accurate textures, present multiple textured tiles for blind perception experience, and present five application scenarios with fabricated 3D models.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {723},
numpages = {5},
keywords = {Personal Fabrication; Digital Fabrication; 3D Printing; Generative AI.},
location = {
},
series = {CHI EA '25}
}

@inbook{10.1145/3715668.3735617,
author = {Guriundefined\u{a}, Alexandra-Elena},
title = {Exploring the Intersection of UI Accessibility and Generative AI: A Framework for Human-AI Collaboration},
year = {2025},
isbn = {9798400714863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715668.3735617},
abstract = {Despite decades of accessibility guidelines, digital interfaces remain largely inaccessible, with over 95% of websites failing basic WCAG standards. My doctoral research examines how generative AI transforms accessibility implementation by investigating how Al-driven tools comply with accessibility standards, what patterns emerge in human-AI collaboration for accessible design, and how we can develop frameworks to guide AI systems toward more accessible outcomes. Through mixed-method studies-evaluating Al- generated interfaces, challenging Al's understanding of accessibility principles, and interviewing professional designers I've identified key compliance patterns, discovered AI limitations, and developed novel interactive prompt engineering methods. This work contributes both theoretical understanding of human-AI collaboration and practical frameworks for integrating generative AI into accessible interface design workflows.},
booktitle = {Companion Publication of the 2025 ACM Designing Interactive Systems Conference},
pages = {116–120},
numpages = {5}
}

@inproceedings{10.1145/3706599.3719966,
author = {Jiang, Lucy and Zhu, Amanda and Oppegaard, Brett},
title = {Audio Description Automatons: Exploring Perspectives on Personas for Generative AI Description Writing Assistants},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719966},
doi = {10.1145/3706599.3719966},
abstract = {Visual media is often made accessible to blind and low vision (BLV) people through audio description (AD), typically written by experts. Prior efforts to increase the scale of description output have involved sighted novices as describers or used generative AI (GenAI) to automatically convert images to text; however, description quality remains a concern. To support novice describers in writing high quality descriptions, we designed and developed a GenAI-powered online tool, “Guidedogs,” featuring five dogs with unique names, images, and voices that provided immediate and varied feedback on draft descriptions. We piloted the tool during a large hackathon-style description workshop in 2024. Through 17 semi-structured interviews, we explored the efficacy of using metaphors as personas for AI assistants and gathered insights on participants’ perceptions on using AI for accessibility purposes. We contribute preliminary insights on generative AI assistant personas in an accessibility context and share design considerations to guide future work.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {83},
numpages = {7},
keywords = {audio description, image description, generative AI, personas, blind, low vision, AI assistant},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3663548.3675631,
author = {Adnin, Rudaiba and Das, Maitraye},
title = {"I look at it as the king of knowledge": How Blind People Use and Understand Generative AI Tools},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675631},
doi = {10.1145/3663548.3675631},
abstract = {The proliferation of Generative Artificial Intelligence (GenAI) tools has brought a critical shift in how people approach information retrieval and content creation in diverse contexts. Yet, we have limited understanding of how blind people use and make sense of GenAI systems. To bridge this gap, we report findings from interviews with 19 blind individuals who incorporate mainstream GenAI tools like ChatGPT and Be My AI in their everyday practices. Our findings reveal how blind users navigate accessibility issues, inaccuracies, hallucinations, and idiosyncracies associated with GenAI and develop interesting (but often flawed) mental models of how these tools work. We discuss key considerations for rethinking access and information verification in GenAI tools, unpacking erroneous mental models among blind users, and reconciling harms and benefits of GenAI from an accessibility perspective.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {64},
numpages = {14},
keywords = {Accessibility, ChatGPT, Generative AI, blind, visual impairment},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@article{10.1007/s10209-024-01108-z,
author = {L\'{o}pez-Gil, Juan-Miguel and Pereira, Juanan},
title = {Turning manual web accessibility success criteria into automatic: an LLM-based approach},
year = {2024},
issue_date = {Mar 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {1},
issn = {1615-5289},
url = {https://doi.org/10.1007/s10209-024-01108-z},
doi = {10.1007/s10209-024-01108-z},
abstract = {Web accessibility evaluation is a costly process that usually requires manual intervention. Currently, large language model (LLM) based systems have gained popularity and shown promising capabilities to perform tasks that seemed impossible or required programming knowledge specific to a given area or were supposed to be impossible to be performed automatically. Our research explores whether an LLM-based system would be able to evaluate web accessibility success criteria that require manual evaluation. Three specific success criteria of the Web Content Accessibility Guidelines (WCAG) that currently require manual checks were tested: 1.1.1 Non-text Content, 2.4.4 Link Purpose (In Context), and 3.1.2 Language of Parts. LLM-based scripts were developed to evaluate the test cases. Results were compared against current web accessibility evaluators. While automated accessibility evaluators were unable to reliably test the three WCAG criteria, often missing or only warning about issues, the LLM-based scripts successfully identified accessibility issues the tools missed, achieving overall 87.18% detection across the test cases. Conclusion The results demonstrate LLMs can augment automated accessibility testing to catch issues that pure software testing misses today. Further research should expand evaluation across more test cases and types of content.},
journal = {Univers. Access Inf. Soc.},
month = mar,
pages = {837–852},
numpages = {16},
keywords = {Web accessibility, WCAG, Automated testing, Large language models, ChatGPT}
}

@inproceedings{10.1145/3613904.3642325,
author = {Das, Maitraye and Fiannaca, Alexander J. and Morris, Meredith Ringel and Kane, Shaun K. and Bennett, Cynthia L.},
title = {From Provenance to Aberrations: Image Creator and Screen Reader User Perspectives on Alt Text for AI-Generated Images},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642325},
doi = {10.1145/3613904.3642325},
abstract = {AI-generated images are proliferating as a new visual medium. However, state-of-the-art image generation models do not output alternative (alt) text with their images, rendering them largely inaccessible to screen reader users (SRUs). Moreover, less is known about what information would be most desirable to SRUs in this new medium. To address this, we invited AI image creators and SRUs to evaluate alt text prepared from various sources and write their own alt text for AI images. Our mixed-methods analysis makes three contributions. First, we highlight creators’ perspectives on alt text, as creators are well-positioned to write descriptions of their images. Second, we illustrate SRUs’ alt text needs particular to the emerging medium of AI images. Finally, we discuss the promises and pitfalls of utilizing text prompts written as input for AI models in alt text generation, and areas where broader digital accessibility guidelines could expand to account for AI images.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {900},
numpages = {21},
keywords = {AI art, Accessibility, Alt text, Blind, Screen reader users, Text-to-Image},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3733155.3737913,
author = {Patel, Pooja and Melcer, Edward Franklin},
title = {Exploring Student Developers’ Trust, Adoption, and Usage of AI-Powered Coding Assistants for Web Accessibility Compliance},
year = {2025},
isbn = {9798400714023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733155.3737913},
doi = {10.1145/3733155.3737913},
abstract = {Web accessibility is a cornerstone of inclusive digital design, yet developers often face challenges in effectively implementing best practices. AI development assistants (AIDA) have emerged as potential solutions, offering automated code suggestions and accessibility recommendations. However, little is known about how student developers perceive and integrate these tools to ensure compliance with web accessibility standards. To address this gap, we conducted an exploratory study using semi-structured interviews with 14 student web developers to understand their experiences with AIDA as well as barriers to adoption. The findings emphasized that confidence in AI-generated accessibility suggestions was often limited, conditional, and shaped by student developer perceptions of reliability, transparency, and contextual awareness.},
booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {298–301},
numpages = {4},
keywords = {Web Accessibility Compliance, AI-powered Coding Assistants, Student Developers, Developer Trust},
location = {
},
series = {PETRA '25}
}

@inproceedings{10.1145/3706370.3731641,
author = {Coronado, Angelo and Carvalho, Sergio T and Berretta, Luciana},
title = {See Through My Eyes: Using Multimodal Large Language Model for Describing Rendered Environments to Blind People},
year = {2025},
isbn = {9798400713910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706370.3731641},
doi = {10.1145/3706370.3731641},
abstract = {Extended Reality (XR) is quickly expanding “as the next major technology wave in personal computing”. Nevertheless, this expansion and adoption could also exclude certain disabled users, particularly people with visual impairment (VIP). According to the World Health Organization (WHO) in their 2019 publication, there were at least 2.2 billion people with visual impairment, a number that is also estimated to have increased in recent years. Therefore, it is important to include disabled users, especially visually impaired people, in the design of Head-Mounted Displays and Extended Reality environments. Indeed, this objective can be pursued by incorporating Multimodal Large Language Model (MLLM) technology, which can assist visually impaired people. As a case study, this study employs different prompts that result in environment descriptions from an MLLM integrated into a virtual reality (VR) escape room. Therefore, six potential prompts were engineered to generate valuable outputs for visually impaired users inside a VR environment. These outputs were evaluated using the G-Eval, and VIEScore metrics. Even though, the results show that the prompt patterns provided a description that aligns with the user’s point of view, it is highly recommended to evaluate these outputs through “expected outputs” from Orientation and Mobility Specialists, and Sighted Guides. Furthermore, the subsequent step in the process is to evaluate these outputs by visually impaired people themselves to identify the most effective prompt pattern.},
booktitle = {Proceedings of the 2025 ACM International Conference on Interactive Media Experiences},
pages = {451–457},
numpages = {7},
keywords = {accessibility, blind, visual impairment, LLM, spatial cognition, rendered environments, virtual reality},
location = {
},
series = {IMX '25}
}

@article{10.1016/j.chb.2023.107855,
author = {Said, Nadia and Potinteu, Andreea E. and Brich, Irina and Buder, J\"{u}rgen and Schumm, Hanna and Huff, Markus},
title = {An artificial intelligence perspective: How knowledge and confidence shape risk and benefit perception},
year = {2023},
issue_date = {Dec 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {149},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2023.107855},
doi = {10.1016/j.chb.2023.107855},
journal = {Comput. Hum. Behav.},
month = dec,
numpages = {25},
keywords = {Artificial intelligence, Knowledge, Confidence, Attitudes, Risk-benefit perception, Metacognition}
}

@inproceedings{10.1145/3650104.3652908,
author = {Acosta-Salgado, Linda and Daviet, Jean-David and Jeanson, Lisa},
title = {Improving Web Accessibility through Artificial Intelligence: A Focus on Image Description Generation: Am\'{e}liorer l'Accessibilit\'{e} des Sites Web gr\^{a}ce \`{a} l'Intelligence Artificielle : Focus sur la G\'{e}n\'{e}ration de Descriptions d'Images},
year = {2024},
isbn = {9798400706080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650104.3652908},
doi = {10.1145/3650104.3652908},
abstract = {The accessibility of websites has become imperative in an increasingly connected world. However, many websites remain inaccessible to certain audiences, particularly the visually impaired. One often overlooked aspect is the description of non-textual elements, especially images. This study explores how artificial intelligence can be utilized to enhance image accessibility on websites. Initially, we identified generative models for alternative image text descriptions and explored the last version of ChatGPT-4V. These models were tested on fifty images, and the generated descriptions were evaluated by a web content generation expert. The results of this experiment emphasize the importance of these models in reducing the time required for description generation. However, we observed that the obtained results did not meet the quality requirements for descriptive texts that could render images more accessible. Regarding Chat GPT-4V, the descriptions are of better quality, though its current efficiency is hampered by its slowness.L'accessibilit\'{e} des sites Web est devenue un imp\'{e}ratif dans un monde de plus en plus connect\'{e}. Cependant, de nombreux sites web restent inaccessibles \`{a} certains publics, et en particulier aux mal voyants. Un aspect souvent n\'{e}glig\'{e} est la description des \'{e}l\'{e}ments non-textuels, notamment des images. Cette \'{e}tude explore comment l'intelligence artificielle peut \^{e}tre utilis\'{e}e pour am\'{e}liorer l'accessibilit\'{e} des images sur un site web. Tout d'abord, nous avons identifi\'{e} des mod\`{e}les g\'{e}n\'{e}ratifs de textes alternatifs aux images et nous avons explor\'{e} la derni\`{e}re version de ChatGPT-4 Vision. Ces mod\`{e}les ont \'{e}t\'{e} test\'{e}s sur cinquante images et les descriptions g\'{e}n\'{e}r\'{e}es ont \'{e}t\'{e} \'{e}valu\'{e}es par un expert en production de contenus pour les sites web. Les r\'{e}sultats de cette exp\'{e}rimentation soulignent l'importance de ces mod\`{e}les pour r\'{e}duire le temps n\'{e}cessaire \`{a} la g\'{e}n\'{e}ration de descriptions. En revanche, nous avons constat\'{e} que les r\'{e}sultats obtenus ne r\'{e}pondent pas aux exigences de qualit\'{e} de textes descriptifs susceptibles de rendre les images plus accessibles. Concernant ChatGPT-4V, les descriptions sont de meilleure qualit\'{e}, bien que son efficacit\'{e} actuelle soit limit\'{e}e par sa lenteur.},
booktitle = {Proceedings of the 35th International Francophone Conference on Human-Computer Interaction},
articleno = {5},
numpages = {13},
keywords = {Web accessibility, alternative text, generative models, image description},
location = {Paris, France},
series = {IHM '24}
}

@inproceedings{10.1145/3706599.3719277,
author = {el Kordy, Jasmin},
title = {Steering Blind Algorithms: Exploring the Impact of Generative AI on the Role of Designers},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719277},
doi = {10.1145/3706599.3719277},
abstract = {Recent advancements in AI, particularly those focused on replicating creativity, have sparked ongoing discussions about their impact on the role of designers. This paper presents the findings from qualitative interviews with 17 design professionals focused on strategic design work, exploring their perceptions and experiences with AI tools, as well as their views on how their roles may evolve in the future. The results highlight the diverse ways in which designers are leveraging AI in their work as well as the resulting impact on their roles, emphasizing the continued significance of expert human designers in steering development toward meaningful outcomes. The findings underscore the continued importance of contextual awareness, interdisciplinary collaboration, and human-centered design in crafting meaningful solutions, and highlight the role of relational expertise in design that manifests through interactions.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {904},
numpages = {7},
keywords = {design professionals, designer-AI collaboration, generative AI},
location = {
},
series = {CHI EA '25}
}

@article{10.1134/S1064230725700431,
author = {Averkin, A. N. and Volkov, E. N. and Yarushev, S. A.},
title = {Hybrid Method of Image Analysis Based on Artificial Intelligence Technologies and Fuzzy Sets},
year = {2025},
issue_date = {Jun 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {64},
number = {3},
issn = {1064-2307},
url = {https://doi.org/10.1134/S1064230725700431},
doi = {10.1134/S1064230725700431},
journal = {J. Comput. Syst. Sci. Int.},
month = aug,
pages = {460–473},
numpages = {14},
keywords = {hybrid method, image analysis, artificial intelligence, fuzzy sets, machine learning, computer vision, image processing, neural networks, intelligent systems, pattern recognition}
}

@inproceedings{10.1007/978-3-031-66955-2_12,
author = {Zokay, Mustapha and Saylani, Hicham},
title = {Identification of&nbsp;Skin Diseases Based on&nbsp;Blind Chromophore Separation and&nbsp;Artificial Intelligence},
year = {2024},
isbn = {978-3-031-66954-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-66955-2_12},
doi = {10.1007/978-3-031-66955-2_12},
abstract = {We propose in this paper a new approach for identifying skin diseases from RGB dermatological images. Based on Blind Source Separation and Artificial Intelligence (AI), this approach proceeds in two steps. We begin by estimating the concentrations of the three main skin Chromophore separately, adopting a new source separation technique that exploits both their spatial sparsity and their positivity. We then utilize these concentrations to extract the more relevant features for classification in our second step using AI, rather than those directly extracted from the three spectral bands of the image, as used by most existing methods. The results of tests performed on two different databases of RGB dermatological images of melanoma and nevus demonstrate the superiority of our approach, in terms of melanoma identification, compared to two types of existing methods based on AI.},
booktitle = {Medical Image Understanding and Analysis: 28th Annual Conference, MIUA 2024, Manchester, UK, July 24–26, 2024, Proceedings, Part I},
pages = {173–187},
numpages = {15},
keywords = {Dermatological RGB image, Blind Source Separation, Chromophore, Artificial Intelligence, Skin diseases identification, Melanoma, Nevus},
location = {Manchester, United Kingdom}
}

@inproceedings{10.1145/3663548.3688513,
author = {Mowar, Peya and Peng, Yi-Hao and Steinfeld, Aaron and Bigham, Jeffrey P},
title = {Tab to Autocomplete: The Effects of AI Coding Assistants on Web Accessibility},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3688513},
doi = {10.1145/3663548.3688513},
abstract = {A long-standing challenge in accessible computing has been to get developers to produce the accessible UI code necessary for assistive technologies to work properly. AI coding assistants (e.g., Github Copilot) potentially offer a new opportunity to make UI code more accessible automatically, but it is unclear how their use impacts code accessibility and what developers need to know in order to use them effectively. In this paper, we report on a study where developers untrained in accessibility were tasked with building web UI components with and without an AI coding assistant. Our findings suggest that while current AI coding assistants show potential for creating more accessible UIs, they currently require accessibility awareness and expertise, limiting their expected impact.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {106},
numpages = {6},
keywords = {AI Coding Assistants, Empirical Studies in HCI, Web Accessibility},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@article{10.1016/j.neucom.2023.126895,
author = {Ansari, Sam and Alatrany, Abbas Saad and Alnajjar, Khawla A. and Khater, Tarek and Mahmoud, Soliman and Al-Jumeily, Dhiya and Hussain, Abir Jaafar},
title = {A survey of artificial intelligence approaches in blind source separation},
year = {2023},
issue_date = {Dec 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {561},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2023.126895},
doi = {10.1016/j.neucom.2023.126895},
journal = {Neurocomput.},
month = dec,
numpages = {26},
keywords = {Artificial intelligence, Blind source separation, Deep learning, Independent component analysis, Machine learning}
}

@inproceedings{10.1145/3697355.3697420,
author = {Yuan, Hang},
title = {Application and Comparison of Search Algorithms in Artificial Intelligence},
year = {2024},
isbn = {9798400717529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3697355.3697420},
doi = {10.1145/3697355.3697420},
abstract = {Searching for people after an earthquake is an important part of emergency rescue work. Finding trapped people in a timely and accurate manner is crucial to reducing casualties and improving rescue efficiency. Traditional blind search algorithms are often inefficient due to the lack of effective guidance strategies. Heuristic search algorithms improve search efficiency by combining heuristic functions, but they still have limitations in complex environments. To solve this problem, this paper proposes an improved Q-learning algorithm based on reinforcement learning that combines dynamic exploration rate, experience replay, and target network for searching for people after an earthquake. The algorithm reduces the randomness in the exploration process by dynamically adjusting the exploration rate, uses experience replay to improve learning efficiency, and introduces a target network to stabilize the training process. Experimental results show that the improved reinforcement learning Q-learning algorithm is significantly superior to traditional blind search and heuristic search algorithms in terms of search efficiency and accuracy.},
booktitle = {Proceedings of the 2024 8th International Conference on Big Data and Internet of Things},
pages = {392–396},
numpages = {5},
keywords = {Artificial Intelligence, Search Algorithm, Reinforcement Learning, Q-learning Algorithm},
location = {
},
series = {BDIOT '24}
}

@inproceedings{10.1145/3544549.3585706,
author = {Cho, Jaemin and Kim, Hee Jae},
title = {Dimensional alt text: Enhancing Spatial Understanding through Dimensional Layering of Image Descriptions for Screen Reader Users},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585706},
doi = {10.1145/3544549.3585706},
abstract = {Over the past decade, there has been a significant improvement in the quality of images we see on the web, and image processing technologies such as monocular depth estimation are opening up new possibilities for various applications. However, despite these developments, how we utilize image descriptions for image accessibility has remained stagnant since alt text was introduced with HTML 2.0 in 1995. This paper presents the concept of Dimensional alt text, which enables users to navigate image descriptions through three-dimensional layers: the foreground, middle ground, and background. Our research findings suggest that providing space for image descriptions on each dimensional layer can assist users in building a mental image of the photo, resulting in better spatial understanding. Our discussion for future work is to extend the use case of the prototype to a broader range of users and investigate a hybrid authoring model that combines human authorship with AI assistance.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {86},
numpages = {6},
keywords = {Accessibility, Alt Text, Alt Text Authoring, Depth Map, Dimensional Alt Text, Image Description, Inclusive Design, Screen Readers, Visual Impairment},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@article{10.1145/3659624,
author = {Cuadra, Andrea and Breuch, Justine and Estrada, Samantha and Ihim, David and Hung, Isabelle and Askaryar, Derek and Hassanien, Marwan and Fessele, Kristen L. and Landay, James A.},
title = {Digital Forms for All: A Holistic Multimodal Large Language Model Agent for Health Data Entry},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659624},
doi = {10.1145/3659624},
abstract = {Digital forms help us access services and opportunities, but they are not equally accessible to everyone, such as older adults or those with sensory impairments. Large language models (LLMs) and multimodal interfaces offer a unique opportunity to increase form accessibility. Informed by prior literature and needfinding, we built a holistic multimodal LLM agent for health data entry. We describe the process of designing and building our system, and the results of a study with older adults (N =10). All participants, regardless of age or disability status, were able to complete a standard 47-question form independently using our system---one blind participant said it was "a prayer answered." Our video analysis revealed how different modalities provided alternative interaction paths in complementary ways (e.g., the buttons helped resolve transcription errors and speech helped provide more options when the pre-canned answer choices were insufficient). We highlight key design guidelines, such as designing systems that dynamically adapt to individual needs.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = may,
articleno = {72},
numpages = {39},
keywords = {Accessibility, Artifact or System, Field Study, Health - Clinical, Input Techniques, Interaction Design, Mobile Devices: Phones/Tablets, Older Adults, Prototyping/Implementation, Qualitative Methods, Text/Speech/Language, User Experience Design}
}

@phdthesis{10.5555/AAI29260425,
author = {Moraga, Jaime F. and Wendy, Zhou, and Hua, Wang, and Ge, Jin, and Soutir, Bandyopadhyay,},
advisor = {Sebnem, Duzgun,},
title = {Geothermal AI: An Artificial Intelligence for Early Stage Geothermal Exploration},
year = {2022},
isbn = {9798357536075},
publisher = {Colorado School of Mines},
address = {USA},
abstract = {Exploration of geothermal resources involves analysis and management of a large number of uncertainties, which makes investment and operations decisions challenging. Remote Sensing (RS), Machine Learning (ML) and Artificial Intelligence (AI) have potential in managing the challenges of geothermal exploration. This thesis presents a methodology that integrates RS, ML and AI to create an initial assessment of geothermal potential, by resorting to known indicators of geothermal areas – namely mineral markers, surface temperature, faults and deformation. The method introduced in this thesis was implemented in two sites (Brady and Desert Peak geothermal sites) that are close to each other but have different characteristics (Brady having clear surface manifestations and Desert Peak being a blind site). Various satellite images and geospatial data were processed for mineral markers, temperature, faults and deformation and then ML methods were implemented to obtain patterns of surface manifestation related to geothermal sites. The resulting Geothermal AI uses these patterns from surface manifestations to predict geothermal potential of each pixel. The Geothermal AI was tested using independent data sets obtaining accuracy of 92-95%. The Geothermal AI was also tested by training on one site and executing it for the other site to predict the geothermal / non-geothermal delineation; in this task, which requires generalization, the Geothermal AI performed quite well in prediction with 72-76% accuracy.},
note = {AAI29260425}
}

@article{10.1016/j.compbiomed.2024.109329,
author = {Lin, Senlin and Ma, Yingyan and Li, Liping and Jiang, Yanwei and Peng, Yajun and Yu, Tao and Qian, Dan and Xu, Yi and Lu, Lina and Chen, Yingyao and Zou, Haidong},
title = {Cost-effectiveness and cost-utility of community-based blinding fundus diseases screening with artificial intelligence: A modelling study from Shanghai, China},
year = {2024},
issue_date = {Dec 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {183},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2024.109329},
doi = {10.1016/j.compbiomed.2024.109329},
journal = {Comput. Biol. Med.},
month = dec,
numpages = {9},
keywords = {Blinding eye disease, Screening, Artificial intelligence, Process reengineering, Health economic evaluation}
}

@phdthesis{10.5555/AAI30322063,
author = {Hu, Fenghe},
title = {Artificial Intelligence Supported Cellular-Connected Extended Reality},
year = {2022},
publisher = {University of London, King's College (United Kingdom)},
abstract = {Extended Reality (XR), including augmented (AR), mixed (MR), and virtual reality (VR), is recognized as the key application for next-generation network services. The major service device, head-mounted display (HMD), is also highly expected to continue powering the growth of the consumer electronic market after the personal mobile phone. The internal motivation for such growth is that XR extends the edge of imagination and revolutionize the interaction between human and information by integrating virtual information with the real world. Thus, to fully explore the potential of XR services, enabling mobility through the cellular network is a definite must, which allows the XR service to reach users anytime and anywhere. However, due to the crucial bandwidth and latency requirements, the current XR devices either require a wired connection for high-resolution and low latency video transmission or perform on-device rendering with limited battery life and image quality. The wireless XR schemes, especially cellular networks, struggle to fulfill all these requirements. The lack of mobility support blocks the massive marketization of mobile XR. This thesis features a comprehensive study of the background study and enablers for mobile XR service, especially providing solutions for mobility with the support of machine learning methods. The main contribution of this thesis includes: quality-of-service (QoS) definition for mobile XR services, enhancing the reliability of cellular XR by cooperative transmission, optimizing the cooperate decisions with federated distributed deep reinforcement learning, hierarchical deep reinforcement learning for joint optimization of scheduling and cooperation problems, and the theoretical analysis of the federated multi-agent algorithm. The thesis firstly explains and shows background studies of QoS definitions for different XR use cases under current technical stages from the aspect of wireless communication. In literature, XR QoS metrics are mostly concluded as high bandwidth and low latency, which are oversimplified and too crucial. Adopting these metrics usually results in discussions of mmWave, optical, or Terahertz transmission without the hope of commercialization. Thus, before investigating the enabling technologies of mobile XR, I need to deeply and clearly define the XR QoS. To study the definition of QoS metrics, I built several testbeds with state-of-the-art devices. I built a fourth-generation cellular network testbed with soft-defined radio devices for several popular XR use cases, including unmanned vehicle control and XR conference applications. We measure their requirements and performance. We also build a tiled-DASH broadcast system to investigate the characteristics of XR 360° video traffic. Based on testbeds' results, we show that XR service generally requires consistently low-latency and high bandwidth connection, which is defined as a combination of ultra-reliable low latency communications and Enhanced Mobile Broadband traffic by standards. However, we also notice that the QoS metrics can largely vary for different use cases. Given the example of social media, the users request video messages on transportation with limited interaction contents and highly tolerable to low bandwidth and high latency connection. Thus, it is still possible for cellular networks to concentrate resources on metrics that matter and provide XR services that vary between use cases with the help of certain cellular networks or media technologies. In the sequel, I first recognized that the key point for mobile XR is reliability with testbed results. Unlike conventional cellular services, the jitter or glitch can immediately bring QoS down for XR services. In conventional video streaming, the users can easily load minutes of future resources into their buffer when connection allows in case of sudden connection loss. But it is not possible for XR services with motion correlated resources requests. Thus, the network must provide seamless service for XR users during the entire service duration. Although in the white paper on fifth-generation (5G) cellular networks, the network should constantly offer at least 50Mbps bandwidth. The blind spot still exists in the cell-edge area caused by interference or blockage. To extend the coverage and ensure QoS for XR services, it is possible to group neighboring cells together and jointly serve the users in the cell-edge area. Technologies like Coordinated Multipoint and cell-free MIMO are introduced to standard following this idea. However, the complexity of searching for optimum grouping decisions grows exponentially with the number of participating cells, i.e. dimension explosion. Current solutions, such as greedy or game theorem methods, can only obtain sub-optimal decisions but with relatively low complexity. They either fail in local optimal or require strong assumptions. In this part of the thesis, I applied deep reinforcement learning (DRL) algorithms to solve this problem, which is proved effective for many wireless communication problems. The algorithm is capable to learn, optimize, and adjust the action policy following the environment observations and corresponding reward. The capability of such algorithms is further extended with the support of the neural network and has been shown to outperform many conventional algorithms. But the dimension explosion still limiting DRL, I discussed the distributed deep reinforcement learning, which allows each base station to make its own decisions by optimizing local policy with the local environment and neighborhood decisions, thus significantly reducing the dimensions of each base station's problem. The algorithm then suggests the grouping decision based on the features extracted from the surrounding environment of the single base station. But such features can be inaccurate due to the non-stationary environment caused by unknown neighbors' policies, which limits the performance of distributed deep reinforcement learning. Such a non-stationary environment de-stabilizes the learning procedure and increases the difficulty of convergence. To further enhance the performance, I tried federated learning methods that allow the information (intention and knowledge) exchange among the cells by sharing parameters among the network. I proved that federated learning methods can significantly improve the stability and convergence speed of distributed deep reinforcement learning. My simulation results show-case that the federated distributed deep reinforcement learning can outperform the conventional methods. Besides grouping decisions, there exist many other optimization problems in the transmission pipeline, for example, scheduling. These problems can correlate to each other, while individually optimizing each problem may result in sub-optimal solutions. In the third part of this thesis, I considered a joint optimization of scheduling and cooperative grouping, where the algorithm decides both transmission and re-transmission of video resources and the grouping decision of cooperative transmission. However, the complexity of such an algorithm again increases dramatically for the combined optimization problem. To solve that, I applied hierarchical deep reinforcement learning, which is designed for joint optimization for two-step optimization problems. My results clearly show that it is beneficial to jointly optimize these correlated problems. In the fourth part of my thesis, I performed a comprehensive analysis of the observed gain of federated learning in distributed reinforcement learning in the previous chapter. I re-defined the problem as a networked Markov Decision Process and provide the convergence proof. I also applied the informational model for deep reinforcement learning to measure the information gained during each information exchange. With the results, I quantified the convergence speed and the cost of backhaul traffic during learning, which is proportional to information exchange frequency. My theoretical analysis shows that the exchange of network parameters or experience of considered optimization problem among base stations is useful even with low frequency, which guides efficient algorithm designs and hyper-parameters tuning. Finally, I concluded the thesis with the other mobile XR solutions introduced in the cellular network and some inspirations for possible future works, especially with the reinforcement learning algorithms.},
note = {AAI30322063}
}

@article{10.1145/3519301,
author = {Lv, Zhihan and Chen, Dongliang and Feng, Hailin and Wei, Wei and Lv, Haibin},
title = {Artificial Intelligence in Underwater Digital Twins Sensor Networks},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3519301},
doi = {10.1145/3519301},
abstract = {The particularity of the marine underwater environment has brought many challenges to the development of underwater sensor networks (UWSNs). This research realized the effective monitoring of targets by UWSNs and achieved higher quality of service in various applications such as communication, monitoring, and data transmission in the marine environment. After analysis of the architecture, the marine integrated communication network system (MICN system) is constructed based on the maritime wireless Mesh network (MWMN) by combining with the UWSNs. A distributed hybrid fish swarm optimization algorithm (FSOA) based on mobility of underwater environment and artificial fish swarm (AFS) theory is proposed in response to the actual needs of UWSNs. The proposed FSOA algorithm makes full use of the perceptual communication of sensor nodes and lets the sensor nodes share the information covered by each other as much as possible, enhancing the global search ability. In addition, a reliable transmission protocol NC-HARQ is put forward based on the combination of network coding (NC) and hybrid automatic repeat request (HARQ). In this work, three sets of experiments are performed in an area of 200 \texttimes{} 200 \texttimes{} 200 m. The simulation results show that the FSOA algorithm can fully cover the events, effectively avoid the blind movement of nodes, and ensure consistent distribution density of nodes and events. The NC-HARQ protocol proposed uses relay nodes for retransmission, and the probability of successful retransmission is much higher than that of the source node. At a distance of more than 2,000 m, the successful delivery rate of data packets is as high as 99.6%. Based on the MICN system, the intelligent ship constructed with the digital twins framework can provide effective ship operating state prediction information. In summary, this study is of great value for improving the overall performance of UWSNs and advancing the monitoring of marine data information.},
journal = {ACM Trans. Sen. Netw.},
month = apr,
articleno = {39},
numpages = {27},
keywords = {Marine monitoring, underwater sensor networks, digital twins, artificial intelligence}
}

@article{10.1007/s00146-024-01912-4,
author = {Chen, Meimei and Hong, Bin},
title = {When will the blind be able to take their first steps with GDR guidance under artificial intelligence?},
year = {2024},
issue_date = {Feb 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {40},
number = {2},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-024-01912-4},
doi = {10.1007/s00146-024-01912-4},
journal = {AI Soc.},
month = apr,
pages = {1157–1159},
numpages = {3}
}

@article{10.1007/s00500-024-09873-y,
author = {Balaha, Hossam Magdy and El-Gendy, Eman M. and Saafan, Mahmoud M.},
title = {DMDRDF: diabetes mellitus and retinopathy detection framework using artificial intelligence and feature selection},
year = {2024},
issue_date = {Oct 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {19},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-024-09873-y},
doi = {10.1007/s00500-024-09873-y},
abstract = {Diabetes mellitus is one of the most common diseases affecting patients of different ages. Diabetes can be controlled if diagnosed as early as possible. One of the serious complications of diabetes affecting the retina is diabetic retinopathy. If not diagnosed early, it can lead to blindness. Our purpose is to propose a novel framework, named DMDRDF, for early and accurate diagnosis of diabetes and diabetic retinopathy. The framework consists of two phases, one for diabetes mellitus detection (DMD) and the other for diabetic retinopathy detection (DRD). The novelty of DMD phase is concerned in two contributions. Firstly, a novel feature selection approach called Advanced Aquila Optimizer Feature Selection (A2OFS) is introduced to choose the most promising features for diagnosing diabetes. This approach extracts the required features from the results of laboratory tests while ignoring the useless features. Secondly, a novel classification approach (CA) using five modified machine learning (ML) algorithms is used. This modification of the ML algorithms is proposed to automatically select the parameters of these algorithms using Grid Search (GS) algorithm. The novelty of DRD phase lies in the modification of 7 CNNs using Aquila Optimizer for the classification of diabetic retinopathy. The reported results concerning the DMD datasets shows that AO reports best performance metrics in the feature selection process with the help of modified ML classifiers. The best achieved accuracy is 98.65% with the GS-ERTC model and max-absolute scaling on the “Early Stage Diabetes Risk Prediction Dataset” dataset. Also, from the reported results concerning the DRD datasets, the AOMobileNet is considered a suitable model for this problem as it outperforms the other modified CNN models with accuracy of 95.80% on the “The SUSTech-SYSU dataset” dataset.},
journal = {Soft Comput.},
month = aug,
pages = {11393–11420},
numpages = {28},
keywords = {Diabetes mellitus (DM), Diabetic retinopathy (DR), Deep learning (DL), Feature selection, Machine learning (ML), Optimization algorithms}
}

@article{10.1007/s10916-021-01795-8,
author = {Malerbi, Fernando Korn and Mendes, Giovana and Barboza, Nathan and Morales, Paulo Henrique and Montargil, Roseanne and Andrade, Rafael Ernane},
title = {Diabetic Macular Edema Screened by Handheld Smartphone-based Retinal Camera and Artificial Intelligence},
year = {2022},
issue_date = {Jan 2022},
publisher = {Plenum Press},
address = {USA},
volume = {46},
number = {1},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-021-01795-8},
doi = {10.1007/s10916-021-01795-8},
abstract = {Our aim was to assess the tomographic presence of diabetic macular edema in type 2 diabetes patients screened for diabetic retinopathy with color fundus photographs and an artificial intelligence algorithm. Color fundus photographs obtained with a low-cost smartphone-based handheld retinal camera were analyzed by the algorithm; patients with suspected macular lesions underwent ocular coherence tomography. A total of 366 patients were screened; diabetic macular edema was suspected in 34 and confirmed in 29 individuals, with average age 60.5 ± 10.9&nbsp;years and glycated hemoglobin 9.8 ± 2.4%; use of insulin, statins, and aspirin were reported in 44.8%, 37.9%, and 34.5% of individuals, respectively; systemic blood hypertension, dyslipidemia, abdominal obesity, chronic kidney disease, and risk for diabetic foot ulcers were present in 100%, 58.6%, 62.1%, 48.3%, and 27.5% of individuals, respectively. Proliferative diabetic retinopathy was present in 31% of patients with macular edema; severity level was associated with albuminuria (p = 0.028). Eyes with macular edema had average central macular thickness 329.89 ± 80.98&nbsp;mμ; intraretinal cysts, sub retinal fluid, hyper-reflective foci, epiretinal membrane, and vitreomacular traction were found in 87.2%, 6.4%, 85.1%, 10.6%, and 6.4% of eyes, respectively. Diabetic retinopathy screening overwhelms health systems and is typically based on color fundus photographs, with high false-positive rates for the detection of diabetic macular edema. The present, semi-automated strategy comprising artificial intelligence algorithms integrated with smartphone-based retinal cameras could improve screening in low-resource settings with limited availability of ocular coherence tomography, allowing increased access rates and ultimately contributing to tackle preventable blindness.},
journal = {J. Med. Syst.},
month = jan,
numpages = {6},
keywords = {Diabetic retinopathy, Artificial intelligence, Mobile health, Public health}
}

@phdthesis{10.5555/AAI29341576,
author = {Nuaimi, Marwan Mohammed Al and Samuel, Ameri, and Kashy, Aminian, and Mehrdad, Zamirian, and Hassan, Amini,},
advisor = {Shahab, Mohaghegh,},
title = {Application of Artificial Intelligence for CO2 Storage in Saline Aquifer (Smart Proxy for Snap-Shot in Time)},
year = {2022},
isbn = {9798351496634},
publisher = {West Virginia University},
address = {USA},
abstract = {In recent years, artificial intelligence (AI) and machine learning (ML) technology have grown in popularity. Smart Proxy Models (SPM) are AI/ML based data-driven models which have proven to be quite crucial in petroleum engineering domain with abundant data, or operations in which large surface/ subsurface volume of data is generated. Climate change mitigation is one application of such technology to simulate and monitor CO2 injection into underground formations. The goal of the SPM developed in this study is to replicate the results (in terms of pressure and saturation outputs) of the numerical reservoir simulation model (CMG) for CO2 injection into saline aquifers. In so doing, the artificial intelligence model was used to particularly predict the pressure distribution as well as carbon dioxide plume at any time-step throughout the period of injection and post-injection. There are four injectors injecting approximately two million metric tons of CO2 per year for a period of ten years. The project seeks to unravel what happens to CO2 and pressure during and after the injection process, commonly referred to as injection and post-injection periods. This process was monitored for 10 years of injection and 190 years of post-injection. There are 46 geologic realizations of the porosity and permeability distributions which along with some 300 static and dynamic data and features extracted from the model are used as the main input to the artificial neuron network for training, calibration and validation. The dataset produced is then distributed into three major parts; the training dataset, which is majorly aimed at training smart proxy model, the calibration dataset which is majorly a watchdog, and a blind validation which is used to perform the final evaluation on the model after it achieves the desired training accuracy. The results show that the developed SPM can successfully mimic the pressure and CO2 behavior of the CMG outputs which are determining factors of the amount and safety of CO2 sequestration. When implemented on a large scale, this technology has the potential to be very competitive with existing numerical reservoir simulators, providing an additional toolbox for petroleum engineers and CO2 sequestration specialists to monitor the pressure and CO2 plume, as well as perform uncertainty quantification and optimization.},
note = {AAI29341576}
}

@article{10.1155/2022/6831049,
author = {Dong, Tianyu and Wang, Qiong and Meng, Lingxing and Tang, Yajuan},
title = {Research on Satisfaction of Driverless Function Based on the Artificial Intelligence Algorithm},
year = {2022},
issue_date = {2022},
publisher = {IOS Press},
address = {NLD},
volume = {2022},
issn = {1574-017X},
url = {https://doi.org/10.1155/2022/6831049},
doi = {10.1155/2022/6831049},
abstract = {With the popularization of driverless technology, more and more vehicles have begun to apply this technology. Under these circumstances, user satisfaction with driverless function begins to have an increasing impact on car sales. This paper adopts the KANO model, a two-dimensional model, to analyze users’ attitudes toward driverless functions. It is found that users have a high dependence on functions such as forward collision warning, autonomous emergency braking, and stated-speed sign recognition. Therefore, relevant enterprises should continue to develop these functions. Besides, users have high expectations for functions such as blind spot detection systems and rear cross-traffic alerts. Enterprises can achieve more support from users by optimizing these functions. Functions like lane-keeping assist, lane departure warning, parking distance control, and door open warning systems belong to indifferent attributes of driverless function, which are not cared about by users. There is no need for enterprises to optimize these functions. However, the lane change assist system has been criticized by users and should be improved by corresponding manufacturers.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {7}
}

@inproceedings{10.1145/3502178.3529111,
author = {Jeanneret Medina, Maximiliano and Lalanne, Denis and Baudet, C\'{e}dric},
title = {Human-Computer Interaction in Artificial Intelligence for Blind and Vision Impairment: An Interpretative Literature Review Based on Bibliometrics: L’interaction humain-machine en intelligence artificielle pour les aveugles et d\'{e}ficients visuels : Une revue de litt\'{e}rature interpr\'{e}tative fond\'{e}e sur la bibliom\'{e}trie},
year = {2022},
isbn = {9781450391986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502178.3529111},
doi = {10.1145/3502178.3529111},
abstract = {The rise of artificial intelligence and particularly machine learning conduct to an emerging landscape of intelligent interactive systems. Such technologies help clinicians to detect diseases from medical imaging, and allow to describe the visual world to people with visual impairment. However, this new technological landscape comes with a set of HCI challenges. To better understand the importance of HCI in AI, we focused on blind and vision impairment as a representative application domain. Using bibliometric techniques, we retained 187 scientific publications organized in three clusters. Our findings show that HCI is absent in research related to medical computer systems but has moderate importance when the aim is to assist BVI in their daily life.},
booktitle = {Adjunct Proceedings of the 33rd Conference on l'Interaction Humain-Machine},
articleno = {3},
numpages = {6},
keywords = {Artificial Intelligence, Bibliom\'{e}trie, Bibliometrics, Blind, D\'{e}ficience Visuelle, Human-Computer Interaction, Intelligence Artificielle, Interaction-Humain Machine, Review, Revue, Visual Impairment},
location = {Namur, Belgium},
series = {IHM '22 Adjunct}
}

@article{10.1016/j.compbiomed.2024.109556,
author = {Vieira, Cleverson and Rocha, Leonardo and Guimar\~{a}es, Marcelo and Dias, Diego},
title = {Exploring transparency: A comparative analysis of explainable artificial intelligence techniques in retinography images to support the diagnosis of glaucoma},
year = {2025},
issue_date = {Feb 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {185},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2024.109556},
doi = {10.1016/j.compbiomed.2024.109556},
journal = {Comput. Biol. Med.},
month = feb,
numpages = {17},
keywords = {Machine learning, Artificial intelligence, Explainable artificial intelligence, Convolutional neural networks, Glaucoma}
}

@inproceedings{10.1145/3746058.3758401,
author = {Rahman, Adil and Khan, Rifat Rahman and Hong, Jonggi and Valencia, Stephanie and Heo, Seongkook},
title = {CustomSight: Enhancing LLM-Powered Visual Assistance for Blind Individuals using Goal-Directed Dynamic Filters},
year = {2025},
isbn = {9798400720369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746058.3758401},
doi = {10.1145/3746058.3758401},
abstract = {LLM-powered assistive technologies (ATs) have enabled blind and visually impaired (BVI) users to query personalized, goal-oriented information about their visual environment. However, the accuracy of system responses depends heavily on well-framed, query-relevant images, which can be difficult for BVI users to capture. We present CustomSight, an LLM-powered AT that helps BVI users effectively query visual information by providing task-aware, real-time guidance to frame the camera and automatically capture images when relevant content is in view. When a user issues a query, CustomSight generates a Dynamic Filter—a custom pipeline that encodes logic tied to the user’s intent, monitors the live feed, and triggers context-aware feedback and image capture. The captured image is sent to the LLM to fetch accurate visual information.},
booktitle = {Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {86},
numpages = {3},
keywords = {Accessibility; Blind and Low Vision; Assistive Technology; LLMs},
location = {
},
series = {UIST Adjunct '25}
}

@inproceedings{10.1145/3696593.3696614,
author = {Doush, Iyad Abu and Kassem, Reem},
title = {Evaluating AI-Generated Web Code for Accessibility Compliance: A Metric-Driven Approach},
year = {2025},
isbn = {9798400707292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696593.3696614},
doi = {10.1145/3696593.3696614},
abstract = {The rapid advancement of generative AI models presents many new opportunities and challenges in the domain of web accessibility. This paper aims to evaluate the accuracy of different generative AI models in producing web codes that adhere to accessibility standards. Two of the most popular AI chatbots, ChatGPT and Microsoft Copilot, were tested to assess their performance. Our methodology involves utilizing a foundational prompt design. To test the models’ functionality, nine different web code examples were chosen. Before testing the chatbot, the ideal codes of the nine examples had to be developed to use them for comparison later. After providing the foundational prompt to the chatbot, the next prompt asks the chatbot to generate the required code. Based on the generated code, a follow-up prompt might be needed to fix any issues in the generated code or to further clarify the requirement. The generated code was evaluated using a comprehensive metric-based approach, ensuring it met the necessary accessibility requirements and usability enhancements. Five different metrics were chosen: correctness and functionality of the code, cross-browser compatibility, the number of prompts needed to produce the required code, the severity of accessibility issues, and conformance to standards, which included testing the code’s compatibility with both the keyboard and screen reader. Our findings reveal significant insights into the capabilities and limitations of generative AI technology in creating accessible web code. The results showed great potential for both chosen chatbots and highlighted the importance of well-crafted prompts. In terms of web accessibility, the chatbots had similar results, however ChatGPT produced clearer webpages that demonstrated the concept in a better way when compared to Copilot.},
booktitle = {Proceedings of the 11th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {338–344},
numpages = {7},
keywords = {Web accessibility, WCAG, ARIA, Generative AI, Prompt engineering, Evaluation metrics, Digital accessibility},
location = {
},
series = {DSAI '24}
}

@inproceedings{10.1145/3744257.3744276,
author = {Sunkara, Mohan and Kolgar Nayak, Akshay and Kalari, Sandeep and Prakash, Yash and Jayarathna, Sampath and Lee, Hae-Na and Ashok, Vikas},
title = {Adapting Online Customer Reviews for Blind Users: A Case Study of Restaurant Reviews},
year = {2025},
isbn = {9798400718823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744257.3744276},
doi = {10.1145/3744257.3744276},
abstract = {Online reviews have become an integral aspect of consumer decision-making on e-commerce websites, especially in the restaurant industry. Unlike sighted users who can visually skim through the reviews, perusing reviews remains challenging for blind users, who rely on screen reader assistive technology that supports predominantly one-dimensional narration of content via keyboard shortcuts. In an interview study, we uncovered numerous pain points of blind screen reader users with online restaurant reviews, notably, the listening fatigue and frustration after going through only the first few reviews. To address these issues, we developed QuickCue assistive tool that performs aspect-focused sentiment-driven summarization to reorganize the information in the reviews into an alternative, thematically-organized presentation that is conveniently perusable with a screen reader. At its core, QuickCue utilizes a large language model to perform aspect-based joint classification for grouping reviews, followed by focused summarizations within the groups to generate concise representations of reviewers’ opinions, which are then presented to the screen reader users via an accessible interface. Evaluation of QuickCue in a user study with 10 participants showed significant improvements in overall usability and task workload compared to the status quo screen reader.},
booktitle = {Proceedings of the 22nd International Web for All Conference},
pages = {135–146},
numpages = {12},
keywords = {blind, screen reader, visual impairment, assistive technology, online discussion forum, large language model},
location = {
},
series = {W4A '25}
}

@inproceedings{10.1145/3704637.3734738,
author = {Murray, Michael and Motahar, Tamanna and Cakmak, Maya},
title = {Towards More Accessible Open Source AI Platforms},
year = {2025},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704637.3734738},
doi = {10.1145/3704637.3734738},
abstract = {AI education is rapidly becoming essential as artificial intelligence transforms industries, yet students with disabilities often encounter significant barriers to learning and engagement. This paper examines accessibility challenges encountered by learners with visual, cognitive, and physical disabilities when using foundational tools for AI development. Using HuggingFace, an influential open-source platform, as a case study, we analyze barriers such as insufficient screen reader support, complex interfaces, and information overload. We propose design recommendations to promote equity and inclusivity in AI tools, aiming to empower diverse learners to thrive in AI education. Our work highlights the importance of inclusive design for CS educators, researchers, and policymakers.},
booktitle = {Proceedings of the 2025 Conference on Research on Equitable and Sustained Participation in Engineering, Computing, and Technology},
pages = {272–278},
numpages = {7},
keywords = {accessibility, ai education, cs education, disability, inclusive design},
location = {Newark, NJ, USA},
series = {RESPECT 2025}
}

@inproceedings{10.1145/3706598.3713433,
author = {Tang, Xinru and Abdolrahmani, Ali and Gergle, Darren and Piper, Anne Marie},
title = {Everyday Uncertainty: How Blind People Use GenAI Tools for Information Access},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713433},
doi = {10.1145/3706598.3713433},
abstract = {Generative AI (GenAI) tools promise to advance non-visual information access but introduce new challenges due to output errors, hallucinations, biases, and constantly changing capabilities. Through interviews with 20 blind screen reader users who use various GenAI applications for diverse tasks, we show how they approached information access with everyday uncertainty, or a mindset of skepticism and criticality towards both AI- and human-mediated assistance as well as information itself. Instead of expecting information to be ‘correct’ and ‘complete’, participants extracted cues from error-prone information sources; treated all information as tentative; acknowledged and explored information subjectivity; and constantly adjusted their expectations and strategies considering the politics around access. The concept of everyday uncertainty situates GenAI tools among the interconnected assistive applications, humans, and sociomaterial conditions that both enable and hinder the ongoing production of access. We discuss the implications of everyday uncertainty for future design and research.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {63},
numpages = {17},
keywords = {uncertainty, generative artificial intelligence, accessibility, blind, screen reader users},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3663548.3675660,
author = {Seo, JooYoung and Kamath, Sanchita S. and Zeidieh, Aziz and Venkatesh, Saairam and McCurry, Sean},
title = {MAIDR Meets AI: Exploring Multimodal LLM-Based Data Visualization Interpretation by and with Blind and Low-Vision Users},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675660},
doi = {10.1145/3663548.3675660},
abstract = {This paper investigates how blind and low-vision (BLV) users interact with multimodal large language models (LLMs) to interpret data visualizations. Building upon our previous work on the multimodal access and interactive data representation (MAIDR) framework, our mixed-visual-ability team co-designed maidrAI, an LLM extension providing multiple AI responses to users’ visual queries. To explore generative AI-based data representation, we conducted user studies with 8 BLV participants, tasking them with interpreting box plots using our system. We examined how participants personalize LLMs through prompt engineering, their preferences for data visualization descriptions, and strategies for verifying LLM responses. Our findings highlight three dimensions affecting BLV users’ decision-making process: modal preference, LLM customization, and multimodal data representation. This research contributes to designing more accessible data visualization tools for BLV users and advances the understanding of inclusive generative AI applications.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {57},
numpages = {31},
keywords = {Accessibility, Blind, Data Visualization, Generative AI, Large Language Models, Low Vision, Multimodality, Screen Readers},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3706598.3714005,
author = {Jones, Katherine and Grayson, Martin and Morrison, Cecily and Leonards, Ute and Metatla, Oussama},
title = {"Put Your Hands Up": How Joint Attention Is Initiated Between Blind Children And Their Sighted Peers},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714005},
doi = {10.1145/3706598.3714005},
abstract = {Initiating joint attention (JA) is a fundamental first step in social interactions. In sighted individuals, it relies predominantly on visual cues, such as gaze and hand gestures. These features can reduce opportunities for blind and visually impaired (BVI) and sighted people to interact. Understanding the strategies to navigate these challenges is necessary to develop technology that can facilitate more inclusive JA. To address this, we conducted a longitudinal case study of five children with mixed visual abilities engaging in activities rich with JA opportunities. In a teacher-led classroom, the children experimented with the use of an AI-powered headset designed to support BVI people in social situations. Interaction analysis established that situational complexity affects the children’s responses to initiation attempts. Furthermore, the headset adds to this complexity, affecting the frequency and reactions to attempts to initiate JA. The findings informed the creation of a JA initiation framework and suggestions for future design.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {555},
numpages = {18},
keywords = {Joint Attention, Joint Engagement, Social Interaction, Blind, Visually Impaired, Children, Inclusion, Mixed-Visual Ability, AI, Headset},
location = {
},
series = {CHI '25}
}

@article{10.1145/3654768.3654775,
author = {Zhang, Lotus},
title = {Designing Accessible Content Creation Support with Blind and Low Vision Creators},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
number = {137},
issn = {1558-2337},
url = {https://doi.org/10.1145/3654768.3654775},
doi = {10.1145/3654768.3654775},
abstract = {Today, digital creative tools are still largely designed without disabled people in mind and thus require retrofitting to achieve basic accessibility. As more creative tools begin to incorporate generative artificial intelligence, I propose a research agenda that centers the design of human-AI co-creation experiences on disabled creators and leverages such technology for accessibility from the start. Specifically, I focus on researching ways that AI-assisted creative tools could be designed to lift the expression ceiling and reduce effort for blind and low vision creators. Starting with a formative mixed-method study that uncovers the blind and low vision community's needs for visual content creation and editing support, this dissertation explores and designs accessible content creation support for three highly desired visual creative tasks: (1) private visual content obfuscation, (2) social media video editing, and (3) aesthetic visual content authoring. Together, I believe this dissertation will contribute actionable insights to lower the barriers to expressive and efficient digital content creation for blind and low vision creators.},
journal = {SIGACCESS Access. Comput.},
month = mar,
articleno = {7},
numpages = {1}
}

@inproceedings{10.1145/3750069.3750122,
author = {Cardia, Marco and Buzzi, Marina and Galesi, Giulio and Leporini, Barbara},
title = {A Descriptive Review of Image Datasets for Accessible Alternative Descriptions in STEM Domains},
year = {2025},
isbn = {9798400721021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3750069.3750122},
doi = {10.1145/3750069.3750122},
abstract = {STEM disciplines rely heavily on visual content, such as charts, diagrams, formulas. These figures are often inaccessible to blind or visually impaired users due to the lack of meaningful alternative text. While automated image captioning has progressed, existing datasets are largely oriented toward general images and overlook the structural and semantic complexity of STEM visual contents. This paper presents a descriptive review of publicly available image datasets, evaluating their applicability for generating accessible descriptions of STEM images. Our analysis reveals major gaps: limited support for complex scientific content, shallow annotations, and little consideration for accessibility standards. We argue for the creation of a specialized dataset with rich, structured annotations aligned with accessibility goals. By identifying critical gaps, this work supports the development of AI tools and datasets that enhance inclusive access to STEM content.},
booktitle = {Proceedings of the 16th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {9},
numpages = {7},
keywords = {Alternative Descriptions, Alternative Text, Accessible Alternative Descriptions, Accessibility, Visually Impaired Students, Accessible Images, Image Captioning, STEM},
location = {
},
series = {CHItaly '25}
}

@article{10.1016/j.compbiomed.2024.108469,
author = {Garc\'{e}s-Jim\'{e}nez, Alberto and Polo-Luque, Mar\'{\i}a-Luz and G\'{o}mez-Pulido, Juan A. and Rodr\'{\i}guez-Puyol, Diego and G\'{o}mez-Pulido, Jos\'{e} M.},
title = {Predictive health monitoring: Leveraging artificial intelligence for early detection of infectious diseases in nursing home residents through discontinuous vital signs analysis},
year = {2024},
issue_date = {May 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {174},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2024.108469},
doi = {10.1016/j.compbiomed.2024.108469},
journal = {Comput. Biol. Med.},
month = may,
numpages = {13},
keywords = {Artificial intelligence, Elderly, Infectious disease, Machine learning, Nursing home, Prognosis, Remote patient monitoring}
}

@inproceedings{10.24963/ijcai.2024/1023,
author = {Moon, Yunseo and Lee, Hyunmin and Oh, SeungYoung and Jung, Hyunggu},
title = {SaGol: using MiniGPT-4 to generate alt text for improving image accessibility},
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/1023},
doi = {10.24963/ijcai.2024/1023},
abstract = {SaGol is an AI-powered application to improve image accessibility for people with visual impairments (PVI) users. Alternative (alt) text, a general method of web accessibility for PVI users, is text or phrases that describe images on a website in an understandable way. SaGol generates alt text with the images on the user's smartphone using a vision large language model called MiniGPT-4. SaGol searches for similar images based on the generated alt text. We evaluated the length of alt text and the search accuracy. This paper shows a potential opportunity to improve image accessibility for PVI users.},
booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
articleno = {1023},
numpages = {4},
location = {Jeju, Korea},
series = {IJCAI '24}
}

@inproceedings{10.1145/3508259.3508269,
author = {Shrestha, Raju},
title = {A Neural Network Model and Framework for an Automatic Evaluation of Image Descriptions based on NCAM Image Accessibility Guidelines},
year = {2022},
isbn = {9781450384162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508259.3508269},
doi = {10.1145/3508259.3508269},
abstract = {Millions of people who are either blind or visually impaired have difficulty understanding the content in an image. To address the problem textual image descriptions or captions are provided separately or as alternative texts on the web so that the users can read them through a screen reader. However, most of the image descriptions provided are inadequate to make them accessible enough. Image descriptions could be written either manually or automatically generated using software tools. There are tools, methods, and metrics used to evaluate the quality of the generated text. However, almost all of them are word-similarity-based and generic. Even though there are standard guidelines such as WCAG2.0 and NCAM image accessibility guidelines, they are rarely used in the evaluation of image descriptions. In this paper, we propose a neural network-based framework and models for an automatic evaluation of image descriptions in terms of compliance with the NCAM guidelines. A custom dataset was created from a widely used Flickr8K dataset to train and test the models. The experimental results show the proposed framework performing very well with an average accuracy of above 98%. We believe that the framework could be helpful and useful for the authors of image descriptions in writing accessible image descriptions for the users.},
booktitle = {Proceedings of the 2021 4th Artificial Intelligence and Cloud Computing Conference},
pages = {68–73},
numpages = {6},
location = {Kyoto, Japan},
series = {AICCC '21}
}

@inproceedings{10.1145/3677846.3677855,
author = {Shen, Ming and Huang, Gang and Wu, Yuxuan and Song, Shuyi and Zhou, Sheng and Li, Liangcheng and Yu, Zhi and Wang, Wei and Bu, Jiajun},
title = {Making Accessible Movies Easily: An Intelligent Tool for Authoring and Integrating Audio Descriptions to Movies},
year = {2024},
isbn = {9798400710308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677846.3677855},
doi = {10.1145/3677846.3677855},
abstract = {Blind and visually impaired (BVI) individuals encounter significant challenges in perceiving the visual content of movies. Audio descriptions (AD) are inserted into speech gaps to describe visual content and storyline for BVI individuals. However, the processes of authoring and integrating AD are laborious, involving tasks such as identifying speech gaps, authoring AD scripts, dubbing, and integrating them into the movie. To streamline these processes, we introduce EasyAD, an intelligent tool to automate these processes. EasyAD utilizes character recognition technology to identify speech gaps and utilizes speech synthesis technology for AD dubbing. EasyAD addresses the misidentification of the background music of existing methods, and for the first time applies a multimodal large language model in the tool to generate AD. EasyAD is currently operational at the China Braille Library and we invite 6 AD authors for a user study. The results demonstrate that with the use of EasyAD, the processing time for a medium-difficulty movie is reduced by nearly 50%, reducing the workload of AD authors and accelerating accessible movie production in China. EasyAD leverages the advantages of AI technologies, especially multimodal large language models, for accessible movie production and benefits BVI individuals.},
booktitle = {Proceedings of the 21st International Web for All Conference},
pages = {160–164},
numpages = {5},
keywords = {accessible movie, accessibility, blind and visually impaired, audio descriptions, multimodal large language model},
location = {Singapore, Singapore},
series = {W4A '24}
}

@inproceedings{10.1145/3746058.3758468,
author = {Chang, Ruei-Che},
title = {Enabling Real-World Assistive Agents: From Live Vision to Proactive Context-Aware Information Delivery},
year = {2025},
isbn = {9798400720369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746058.3758468},
doi = {10.1145/3746058.3758468},
abstract = {Interacting with the real world is a fundamental part of daily life, yet it remains challenging for individuals who are blind or visually impaired (BVI). It demands live, contextual understanding of dynamic environments, along with interactive, multimodal communication to fulfill their sensory and cognitive needs. To address this, my dissertation develops assistive AI systems and frameworks that observe the real world through multimodal sensing, reason about essential information in response to user contexts, and deliver human-like verbal communication to support real-world understanding. First, I explored the design insights of assistive AI agents by investigating how BVI users interact with human-like video AI systems across diverse real-world contexts. Second, with the identified insights, such as a lack of proactivity, I developed a mobile application that analyzes live camera feeds to generate real-time visual descriptions aligned with user goals, delivering them in harmony with the audio environment. Lastly, I extend it with a set of human-like capabilities, such as memory, spatial understanding, and the ability to infer intent from natural interactions, to act as a long-term assistive companion. Ultimately, my dissertation advances a paradigm shift from digital agents to real-world assistive agents that enhance the independence and agency of BVI individuals.},
booktitle = {Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {2},
numpages = {5},
keywords = {Visual descriptions, blind, visually impaired, assistive technology, accessibility, context-aware, customization, LLM, real world, sound},
location = {
},
series = {UIST Adjunct '25}
}

@article{10.1007/s11042-022-12577-w,
author = {Afif, Mouna and Ayachi, Riadh and Said, Yahia and Pissaloux, Edwige and Atri, Mohamed},
title = {An efficient object detection system for indoor assistance navigation using deep learning techniques},
year = {2022},
issue_date = {May 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {81},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-022-12577-w},
doi = {10.1007/s11042-022-12577-w},
abstract = {Building new systems used for indoor objects detection and indoor assistance navigation presents a very crucial task especially in artificial intelligence and computer science fields. The number of blind and visually impaired persons (VIP) is increasing day by day. In order to help this category of persons, we propose to develop a new indoor object-detection system based on deep convolutional neural networks (DCNNs). The proposed system is developed based on the one-stage neural network RetinaNet. In order to train and evaluate the developed system, we propose to build a new indoor objects dataset which also presents 11,000 images containing 24 indoor landmark objects highly valuable for indoor assistance navigation. The proposed dataset provides a high intra and inter-class variation and various challenging conditions which aim to build a robust detection system for blind and visually impaired people (VIP) mobility. Experimental results prove the high detection performances of the developed indoor objects detection and recognition system. We obtained a detection accuracy reaching up to 98.75% mAP and 62 FPS as a detection speed.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {16601–16618},
numpages = {18},
keywords = {Indoor navigation, Blind and visually impaired persons, Deep learning, Deep convolutional neural networks (DCNN), Indoor object detection}
}

@inproceedings{10.1145/3544548.3581145,
author = {Pucci, Emanuele and Possaghi, Isabella and Cutrupi, Claudia Maria and Baez, Marcos and Cappiello, Cinzia and Matera, Maristella},
title = {Defining Patterns for a Conversational Web},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581145},
doi = {10.1145/3544548.3581145},
abstract = {Conversational agents are emerging as channels for a natural and accessible interaction with digital services. Their benefits span across a wide range of usage scenarios and address visual impairments and any situational impairments that may take advantage of voice-based interactions. A few works highlighted the potential and the feasibility of adopting conversational agents for making the Web truly accessible for everyone. Yet, there is still a lack of concrete guidance in designing conversational experiences for browsing the Web. This paper illustrates a human-centered process that involved 26 blind and visually impaired people to investigate their difficulties when using assistive technology for accessing the Web, and their attitudes and preferences on adopting conversational agents. In response to the identified challenges, the paper introduces patterns for conversational Web browsing. It also discusses design implications that can promote Conversational AI as a technology to enhance Web accessibility.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {118},
numpages = {17},
keywords = {Conversational Patterns, Conversational UIs, Conversational Web Browsing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1016/j.jpdc.2025.105085,
author = {Afif, Mouna and Ayachi, Riadh and Said, Yahia and Atri, Mohamed},
title = {Deep embedded lightweight CNN network for indoor objects detection on FPGA},
year = {2025},
issue_date = {Jul 2025},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {201},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2025.105085},
doi = {10.1016/j.jpdc.2025.105085},
journal = {J. Parallel Distrib. Comput.},
month = may,
numpages = {7},
keywords = {Indoor objects detection, Blind and visually impaired, Deep learning, Embedded implementation, VITIS AI, Xilinx ZCU 102}
}

@inproceedings{10.1007/978-3-031-62846-7_47,
author = {Darvishy, Alireza},
title = {Accessible Mobility for Persons with Disabilities: Introduction to the Special Thematic Sessiom},
year = {2024},
isbn = {978-3-031-62845-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-62846-7_47},
doi = {10.1007/978-3-031-62846-7_47},
abstract = {This paper presents and summarizes nine Special Thematic Session (STS) papers focusing on navigational assistive technologies for persons with disabilities, particularly for blind and visually impaired (BVI) individuals. The selected papers cover a wide range of topics, including step length estimation, utilization of technology in orientation and mobility (O&amp;M), thermal-tactile biofeedback, ICT trends in extraocular muscle prosthesis implants, sound-based navigation, indoor wayfinding applications, digital tools for navigation training, AI-based sensor orientation correction, and accessible wayfinding tools. Along with summarizing their approaches and key findings, this paper touches on the common methods, themes, and emerging trends across the papers.},
booktitle = {Computers Helping People with Special Needs: 19th International Conference, ICCHP 2024, Linz, Austria, July 8–12, 2024, Proceedings, Part I},
pages = {395–399},
numpages = {5},
keywords = {People with Disabilities, Visual Impairment, Navigation, Wayfinding, Mobility, Assistive Technologies},
location = {Linz, Austria}
}

@article{10.1007/s00146-024-01883-6,
author = {Kintonova, Aliya and Gabdreshov, Galimzhan and Yensebaev, Timur and Sadykova, Rizvangul and Yensebayev, Nurbek and Kulbasov, Sultan and Magzymov, Daulet},
title = {Experiment on teaching visually impaired and blind children using a mobile electronic alphabetic braille trainer},
year = {2024},
issue_date = {Feb 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {40},
number = {2},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-024-01883-6},
doi = {10.1007/s00146-024-01883-6},
abstract = {The article considers a pressing problem in the field of inclusive education: creating a comfortable learning environment for the effective education of children with special needs. In this article, a mobile electronic alphabet Braille simulator is an element of the learning environment for children with special needs. The article describes an experiment on teaching visually impaired and blind children using a mobile electronic Braille alphabet simulator. The mobile electronic Braille alphabet trainer, based on new advanced technology, was developed by Kazakh scientist-inventor Galimzhan Gabdreshov under the brand of his company SEZUAL. It was developed to reduce the time that blind children need to learn to read. The relevance of the study lies in the need to educate visually impaired and blind children through the introduction of modern technologies. Children entering primary, mainstream and special needs schools have been learning Braille for several years with varying degrees of success. The article shows the result of an experiment on teaching children using an electronic Braille alphabet simulator and without this simulator. The experiment showed the benefits of teaching blind children using a Braille simulator. An experimental study on the implementation of the SEZUAL electronic alphabet simulator with Braille was carried out in schools for the blind in the cities of Atyrau and Astana. A way to improve a mobile electronic alphabet Braille simulator is to make a simultaneous sound signal when a child presses the keys of the Braille simulator, with different Braille characters. To enhance the impact on the associative zone of the cerebral cortex, in the Braille simulator, sound signals of each key are added to the touch keys, which, like visual signals, are processed by the associative zone of the cerebral cortex and help to consolidate the perception of the simulator keys. This article describes in detail an experiment on the assimilation of educational material by blind and visually impaired children using a combination of parameters of sensorimotor and auditory perception of children. To clarify the peculiarities of the perception of blind and visually impaired respondents, their leading representative systems and modalities were identified before the experiment. The test was conducted based on the educational institution Atyrau Regional School for Gifted Children, Republic of Kazakhstan.},
journal = {AI Soc.},
month = mar,
pages = {1059–1074},
numpages = {16},
keywords = {Teaching methods, Braille, Alphabet for the blind, Visually impaired, Training program, Sensorimotor organs}
}

@article{10.3233/JIFS-220772,
author = {Mishra, Anju and Singh, Laxman and Pandey, Mrinal and Lakra, Sachin},
title = {Image based early detection of diabetic retinopathy: A systematic review on Artificial Intelligence (AI) based recent trends and approaches},
year = {2022},
issue_date = {2022},
publisher = {IOS Press},
address = {NLD},
volume = {43},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-220772},
doi = {10.3233/JIFS-220772},
abstract = {Diabetic Retinopathy (DR) is a disease that damages the retina of the human eye due to diabetic complications, resulting in a loss of vision. Blindness may be avoided If the DR disease is detected at an early stage. Unfortunately, DR is irreversible process, however, early detection and treatment of DR can significantly reduce the risk of vision loss. The manual diagnosis done by ophthalmologists on DR retina fundus images is time consuming, and error prone process. Nowadays, machine learning and deep learning have become one of the most effective approaches, which have even surpassed the human performance as well as performance of traditional image processing-based algorithms and other computer aided diagnosis systems in the analysis and classification of medical images. This paper addressed and evaluated the various recent state-of-the-art methodologies that have been used for detection and classification of Diabetic Retinopathy disease using machine learning and deep learning approaches in the past decade.Furthermore, this study also provides the authors observation and performance evaluation of available research using several parameters, such as accuracy, disease status, and sensitivity. Finally, we conclude with limitations, remedies, and future directions in DR detection. In addition, various challenging issues that need further study are also discussed.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6709–6741},
numpages = {33},
keywords = {Retinal fundus images, machine learning, deep learning, classification, Diabetic retinopathy}
}

@article{10.1016/j.engappai.2025.110713,
author = {Wang, Dong and Liu, Jian and Deng, Lijun and Cao, Peng and Liu, Li},
title = {Fast and intelligent measurement of the ventilation resistance coefficient for the whole mine based on sparse measurement points},
year = {2025},
issue_date = {Jul 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2025.110713},
doi = {10.1016/j.engappai.2025.110713},
journal = {Eng. Appl. Artif. Intell.},
month = may,
numpages = {15},
keywords = {Artificial intelligence, Differential evolution, Intelligent measurement, Sparse measurement points, Ventilation resistance coefficient, Whole ventilation network}
}

@inproceedings{10.1609/aaai.v38i21.30556,
author = {Shende, Tanisha},
title = {AI-enhanced art appreciation: generating text from artwork to promote inclusivity},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i21.30556},
doi = {10.1609/aaai.v38i21.30556},
abstract = {Visual art facilitates expression, communication, and connection, yet it remains inaccessible to those who are visually-impaired and those who lack the resources to understand the techniques and history of art. In this work, I propose the development of a generative AI model that generates a description and interpretation of a given artwork. Such research can make art more accessible, support art education, and improve the ability of AI to understand and translate between creative media. Development will begin with a formative study to assess the needs and preferences of blind and low vision people and art experts. Following the formative study, the basic approach is to train the model on a database of artworks and their accompanying descriptions, predict sentiments from extracted visual data, and generate a paragraph closely resembling training textual data and incorporating sentiment analysis. The model will then be evaluated quantitatively through metrics like METEOR and qualitatively through Turing tests in an iterative process.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2833},
numpages = {3},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.1145/3663548.3675659,
author = {Alharbi, Rahaf and Lor, Pa and Herskovitz, Jaylin and Schoenebeck, Sarita and Brewer, Robin N.},
title = {Misfitting With AI: How Blind People Verify and Contest AI Errors},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675659},
doi = {10.1145/3663548.3675659},
abstract = {Blind people use artificial intelligence-enabled visual assistance technologies (AI VAT) to gain visual access in their everyday lives, but these technologies are embedded with errors that may be difficult to verify non-visually. Previous studies have primarily explored sighted users’ understanding of AI output and created vision-dependent explainable AI (XAI) features. We extend this body of literature by conducting an in-depth qualitative study with 26 blind people to understand their verification experiences and preferences. We begin by describing errors blind people encounter, highlighting how AI VAT fails to support complex document layouts, diverse languages, and cultural artifacts. We then illuminate how blind people make sense of AI through experimenting with AI VAT, employing non-visual skills, strategically including sighted people, and cross-referencing with other devices. Participants provided detailed opportunities for designing accessible XAI, such as affordances to support contestation. Informed by disability studies framework of misfitting and fitting, we unpacked harmful assumptions with AI VAT, underscoring the importance of celebrating disabled ways of knowing. Lastly, we offer practical takeaways for Responsible AI practice to push the field of accessible XAI forward.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {61},
numpages = {17},
keywords = {Accessibility, Artificial Intelligence, Be My Eyes, Blind people, Explainability, Seeing AI, Verification, Visual Assistance Technology},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@article{10.1007/s00500-023-09177-7,
author = {Afif, Mouna and Ayachi, Riadh and Said, Yahia and Atri, Mohamed},
title = {An indoor scene recognition system based on deep learning evolutionary algorithms},
year = {2023},
issue_date = {Nov 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-023-09177-7},
doi = {10.1007/s00500-023-09177-7},
abstract = {Blind and visually impaired (BVI) face various problems during their navigation. Being unable to rely on sight greatly restricts their capacity to learn information about their surroundings. Scene recognition is crucial in improving life quality for BVI. Scene recognition systems can  recognize and characterize the visual world using powerful artificial intelligence algorithms, allowing users to receive a critical overview. Indoor scene recognition systems are crucial for BVI to explore their environment. These systems are essential for increasing their independence, safety, and overall quality of life. Developing technology that allows BVI to perceive their environment and enable them to  navigate and interact with the world on their own is extremely important. We propose in this paper a scene recognition system to assist BVI in their daily activities. The proposed work was developed on top of an efficient set of deep learning techniques called “Deep Evolutionary Algorithms (DEAs)”. DEAs are a type of algorithm that solves complicated search problems by combining the principles of evolutionary computing with deep learning. DEAs provide optimization techniques inspired by the process of natural selection. They iteratively develop a population of potential networks to identify optimum or near-optimal networks to solve complicated problems through genetic methods including mutation, crossover, and selection. To ensure the efficiency of the proposed work, extensive experiments have been conducted using two benchmark datasets the MIT 67 dataset and the Scene 15 dataset. New state-of-the-art results have been ensured by the proposed work in terms of recognition accuracy.},
journal = {Soft Comput.},
month = sep,
pages = {15581–15594},
numpages = {14},
keywords = {Indoor scene recognition, Blinds and visually impaired, Deep learning, Evolutionary algorithms}
}

@inproceedings{10.1145/3677846.3677861,
author = {Xu, Andi and Cai, Minyu and Hou, Dier and Chang, Ruei-Che and Guo, Anhong},
title = {ImageExplorer Deployment: Understanding Text-Based and Touch-Based Image Exploration in the Wild},
year = {2024},
isbn = {9798400710308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677846.3677861},
doi = {10.1145/3677846.3677861},
abstract = {Blind and visually-impaired (BVI) users often rely on alt-texts to understand images. AI-generated alt-texts can be scalable and efficient but may lack details and are prone to errors. Multi-layered touch interfaces, on the other hand, can provide rich details and spatial information, but may take longer to explore and cause higher mental load. To understand how BVI users leverage these two methods, we deployed ImageExplorer, an iOS app on the Apple App Store that provides multi-layered image information via both text-based and touch-based interfaces with customizable levels of granularity. Across 12 months, 371 users uploaded 651 images and explored 694 times. Their activities were logged to help us understand how BVI users consume image captions in the wild. This work informs a holistic understanding of BVI users’ image exploration behavior and influential factors. We provide design implications for future models of image captioning and visual access tools.},
booktitle = {Proceedings of the 21st International Web for All Conference},
pages = {59–69},
numpages = {11},
keywords = {Accessibility, Alt Text, Blind, Deployment, Image Caption, ImageExplorer, Screen Reader, Touch Exploration, Visual Impairment},
location = {Singapore, Singapore},
series = {W4A '24}
}

@article{10.1007/s10586-021-03419-9,
author = {Afif, Mouna and Ayachi, Riadh and Atri, Mohamed},
title = {Indoor objects detection system implementation using multi-graphic processing units},
year = {2022},
issue_date = {Feb 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-021-03419-9},
doi = {10.1007/s10586-021-03419-9},
abstract = {Indoor objects detection and recognition plays an important role in computer science and artificial intelligence fields. This task plays also a crucial role especially for blind and visually impaired persons (VIP) assistance navigation. Aiming to address this problem, we propose in this paper to develop a new indoor object detection system based on deep learning algorithms. Unfortunately, this type of algorithms requires heavy computational resources, and energy consumption. To address this problem, we propose a CUDA multi-GPU framework implementation of the proposed system. Generally deep learning based algorithms require huge amount of data to train and test networks. We propose to develop a new indoor dataset which consists of 11,000 indoor images containing 25 indoor landmark objects highly recommended for blind and VIP navigation. Based on the obtained results, the developed system shows big efficiency in terms of detection accuracy as well as processing time.},
journal = {Cluster Computing},
month = feb,
pages = {469–483},
numpages = {15},
keywords = {Parallel computing, Deep learning, GPU implementation, Assistance navigation, Deep convolutional neural network (DCNNs), Indoor object detection}
}

@article{10.1016/j.engappai.2025.110789,
author = {Hu, Bo and Xu, Shiyu and Liu, Lu and Liu, Rongxin and Sun, Mingzhu and Zhao, Xin},
title = {Adaptive self-evolving extreme learning machine-based terminal sliding mode control with application in retinal vein injection},
year = {2025},
issue_date = {Jul 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2025.110789},
doi = {10.1016/j.engappai.2025.110789},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
numpages = {15},
keywords = {Retinal vein injection, Neural networks, Extreme leaning machine, Self-evolving mechanism, Terminal sliding mode control}
}

@inproceedings{10.1007/978-3-031-98281-1_9,
author = {Kawulok, Mateusz and Ma\'{c}kowski, Micha\l{} and Rosiak, Maria},
title = {A Method and Semi-automated AI Tool Supporting Tutors in Preparing Audio-Tactile Exercises for Blind Students},
year = {2025},
isbn = {978-3-031-98280-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-98281-1_9},
doi = {10.1007/978-3-031-98281-1_9},
abstract = {The article presents a method and tool that use deep learning to enable the preparation of educational graphic materials adapted to the needs of visually impaired people, focusing on supporting the teacher's work in adapting exercises to an alternative audio-tactile form. The challenges educators face, the proposed alternative in the form of an artificial intelligence tool, and the research carried out are presented. The research group comprised 9 teachers with extensive experience working with visually impaired students, and the study spanned one semester in a high school setting. The proposed evaluation method encompassed several factors, including a comparison of the effectiveness of the obtained artificial intelligence models for image analysis, a comparison of the time required for exercise preparation in contrast to previously used solutions, the usability scale of the proposed system measured by subjective indicators, and the reduction of the workload of the tutor by using a standard task-load index. The findings suggest that the proposed method and the teacher support tool attain high-efficiency scores in the quality of the classifier and markedly reduce the workload necessary to adapt educational material to the needs of blind individuals. Further development of the tool toward generative models can contribute to the creation of a comprehensive and automated adaptation tool, thereby reducing exercise preparation time to the requisite minimum.},
booktitle = {Generative Systems and Intelligent Tutoring Systems: 21st International Conference, ITS 2025, Alexandroupolis, Greece, June 2–6, 2025, Proceedings, Part I},
pages = {116–130},
numpages = {15},
keywords = {Alternative Method of Audio-Tactile Presentation, Blind Students Education, Automated Tool for Exercises Preparation},
location = {Alexandroupolis, Greece}
}

@article{10.5555/3722479.3722531,
author = {Saben, Clark and Zeitz, Jessica and Chandrasekar, Prashant},
title = {Enabling Blind and Low-Vision (BLV) Developers with LLM-Driven Code Debugging},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {3},
issn = {1937-4771},
abstract = {BLVRUN is a command line shell script designed to offer developers within the blind and low-vision (BLV) community a succinct and insightful overview of traceback errors. Its primary function involves parsing errors and utilizing a refined large language model to generate informative error summaries. In terms of performance, our model rivals that of well-known models like ChatGPT or AI-chatbot plug-ins tailored for specific Integrated Development Environments (IDEs). Importantly, BLV users can seamlessly integrate this tool into their existing development workflows, eliminating the need for any modifications or adaptations to facilitate debugging tasks.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {204–215},
numpages = {12}
}

@inproceedings{10.1145/3665348.3665354,
author = {Niu, Lianqiang and Bao, Han},
title = {Fast Tactile Paving Segmentation Model Based on Reparameterized Structure},
year = {2024},
isbn = {9798400709562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665348.3665354},
doi = {10.1145/3665348.3665354},
abstract = {In order to solve the problem of low segmentation accuracy due to the interference of noise information such as light and shadow in the current lightweight blind segmentation model, we propose a fast tactile paving segmentation model based on reparameterize. The multi-branch training structure learns spatial features such as tactile paving, light and shadow while compensating for the loss of information in the feature map due to downsampling operations as well as enhancing the learning of its features in the channel dimension. The single-branch structure obtained after reparameterization of the multibranch structure ensures faster inference of the model as well as lower memory access costs. The introduction of coordinate attention allows the model to learn the distribution law of the features of tactile paving, light and shadow, etc. in the spatial dimension and automatically generate the attention map with weight information, effectively reducing the weight of the spatial structural features of noise such as light and shadow. Experiments show that the model's ability to resist interference from environmental noise such as light and shadow has been enhanced and the segmentation accuracy has been further improved under the premise of ensuring the inference speed.},
booktitle = {Proceedings of the 2024 International Conference on Generative Artificial Intelligence and Information Security},
pages = {24–28},
numpages = {5},
location = {Kuala Lumpur, Malaysia},
series = {GAIIS '24}
}

@article{10.1016/j.jvcir.2025.104461,
author = {Liu, Xuelin and Yan, Jiebin and Fang, Yuming and Hou, Jingwen},
title = {Opinion-unaware blind quality assessment of AI-generated omnidirectional images based on deep feature statistics},
year = {2025},
issue_date = {Jul 2025},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2025.104461},
doi = {10.1016/j.jvcir.2025.104461},
journal = {J. Vis. Comun. Image Represent.},
month = aug,
numpages = {10},
keywords = {Opinion-unaware, AI-generated content, Omnidirectional images, Blind image quality assessment}
}

@article{10.3103/S0005105525700979,
author = {Polilova, T. A.},
title = {Accessible Internet: From the WAI Initiative to Russian Practice},
year = {2025},
issue_date = {Oct 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {59},
number = {Suppl 1},
issn = {0005-1055},
url = {https://doi.org/10.3103/S0005105525700979},
doi = {10.3103/S0005105525700979},
journal = {Autom. Doc. Math. Linguist.},
month = oct,
pages = {S47–S58},
numpages = {12},
keywords = {WAI initiative, WCAG recommendations, GOST R 52872-2019, digital content, accessibility for people with disabilities}
}

@inproceedings{10.1145/3613905.3648641,
author = {Wen, Linda Yilin and Morrison, Cecily and Grayson, Martin and Marques, Rita Faia and Massiceti, Daniela and Longden, Camilla and Cutrell, Edward},
title = {Find My Things: Personalized Accessibility through Teachable AI for People who are Blind or Low Vision},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3648641},
doi = {10.1145/3613905.3648641},
abstract = {The opportunity for artificial intelligence, or AI, to enable accessibility is rapidly growing, but widely impactful applications can be challenging to build given the diversity of user need within and across disability communities. Teachable AI systems give users with disabilities a way to leverage the power of AI to personalize applications for their own specific needs. We demonstrate Find My Things as an end-to-end example of applying Teachable AI systems to address the diversity of accessibility needs. An application that can be taught by people who are blind or low vision to find their personal things, Find My Things illustrates the potential Teachable AI holds for accessibility.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {403},
numpages = {6},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3613904.3642713,
author = {Zhang, Lotus and Stangl, Abigale and Sharma, Tanusree and Tseng, Yu-Yun and Xu, Inan and Gurari, Danna and Wang, Yang and Findlater, Leah},
title = {Designing Accessible Obfuscation Support for Blind Individuals’ Visual Privacy Management},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642713},
doi = {10.1145/3613904.3642713},
abstract = {Blind individuals commonly share photos in everyday life. Despite substantial interest from the blind community in being able to independently obfuscate private information in photos, existing tools are designed without their inputs. In this study, we prototyped a preliminary screen reader-accessible obfuscation interface to probe for feedback and design insights. We implemented a version of the prototype through off-the-shelf AI models (e.g., SAM, BLIP2, ChatGPT) and a Wizard-of-Oz version that provides human-authored guidance. Through a user study with 12 blind participants who obfuscated diverse private photos using the prototype, we uncovered how they understood and approached visual private content manipulation, how they reacted to frictions such as inaccuracy with existing AI models and cognitive load, and how they envisioned such tools to be better designed to support their needs (e.g., guidelines for describing visual obfuscation effects, co-creative interaction design that respects blind users’ agency).},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {235},
numpages = {19},
keywords = {accessibility, blind photography, privacy-preservation technology},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@proceedings{10.1145/3656650,
title = {AVI '24: Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {AVI 2024 is the 17th edition of the International Conference on Advanced Visual Interfaces, held in Arenzano, Genoa (IT), in cooperation with ACM, ACM SIGCHI, ACM SIGMM, and ACM SIGWEB.Every two years since 1992, AVI has gathered a vast international community of experts with a wide range of backgrounds. Throughout three decades, AVI has gained and holds a prestigious position among International HCI conferences, boasting a dedicated nucleus of returning participants, but also providing a venue for young researchers to show their achievements and establish contacts with senior community members.AVI 2024 presents a broad and sound scientific program covering traditional AVI topics on information and data visualization, interaction with multimodal user interfaces, augmented and virtual reality, while also addressing emerging topics including the application of generative artificial intelligence in HCI design and evaluation.The program features the presentation of 21 long research papers and 28 short papers selected through a rigorous double-blind reviewing process and organized into sessions on 13 main topics. Furthermore, it includes the presentation of 48 poster papers, 9 demo papers, and 11 doctoral consortium papers, selected through a single-blind reviewing process. Finally, the rich and vibrant program includes 3 keynote talks, 3 tutorials, and 10 workshops addressing some of the most exciting issues in HCI.Submissions to AVI 2024 came from 34 different countries distributed in descending order in Europe, Asia, North America, South America, and Africa.},
location = {Arenzano, Genoa, Italy}
}

@article{10.1016/j.engappai.2025.110871,
author = {Rahman, Sheikh Shah Mohammad Motiur and Salomon, Michel and Demb\'{e}l\'{e}, Sounkalo},
title = {A novel adaptive noise model selection framework for blind denoising of Scanning Electron Microscopy images},
year = {2025},
issue_date = {Aug 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2025.110871},
doi = {10.1016/j.engappai.2025.110871},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
numpages = {13},
keywords = {Scanning Electron Microscopy, Image denoising, Adaptive blind denoising, Shifted Windows Transformer, Convolutional neural network}
}

@inproceedings{10.1007/978-3-032-05185-1_1,
author = {Akumu, Tanya and Elbatel, Marawan and Campello, Victor M. and Osuala, Richard and Martin-Isla, Carlos and Valenzuela, Ignacio and Li, Xiaomeng and Khanal, Bishesh and Lekadir, Karim},
title = {Adaptive Frame Selection for Gestational Age Estimation from Blind Sweep Fetal Ultrasound Videos},
year = {2025},
isbn = {978-3-032-05184-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-05185-1_1},
doi = {10.1007/978-3-032-05185-1_1},
abstract = {The blind sweep ultrasound protocol, coupled with artificial intelligence (AI), offers promising solutions for expanding ultrasound availability in low-resource settings. However, existing AI approaches for gestational age (GA) prediction using bind sweeps face challenges like reliance on manual segmentation, computational inefficiency from high frame volume, and suboptimal sampling strategies that compromise performance, particularly with smaller datasets. We propose SelectGA, a novel framework for automated blind sweep analysis that enables effective fine-tuning of pretrained models through adaptive frame selection for GA prediction. Our approach identifies the most informative and least redundant frames, enhancing both training efficiency and prediction accuracy. Validated on data collected from ultrasound devices in diverse resource environments, SelectGA improves gestational age prediction accuracy by 27% on mean absolute error metrics. These results demonstrate substantially improved generalizability, establishing foundations for sustainable AI adoption in prenatal care across resource-constrained settings. Code is available at: .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2025: 28th International Conference, Daejeon, South Korea, September 23–27, 2025, Proceedings, Part XIV},
pages = {3–12},
numpages = {10},
keywords = {Gestational Age, Fetal Ultrasound, Blind sweep},
location = {Daejeon, Korea (Republic of)}
}

@article{10.1016/j.jvcir.2024.104094,
author = {Hsu, Ling-Yuan},
title = {AI-assisted deepfake detection using adaptive blind image watermarking},
year = {2024},
issue_date = {Apr 2024},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {100},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2024.104094},
doi = {10.1016/j.jvcir.2024.104094},
journal = {J. Vis. Comun. Image Represent.},
month = apr,
numpages = {19},
keywords = {Deepfake detection, Blind image watermarking, Partly sign-altered, Mixed modulation}
}

@inproceedings{10.1145/3706598.3714329,
author = {Mo, Ye and Huang, Gang and li, liangcheng and Deng, Dazhen and Yu, Zhi and Xu, Yilun and Ye, Kai and Zhou, Sheng and Bu, Jiajun},
title = {TableNarrator: Making Image Tables Accessible to Blind and Low Vision People},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714329},
doi = {10.1145/3706598.3714329},
abstract = {The widespread use of image tables presents significant accessibility challenges for blind and low vision (BLV) people, limiting their access to critical data. Despite advancements in artificial intelligence (AI) for interpreting image tables, current solutions often fail to consider the specific needs of BLV users, leading to a poor user experience. To address these issues, we introduce TableNarrator, an innovative system designed to enhance the accessibility of image tables. Informed by accessibility standards and user feedback, TableNarrator leverages AI to generate alternative text tailored to the cognitive and reading preferences of BLV users. It streamlines access through a simple interaction mode and offers personalized options. Our evaluations, from both technical and user perspectives, demonstrate that TableNarrator not only provides accurate and comprehensive table information but also significantly enhances the user experience for BLV people.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {297},
numpages = {17},
keywords = {Accessibility, Assistive Technology, Screen Reader, Image Tables, Computer Vision, Large Language Model},
location = {
},
series = {CHI '25}
}

@article{10.1016/j.engappai.2024.109730,
author = {Tolie, Hamidreza Farhadi and Ren, Jinchang and Chen, Rongjun and Zhao, Huimin and Elyan, Eyad},
title = {Blind sonar image quality assessment via machine learning: Leveraging micro- and macro-scale texture and contour features in the wavelet domain},
year = {2025},
issue_date = {Feb 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {141},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2024.109730},
doi = {10.1016/j.engappai.2024.109730},
journal = {Eng. Appl. Artif. Intell.},
month = feb,
numpages = {16},
keywords = {Blind image quality assessment, Side scan sonar, Forward-looking sonar, Machine learning, Support vector regression, Feature representation}
}

@article{10.1016/j.artmed.2024.102969,
author = {P\'{e}rez-Bueno, Fernando and Engan, Kjersti and Molina, Rafael},
title = {Robust blind color deconvolution and blood detection on histological images using Bayesian K-SVD},
year = {2024},
issue_date = {Oct 2024},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {156},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2024.102969},
doi = {10.1016/j.artmed.2024.102969},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {10},
keywords = {Stain separation, Blood detection, Histological images}
}

@inproceedings{10.1145/3707640.3731916,
author = {Do, Lana and Mirani, Sanjay and Pitcher-Cooper, Charity and Nguyen, Xuan Duy Anh and Bairaboina, Alekya and Yoon, Ilmi},
title = {YouDescribe: Bridging AI Efficiency and Human Insight for Scalable Audio Description},
year = {2025},
isbn = {9798400713972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707640.3731916},
doi = {10.1145/3707640.3731916},
abstract = {Audio description (AD) is critical for making video content accessible to blind and low-vision (BLV) audiences, yet remains largely unavailable for short-form and user-generated videos. YouDescribe addresses this gap by enabling BLV users to request, rate, and contribute to descriptions, while sighted volunteers create or refine AD through a structured, interactive interface. The platform’s latest version introduces drafts generated using Artificial Intelligence (AI) within a Human-in-the-Loop (HITL) workflow, helping describers accelerate the authoring process while preserving narrative accuracy and contextual nuance. Importantly, its accessible design ensures BLV users are not passive consumers, but active contributors—enhancing the inclusivity and relevance of the content. This paper presents YouDescribe’s innovative design and system architecture, exploring how AI-assisted collaboration enhances the scalability, quality, and inclusivity of audio description.},
booktitle = {Adjunct Proceedings of the 4th Annual Symposium on Human-Computer Interaction for Work},
articleno = {10},
numpages = {7},
keywords = {human-in-the-loop, multimodal AI system, audio description, blind and low-vision audiences},
location = {
},
series = {CHIWORK '25 Adjunct}
}

@inproceedings{10.1145/3749012.3749042,
author = {Chamaidi, Theodora and Charoupia, Helen and Stavrakis, Modestos},
title = {More-than-a-prediction: reframing AI's role in more-than-human design of interactions},
year = {2025},
isbn = {9798400715617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3749012.3749042},
doi = {10.1145/3749012.3749042},
abstract = {Artificial intelligence (AI) is increasingly integrated into design processes and presented as a predictive or generative partner. Within Human-Computer Interaction (HCI), this role can extend into co-design contexts. Yet in more-than-human and multispecies design, using such tools would risk the reinforcement of anthropocentric biases and obscure non-human perspectives. This paper aims to challenge the narratives of AI as a co-design agent and instead propose a reframing of AI as a reflective companion that aids designers by exposing ethical blind spots, checking for human-centred biases, and encouraging the critical reflection of designers. Applying the BACT framework (Beings, Activities, Context, Technologies), initially developed for multispecies design, we suggest how AI systems can be trained to use such structures to enable post-design reflection. In this reframing, BACT is a template for how existing design tools can be used to guide AI towards more inclusive and ethically aware analysis. Instead of providing solutions, AI is a prompt towards a path to more reflective, multispecies-sensitive design practices.},
booktitle = {Proceedings of the 3rd International Conference of the ACM Greek SIGCHI Chapter},
pages = {27–32},
numpages = {6},
keywords = {Artificial Intelligence, Human-Computer Interaction, More-than-Human Design, Multispecies Design, human-AI collaboration},
location = {
},
series = {CHIGreece '25}
}

@article{10.1145/3654768.3654772,
author = {Herskovitz, Jaylin},
title = {DIY Assistive Software: End-User Programming for Personalized Assistive Technology},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
number = {137},
issn = {1558-2337},
url = {https://doi.org/10.1145/3654768.3654772},
doi = {10.1145/3654768.3654772},
abstract = {Existing assistive technologies often fail to support the unique and personalized needs of blind and visually impaired (BVI) people. Thus, BVI people have become domain experts in customizing and 'hacking' assistive technology in order to creatively suit their needs. However, current assistive technologies are difficult to hack and alter at the individual level due to the skill and resources required. Due to these challenges, new approaches to supporting Do-It-Yourself (DIY) technology creation are needed to make applying the DIY concept to high-tech assistive technologies for BVI people more feasible. Through my dissertation, I aim to better support DIY assistive software design and development, so that BVI people can design, create, and customize assistive software for themselves and their communities. To accomplish this, I plan to address three primary research goals. First, through qualitative interviews, I present an understanding of why and how BVI people engage in assistive technology customization and adaptation, and the challenges and opportunities that arise from this practice. Second, I aim to investigate how end-user programming can be applied as an approach to DIY assistive technology creation, by designing, building, and evaluating a prototype end-user programming tool. Finally, I aim to understand and improve the real-world feasibility of end-user programming in this domain, by designing tools to improve the accessibility of debugging visual technologies. By addressing these primary research questions, my planned dissertation contributes to the HCI and accessibility community's understanding of DIY assistive software, accessible end-user programming, and accessible debugging techniques. By supporting BVI in creating and modifying their assistive software, this research aims to promote the democratization of technology creation and support BVI people in having greater control over AI-based technologies in their lives.},
journal = {SIGACCESS Access. Comput.},
month = mar,
articleno = {4},
numpages = {1}
}

@inproceedings{10.1145/3706599.3720150,
author = {Tsutsui, Ayaka and Tanaka, Kengo and Yu, Yaman and Ryskeldiev, Bektur},
title = {"I Am a Blind Seller!": Picture Taking Assistance for Visually Impaired Individuals for Participation as Sellers in Customer to Customer (C2C) Marketplaces},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720150},
doi = {10.1145/3706599.3720150},
abstract = {Consumer-to-consumer (C2C) marketplaces have grown as platforms for buying and selling products, driven by interest in sustainable and cost-effective shopping. However, the reliance on visual communication, especially product photos, presents barriers for visually impaired (BLV) individuals who often struggle to capture high-quality images independently. To identify the challenges of product photography, we conducted semi-structured interviews with N = 6 BLV participants. Based on the findings, we developed a prototype that integrates real-time object detection for audio-assisted composition during photography and interaction with Generative AI (GenAI) after capture. We evaluated the prototype with N = 4 BLV participants, comparing it to conventional photography applications. Our findings highlight the unique demands of photography in C2C marketplace and demonstrate the potential of GenAI tools to empower BLV sellers to participate effectively and independently.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {10},
numpages = {10},
keywords = {Photo Assistance, Accessibility, Generative AI, Design, Blind or low vision},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1007/978-981-99-9785-5_25,
author = {Huang, Rong and Chen, Li and Zheng, Jun and Zhang, Quanxin and Yu, Xiao},
title = {Adversarial Attacks Against Object Detection in&nbsp;Remote Sensing Images},
year = {2023},
isbn = {978-981-99-9784-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-9785-5_25},
doi = {10.1007/978-981-99-9785-5_25},
abstract = {With the continuous development of artificial intelligence technology and the increasing richness of remote sensing data, deep convolutional neural networks(DNNs) have been widely used in the field of remote sensing images. Object detection in remote sensing images has achieved considerable progress due to DNNs. However, DNNs have shown their vulnerability to adversarial attacks. The object detection models in remote sensing images also have this vulnerability. The complexity of remote sensing object detection models makes it difficult to implement adversarial attacks. In this work, we propose an adversarial attack method against the remote sensing object detection model based on the L∞norm, which can make the detector blind–that is, the detector misses a large number of objects in the image. Because some remote sensing images are too large, we also designed a pre-processing method to segment and pre-process the huge images, which is combined with the attack method. Our proposed attack method can effectively perform adversarial attacks on remote sensing object detection models.},
booktitle = {Artificial Intelligence Security and Privacy: First International Conference on Artificial Intelligence Security and Privacy, AIS&amp;P 2023, Guangzhou, China, December 3–5, 2023, Proceedings, Part I},
pages = {358–367},
numpages = {10},
keywords = {Adversarial Attack, Remote Sensing Images, Object Detection models},
location = {Guangzhou, China}
}

@inproceedings{10.1007/978-981-96-0351-0_25,
author = {Xu, Peijie and Song, Andy and Wang, Ke},
title = {Learning Low-Energy Consumption Obstacle Detection Models for&nbsp;the&nbsp;Blind},
year = {2024},
isbn = {978-981-96-0350-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-96-0351-0_25},
doi = {10.1007/978-981-96-0351-0_25},
abstract = {The study aims for low-energy consumption models that can detect obstacles on a head-mounted smart device for the vision impaired. Under this setting, long operational time with limited battery power is obviously important, so small but accurate models would be highly desirable. To achieve this goal, a comprehensive investigation is conducted to establish the most suitable model that can balance accuracy, real-time performance, and energy consumption. The results suggest that a highly efficient wearable obstacle detection solution for the blind is feasible. The established model can deal with the large variations introduced by the person’s natural head turns with a low energy footprint. In addition, our study shows that deep learning models, even tiny ones, are not ideal for this scenario, due to their high computational costs and delays.},
booktitle = {AI 2024: Advances in Artificial Intelligence: 37th Australasian Joint Conference on Artificial Intelligence, AI 2024, Melbourne, VIC, Australia, November 25–29, 2024, Proceedings, Part II},
pages = {335–347},
numpages = {13},
keywords = {Low-energy consumption, Obstacle avoidance, Blind and vision impairments},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3594806.3596542,
author = {Othman, Achraf and Dhouib, Amira and Nasser Al Jabor, Aljazi},
title = {Fostering websites accessibility: A case study on the use of the Large Language Models ChatGPT for automatic remediation},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3596542},
doi = {10.1145/3594806.3596542},
abstract = {The use of automated accessibility testing tools remains a common practice for evaluating web accessibility. However, the results obtained from these tools may not always provide a comprehensive and complete view of a site's accessibility status. The main purpose of this study is to improve web accessibility by automatically remediating non-accessible ones using Large Language Models (LLM), particularly ChatGPT. The effectiveness of the used model in detecting and remediating accessibility issues to ensure compliance with the Web Content Accessibility Guidelines (WCAG 2.1) is also discussed. By using ChatGPT as a remediation tool, this study investigates the potential of LLM in improving web accessibility. In the case study, two websites that did not adhere to the WCAG 2.1 guidelines were selected as the primary experimental subjects for the study. These websites were assessed using the web accessibility evaluation tool, WAVE, to detect accessibility issues. The identified issues served then as the basis for remediation using ChatGPT. The effectiveness of the used advanced language model as a web accessibility remediation tool was evaluated by comparing its findings with those obtained from manual accessibility testing. The results of this comparison have significant implications for stakeholders involved in achieving WCAG compliance and contribute to the development of more accessible online platforms for individuals with disabilities.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {707–713},
numpages = {7},
location = {Corfu, Greece},
series = {PETRA '23}
}

@inproceedings{10.1145/3746058.3758413,
author = {Zhong, Mingyuan and Mallavarapu, Ajit and Nie, Qing},
title = {Caption: Generating Informative Content Labels for Image Buttons Using Next-Screen Context},
year = {2025},
isbn = {9798400720369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746058.3758413},
doi = {10.1145/3746058.3758413},
abstract = {We present Caption, an LLM-powered content label generation tool for visual interactive elements on mobile devices. Content labels are essential for screen readers to provide announcements for image-based elements, but are often missing or uninformative due to developer neglect. Automated captioning systems attempt to address this, but are limited to on-screen context, often resulting in inaccurate or unspecific labels. To generate more accurate and descriptive labels, Caption collects next-screen context on interactive elements by navigating to the destination screen that appears after an interaction and incorporating information from both the origin and destination screens. Preliminary results show Caption generates more accurate labels than both human annotators and an LLM baseline. We expect Caption to empower developers by providing actionable accessibility suggestions and directly support on-demand repairs by screen reader users.},
booktitle = {Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {130},
numpages = {3},
keywords = {Mobile accessibility, content labels, UI understanding, LLMs.},
location = {
},
series = {UIST Adjunct '25}
}

@inproceedings{10.1145/3746058.3758437,
author = {Wei, Yize and Bin Adam, Ibnu Taimiyyah and Wu, Hanjun and Messerschmidt, Moritz Alexander and Ooi, Wei Tsang and Jouffrais, Christophe and Nanayakkara, Suranga},
title = {Towards LLM-powered Drone Companion for Blind and Low Vision Users},
year = {2025},
isbn = {9798400720369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746058.3758437},
doi = {10.1145/3746058.3758437},
abstract = {In this work, we present an initial step towards an LLM-powered assistive drone for Blind and Low Vision (BLV) users. Through a formative study (N=9), we identified envisioned use cases and desired interaction modalities. Then, we took a participatory design approach to build a prototype, incorporating the feedback received from 3 BLV users, as well as 5 domain experts. We found that participants were receptive to the idea of an LLM-powered assistive drone and envisioned using such a system to address unmet needs.},
booktitle = {Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {121},
numpages = {4},
keywords = {Large Language Models, Drones, Accessibility},
location = {
},
series = {UIST Adjunct '25}
}

@inproceedings{10.1007/978-981-99-9119-8_2,
author = {Xie, Haidong and Xiang, Xueshuang and Dong, Bin and Liu, Naijin},
title = {Blind Adversarial Training: Towards Comprehensively Robust Models Against Blind Adversarial Attacks},
year = {2023},
isbn = {978-981-99-9118-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-9119-8_2},
doi = {10.1007/978-981-99-9119-8_2},
abstract = {Adversarial training (AT) aims to improve models’ robustness against adversarial attacks by mixing clean data and adversarial examples (AEs) into training. Most existing AT approaches can be grouped into restricted and unrestricted approaches. Restricted AT requires a prescribed uniform budget for AEs during training, with the obtained results showing high sensitivity to the budget. In contrast, unrestricted AT uses unconstrained AEs, and these overestimated AEs significantly lower the clean accuracy and robustness against small budget attacks. Thus, the existing AT approaches find it difficult to obtain a comprehensively robust model when confronting attacks with an unknown budget, which we name blind adversarial attacks. Considering this problem, this paper proposes a novel AT approach named blind adversarial training (BAT). The main idea is to use a cutoff-scale strategy to adaptively estimate a nonuniform budget to modify the AEs used in training, ensuring that the strengths of the AEs are dynamically located in a reasonable range and ultimately improving the comprehensive robustness of the AT model. We include a theoretical investigation on a toy classification problem to guarantee the improvement of BAT. The experimental results also demonstrate that BAT can achieve better comprehensive robustness than AT with several AEs.},
booktitle = {Artificial Intelligence: Third CAAI International Conference, CICAI 2023, Fuzhou, China, July 22–23, 2023, Revised Selected Papers, Part II},
pages = {15–26},
numpages = {12},
keywords = {Adversarial Training, Comprehensive Robustness, Blind Adversarial Attacks},
location = {Fuzhou, China}
}

@inproceedings{10.1145/3597638.3608395,
author = {Morrison, Cecily and Grayson, Martin and Marques, Rita Faia and Massiceti, Daniela and Longden, Camilla and Wen, Linda and Cutrell, Edward},
title = {Understanding Personalized Accessibility through Teachable AI: Designing and Evaluating Find My Things for People who are Blind or Low Vision},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608395},
doi = {10.1145/3597638.3608395},
abstract = {The opportunity for artificial intelligence, or AI, to enable accessibility is rapidly growing, but widely impactful applications can be challenging to build given the diversity of user need within and across disability communities. Teachable AI systems give users with disabilities a way to leverage the power of AI to personalize applications for their own specific needs, as long as the effort of providing examples is balanced with the benefit of the personalization received. As an example, this paper presents the design and evaluation of Find My Things, an end-to-end application that can be taught by people who are blind or low vision to find their personal things. Through synthesis of the design process, this paper offers design considerations for the teaching loop that is so critical to realizing the power of teachable AI for accessibility.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {31},
numpages = {12},
keywords = {Accessibility, Artificial Intelligence, Teachable AI},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1007/978-3-031-60881-0_18,
author = {Bekkvik Szentirmai, Attila},
title = {Enhancing Accessible Reading for All with Universally Designed Augmented Reality – AReader: From Audio Narration to Talking AI Avatars},
year = {2024},
isbn = {978-3-031-60880-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-60881-0_18},
doi = {10.1007/978-3-031-60881-0_18},
abstract = {This paper introduces AReader, an innovative approach that combines Universal Design (UD) and Augmented Reality (AR) to enhance the universal accessibility of paper-based textual content. It targets a broad audience, including individuals with print and situational disabilities, emphasizing that reading challenges can affect anyone. The paper details the practical implementation of UD in AR, focusing on AR's context-aware, multimodal capabilities, and highlights a novel Visual Screen Reader feature. This feature is presented in two concepts: Text-to-Audio Narration with Visual Reading Guide and Text-to-AI-Video Narration. The paper also outlines a set of features and hands-on UD practices adaptable to a wide variety of AR applications. User testing has demonstrated that the prototype effectively enhances reading accessibility and usability across various user groups. The findings suggest that integrating UD principles with AR technologies can foster inclusive solutions, benefiting not only individuals with special needs but also contributing to broader societal inclusion.},
booktitle = {Universal Access in Human-Computer Interaction: 18th International Conference, UAHCI 2024, Held as Part of the 26th HCI International Conference, HCII 2024, Washington, DC, USA, June 29 – July 4, 2024, Proceedings, Part II},
pages = {282–300},
numpages = {19},
keywords = {Universal Design, Augmented Reality, Assistive Technology, Text-to-Speech, Inclusive Technology, Digital Accessibility},
location = {Washington DC, USA}
}

@inproceedings{10.1609/aaai.v39i5.32506,
author = {Li, Junyi and Zhang, Zhilu and Zuo, Wangmeng},
title = {Rethinking transformer-based blind-spot network for self-supervised image denoising},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i5.32506},
doi = {10.1609/aaai.v39i5.32506},
abstract = {Blind-spot networks (BSN) have been prevalent neural architectures in self-supervised image denoising (SSID). However, most existing BSNs are conducted with convolution layers. Although transformers have shown the potential to overcome the limitations of convolutions in many image restoration tasks, the attention mechanisms may violate the blind-spot requirement, thereby restricting their applicability in BSN. To this end, we propose to analyze and redesign the channel and spatial attentions to meet the blind-spot requirement. Specifically, channel self-attention may leak the blind-spot information in multi-scale architectures, since the downsampling shuffles the spatial feature into channel dimensions. To alleviate this problem, we divide the channel into several groups and perform channel attention separately. For spatial self-attention, we apply an elaborate mask to the attention matrix to restrict and mimic the receptive field of dilated convolution. Based on the redesigned channel and window attentions, we build a Transformer-based Blind-Spot Network (TBSN), which shows strong local fitting and global perspective abilities. Furthermore, we introduce a knowledge distillation strategy that distills TBSN into smaller denoisers to improve computational efficiency while maintaining performance. Extensive experiments on real-world image denoising datasets show that TBSN largely extends the receptive field and exhibits favorable performance against state-of-the-art SSID methods. Code — https://github.com/nagejacob/TBSN},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {533},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1145/3729605.3729646,
author = {Gu, Xiu and Yuan, Tianyi},
title = {An Empirical Study on AI-Driven Text Mining for Graduate Thesis Quality Management in Higher Education},
year = {2025},
isbn = {9798400714405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3729605.3729646},
doi = {10.1145/3729605.3729646},
abstract = {In recent years, artificial intelligence(AI) technology has developed rapidly and is widely used in various educational scenarios and administrative management in the field of higher education. This paper takes Text-Mining process of blind review comments of master's theses from a university as an example to explore the application effectiveness of AI technology in the quality management of master's thesis. Through empirical research, it provides a case reference for the application of AI in higher education management and a replicable integration program for similar institutions. This study used a three- phase approach to analyse blind review comments, namely independent use of AI (ChatGPT), independent use of traditional text analysis tools (NVIVO), and collaborative analysis combining the two. It was found that despite the significant advantages of AI in terms of processing speed and scale, the AI's output highly depends on the user's prompts and pre-training. Moreover, human intervention was critical to the stability and accuracy of AI's output.},
booktitle = {Proceedings of the 2025 International Conference on Big Data and Informatization Education},
pages = {232–236},
numpages = {5},
keywords = {Artificial intelligence(AI), Blind review comments, Higher education, Thesis quality management},
location = {
},
series = {ICBDIE '25}
}

@inproceedings{10.1609/aaai.v39i5.32530,
author = {Li, Xudong and Zhang, Yan and Shen, Yunhang and Li, Ke and Hu, Runze and Zheng, Xiawu and Zhao, Sicheng},
title = {Feature denoising diffusion model for blind image quality assessment},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i5.32530},
doi = {10.1609/aaai.v39i5.32530},
abstract = {Blind Image Quality Assessment (BIQA) aims to evaluate image quality in line with human perception, without reference benchmarks. Currently, deep learning BIQA methods typically depend on using features from high-level tasks for transfer learning. However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features. In this paper, we take an initial step toward exploring the diffusion model for feature denoising in BIQA, namely Perceptual Feature Diffusion for IQA (PFD-IQA), which aims to remove noise from quality-aware features. Specifically, 1) we propose a Perceptual Prior Discovery and Aggregation module to establish two auxiliary tasks to discover potential low-level features in images that are used to aggregate perceptual textual prompt conditions for the diffusion model. 2) we propose a Perceptual Conditional Feature Refinement strategy, which matches noisy features to predefined denoising trajectories and then performs exact feature denoising based on textual prompt conditions. By incorporating a lightweight denoiser and requiring only a few feature denoising steps (e.g., just five iterations), our PFD-IQA framework achieves superior performance across eight standard BIQA datasets, validating its effectiveness.},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {557},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1145/3701716.3715565,
author = {Pati, Swostik and Zaki, Yasir},
title = {Evaluating the Efficacy of Next.js: A Comparative Analysis with React.js on Performance, SEO, and Global Network Equity},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715565},
doi = {10.1145/3701716.3715565},
abstract = {This paper investigates the efficacy of Next.js as a framework addressing the challenges posed by React.js, particularly in performance, SEO, and equitable web accessibility. By constructing identical websites and web applications in both frameworks, we aim to evaluate the frameworks' behavior under diverse network conditions and capabilities. Beyond quantitative metrics like First Contentful Paint (FCP) and Time to Interactive (TTI), we incorporate qualitative user feedback to assess real-world usability. Our motivation stems from bridging the digital divide exacerbated by client-side rendering (CSR) frameworks and validating investments in modern technologies for businesses and institutions. Employing a novel LLM-assisted migration workflow, this paper also demonstrates the ease with which developers can transition from React.js to Next.js. Our results highlight Next.js's promise of better overall performance, without any degradation in user interaction experience, showcasing its potential to mitigate disparities in web accessibility and foster global network equity, thereby emerging as a compelling framework for shaping an inclusive web.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1239–1243},
numpages = {5},
keywords = {client/server side rendering, next.js, react.js, seo optimization},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1609/aaai.v38i20.30200,
author = {Choi, Hyunmin and Woo, Simon S. and Kim, Hyoungshick},
title = {Blind-Touch: homomorphic encryption-based distributed neural network inference for privacy-preserving fingerprint authentication},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i20.30200},
doi = {10.1609/aaai.v38i20.30200},
abstract = {Fingerprint authentication is a popular security mechanism for smartphones and laptops. However, its adoption in web and cloud environments has been limited due to privacy concerns over storing and processing biometric data on servers. This paper introduces Blind-Touch, a novel machine learning-based fingerprint authentication system leveraging homomorphic encryption to address these privacy concerns. Homomorphic encryption allows computations on encrypted data without decrypting. Thus, Blind-Touch can keep fingerprint data encrypted on the server while performing machine learning operations. Blind-Touch combines three strategies to efficiently utilize homomorphic encryption in machine learning: (1) It optimizes the feature vector for a distributed architecture, processing the first fully connected layer (FC-16) in plaintext on the client side and the subsequent layer (FC-1) post-encryption on the server, thereby minimizing encrypted computations; (2) It employs a homomorphic encryption-compatible data compression technique capable of handling 8,192 authentication results concurrently; and (3) It utilizes a clustered server architecture to simultaneously process authentication results, thereby enhancing scalability with increasing user numbers. Blind-Touch achieves high accuracy on two benchmark fingerprint datasets, with a 93.6% F1-score for the PolyU dataset and a 98.2% F1-score for the SOKOTO dataset. Moreover, Blind-Touch can match a fingerprint among 5,000 in about 0.65 seconds. With its privacy-focused design, high accuracy, and efficiency, Blind-Touch is a promising alternative to conventional fingerprint authentication for web and cloud applications.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2452},
numpages = {10},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.1609/aaai.v39i2.32216,
author = {Chen, Sen and Liu, Hongying and Fang, Chaowei and Shang, Fanhua and Liu, Yuanyuan and Wan, Liang and Jiang, Dongmei and Wang, Yaowei},
title = {Unsupervised degradation representation aware transform for real-world blind image super-resolution},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i2.32216},
doi = {10.1609/aaai.v39i2.32216},
abstract = {Blind image super-resolution (blind SR) aims to restore a high-resolution (HR) image from a low-resolution (LR) image with unknown degradation. Many existing methods explicitly estimate degradation information from various LR images. However, in most cases, image degradations are independent of image content. Their estimations may be influenced by the image content resulting in inaccuracy. Unlike existing works, we design a dual-encoder for degradation representation (DEDR) to preclude the influence of image content from LR images. This benefits in extracting the intrinsic degradation representation more accurately. To the best of our knowledge, this paper is the first work that estimates degradation representation through filtering out image content. Based on the degradation representation extracted by DEDR, we present a novel framework, named degradation representation aware transform network (DRAT) for blind SR. We propose global degradation aware (GDA) blocks to propagate degradation information across spatial and channel dimensions, in which a degradation representation transform module (DRT) is introduced to render features degradation-aware, thereby enhancing the restoration of LR images. Extensive experiments are conducted on three benchmark datasets (including Gaussian 8, DIV2KRK, and real-world datasets) under large scaling factors with complex degradations. The experimental results demonstrate that DRAT surpasses state-of-the-art supervised kernel estimation and unsupervised degradation representation methods. Code — https://github.com/KKKc3231/DRAT},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {243},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1609/aaai.v39i3.32242,
author = {Chen, Zikang and Jiang, Tao and Hu, Xiaowan and Zhang, Wang and Li, Huaqiu and Wang, Haoqian},
title = {Spatiotemporal blind-spot network with calibrated flow alignment for self-supervised video denoising},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i3.32242},
doi = {10.1609/aaai.v39i3.32242},
abstract = {Self-supervised video denoising aims to remove noise from videos without relying on ground truth data, leveraging the video itself to recover clean frames. Existing methods often rely on simplistic feature stacking or apply optical flow without thorough analysis. This results in suboptimal utilization of both inter-frame and intra-frame information, and it also neglects the potential of optical flow alignment under self-supervised conditions, leading to biased and insufficient de-noising outcomes. To this end, we first explore the practicality of optical flow in the self-supervised setting and introduce a SpatioTemporal Blind-spot Network (STBN) for global frame feature utilization. In the temporal domain, we utilize bidirectional blind-spot feature propagation through the proposed blind-spot alignment block to ensure accurate temporal alignment and effectively capture long-range dependencies. In the spatial domain, we introduce the spatial receptive field expansion module, which enhances the receptive field and improves global perception capabilities. Additionally, to reduce the sensitivity of optical flow estimation to noise, we propose an unsupervised optical flow distillation mechanism that refines fine-grained inter-frame interactions during optical flow alignment. Our method demonstrates superior performance across both synthetic and real-world video denoising datasets. Code — https://github.com/ZKCCZ/STBN},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {269},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1609/aaai.v39i8.32944,
author = {Xie, Lianxin and Zheng, Bingbing and Xue, Wen and Zhang, Yunfei and Jiang, Le and Xu, Ruotao and Wu, Si and Wong, Hau-San},
title = {Discrete prior-based temporal-coherent content prediction for blind face video restoration},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i8.32944},
doi = {10.1609/aaai.v39i8.32944},
abstract = {Blind face video restoration aims to restore high-fidelity details from videos subjected to complex and unknown degradations. This task poses a significant challenge of managing temporal heterogeneity while at the same time maintaining stable face attributes. In this paper, we introduce a Discrete Prior-based Temporal-Coherent content prediction transformer to address the challenge, and our model is referred to as DP-TempCoh. Specifically, we incorporate a spatial-temporal-aware content prediction module to synthesize high-quality content from discrete visual priors, conditioned on degraded video tokens. To further enhance the temporal coherence of the predicted content, a motion statistics modulation module is designed to adjust the content, based on discrete motion priors in terms of cross-frame mean and variance. As a result, the statistics of the predicted content can match with that of real videos over time. By performing extensive experiments, we verify the effectiveness of the design elements and demonstrate the superior performance of our DP-TempCoh in both synthetically and naturally degraded video restoration.},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {971},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1609/aaai.v39i21.34425,
author = {Zhang, Yudong and Xie, Ruobing and Chen, Jiansheng and Sun, Xingwu and Kang, Zhanhui and Wang, Yu},
title = {Enhancing contrastive learning inspired by the philosophy of "the blind men and the elephant"},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i21.34425},
doi = {10.1609/aaai.v39i21.34425},
abstract = {Contrastive learning is a prevalent technique in self-supervised vision representation learning, typically generating positive pairs by applying two data augmentations to the same image. Designing effective data augmentation strategies is crucial for the success of contrastive learning. Inspired by the story of the blind men and the elephant, we introduce JointCrop and JointBlur. These methods generate more challenging positive pairs by leveraging the joint distribution of the two augmentation parameters, thereby enabling contrastive learning to acquire more effective feature representations. To the best of our knowledge, this is the first effort to explicitly incorporate the joint distribution of two data augmentation parameters into contrastive learning. As a plug-and-play framework without additional computational overhead, JointCrop and JointBlur enhance the performance of SimCLR, BYOL, MoCo vl, MoCo v2, MoCo v3, SimSiam, and Dino baselines with notable improvements. Code — https://github.com/btzyd/JointCrop, Extended version — http://arxiv.org/abs/2412.16522},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2527},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1145/3698205.3729553,
author = {Ye, Lilian and Savi, Alexander O. and Hofman, Abe D.},
title = {Assessing Reliability in AI-Powered Learning Systems with A/A Tests},
year = {2025},
isbn = {9798400712913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698205.3729553},
doi = {10.1145/3698205.3729553},
abstract = {The rapid evolution of Artificial Intelligence (AI) has expanded access to large-scale online adaptive learning systems. Such AI-powered systems strive to deliver personalized learning experiences, often by means of advanced algorithms that continuously model learner behavior. Ensuring the reliability of these systems is fundamental, otherwise their ability to optimize individual learning paths and inform decision-making is undermined. In order to trust such systems, learners with identical profiles should follow highly similar learning trajectories. But how can we evaluate the reliability of these dynamic learning environments, especially in systems that are continuously developed and updated? This paper demonstrates the effectiveness of A/A testing --- large-scale double-blind experiments with identical conditions --- in systematically evaluating the reliability of AI-powered learning environments. We illustrate this by assessing the reliability of student model parameters in a large-scale online arithmetic learning platform that is driven by a well-studied and powerful explainable AI algorithm. We duplicated the item bank of a newly developed game and randomly assigned 50% of the players to one of two identical versions, which were launched simultaneously in the live environment. We then analyzed the reliability of item difficulty convergence, the stability of student ability estimates in the new game, and their relationship to ability estimates from other arithmetic games, as well as patterns in student errors. Our results indicate that the student model parameters are stable across the two variants, highlighting A/A testing as a valuable tool for assessing the reliability of large-scale AI-powered learning systems. We discuss its advantages and suggest future directions for adapting the approach, while considering its relevance in dynamic learning environments.},
booktitle = {Proceedings of the Twelfth ACM Conference on Learning @ Scale},
pages = {13–23},
numpages = {11},
keywords = {a/a testing, adaptive learning, convergence, reliability assessment, system monitoring},
location = {Palermo, Italy},
series = {L@S '25}
}

@inproceedings{10.1145/3717511.3747078,
author = {Stark, Maja and Sch\"{u}rrer, Dagmar and Knaut, Andrea},
title = {Who Am I Talking To? Critical Reflections on Socially Interactive Agents in Current Media Art},
year = {2025},
isbn = {9798400715082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3717511.3747078},
doi = {10.1145/3717511.3747078},
abstract = {With their aesthetic, multi-sensory, and often radical approaches, artists are predestined to provide impulses for critical reflection and the necessary social discourse of Socially Interactive Agents (SIAs). For this paper, five outstanding works from female media artists were selected, most of them discussing these technological entities within a Feminist Artificial Intelligence (FAI) framework that addresses biases and inequities triggered, adopted or reinforced by AI. Drawing on the method of qualitative comparison that is well-established in most humanities disciplines, the authors approach these artworks from the complementary perspectives of art studies and computer science. Building upon descriptions of the artworks, similarities and differences will be outlined and interpreted to reveal what new perspectives these artists bring into the social discourse of SIAs. The comparison is structured along three topics: the digital implementation of the artworks, the confrontation of user expectations towards AI, and the reflection of human-machine relation. This analysis will lead to a discussion of the findings in the context of current research and of potential blind spots in science and society as identified by the artists. The paper will conclude with a contextualization of the artists’ practices within the FAI framework, a differentiation from the Responsible Artificial Intelligence approach, and learnings for future SIA development.},
booktitle = {Proceedings of the 25th ACM International Conference on Intelligent Virtual Agents},
articleno = {20},
numpages = {10},
keywords = {Critical Reflection, Feminist Artificial Intelligence, Media Art, Socially Interactive Agents, Virtual Agents},
location = {
},
series = {IVA '25}
}

@article{10.1016/j.chb.2024.108352,
author = {Klingbeil, Artur and Gr\"{u}tzner, Cassandra and Schreck, Philipp},
title = {Trust and reliance on AI — An experimental study on the extent and costs of overreliance on AI},
year = {2024},
issue_date = {Nov 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {160},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2024.108352},
doi = {10.1016/j.chb.2024.108352},
journal = {Comput. Hum. Behav.},
month = nov,
numpages = {10},
keywords = {Human-computer interaction, Behavioral experiment, Reliance behavior, Trust attitude, Overreliance, Algorithm appreciation}
}

@article{10.1007/s00530-024-01259-2,
author = {Wan, Guangquan and Lian, Guanghui and Yao, Lan},
title = {Reducing blind spots in esophagogastroduodenoscopy examinations using a novel deep learning model},
year = {2024},
issue_date = {Apr 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {2},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-024-01259-2},
doi = {10.1007/s00530-024-01259-2},
abstract = {The intricate architecture of gastric anatomy coupled with the complexities inherent in esophagogastroduodenoscopy (EGD) procedures can lead to blind spots during examinations. These blind spots refer to anatomical locations not visualized during EGD examinations, potentially impacting timely diagnoses and treatments, and exacerbating patient conditions. Therefore, developing artificial intelligence (AI) for monitoring and reducing blind spots in EGD examinations is crucial. This study introduces MMCNet, a novel deep-learning model for classifying anatomical locations in EGD images. The model-based AI system can promptly alert the physician when it fails to recognize all anatomical locations in real-time EGD examinations, thereby enabling the monitoring and reduction of blind spots. To validate its efficacy, comprehensive experimental assessments compared MMCNet with established deep learning models. The results confirm MMCNet’s high accuracy rate of 97.25% in recognizing anatomical locations in EGD images. Moreover, its notably compact memory size of 4.16M contributes to reduced memory requirements. With its accuracy and small model size, the model demonstrates significant potential as an effective tool for computer-assisted blind spot detection. Additionally, this study presents a comprehensive workflow for applying deep learning models to address practical issues, which can be easily adapted for similar tasks.},
journal = {Multimedia Syst.},
month = mar,
numpages = {14},
keywords = {Artificial intelligence, Deep learning, Medical image processing, Esophagogastroduodenoscopy}
}

@inproceedings{10.1609/aaai.v38i7.28499,
author = {Yin, Ziyi and Ye, Muchao and Zhang, Tianrong and Wang, Jiaqi and Liu, Han and Chen, Jinghui and Wang, Ting and Ma, Fenglong},
title = {VQAttack: transferable adversarial attacks on visual question answering via pre-trained models},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i7.28499},
doi = {10.1609/aaai.v38i7.28499},
abstract = {Visual Question Answering (VQA) is a fundamental task in computer vision and natural language process fields. Although the "pre-training &amp; finetuning" learning paradigm significantly improves the VQA performance, the adversarial robustness of such a learning paradigm has not been explored. In this paper, we delve into a new problem: using a pre-trained multimodal source model to create adversarial image-text pairs and then transferring them to attack the target VQA models. Correspondingly, we propose a novel VQATTACK model, which can iteratively generate both image and text perturbations with the designed modules: the large language model (LLM)-enhanced image attack and the cross-modal joint attack module. At each iteration, the LLM-enhanced image attack module first optimizes the latent representation-based loss to generate feature-level image perturbations. Then it incorporates an LLM to further enhance the image perturbations by optimizing the designed masked answer anti-recovery loss. The cross-modal joint attack module will be triggered at a specific iteration, which updates the image and text perturbations sequentially. Notably, the text perturbation updates are based on both the learned gradients in the word embedding space and word synonym-based substitution. Experimental results on two VQA datasets with five validated models demonstrate the effectiveness of the proposed VQATTACK in the transferable attack setting, compared with state-of-the-art baselines. This work reveals a significant blind spot in the "pre-training &amp; fine-tuning" paradigm on VQA tasks. The source code can be found in the link https://github.com/ericyinyzy/VQAttack.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {751},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@article{10.1016/j.engappai.2023.105838,
author = {Langarica, Sa\'{u}l and N\'{u}\~{n}ez, Felipe},
title = {Contrastive blind denoising autoencoder for real time denoising of industrial IoT sensor data},
year = {2023},
issue_date = {Apr 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2023.105838},
doi = {10.1016/j.engappai.2023.105838},
journal = {Eng. Appl. Artif. Intell.},
month = apr,
numpages = {11},
keywords = {Cyber–physical systems, Autoencoders, Noise contrastive estimation, Self-supervised learning}
}

@inproceedings{10.1145/3744367.3744388,
author = {Cao, Yajing and Wu, Yafen},
title = {Dynamic Generation Model of AI-Personalized Learning Paths Based on Educational Psychology},
year = {2025},
isbn = {9798400715068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744367.3744388},
doi = {10.1145/3744367.3744388},
abstract = {As  the global educational digitization moving forward to the stage of “intense personalization”, the prevalent AI learning systems that depend on “behavior - related data + knowledge networks” are confronted with issues of “data partiality” and “psychological blind areas”. This research puts forward a dynamic mapping model of “psychological traits - learning trajectories”, which demolishes the dualism between “data - propelled” and “theory - impelled” methods through the construction of an AI framework with educational psychology at its core. It utilizes streamlined multimodal data amalgamation to carry out real - time psychological state appraisals and spurs on the ethical progress of technology by quantifying the human - oriented degree of AI systems. The study incorporates incremental prompting techniques, tiered designs, and a blend of expert reviews and case simulations for verification. Even though the model meets practical difficulties, it propels the scientific and human - friendly development of educational technology. Suggestions encompass the short - term development of compatible add - ons and cross - cultural trials, as well as the long - term integration of EEG technology and the setting up of shared data repositories.},
booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Educational Systems},
pages = {124–130},
numpages = {7},
keywords = {AI personalized learning, dynamic path generation, educational psychology, multimodal data fusion},
location = {
},
series = {ICAIES '25}
}

@inproceedings{10.1145/3715275.3732162,
author = {Alam, Sabriya Maryam and Abdulhai, Marwa and Salehi, Niloufar},
title = {Blind Faith? User Preference and Expert Assessment of AI-Generated Religious Content},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732162},
doi = {10.1145/3715275.3732162},
abstract = {The increasing use of AI tools like ChatGPT for religious information-seeking among Muslims raises critical questions about the intersection of technology, faith, and expertise. This mixed-methods study investigates user and expert evaluations of AI-generated religious content, through both quantitative and qualitative analysis of user preferences and expert feedback elicited through interviews, surveys, and expert consultations. Our findings reveal a significant disconnect: despite expressing distrust in AI for religious guidance and stating a preference for scholarly answers, Muslim users overwhelmingly preferred AI-generated responses to Islamic questions in blind evaluations, favoring them in 81.3% of cases. Quantitative analysis showed significant differences in ratings, including along dimensions of perceived accuracy, usefulness, completeness, and clarity. AI answers were also rated more favorably when the source was unknown, compared to when it was known. This positive sentiment persisted with source awareness, when users asked AI their own religious questions and evaluated the responses as high quality. However, expert evaluations revealed critical deficiencies in AI responses, highlighting users’ limited ability to recognize such shortcomings. These findings suggest the need to move beyond purely user-centric approaches to AI alignment in sensitive domains, prioritizing both user and expert values in future LLM development.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2451–2479},
numpages = {29},
keywords = {Human-Computer Interaction, AI Alignment, Large Language Models, User Preferences, Religious Information-Seeking},
location = {
},
series = {FAccT '25}
}

@article{10.1016/j.engappai.2025.110981,
author = {Yan, Qiuhai and Jiang, Aiwen and Chen, Kang and Peng, Long and Yi, Qiaosi and Zhang, Chunjie},
title = {Textual prompt guided image restoration},
year = {2025},
issue_date = {Sep 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2025.110981},
doi = {10.1016/j.engappai.2025.110981},
journal = {Eng. Appl. Artif. Intell.},
month = aug,
numpages = {12},
keywords = {Textual prompt, Image restoration, Low-level vision, Multi-modal, Human-in-the-loop}
}

@inproceedings{10.1007/978-3-031-87657-8_16,
author = {Arora, Aayush and Jena, Junali Jasmine and Chowdhury, Pradipto and Gourisaria, Mahendra Kumar and Parui, Sricheta},
title = {Early Detection of Diabetic Retinopathy: An Explainable AI Approach Using DR-NET},
year = {2025},
isbn = {978-3-031-87656-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-87657-8_16},
doi = {10.1007/978-3-031-87657-8_16},
abstract = {Diabetic Retinopathy (DR) is one of the most common complications of diabetes and a major cause of blindness in the world. Screening is very important in the management and prevention of the diseases from getting worse. This paper aims at evaluating the effectiveness of CNN models including VGG16, VGG19, ResNet50, ResNet101, MobileNet, InceptionResNetV2, and DenseNet for the identification of Diabetic Retinopathy from retinal images. For the purpose of improving model interpretability, Explainable Artificial Intelligence (XAI) methods are applied, underlining the need to explain the decision-making of deep learning models. Feature importance analysis is performed in order to determine which of the retinal features have the most impact on the model. This work does not only give an insight into the biomarkers of Diabetic Retinopathy, but also helps clinicians in decision making for diagnosis and management of the condition. The combination of deep learning with Explainable AI (XAI) methods meets one of the most important requirements for AI in healthcare, namely, the ability of clinicians to understand the models they employ. Out of all the models, the custom built Deep CNN model named “DR NET V2” which was built using the Fast AI framework had the highest accuracy of 84.29%. This result has shown that the proposed DR NET V2 model is capable of detecting Diabetic Retinopathy with high accuracy, thus its applicability in real world scenarios. This work adds to the current research on creating accurate and explainable models for early identification and monitoring of Diabetic Retinopathy.},
booktitle = {Pattern Recognition. ICPR 2024 International Workshops and Challenges: Kolkata, India, December 1, 2024, Proceedings, Part I},
pages = {225–239},
numpages = {15},
keywords = {Diabetic Retinopathy (DR), Deep Learning, Convolutional Neural Networks (CNN), Explainable Artificial Intelligence (XAI), Retinal Image Analysis},
location = {Kolkata, India}
}

@inproceedings{10.1609/aaai.v38i5.28220,
author = {Min, Zijian and Hassan, Gundu Mohamed and Jo, Geun-Sik},
title = {Robust blind text image deblurring via maximum consensus framework},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i5.28220},
doi = {10.1609/aaai.v38i5.28220},
abstract = {The blind text image deblurring problem presents a formidable challenge, requiring the recovery of a clean and sharp text image from a blurry version with an unknown blur kernel. Sparsity-based strategies have demonstrated their efficacy by emphasizing the sparse priors of the latent image and kernel. However, these existing strategies have largely neglected the influence of additional noise, imposing limitations on their performance. To overcome this limitation, we propose a novel framework designed to effectively mitigate the impact of extensive noise prevalent in blurred images. Our approach centers around a robust Maximum Consensus Framework, wherein we optimize the quantity of interest from the noisy blurry image based on the maximum consensus criterion. Furthermore, we propose the integration of the Alternating Direction Method of Multipliers (ADMM) and the Half-Quadratic Splitting (HQS) method to effectively address the computationally intractable ℓ0 norm problem. This innovative strategy enables improvements in the deblurring performance of blurry text images with the additional synthetic noise. Experimental evaluations conducted on various noisy blurry text images demonstrate the superiority of the proposed approach over existing methods.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {472},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.1609/aaai.v38i2.27889,
author = {Chen, Zhengrui and Lu, Liying and Yuan, Ziyang and Zhu, Yiming and Li, Yu and Yuan, Chun and Deng, Weihong},
title = {Blind face restoration under extreme conditions: leveraging 3D-2D prior fusion for superior structural and texture recovery},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i2.27889},
doi = {10.1609/aaai.v38i2.27889},
abstract = {Blind face restoration under extreme conditions entails reconstructing high-quality facial images from severely degraded inputs, often characterized by poor quality and extreme facial poses. These challenges lead to errors in facial structure and unnatural artifacts in restored images. This paper demonstrates the efficacy of leveraging 3D priors to address structural deficiencies in 2D priors while preserving texture details. We introduce FREx (Face Restoration under Extreme conditions), which integrates structure-accurate 3D priors and texture-rich 2D priors into pretrained generative networks. To fuse information from 3D and 2D priors, we propose an adaptive weight module that adjusts feature importance based on input image conditions. This approach enables our model to restore accurate facial structures and natural appearances, even when images suffer significant information loss due to degradation and extreme poses. Extensive experiments on synthetic and real-world datasets validate the effectiveness of our approach.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {141},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@article{10.1016/j.cosrev.2023.100555,
author = {Singh, Mandhatya and Kanroo, Muhammad Suhaib and Kawoosa, Hadia Showkat and Goyal, Puneet},
title = {Towards accessible chart visualizations for the non-visuals: Research, applications and gaps},
year = {2023},
issue_date = {May 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2023.100555},
doi = {10.1016/j.cosrev.2023.100555},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {21},
keywords = {Accessibility, Chart visualization, Chart understanding, Blind, Visually impaired, Document intelligence}
}

@article{10.1016/j.engappai.2022.105092,
author = {Yin, Pengfeng and Liu, Zhonghua and Wu, Di and Huo, Hua and Wang, Haijun and Zhang, Kaibing},
title = {Unsupervised simple Siamese representation learning for blind super-resolution},
year = {2022},
issue_date = {Sep 2022},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {114},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2022.105092},
doi = {10.1016/j.engappai.2022.105092},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
numpages = {10},
keywords = {Image super-resolution, Blind super-resolution, Siamese network, DRAN}
}

@article{10.1016/j.eswa.2022.117134,
author = {Hsu, Ling-Yuan and Hu, Hwai-Tsu and Chou, Hsien-Hsin},
title = {A high-capacity QRD-based blind color image watermarking algorithm incorporated with AI technologies},
year = {2022},
issue_date = {Aug 2022},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {199},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2022.117134},
doi = {10.1016/j.eswa.2022.117134},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {15},
keywords = {Blind image watermarking, QR decomposition, Particle swarm optimization, Super-resolution convolutional neural network, Watermark retrieval assurance}
}

@article{10.1002/aaai.12128,
author = {Lee, Minsu and Heo, Yu‐Jung and Choi, Seongho and Choi, Woo Suk and Zhang, Byoung‐Tak},
title = {Video Turing Test: A first step towards human‐level AI},
year = {2023},
issue_date = {Winter 2023},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {44},
number = {4},
issn = {0738-4602},
url = {https://doi.org/10.1002/aaai.12128},
doi = {10.1002/aaai.12128},
abstract = {The development of artificial intelligence (AI) agents capable of human‐level understanding of video content and conducting conversations with humans on this basis is a promising application that people expect. However, this is a challenging task that requires the holistic integration of multimodal information with temporal dependencies and reasoning, as well as social and physical commonsense. In addition, the development of appropriate systematic evaluation methods is essential. In this context, we introduce the Video Turing Test (VTT), a blind test used to evaluate human‐likeness in terms of video comprehension ability. Moreover, we propose Vincent as a video understanding AI. We explain the configuration of VTT, the architecture of Vincent to prepare for VTT and the proposed evaluation methods for video comprehension. We also estimate the current intelligence level of AI based on our results and discuss future research directions.},
journal = {AI Mag.},
month = dec,
pages = {537–554},
numpages = {18}
}

@inproceedings{10.1007/978-3-031-42682-7_28,
author = {Theophilou, Emily and Lomonaco, Francesco and Donabauer, Gregor and Ognibene, Dimitri and S\'{a}nchez-Reina, Roberto J. and Hern\`{a}ndez-Leo, Davinia},
title = {AI and&nbsp;Narrative Scripts to&nbsp;Educate Adolescents About Social Media Algorithms: Insights About AI Overdependence, Trust and&nbsp;Awareness},
year = {2023},
isbn = {978-3-031-42681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-42682-7_28},
doi = {10.1007/978-3-031-42682-7_28},
abstract = {Social Media Artificial Intelligence algorithms provide users with engaging and personalized content. Yet, the personalization of algorithms may have a negative impact on users who lack AI literacy. The limited understanding of SM algorithms among the population suggest that adolescents are more likely to place blind trust in the information they consume, exposing them to negative consequences (misinformation, filter bubbles and echo chambers). We therefore propose an intervention with a narrative scripts approach to raise awareness of AI algorithms in SM. To foster an authentic learning experience and question adolescents’ trust in AI, we deploy a low-accuracy AI image classifier. A quasi-experimental study was conducted among 144 high-school students in Barcelona, Spain. The results show that the narrative scripts intervention improved students’ awareness of SM algorithms and shaped more critical attitudes towards them. A comparison of students’ choices between human predictions and those produced by a low-accuracy AI classifier shows a lack of AI overdependence. Information about predictions’ source did not affect students’ trust or learning about AI. These findings contribute towards SM algorithms education and share insight into the effect of deploying low-accuracy detectors in learning technology interventions.},
booktitle = {Responsive and Sustainable Educational Futures: 18th European Conference on Technology Enhanced Learning, EC-TEL 2023, Aveiro, Portugal, September 4–8, 2023, Proceedings},
pages = {415–429},
numpages = {15},
keywords = {Social media algorithms education, Low-accuracy image classification, Adolescents AI trust, AI overdependence},
location = {Aveiro, Portugal}
}

@inproceedings{10.1145/3746059.3747641,
author = {Killough, Daniel and Feng, Justin and Ching, Zheng Xue and Wang, Daniel and Dyava, Rithvik and Tian, Yapeng and Zhao, Yuhang},
title = {VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747641},
doi = {10.1145/3746059.3747641},
abstract = {Virtual Reality (VR) is inaccessible to blind people. While research has investigated many techniques to enhance VR accessibility, they require additional developer effort to integrate. As such, most mainstream VR apps remain inaccessible as the industry de-prioritizes accessibility. We present VRSight, an end-to-end system that recognizes VR scenes post hoc through a set of AI models (e.g., object detection, depth estimation, LLM-based atmosphere interpretation) and generates tone-based, spatial audio feedback, empowering blind users to interact in VR without developer intervention. To enable virtual element detection, we further contribute DISCOVR, a VR dataset consisting of 30 virtual object classes from 17 social VR apps, substituting real-world datasets that remain not applicable to VR contexts. Nine participants used VRSight to explore an off-the-shelf VR app (Rec Room), demonstrating its effectiveness in facilitating social tasks like avatar awareness and available seat identification.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {49},
numpages = {17},
keywords = {Virtual reality, accessibility, computer vision, artificial intelligence, blindness, spatial audio, screen reader},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1609/aaai.v39i5.32593,
author = {Liu, Siyu and Duan, Zheng-Peng and OuYang, Jia and Fu, Jiayi and Park, Hyunhee and Liu, Zikun and Guo, Chun-Le and Li, Chongyi},
title = {FaceMe: robust blind face restoration with personal identification},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i5.32593},
doi = {10.1609/aaai.v39i5.32593},
abstract = {Blind face restoration is a highly ill-posed problem due to the lack of necessary context. Although existing methods produce high-quality outputs, they often fail to faithfully preserve the individual's identity. In this paper, we propose a personalized face restoration method, FaceMe, based on a diffusion model. Given a single or a few reference images, we use an identity encoder to extract identity-related features, which serve as prompts to guide the diffusion model in restoring high-quality and identity-consistent facial images. By simply combining identity-related features, we effectively minimize the impact of identity-irrelevant features during training and support any number of reference image inputs during inference. Additionally, thanks to the robustness of the identity encoder, synthesized images can be used as reference images during training, and identity changing during inference does not require fine-tuning the model. We also propose a pipeline for constructing a reference image training pool that simulates the poses and expressions that may appear in real-world scenarios. Experimental results demonstrate that our FaceMe can restore high-quality facial images while maintaining identity consistency, achieving excellent performance and robustness. Code — https://modyu-liu.github.io/FaceMe_Homepage},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {620},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@article{10.1007/s00500-023-08604-z,
author = {Lettieri, Nicola and Guarino, Alfonso and Zaccagnino, Rocco and Malandrino, Delfina},
title = {Keeping judges in the loop: a human–machine collaboration strategy against the blind spots of AI in criminal justice},
year = {2023},
issue_date = {Aug 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {16},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-023-08604-z},
doi = {10.1007/s00500-023-08604-z},
abstract = {While seeping into every aspect of our lives, intelligent systems carry serious concerns, many of which stem from the need to secure control over machines ensuring they turn into an enhancement and not into a threat to human societies. Issues are not lacking. Whether one considers recruiting platforms perpetuating historical patterns of discrimination or AI-driven systems unfairly evaluating the creditworthiness of natural persons, it clearly appears vital to firmly place human beings with their rights and needs at the center of intelligent technologies. The paper tackles this kind of issue by focusing on the use of artificial intelligence in criminal justice, where predictive analytics and AI-driven decision systems have proven capable not only of enhancing the fight against crime but also of bringing the risk of opacity, aberrations, and injustices. We present a human–machine collaboration strategy providing judges with the advantages of AI and computational heuristics while offering control and understanding of the role played by machines. Drawing from a research that led to the development of an experimental platform supporting judges and public prosecutors dealing with organized crime, the strategy revolves around two components: (i) an online learning model designed to support judges in the evaluation of criminal dangerousness of individuals and groups capable of learning from users’ feedback; (ii) a human–computer interaction component exploiting visual metaphors to ease judges’ interaction with data, AI and other heuristics. The main contribution of the work is a novel and viable declension of the Human-centered AI paradigm in justice administration.},
journal = {Soft Comput.},
month = jun,
pages = {11275–11293},
numpages = {19},
keywords = {Human–machine collaboration, Machine learning digital justice, Legal analytics, Computational crime analysis}
}

@article{10.1016/j.engappai.2023.106494,
author = {Yin, Pengfei and Liu, Zhonghua and Wu, Di and Huo, Hua and Wang, Haijun and Zhang, Kaibing},
title = {Corrigendum to “Unsupervised Simple Siamese Representation Learning for Blind Super-Resolution” [Eng. Appl. Artif. Intell. 114 (2022) 105092]},
year = {2023},
issue_date = {Aug 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {123},
number = {PC},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2023.106494},
doi = {10.1016/j.engappai.2023.106494},
journal = {Eng. Appl. Artif. Intell.},
month = aug,
numpages = {1}
}

@inproceedings{10.1007/978-981-99-7025-4_2,
author = {Wei, Zihao and Huang, Yidong and Chen, Yuang and Zheng, Chenhao and Gao, Jingnan},
title = {A-ESRGAN: Training Real-World Blind Super-Resolution with&nbsp;Attention U-Net Discriminators},
year = {2023},
isbn = {978-981-99-7024-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-7025-4_2},
doi = {10.1007/978-981-99-7025-4_2},
abstract = {Generative adversarial networks (GANs) have recently made great progress in blind image super-resolution (SR) with their superiority in learning mappings between manifolds, which benefits the reconstruction of image’s textural details. Recent works have largely focused on designing more realistic degradation models, or constructing a more powerful generator structure but neglected the ability of discriminators in improving visual performances. In this paper, we present A-ESRGAN, a GAN model for blind SR tasks featuring an attention U-Net based, multi-scale discriminator that can be seamlessly integrated with other generators. To our knowledge, this is the first work to introduce attention U-Net structure as the discriminator of GAN to solve blind SR problems. And the paper also gives an interpretation of the mechanism behind multi-scale attention U-Net that brings performance breakthrough to the model. Experimental results demonstrate the superiority of our A-ESRGAN over state-of-the-art level performance in terms of quantitative metrics and visual quality. The code can be find in .},
booktitle = {PRICAI 2023: Trends in Artificial Intelligence: 20th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2023, Jakarta, Indonesia, November 15–19, 2023, Proceedings, Part III},
pages = {16–27},
numpages = {12},
keywords = {Blind Super Resolution, Generative adversarial networks, attention mechanism, Multi-scale, U-Net},
location = {Jakarta, Indonesia}
}

@inproceedings{10.1007/978-981-99-7025-4_13,
author = {Pan, Qingyi and Guo, Ning and Qingge, Letu and Zhang, Jingyi and Yang, Pei},
title = {PMT-IQA: Progressive Multi-task Learning for&nbsp;Blind Image Quality Assessment},
year = {2023},
isbn = {978-981-99-7024-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-7025-4_13},
doi = {10.1007/978-981-99-7025-4_13},
abstract = {Blind image quality assessment (BIQA) remains challenging due to the diverse types of distortion and variable image content, which complicates the distortion patterns crossing different scales and aggravates the difficulty of the regression problem for BIQA. However, existing BIQA methods often fail to consider multi-scale distortion patterns and image content, and there has limited research on improving the performance of quality regression models through specific learning strategies. In this paper, we propose a simple yet effective Progressive Multi-Task Image Quality Assessment (PMT-IQA) model, which contains a multi-scale feature extraction module (MS) and a progressive multi-task learning module (PMT), to help the model learn complex distortion patterns and better optimize the regression issue to align with the law of human learning process from easy to hard. To verify the effectiveness of the proposed PMT-IQA model, we conduct experiments on four widely used public datasets, and the experimental results indicate that the performance of PMT-IQA is superior to the comparison approaches, and both MS and PMT modules improve the model’s performance. The source code for this study is available at .},
booktitle = {PRICAI 2023: Trends in Artificial Intelligence: 20th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2023, Jakarta, Indonesia, November 15–19, 2023, Proceedings, Part III},
pages = {153–164},
numpages = {12},
keywords = {Blind image quality assessment, easy-to-hard effect, multi-scale feature, progressive multi-task learning},
location = {Jakarta, Indonesia}
}

@article{10.1016/j.engappai.2025.110604,
author = {Guo, Yanfei and Yang, Chenglong and Du, Hangli and Zhang, Yuanke and Ma, Fei and Yuan, Shasha},
title = {Dynamic momentum contrastive learning network for diabetic retinopathy grading},
year = {2025},
issue_date = {Jul 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2025.110604},
doi = {10.1016/j.engappai.2025.110604},
journal = {Eng. Appl. Artif. Intell.},
month = may,
numpages = {16},
keywords = {Diabetic retinopathy grading, Class imbalance, Multi-class classification, Dynamic contrastive learning, Dual attention mechanism}
}

@article{10.1177/0894439320980118,
author = {Janssen, Marijn and Hartog, Martijn and Matheus, Ricardo and Yi Ding, Aaron and Kuk, George},
title = {Will Algorithms Blind People? The Effect of Explainable AI and Decision-Makers’ Experience on AI-supported Decision-Making in Government},
year = {2022},
issue_date = {Apr 2022},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {40},
number = {2},
issn = {0894-4393},
url = {https://doi.org/10.1177/0894439320980118},
doi = {10.1177/0894439320980118},
abstract = {Computational artificial intelligence (AI) algorithms are increasingly used to support decision making by governments. Yet algorithms often remain opaque to the decision makers and devoid of clear explanations for the decisions made. In this study, we used an experimental approach to compare decision making in three situations: humans making decisions (1) without any support of algorithms, (2) supported by business rules (BR), and (3) supported by machine learning (ML). Participants were asked to make the correct decisions given various scenarios, while BR and ML algorithms could provide correct or incorrect suggestions to the decision maker. This enabled us to evaluate whether the participants were able to understand the limitations of BR and ML. The experiment shows that algorithms help decision makers to make more correct decisions. The findings suggest that explainable AI combined with experience helps them detect incorrect suggestions made by algorithms. However, even experienced persons were not able to identify all mistakes. Ensuring the ability to understand and traceback decisions are not sufficient for avoiding making incorrect decisions. The findings imply that algorithms should be adopted with care and that selecting the appropriate algorithms for supporting decisions and training of decision makers are key factors in increasing accountability and transparency.},
journal = {Soc. Sci. Comput. Rev.},
month = apr,
pages = {478–493},
numpages = {16},
keywords = {AI, artificial intelligence, decision making, e-government, algorithmic governance, transparency, accountability, XAI, experiment, data-driven government}
}

@inproceedings{10.1609/aaai.v38i5.28236,
author = {Pan, Wensheng and Gao, Timin and Zhang, Yan and Zheng, Xiawu and Shen, Yunhang and Li, Ke and Hu, Runze and Liu, Yutao and Dai, Pingyang},
title = {Semi-supervised blind image quality assessment through knowledge distillation and incremental learning},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i5.28236},
doi = {10.1609/aaai.v38i5.28236},
abstract = {Blind Image Quality Assessment (BIQA) aims to simulate human assessment of image quality. It has a great demand for labeled data, which is often insufficient in practice. Some researchers employ unsupervised methods to address this issue, which is challenging to emulate the human subjective system. To this end, we introduce a unified framework that combines semi-supervised and incremental learning to address the mentioned issue. Specifically, when training data is limited, semi-supervised learning is necessary to infer extensive unlabeled data. To facilitate semi-supervised learning, we use knowledge distillation to assign pseudo-labels to unlabeled data, preserving analytical capability. To gradually improve the quality of pseudo labels, we introduce incremental learning. However, incremental learning can lead to catastrophic forgetting. We employ Experience Replay by selecting representative samples during multiple rounds of semi-supervised learning, to alleviate forgetting and ensure model stability. Experimental results show that the proposed approach achieves state-of-the-art performance across various benchmark datasets. After being trained on the LIVE dataset, our method can be directly transferred to the CSIQ dataset. Compared with other methods, it significantly outperforms unsupervised methods on the CSIQ dataset with a marginal performance drop (-0.002) on the LIVE dataset. In conclusion, our proposed method demonstrates its potential to tackle the challenges in real-world production processes.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {488},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@article{10.1016/j.engappai.2025.110010,
author = {Sivakumar, Rishikesh and Penkova, Anita},
title = {Enhancing glaucoma detection through multi-modal integration of retinal images and clinical biomarkers},
year = {2025},
issue_date = {Mar 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {143},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2025.110010},
doi = {10.1016/j.engappai.2025.110010},
journal = {Eng. Appl. Artif. Intell.},
month = mar,
numpages = {13},
keywords = {Glaucoma detection, Vision Transformers, Convolutional Neural Networks, Machine Learning, Clinical Biomarkers}
}

@article{10.1016/j.artint.2023.103883,
author = {Gerevini, Alfonso E. and Lipovetzky, Nir and Percassi, Francesco and Saetti, Alessandro and Serina, Ivan},
title = {Width-based search for multi agent privacy-preserving planning},
year = {2023},
issue_date = {May 2023},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {318},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2023.103883},
doi = {10.1016/j.artint.2023.103883},
journal = {Artif. Intell.},
month = may,
numpages = {29},
keywords = {Planning, Multi-agent planning, Privacy-preserving planning, Distributed planning}
}

@article{10.1007/s10462-023-10532-1,
author = {Alajlan, Abrar M. and Razaque, Abdul},
title = {ESOA-HGRU: egret swarm optimization algorithm-based hybrid gated recurrent unit for classification of diabetic retinopathy},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {56},
number = {Suppl 2},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-023-10532-1},
doi = {10.1007/s10462-023-10532-1},
abstract = {Diabetes is a chronic disease that affects people all over the world and raises the glucose level in the blood as a result of a lack of insulin. Diabetic Retinopathy causes retinal eye disease, which impairs vision and eventually results in blindness. The two classifications of Diabetic Retinopathy based on retinal indicators are Non-Proliferative and Proliferative Diabetic Retinopathy. The Diabetic Retinopathy diagnosis is a time-consuming process for professionals. But the use of handcrafted features limits this method’s performance. To identify Diabetic Retinopathy at an early stage, we propose an Egret Swarm optimized hybrid Mask Region-Based Convolutional Neural Network-Bidirectional Gated Recurrent Unit approach which identifies the interdependencies between different Diabetic Retinopathy stages. Initially, the input samples are preprocessed using data augmentation and partitioned into training and testing data. The parameter of the Hybrid Mask Region-Based Convolutional Neural Network-Bidirectional Gated Recurrent Unit model is optimized through the Egret Swarm Optimization algorithm to minimize the loss of the classifier. Here, Egret Swarm optimized hybrid Hybrid Mask Region-Based Convolutional Neural Network-Bidirectional Gated Recurrent Unit architecture is utilized to classify various Diabetic Retinopathy stages. This paper uses three large baseline datasets: Idrid, APTOS 2019 blindness detection, and Zenodo dataset. The simulation results demonstrated that the proposed technique achieved improved precision, recall, and F-measure scores which are nearly equal to 99.1%, 98.9%, and 99%.},
journal = {Artif. Intell. Rev.},
month = sep,
pages = {1617–1646},
numpages = {30},
keywords = {Diabetic retinopathy, Diabetes retinopathy, Hemorrhaging, Biomedical imaging, Egret swarm optimization algorithm, Hybrid mask RCNN-bidirectional gated recurrent unit}
}

@inproceedings{10.1609/aaai.v39i1.32015,
author = {Kou, Feifei and Yao, Yuhan and Yao, Siyuan and Wang, Jiahao and Shi, Lei and Li, Yawen and Kang, Xuejing},
title = {IWRN: a robust blind watermarking method for artwork image copyright protection against noise attack},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i1.32015},
doi = {10.1609/aaai.v39i1.32015},
abstract = {Adding imperceptible watermarks to artwork images, such as paintings and photographs, can effectively safeguard the copyright of these images without compromising their usability. However, existing blind watermarking techniques encounter two major challenges in addressing this task: imperceptibility and robustness, particularly when subjected to various noise attacks. In this paper, we propose a blind watermarking method for artwork image copyright protection, IWRN, which can ensure both the Imperceptibility of the Watermark and Robustness against Noise attacks. For imperceptibility, we design a Learnable Wavelet Network (LWN) to adaptively embed the watermark into the high-frequency region where the watermark has better invisibility. For robustness, we establish a Deform-Attention based Invertible Neural Network (DA-INN) with a decoding optimization, which offers the advantage of computational reversion, and combines the deform-attention mechanism and decoding optimization to enhance the model's resistance against noises. Additionally, we design a Joint Contrast Learning (JCL) mechanism to improve imperceptibility and robustness simultaneously. Experiments show that our IWRN outperforms other state-of-the-art blind watermarking methods, achieves an average performance of 46.74 PSNR and 99.91% accuracy across three datasets when facing 12 kinds of noise attacks. Code — https://github.com/BUPT-SN/IWRN},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {42},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@article{10.1016/j.engappai.2025.110380,
author = {Zhou, Ziqiang and Sun, Baojiang and Sun, Qian},
title = {Characterizing the hydrodynamic and mechanical properties of hydraulic fractured shale plays using a Kolmogorov-Arnold-Network-assisted data assimilation approach},
year = {2025},
issue_date = {May 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {147},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2025.110380},
doi = {10.1016/j.engappai.2025.110380},
journal = {Eng. Appl. Artif. Intell.},
month = apr,
numpages = {16},
keywords = {Artificial intelligence, Kolmogorov-Arnold network, Ensemble Kalman filter, Hydraulic fracture characteristics, Shale plays}
}

@article{10.1145/3654768.3654771,
author = {Gamage, Bhanuka},
title = {AI-Enabled Smart Glasses for People with Severe Vision Impairments},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
number = {137},
issn = {1558-2337},
url = {https://doi.org/10.1145/3654768.3654771},
doi = {10.1145/3654768.3654771},
abstract = {Over the last decade, there has been significant research on how smart assistive devices with artificial intelligence (AI) built into them can assist people with severe vision impairments to comprehend their surroundings. These devices come in various forms such as smartphone applications, smart-glasses, and smart canes. Smart glasses have gained popularity lately due to recent technological advancements, as well as their natural position in front of the user's eyes. However, there has been limited research to understand how people with severe vision impairments would prefer to interact with them. The objective of this project is to investigate the use of AI-enabled smart-glasses to aid individuals with severe vision impairments. The research aims to comprehend the differences between the types of research conducted by researchers and the needs and desires of the community. The study will utilise a Design Thinking approach and involve vision-impaired users throughout the project by utilising co-design methods. The research will involve the development of smart-glass application prototypes through iterative case studies with individuals who are blind, have low-vision, and have cerebral vision impairment (CVI). The goal is to understand the users' preferred interaction model from their lived experiences. The outcome of the research will also be a software architecture that enables people with severe vision impairments to seamlessly access information about their surroundings.},
journal = {SIGACCESS Access. Comput.},
month = mar,
articleno = {3},
numpages = {1}
}

@inproceedings{10.1007/978-3-031-98420-4_5,
author = {Simon, Sebastian and Sankaranarayanan, Sreecharan and Tajik, Elham and Borchers, Conrad and Shahrokhian, Bahar and Balzan, Francesco and Strau\ss{}, Sebastian and Viswanathan, Sree Aurovindh and Ata\c{s}, Amine Hatun and \v{C}arapina, Mia and Liang, Li and Celik, Berkan},
title = {Comparing a Human’s and a Multi-Agent System’s Thematic Analysis: Assessing Qualitative Coding Consistency},
year = {2025},
isbn = {978-3-031-98419-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-98420-4_5},
doi = {10.1007/978-3-031-98420-4_5},
abstract = {Large Language Models (LLMs) have demonstrated fluency in text generation and reasoning tasks. Consequently, the field has probed the ability of LLMs to automate qualitative analysis, including inductive thematic analysis (iTA), previously achieved through human reasoning only. Studies using LLMs for iTA have yielded mixed results so far. LLMs have successfully been used for isolated steps of iTA in hybrid setups. With recent advances in multi-agent systems (MAS) enabling complex reasoning and task execution through multiple, collaborating LLM agents, the first results point towards the possibility of automating sequences of the iTA process. However, previous work especially lacks methodological standards for assessing the reliability and validity of LLM-derived iTA. Thus, in this paper, we propose a method for assessing the quality of iTA systems based on consistency with human coding on a benchmark dataset. We present criteria for benchmark datasets and an expert blind review with this method on two iTA outputs: one iTA conducted by domain experts, and another fully automated with a MAS built on the Claude 3.5 Sonnet LLM. Results indicate a high level of consistency and contribute evidence that complex qualitative analysis methods common in AIED research can be carried out by MAS.},
booktitle = {Artificial Intelligence in Education: 26th International Conference, AIED 2025, Palermo, Italy, July 22–26, 2025, Proceedings, Part III},
pages = {60–73},
numpages = {14},
keywords = {Large Language Models, Qualitative Coding, Thematic Analysis, Inductive Analysis, Claude 3.5 Sonnet, Agentic LLMs, Multi-Agent Systems},
location = {Palermo, Italy}
}

@inproceedings{10.1145/3696474.3708460,
author = {Jiang, Bin and Qian, Jun and Sun, Jun and Guo, Hao and Wang, Daoming and Zhao, Zhigang and Kong, Lingshu},
title = {Visual Blind Spot Detection System for Bus Drivers Based on Lightweight Model},
year = {2025},
isbn = {9798400710100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696474.3708460},
doi = {10.1145/3696474.3708460},
abstract = {For a bus driver, the environment outside the bus door contains a serious visual blind spot. When a bus is parking, waiting for passengers boarding, and exiting the bus station, this blind spot requires the driver to maintain a high degree of attention. In order to improve the driver's perception of the blind spot in this area, a fisheye camera on the top of the bus door is used. A car-grade chip is selected to collect visual images and a lightweight deep learning model is deployed to identify the environment near the door, including objects such as pedestrians, bicycles and cars. Firstly, the collected YUV visual images are corrected for distortion, and then converted into RGB images. Then the RGB images are inputted into the trained deep learning model for human and vehicle recognition. The lightweight model is obtained after the lightweight optimization of the YOLO-FastestV2. And a good recognition accuracy is obtained. The neural network recognition of each image consumes about 60ms. In order to achieve the image collection and display rate of 25 fps, the frame skipping recognition method is adopted, which ensures the continuity of video display and a certain recognition accuracy. Finally, the performances of different deep learning models are compared so as to select a model suitable for the application scenarios and hardware resources. By providing the real time visual images with labels and an alarm signal when needed for the bus driver, the system could effectively improve the driver's ability of blind spot perception.},
booktitle = {Proceedings of the 2024 4th International Joint Conference on Robotics and Artificial Intelligence},
pages = {188–192},
numpages = {5},
keywords = {Blind spot detection, Deep learning, Lightweight model, Vision},
location = {
},
series = {JCRAI '24}
}

@inproceedings{10.1145/3597638.3608955,
author = {Gamage, Bhanuka and Do, Thanh-Toan and Price, Nicholas Seow Chiang and Lowery, Arthur and Marriott, Kim},
title = {What do Blind and Low-Vision People Really Want from Assistive Smart Devices? Comparison of the Literature with a Focus Study},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608955},
doi = {10.1145/3597638.3608955},
abstract = {Over the last decade there has been considerable research into how artificial intelligence (AI), specifically computer vision, can assist people who are blind or have low-vision (BLV) to understand their environment. However, there has been almost no research into whether the tasks (object detection, image captioning, text recognition etc.) and devices (smartphones, smart-glasses etc.) investigated by researchers align with the needs and preferences of BLV people. We identified 646 studies published in the last two and a half years that have investigated such assistive AI techniques. We analysed these papers to determine the task, device and participation by BLV individuals. We then interviewed 24 BLV people and asked for their top five AI-based applications and to rank the applications found in the literature. We found only a weak positive correlation between BLV participants’ perceived importance of tasks and researchers’ focus and that participants prefer conversational agent interface and head-mounted devices.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {30},
numpages = {21},
keywords = {augmented reality, mobile application, recognition, smart devices, virtual reality, wearable},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1007/978-3-031-92591-7_20,
author = {Magay, Alexey and Tripathi, Dhurba and Hao, Yu and Fang, Yi},
title = {A Light and&nbsp;Smart Wearable Platform with&nbsp;Multimodal Foundation Model for&nbsp;Enhanced Spatial Reasoning in&nbsp;People with&nbsp;Blindness and&nbsp;Low Vision},
year = {2025},
isbn = {978-3-031-92590-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-92591-7_20},
doi = {10.1007/978-3-031-92591-7_20},
abstract = {People with blindness and low vision (pBLV) face significant challenges, struggling to navigate environments and locate objects due to limited visual cues. Spatial reasoning is crucial for these individuals, as it enables them to understand and interpret the spatial relationships in their surroundings, enhancing their ability to navigate and interact more safely and independently. Current multi-modal large language (MLLM) models for low vision people lack the spatial reasoning capabilities needed to effectively assist in these tasks. Moreover, there is a notable absence of lightweight, easy-to-use systems that allow pBLV to effectively perceive and interact with their surrounding environment. In this paper, we propose a novel spatial enhanced multi-modal large language model-based approach for visually impaired individuals. By fine-tuning the MLLM to incorporate spatial reasoning capabilities, our method significantly improves the understanding of environmental context, which is critical for navigation and object recognition. The innovation extends to a hardware component, designed as an attachment for glasses, ensuring increased accessibility and ease of use. This integration leverages advanced VLMs to interpret visual data and provide real-time, spatially aware feedback to the user. Our approach aims to bridge the gap between advanced machine learning models and practical, user-friendly assistive devices, offering a robust solution for visually impaired users to navigate their surroundings more effectively and independently. The paper includes an in-depth evaluation using the VizWiz dataset, demonstrating substantial improvements in accuracy and user experience. Additionally, we design a comprehensive dataset to evaluate our method’s effectiveness in real-world situations, demonstrating substantial improvements in accuracy and user experience.},
booktitle = {Computer Vision – ECCV 2024 Workshops: Milan, Italy, September 29–October 4, 2024, Proceedings, Part XII},
pages = {323–339},
numpages = {17},
keywords = {VI assistance, Multi-modal large language model, Wearable device},
location = {Milan, Italy}
}

@article{10.1145/3696318,
author = {Singh, Utkarsha and Divya Venkatesh, Jeevithashree and Muraleedharan, Anujith and Saluja, Kamalpreet Singh and J H, Anamika and Biswas, Pradipta},
title = {Accessibility Analysis of Educational Websites Using WCAG 2.0},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3696318},
doi = {10.1145/3696318},
abstract = {In recent years, the internet has emerged as the primary information hub, encompassing various domains such as education, healthcare, government, and e-commerce. With a vast user base relying on websites for accessing essential information, ensuring web content accessibility has become imperative. Meeting accessibility guidelines is crucial to accommodate users with diverse abilities worldwide. This article examines accessibility variances among education board websites hosting higher-level school leaving examinations. The study evaluates the landing pages of 13 educational websites from both developing and developed countries, employing six Web Content Accessibility Guidelines (WCAG) tools endorsed by the World Wide Web Consortium (W3C). The study has meticulously compiled the outcomes of each assessment tool, delineating the diverse accessibility concerns encountered on each website. Furthermore, alternative recommendations have been proposed to assist web developers in rectifying these issues. The findings underscore a higher prevalence of accessibility issues within the education board websites of developing countries compared to their developed counterparts. Specifically, the analysis revealed prominent text contrast failures among selected web pages from developing nations, while non-text contrast failures were more prevalent in those from developed countries. Notably, the education board websites of India and Japan emerged with the highest number of accessibility issues among the sampled websites. As a way forward, a case study was shown on using LLM for making websites accessible.},
journal = {Digit. Gov.: Res. Pract.},
month = oct,
articleno = {32},
numpages = {28},
keywords = {Developed and developing countries, educational board websites, W3C (world wide web consortium), WCAG (web content accessibility guidelines), web accessibility, web accessibility evaluation tools}
}

@inproceedings{10.1609/aaai.v38i3.28023,
author = {Ji, Liya and Rao, Zhefan and Pan, Sinno Jialin and Lei, Chenyang and Chen, Qifeng},
title = {A diffusion model with state estimation for degradation-blind inverse imaging},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i3.28023},
doi = {10.1609/aaai.v38i3.28023},
abstract = {Solving the task of inverse imaging problems can restore unknown clean images from input measurements that have incomplete information. Utilizing powerful generative models, such as denoising diffusion models, could better tackle the ill-posed issues of inverse problems with the distribution prior of the unknown clean images. We propose a learnable state-estimator-based diffusion model to incorporate the measurements into the reconstruction process. Our method makes efficient use of the pre-trained diffusion models with computational feasibility compared to the conditional diffusion models, which need to be trained from scratch. In addition, our pipeline does not require explicit knowledge of the image degradation operator or make the assumption of its form, unlike many other works that use the pre-trained diffusion models at the test time. The experiments on three typical inverse imaging problems (both linear and non-linear), inpainting, deblurring, and JPEG compression restoration, have comparable results with the state-of-the-art methods.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {275},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.1007/978-3-031-49552-6_39,
author = {Mart\'{\i}nez-D\'{\i}az, Yoanna and Lu\'{e}vano, Luis S. and M\'{e}ndez-V\'{a}zquez, Heydi},
title = {Effectiveness of&nbsp;Blind Face Restoration to&nbsp;Boost Face Recognition Performance at&nbsp;Low-Resolution Images},
year = {2023},
isbn = {978-3-031-49551-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49552-6_39},
doi = {10.1007/978-3-031-49552-6_39},
abstract = {This paper studies the effectiveness of Blind Face Restoration methods to boost the performance of face recognition systems on low-resolution images. We investigate the use of three blind face restoration techniques, which have demonstrated impressive results in generating realistic high-resolution face images. Three state-of-the-art face recognition methods were selected to assess the impact of using the generated high-resolution images on their performance. Our analysis includes both, synthesized and native low-resolution images. The conducted experimental evaluation show that this is still an open research problem.},
booktitle = {Progress in Artificial Intelligence and Pattern Recognition: 8th International Congress on Artificial Intelligence and Pattern Recognition, IWAIPR 2023, Varadero, Cuba, September 27–29, 2023, Proceedings},
pages = {455–467},
numpages = {13},
keywords = {low-resolution, face recognition, blind face restoration},
location = {Varadero, Cuba}
}

@inproceedings{10.1145/3696593.3696622,
author = {Tanwar, Sachin and Rao, P.V. Madhusudhan and Verma, Esha},
title = {Digital Inclusion Uneven: Exploring Visually Challenged Users' Technology Choices: Strong dependence of Visually Challenged Users on Personal Computers despite the advancements in Mobile Technologies and User Experience},
year = {2025},
isbn = {9798400707292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696593.3696622},
doi = {10.1145/3696593.3696622},
abstract = {This research explores the unequal digital inclusion for visually challenged (VC) individuals, focusing on their technology preferences, in a rapidly expanding digital landscape that prioritizes mobile and AI backed technologies. Despite the widespread adoption of mobile devices, the study shows that VC individuals rely heavily on personal computers, despite the convenience and portability of smartphones.The study investigates the limitations of mobile devices that hinder its&nbsp;effectiveness and user-friendliness, resulting in the digital divide by examining the distinct characteristics of personal computers (PCs) that enhance the ability of visually impaired individuals to accomplish their goals more efficiently compared to smartphones. Moreover, the study demonstrates that VC individuals interpret smartphones, typically considered more private and secure for those with normal eyesight, in a completely different manner compared to personal computers.This was accomplished by conducting a qualitative semi-structured interviews with 34 VC&nbsp;individuals in capital city of India. The results emphasize on immediate necessity for more inclusiveness in the development of mobile application design and screen reader technology, to guarantee equal opportunity and involvement for everyone.},
booktitle = {Proceedings of the 11th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {311–316},
numpages = {6},
location = {
},
series = {DSAI '24}
}

@inproceedings{10.1145/3587281.3587296,
author = {Stangl, Abigale and Sadjo, Emma and Emami-Naeini, Pardis and Wang, Yang and Gurari, Danna and Findlater, Leah},
title = {“Dump it, Destroy it, Send it to Data Heaven”: Blind People’s Expectations for Visual Privacy in Visual Assistance Technologies},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587281.3587296},
doi = {10.1145/3587281.3587296},
abstract = {Visual assistance technologies provide people who are blind with access to information about their visual surroundings by digitally connecting them to remote humans or artificial intelligence systems that describe visual content such as objects, people, scenes, and text observed in their live image/video feeds. Prior work has revealed that users have concerns about how such technologies handle private visual content captured in their image/video feeds. Yet, it remains unclear how users want technologies to manage such private content. To fill this gap, we interviewed 16 totally blind individuals to learn about their expectations for visual privacy when using visual assistance technologies. Our findings reveal three overarching user-centered expectations associated with visual privacy-preservation in this domain, as well as the broader ethical challenges involved with developing AI-based privacy-preserving visual assistance technologies.},
booktitle = {Proceedings of the 20th International Web for All Conference},
pages = {134–147},
numpages = {14},
location = {Austin, TX, USA},
series = {W4A '23}
}

@inproceedings{10.1007/978-3-031-34344-5_25,
author = {P\'{e}rez-Bueno, Fernando and Engan, Kjersti and Molina, Rafael},
title = {A Robust BKSVD Method for&nbsp;Blind Color Deconvolution and&nbsp;Blood Detection on&nbsp;H &amp;E Histological Images},
year = {2023},
isbn = {978-3-031-34343-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-34344-5_25},
doi = {10.1007/978-3-031-34344-5_25},
abstract = {Hematoxylin and Eosin (H &amp;E) color variation between histological images from different laboratories degrades the performance of Computer-Aided Diagnosis systems. Histology-specific models to solve color variation are designed taking into account the staining procedure, where most color variations are introduced. In particular, Blind Color Deconvolution (BCD) methods aim to identify the real underlying colors in the image and to separate the tissue structure from the color information. A commonly used assumption is that images are stained with and only with the pure staining colors (e.g., blue and pink for H &amp;E). However, this assumption does not hold true in the presence of common artifacts such as blood, where the blood cells need a third color component to be represented. Blood usually hampers the ability of color standardization algorithms to correctly identify the stains in the image, producing unexpected outputs. In this work, we propose a robust Bayesian K-Singular Value Decomposition (BKSVD) model to simultaneously detect blood and separate color from structure in histological images. Our method was tested on synthetic and real images containing different amounts of blood pixels.},
booktitle = {Artificial Intelligence in Medicine: 21st International Conference on Artificial Intelligence in Medicine, AIME 2023, Portoro\v{z}, Slovenia, June 12–15, 2023, Proceedings},
pages = {207–217},
numpages = {11},
keywords = {Stain Separation, Blood Detection, Histological images},
location = {Portoroz, Slovenia}
}

@article{10.1613/jair.1.16869,
author = {Speck, David and Seipp, Jendrik and Torralba, Alvaro},
title = {Symbolic Search for Cost-Optimal Planning with Expressive Model Extensions},
year = {2025},
issue_date = {Jun 2025},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {82},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.16869},
doi = {10.1613/jair.1.16869},
abstract = {In classical planning, the task is to derive a sequence of deterministic actions that changes the current fully-observable world state into one that satisfies a set of goal criteria. Algorithms for classical planning are domain-independent, i.e., they are not limited to a particular application and instead can be used to solve different types of reasoning problems. The main language for modeling such problems is the Planning Domain Definition Language (PDDL). Even though it provides many language features for expressing a wide range of planning tasks, most of today’s classical planners, especially optimal ones, support only a small subset of its features. The most widely supported fragment is lifted STRIPS plus types and action costs. While this fragment suffices to model some interesting planning tasks, using it to model more realistic problems often incurs a much higher modeling effort. Even if modeling is possible at all, solving the resulting tasks is often infeasible in practice, as the required encoding size increases exponentially. To address these issues, we show how to support more expressive modeling languages natively in optimal classical planning algorithms. Specifically, we focus on symbolic search, a state-of-the-art search algorithm that operates on sets of world states. We show how to extend symbolic search to support classical planning with conditional effects, axioms, and state-dependent action costs. All of these modeling features are expressive in the sense that compiling them away incurs a significant blow-up, so is it often necessary to support them natively. Except for blind (non-symbolic) search, our new symbolic search is the first optimal classical planning algorithm that supports these three modeling extensions in combination, and it even compares favorably to other state-of-the-art approaches that only support a subset of the extensions.},
journal = {J. Artif. Int. Res.},
month = apr,
pages = {1349–1405},
numpages = {57},
keywords = {natural language, machine learning, fake news, transfer learning}
}

@inproceedings{10.1145/3718751.3718789,
author = {Tang, Lu},
title = {Blind Image Quality Assessment Method for Cloud Gaming Based on Fusion Feature},
year = {2025},
isbn = {9798400709753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718751.3718789},
doi = {10.1145/3718751.3718789},
abstract = {With the development of cloud gaming, more and more users are willing to play games in the cloud, and the image quality of the game will directly affect the game experience of the players. Image quality assessment is an important tool to ensure that the image is transmitted and displayed with high quality. Some studies have shown that advanced IQA models designed for natural scene images do not work for unnatural scene images such as cloud gaming. In addition, the existing research on image quality assessment rarely discusses the impact of aesthetic factors on image quality. Therefore, an esthetic feature-assisted blind image quality assessment method for cloud gaming is proposed in this paper. A two-flow convolutional neural network architecture is used to extract both the aesthetic features and distortion features of the image, and the two features are fused into the hyperparameter regression network to calculate the quality of cloud game images. Experiments are carried out on real data sets, and the results show that the proposed method is superior to other classical image quality assessment methods.},
booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management},
pages = {236–241},
numpages = {6},
keywords = {blind image quality assessment, cloud gaming, feature fusion, two-stream CNN},
location = {
},
series = {ICBAR '24}
}

@inproceedings{10.1007/978-3-031-20500-2_11,
author = {Lu, Wei and Sun, Wei and Min, Xiongkuo and Zhang, Zicheng and Wang, Tao and Zhu, Wenhan and Yang, Xiaokang and Zhai, Guangtao},
title = {Blind Surveillance Image Quality Assessment via&nbsp;Deep Neural Network Combined with&nbsp;the&nbsp;Visual Saliency},
year = {2022},
isbn = {978-3-031-20499-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-20500-2_11},
doi = {10.1007/978-3-031-20500-2_11},
abstract = {The intelligent video surveillance system (IVSS) can automatically analyze the content of the surveillance image (SI) and reduce the burden of the manual labour. However, the SIs may suffer quality degradations in the procedure of acquisition, compression, and transmission, which makes IVSS hard to understand the content of SIs. In this paper, we first conduct an example experiment (i.e. the face detection task) to demonstrate that the quality of the SIs has a crucial impact on the performance of the IVSS, and then propose a saliency-based deep neural network for the blind quality assessment of the SIs, which helps IVSS to filter the low-quality SIs and improve the detection and recognition performance. Specifically, we first compute the saliency map of the SI to select the most salient local region since the salient regions usually contain rich semantic information for machine vision and thus have a great impact on the overall quality of the SIs. Next, the convolutional neural network (CNN) is adopted to extract quality-aware features for the whole image and local region, which are then mapped into the global and local quality scores through the fully connected (FC) network respectively. Finally, the overall quality score is computed as the weighted sum of the global and local quality scores. Experimental results on the SI quality database (SIQD) show that the proposed method outperforms all compared state-of-the-art BIQA methods.},
booktitle = {Artificial Intelligence: Second CAAI International Conference, CICAI 2022, Beijing, China, August 27–28, 2022, Revised Selected Papers, Part II},
pages = {136–146},
numpages = {11},
keywords = {Surveillance image, Blind quality assessment, Deep neural network, Visual saliency},
location = {Beijing, China}
}

@inproceedings{10.1145/3544548.3580922,
author = {Sharma, Tanusree and Stangl, Abigale and Zhang, Lotus and Tseng, Yu-Yun and Xu, Inan and Findlater, Leah and Gurari, Danna and Wang, Yang},
title = {Disability-First Design and Creation of A Dataset Showing Private Visual Information Collected With People Who Are Blind},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580922},
doi = {10.1145/3544548.3580922},
abstract = {We present the design and creation of a disability-first dataset, “BIV-Priv,” which contains 728 images and 728 videos of 14 private categories captured by 26 blind participants to support downstream development of artificial intelligence (AI) models. While best practices in dataset creation typically attempt to eliminate private content, some applications require such content for model development. We describe our approach in creating this dataset with private content in an ethical way, including using props rather than participants’ own private objects and balancing multi-disciplinary perspectives (e.g., accessibility, privacy, computer vision) to meet the tangible metrics (e.g., diversity, category, amount of content) to support AI innovations. We observed challenges that our participants encountered during the data collection, including accessibility issues (e.g., understanding foreground vs. background object placement) and issues due to the sensitive nature of the content (e.g., discomfort in capturing some props such as condoms around family members).},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {51},
numpages = {15},
keywords = {accessibility, blind, computer vision, dataset, image description, personal visual data, privacy, private visual content, visual assistance, visual impairments, visual interpretation},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1016/j.engappai.2024.109661,
author = {Yan, Li and Wen, Hu and Wang, Zhenping and Jin, Yongfei and Guo, Jun and Liu, Yin and Fan, Shixing},
title = {Prediction and evaluation of key parameters in coalbed methane pre-extraction based on transformer and inversion model},
year = {2025},
issue_date = {Jan 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {139},
number = {PB},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2024.109661},
doi = {10.1016/j.engappai.2024.109661},
journal = {Eng. Appl. Artif. Intell.},
month = jan,
numpages = {23},
keywords = {Coalbed methane, Industrial safety, Deep learning, Transformer, Sequential models}
}

@inproceedings{10.1609/aaai.v38i18.29978,
author = {Zhou, Zhanpeng and Shen, Wen and Chen, Huixin and Tang, Ling and Chen, Yuefeng and Zhang, Quanshi},
title = {Batch normalization is blind to the first and second derivatives of the loss},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i18.29978},
doi = {10.1609/aaai.v38i18.29978},
abstract = {We prove that when we do the Taylor series expansion of the loss function, the BN operation will block the influence of the first-order term and most influence of the second-order term of the loss. We also find that such a problem is caused by the standardization phase of the BN operation. We believe that proving the blocking of certain loss terms provides an analytic perspective for potential detects of a deep model with BN operations, although the blocking problem is not fully equivalent to significant damages in all tasks on benchmark datasets. Experiments show that the BN operation significantly affects feature representations in specific tasks.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2230},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.1007/978-3-032-05870-6_21,
author = {Fernandes, Leonor and Gon\c{c}alves, Tiago and Matos, Jo\~{a}o and Nakayama, Luis and Cardoso, Jaime S.},
title = {Disentanglement and&nbsp;Assessment of&nbsp;Shortcuts in&nbsp;Ophthalmological Retinal Imaging Exams},
year = {2025},
isbn = {978-3-032-05869-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-05870-6_21},
doi = {10.1007/978-3-032-05870-6_21},
abstract = {Diabetic retinopathy (DR) is a leading cause of vision loss in working-age adults. While screening reduces the risk of blindness, traditional imaging is often costly and inaccessible. Artificial intelligence (AI) algorithms present a scalable diagnostic solution, but concerns regarding fairness and generalization persist. This work evaluates the fairness and performance of image-trained models in DR prediction, as well as the impact of disentanglement as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness was assessed between subgroups of SAs, and disentanglement was applied to reduce bias. All models achieved high DR prediction performance in diagnosing (up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77% AUROC, respectively). Fairness assessment suggests disparities, such as a 10% AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction had varying results, depending on the model selected. Disentanglement improved DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2 and Swin V2 (7% and 3%, respectively). These findings highlight the complexity of disentangling fine-grained features in fundus imaging and emphasize the importance of fairness in medical imaging AI to ensure equitable and reliable healthcare solutions.},
booktitle = {Fairness of AI in Medical Imaging: Third International Workshop, FAIMI 2025, Held in Conjunction with MICCAI 2025, Daejeon, South Korea, September 23, 2025, Proceedings},
pages = {208–217},
numpages = {10},
keywords = {Deep learning, Diabetic retinopathy, Disentanglement, Fairness},
location = {Daejeon, Korea (Republic of)}
}

@inproceedings{10.1007/978-3-032-00652-3_9,
author = {Malayshi, Suhad and Hassasneh, Ahmad},
title = {Transfer Learning-Based Classification of Diabetic Retinopathy Using a Pre-trained InceptionResNet Model},
year = {2025},
isbn = {978-3-032-00651-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-00652-3_9},
doi = {10.1007/978-3-032-00652-3_9},
abstract = {Diabetic retinopathy is an eye disease that is the leading cause of vision loss in one-third of people with diabetes, it progresses silently without any noticeable symptoms from the patient’s side, so the early detection can preserve the patient’s vision loss and improve the quality of life. In this paper, we present a combination of deep learning pre-trained Inception-ResNet model with state-of-the-art image cleaning, enhancement and pre-processing techniques for early classification of diabetic retinopathy by using about 3660 retinal images obtained from the APTOS 2019 blindness detection public dataset. The evaluation of the approach is made based on different measures such as accuracy, precision, recall F1-score, confusion matric, receiver operating characteristic (ROC) curve and kappa score, showed a promising result for accurately classifying the stage of diabetic retinopathy. Our model achieved an accuracy of 98.0%, an average F1 score of 97.6% and a kappa score of 0.970. The results suggest that the approach can contribute to the early detection of diabetic retinopathy and improve the quality of life of people with diabetes.},
booktitle = {Artificial Intelligence in Healthcare: Second International Conference, AIiH 2025, Cambridge, UK, September 8–10, 2025, Proceedings, Part I},
pages = {110–124},
numpages = {15},
keywords = {Diabetics Retinopathy, Medical Image Classification, Deep Learning, Inceptions-ResnetV2, Image Preprocessing, Image Augmentation},
location = {Cambridge, United Kingdom}
}

@article{10.1016/j.artmed.2024.102803,
author = {El Habib Daho, Mostafa and Li, Yihao and Zeghlache, Rachid and Boit\'{e}, Hugo Le and Deman, Pierre and Borderie, Laurent and Ren, Hugang and Mannivanan, Niranchana and Lepicard, Capucine and Cochener, B\'{e}atrice and Couturier, Aude and Tadayoni, Ramin and Conze, Pierre-Henri and Lamard, Mathieu and Quellec, Gwenol\'{e}},
title = {DISCOVER: 2-D multiview summarization of Optical Coherence Tomography Angiography for automatic diabetic retinopathy diagnosis},
year = {2024},
issue_date = {Mar 2024},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {149},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2024.102803},
doi = {10.1016/j.artmed.2024.102803},
journal = {Artif. Intell. Med.},
month = mar,
numpages = {17},
keywords = {Diabetic retinopathy, Optical Coherence Tomography Angiography (OCTA), Deep learning, Interpretability}
}

@inproceedings{10.1007/978-3-031-95841-0_11,
author = {Boulaabi, Meher and Gader, Takwa Ben A\"{\i}cha and Echi, Afef Kacem and Bouraoui, Zied},
title = {Enhancing DR Classification with&nbsp;Swin Transformer and&nbsp;Shifted Window Attention},
year = {2025},
isbn = {978-3-031-95840-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-95841-0_11},
doi = {10.1007/978-3-031-95841-0_11},
abstract = {Diabetic retinopathy (DR) is a leading cause of blindness worldwide, underscoring the importance of early detection for effective treatment. However, automated DR classification remains challenging due to variations in image quality, class imbalance, and pixel-level similarities that hinder model training. To address these issues, we propose a robust preprocessing pipeline incorporating image cropping, Contrast-Limited Adaptive Histogram Equalization (CLAHE), and targeted data augmentation to improve model generalization and resilience. Our approach leverages the Swin Transformer, which utilizes hierarchical token processing and shifted window attention to efficiently capture fine-grained features while maintaining linear computational complexity. We validate our method on the Aptos and IDRiD datasets for multi-class DR classification, achieving accuracy rates of 89.65% and 97.40%, respectively. These results demonstrate the effectiveness of our model, particularly in detecting early-stage DR, highlighting its potential for improving automated retinal screening in clinical settings.},
booktitle = {Artificial Intelligence in Medicine: 23rd International Conference, AIME 2025, Pavia, Italy, June 23–26, 2025, Proceedings, Part II},
pages = {57–61},
numpages = {5},
keywords = {Diabetic Retinopathy, Swin Transformer, Medical Image Classification},
location = {Pavia, Italy}
}

@article{10.1016/j.engappai.2024.108353,
author = {Arsalan, Muhammad and Haider, Adnan and Park, Chanhum and Hong, Jin Seong and Park, Kang Ryoung},
title = {Multiscale triplet spatial information fusion-based deep learning method to detect retinal pigment signs with fundus images},
year = {2024},
issue_date = {Jul 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {133},
number = {PD},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2024.108353},
doi = {10.1016/j.engappai.2024.108353},
journal = {Eng. Appl. Artif. Intell.},
month = jul,
numpages = {21},
keywords = {Fundus images, Single spatial fusion network, Triplet spatial fusion network, deep learning, Computer-aided diagnosis}
}

@article{10.1016/j.engappai.2024.107994,
author = {Lei, Yongjia and Lin, Shuyuan and Li, Zhiying and Zhang, Yachao and Lai, Taotao},
title = {GNN-fused CapsNet with multi-head prediction for diabetic retinopathy grading},
year = {2024},
issue_date = {Jul 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {133},
number = {PA},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2024.107994},
doi = {10.1016/j.engappai.2024.107994},
journal = {Eng. Appl. Artif. Intell.},
month = jul,
numpages = {12},
keywords = {Capsule network, Graph neural network, Diabetic retinopathy grading, Feature fusion, Transfer learning}
}

@inproceedings{10.1145/3613905.3638180,
author = {Perera, Minoli},
title = {Enhancing Productivity Applications for People who are Blind using AI Assistants},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3638180},
doi = {10.1145/3613905.3638180},
abstract = {Productivity applications, such as word processors, spreadsheets and presentations, have become indispensable tools in the workplace, higher education, and personal settings. These applications are primarily accessed by blind users through the use of screen readers, which face numerous accessibility and usability challenges that hinder productivity and independence. My research aims to understand the severity of these challenges, explore the design space for potential AI-based Assistants, and propose design guidelines for more accessible and usable applications. The research employs a Design Thinking and Co-design approach, with the active involvement of blind users throughout the project. I have conducted a survey and a user study to gain insights into blind users’ experiences with productivity applications. The next phase of the project will delve into AI assistant capabilities and interaction techniques aimed at enhancing blind users’ productivity and independence.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {432},
numpages = {6},
keywords = {AI assistants, accessibility, assistive technology, blind, generative AI, productivity applications, screen readers, virtual assistants, voice assistants},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@article{10.1007/s11042-023-14419-9,
author = {Venkat Ragavan, S. and Tarun, A. H. and Yogeeshwar, S. and Vishwath Kumar, B. S. and Sofana Reka, S.},
title = {A realtime portable and accessible aiding system for the blind – a cloud based approach},
year = {2023},
issue_date = {May 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {82},
number = {13},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-023-14419-9},
doi = {10.1007/s11042-023-14419-9},
abstract = {With the rise of AI and Deep Learning technologies, it is now possible to give the visually impaired a sense of sight. This work intends to propose a system, which helps the blind to perceive their surrounding without any extra hand. The system harnesses the power of revolutionary cloud technology, cutting-edge artificial intelligence systems and state of the art language translation technologies for the inevitable cause of assisting the blind. This work mainly focusses on developing a simple gesture-controlled cloud based mobile application, which would allow them to capture their surroundings and help them to navigate through their surroundings in real-time. In this work a real case system architecture is proposed which would analyse the spatial reference of objects in the image and also the custom trained NLP engine generates a description that is narrated in their own native language which stands the unique aspect of the work. The proposed application proves to be a one-stop solution for the visually impaired with real case analysis. To strengthen the analysis of the work, results pertaining to the system architecture emphasising its real-time performance and accessibility are done.},
journal = {Multimedia Tools Appl.},
month = jan,
pages = {20641–20654},
numpages = {14},
keywords = {Image captioning, Object spatial analysis, Optical Character Recognition (OCR), Text to Speech (TTS), Multilingual voice, Language translation}
}

@inproceedings{10.1145/3589462.3589490,
author = {Avolio, Matteo and Fuduli, Antonio and Vocaturo, Eugenio and Zumpano, Ester},
title = {On Detection of Diabetic Retinopathy via Multiple Instance Learning},
year = {2023},
isbn = {9798400707445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589462.3589490},
doi = {10.1145/3589462.3589490},
abstract = {Diabetic Retinopathy (DR) is a complication of diabetes, caused by a damage to the blood vessels in the light-sensitive tissue of the retina. Since it affects the eyes, it can determine visual impairment or even blindness. Considering the number of diabetic patients worldwide, it is clear that effective screening of potential DR patients is of utmost importance. While direct and indirect ophthalmoscopy are the main methods for evaluating DR, artificial intelligence is on the rise in vision care. DR is detectable by analyzing data from patients’ fundus photographs, and is therefore a disease that artificial intelligence tools can effectively support. In this paper, we present some preliminary numerical results obtained in discriminating between eye fundi of healthy individuals and of people with severe diabetic retinopathy, by using a Multiple Instance Learning approach.},
booktitle = {Proceedings of the 27th International Database Engineered Applications Symposium},
pages = {170–176},
numpages = {7},
keywords = {Diabetic Retinopathy Detection, Image Processing, Multiple Instance Learning},
location = {Heraklion, Crete, Greece},
series = {IDEAS '23}
}

@inproceedings{10.1145/3591569.3591590,
author = {Corbaci, Tolga},
title = {Anterior Segment Eye Abnormality Detection},
year = {2023},
isbn = {9781450399616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591569.3591590},
doi = {10.1145/3591569.3591590},
abstract = {Vision is the most critical sense helping us to understand the world around us. Ophthalmology is an area of medicine that deals with the eye and vision. In many remote areas, people do not have access to ophthalmologists, and many go blind for preventable reasons. Awareness about eye health and early diagnosis is essential in eye health to prevent blindness. An artificial intelligence (AI) algorithm that can quickly detect eye disease is valuable and necessary. Anterior segment eye images are essential and easily obtained without additional equipment. In this study, I aimed to build an artificial intelligence algorithm to detect eye diseases from mobile photographs. I extracted and combined anterior segment eye photos from various publicly available datasets and labeled 3938 images as Normal (healthy) and 1094 images as Abnormal (unhealthy). I increased the data diversity by augmenting it with random flips and rotations: and then prepared it for AI training. I re-trained the algorithms trained in ImageNet Visual Recognition Challenge with the transfer learning method. I compared custom and pre-trained models. After evaluating the performance of the models with the test set, 98% accuracy and 97% F1 score were obtained with the Inception-ResNetV2 model.},
booktitle = {Proceedings of the 2023 8th International Conference on Intelligent Information Technology},
pages = {123–131},
numpages = {9},
location = {Da Nang, Vietnam},
series = {ICIIT '23}
}

@inproceedings{10.1145/3568162.3576952,
author = {Asakawa, Chieko},
title = {Interaction Techniques with a Navigation Robot for the Visually Impaired},
year = {2023},
isbn = {9781450399647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568162.3576952},
doi = {10.1145/3568162.3576952},
abstract = {Robotic technology has long been seen as a potential mobility aid for the visually impaired, and the latest advancements in sensing and artificial intelligence have made it a reality. Before starting the project, we researched the capabilities of, and user interactions with guide dogs. We discovered that the rich haptic interaction with the "handle" allows users to comfortably follow a guide dog.We then began designing a navigation robot with a handle that is small and slim enough to be followed in a similar body posture. Our goal was to make the robot natural and seamless in an urban environment, leading us to the concept of the AI Suitcase, a suitcase-shaped robot. In this presentation, after reviewing the capabilities of a guide dog, we will explain the concept, design, and implementation of the AI Suitcase and interaction techniques with the robot. We will also discuss the challenges of implementing such new robotic technologies in the real world, including technical challenges, infrastructure challenges, business models, and social acceptance.BIO: Dr. Chieko Asakawa is an IBM Fellow, working in the area of accessibility. Her initial contribution to the field started from braille digitalization and moved onto the Web accessibility, including the world's first practical voice browser. Since 2010, Chieko is focusing on real world accessibility to help the visually impaired to better comprehend their surroundings and navigate the world by the power of AI. Her latest project is the development of the AI suitcase, a navigation robot for the visually impaired. She has been serving as an IBM Distinguished Service Professor at Carnegie Mellon University since 2014. Dr. Asakawa started to concurrently serve as Chief Executive Director of the Japanese National Museum of Emerging Science and Innovation (Miraikan) since April 2021. In 2013, the government of Japan awarded the Medal of Honor with Purple Ribbon to Dr. Asakawa for her outstanding contributions to accessibility research. She was elected as a foreign member of the US National Academy of Engineering in 2017, inducted into the National Inventors Hall of Fame (NIHF) in 2019. She also received American Foundation for the Blind 2020 Helen Keller Achievement Award.},
booktitle = {Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1},
numpages = {1},
keywords = {assistive technologies, autonomous navigation robot, blind navigation, human-robot interaction},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@inproceedings{10.1145/3663548.3688498,
author = {Collins, Jazmin and Nicholson, Kaylah Myranda and Khadir, Yusuf and Stevenson Won, Andrea and Azenkot, Shiri},
title = {An AI Guide to Enhance Accessibility of Social Virtual Reality for Blind People},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3688498},
doi = {10.1145/3663548.3688498},
abstract = {The rapid growth of virtual reality (VR) has led to increased use of social VR platforms for interaction. However, these platforms lack adequate features to support blind and low vision (BLV) users, posing significant challenges in navigation, visual interpretation, and social interaction. One promising approach to these challenges is employing human guides in VR. However, this approach faces limitations with a lack of availability of humans to serve as guides, or the inability to customize the guidance a user receives from the human guide. We introduce an AI-powered guide to address these limitations. The AI guide features six personas, each offering unique behaviors and appearances to meet diverse user needs, along with visual interpretation and navigation assistance. We aim to use this AI guide in the future to help us understand BLV users’ preferences for guide forms and functionalities.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {130},
numpages = {5},
keywords = {VR, accessibility, blind, low vision},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3708557.3716329,
author = {Duarte, Carlos and Costa, Miguel and Seixas Pereira, Let\'{\i}cia and Guerreiro, Jo\~{a}o},
title = {Expanding Automated Accessibility Evaluations: Leveraging Large Language Models for Heading-Related Barriers},
year = {2025},
isbn = {9798400714092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708557.3716329},
doi = {10.1145/3708557.3716329},
abstract = {Ensuring digital resources are accessible to all users, including those with disabilities, is critical in today’s digital landscape. The growing volume of online content has intensified the need for automated accessibility evaluations to ensure compliance with accessibility guidelines. Yet, existing automated tools are limited in scope, being unable to identify many types of accessibility barriers. Recent advances in AI, particularly large language models (LLMs), offer opportunities to expand the range of automated accessibility checks. This work explores the ability of LLMs to detect accessibility barriers related to web page headings. We developed targeted prompts to help LLMs identify them and evaluated the effectiveness of three models – Llama 3.1, GPT-4o, and GPT-4o mini – in multiple versions of a reference webpage, each featuring different heading-related barriers. Findings reveal that model performance depends on barrier type: Llama 3.1 stands out at detecting structural issues like heading appropriateness and hierarchy, GPT-4o is better at identifying accessible names and semantic substitutions, while GPT-4o mini is the most versatile, handling complex structural modifications and labelling. This study highlights LLM’s potential in advancing web accessibility evaluation and bridging gaps in automated assessments.},
booktitle = {Companion Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {39–42},
numpages = {4},
keywords = {Web Accessibility, LLM, Accessibility Evaluation, Headings, Llama, GPT},
location = {
},
series = {IUI '25 Companion}
}

@inproceedings{10.1109/CoG51982.2022.9893624,
author = {Khan, Ibrahim and Nguyen, Thai Van and Dai, Xincheng and Thawonmas, Ruck},
title = {DareFightingICE Competition: A Fighting Game Sound Design and AI Competition},
year = {2022},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CoG51982.2022.9893624},
doi = {10.1109/CoG51982.2022.9893624},
abstract = {This paper presents a new competition-at the 2022 IEEE Conference on Games (CoG)- called DareFightingICE Competition. The competition has two tracks: a sound design track and an AI track. The game platform for this competition is also called DareFightingICE, a fighting game platform. DareFightingICE is a sound-design-enhanced version of FightingICE, used earlier in a competition at CoG until 2021 to promote artificial intelligence (AI) research in fighting games. In the sound design track, participants compete for the best sound design, given the default sound design of DareFightingICE as a sample, where we define a sound design as a set of sound effects combined with the source code that implements their timing-control algorithm. Participants of the AI track are asked to develop their AI algorithm that controls a character given only sound as the input (blind AI) to fight against their opponent; a sample deep-learning blind AI will be provided by us. Our means to maximize the synergy between the two tracks are also described. This competition serves to come up with effective sound designs for visually impaired players, a group in the gaming community which has been mostly ignored. To the best of our knowledge, DareFightingICE Competition is the first of its kind within and outside of CoG.},
booktitle = {2022 IEEE Conference on Games (CoG)},
pages = {478–485},
numpages = {8},
location = {Beijing, China}
}

@inproceedings{10.24963/ijcai.2024/185,
author = {Zhang, Hangtao and Hu, Shengshan and Wang, Yichen and Zhang, Leo Yu and Zhou, Ziqi and Wang, Xianlong and Zhang, Yanjun and Chen, Chao},
title = {Detector collapse: backdooring object detection to catastrophic overload or blindness in the physical world},
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/185},
doi = {10.24963/ijcai.2024/185},
abstract = {Object detection tasks, crucial in safety-critical systems like autonomous driving, focus on pinpointing object locations. These detectors are known to be susceptible to backdoor attacks. However, existing backdoor techniques have primarily been adapted from classification tasks, overlooking deeper vulnerabilities specific to object detection. This paper is dedicated to bridging this gap by introducing Detector Collapse (DC), a brand-new backdoor attack paradigm tailored for object detection. DC is designed to instantly incapacitate detectors (i.e., severely impairing detector's performance and culminating in a denial-of-service). To this end, we develop two innovative attack schemes: SPONGE for triggering widespread misidentifications and BLINDING for rendering objects invisible. Remarkably, we introduce a novel poisoning strategy exploiting natural objects, enabling DC to act as a practical backdoor in real-world environments. Our experiments on different detectors across several benchmarks show a significant improvement (∼10%-60% absolute and ∼2-7\texttimes{} relative) in attack efficacy over state-of-the-art attacks.},
booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
articleno = {185},
numpages = {9},
location = {Jeju, Korea},
series = {IJCAI '24}
}

@article{10.1007/s10462-022-10231-3,
author = {\"{O}zbay, Erdal},
title = {An active deep learning method for diabetic retinopathy detection in segmented fundus images using artificial bee colony algorithm},
year = {2022},
issue_date = {Apr 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {56},
number = {4},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-022-10231-3},
doi = {10.1007/s10462-022-10231-3},
abstract = {Retinal fundus image analysis (RFIA) is frequently used in diabetic retinopathy (DR) scans to determine the risk of blindness in diabetic patients. Ophthalmologists receive support from various RFIA programs to cope with the detection of visual impairments. In this article, active deep learning (ADL) using new multi-layer architecture for automatic recognition of DR stages is presented. In order to facilitate the detection of retinal lesions in the ADL system preprocessing, the image is segmented using the artificial bee colony (ABC) algorithm with a threshold value determined according to the results of the image histogram. Besides, a tag-efficient convolutional neural networks (CNN) architecture known as ADL-CNN has been developed to automatically extract segmented retinal features. This model has a two-stage process. In the first, images are selected to learn simple or complex retinal features using basic accuracy labels in the training examples. Second, useful masks are provided with key lesion features and segment areas of interest within the retinal image. Performance evaluation of the proposed ADL-CNN model is made by comparing the most advanced methods using the same dataset. The efficiency of the system is made by measuring statistical metrics such as classification accuracy (ACC), sensitivity (SE), specificity (SP), and F-measure. The ADL-CNN model applied to the EyePacs dataset containing 35,122 retinal images yielded 99.66% ACC, 93.76% SE, 96.71% SP, and 94.58% F-measure. In this respect, it can be said that the proposed method shows high performance in detecting DR lesions from various fundus images and determining the severity level.},
journal = {Artif. Intell. Rev.},
month = aug,
pages = {3291–3318},
numpages = {28},
keywords = {Artificial bee colony, Active deep learning, Convolutional neural network, Diabetic retinopathy, Image segmentation}
}

@inproceedings{10.1145/3750069.3750114,
author = {Deshmukh, Raghavendra},
title = {Toward Neurodivergent-Aware Productivity: A Systems andAI-Based Human-in-the-Loop Framework for ADHD-AffectedProfessionals},
year = {2025},
isbn = {9798400721021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3750069.3750114},
doi = {10.1145/3750069.3750114},
abstract = {Digital work environments in IT and knowledge-based sectors demand high levels of attention management, task juggling and self-regulation. For adults with Attention-Deficit/Hyperactivity Disorder (ADHD), these settings often amplify existing challenges such as time blindness, digital distraction, emotional reactivity and executive dysfunction. Such individuals prefer a low-touch, easy-to-use interventions to help them in their day-to-day activities. Conventional productivity tools fall short of supporting the cognitive variability and overload patterns experienced by neurodivergent professionals. This paper introduces a comprehensive framework that blends Systems Thinking, Human-in-the-Loop with Artificial Intelligence (AI), Machine Learning (ML) and privacy-first adaptive agents to support ADHD-affected users in managing digital work. At the heart of the solution is a voice-enabled productivity assistant that senses user behavior—tab usage, application focus, inactivity windows—using lightweight, on-device machine learning. These behavioral cues are analyzed in real-time to infer attention states and deliver adaptive nudges, reflective queries or accountability-based presence (body doubling) designed to co-regulate cognition without disruption. While technically grounded in AI/ML, the design is deeply influenced by Systems Thinking, viewing user attention as a product of dynamic feedback loops. This hybrid approach bridges behavioral sensing with cognitive inclusivity, offering a replicable model for adaptive, neurodivergent first decision support tools in high-distraction work environments.},
booktitle = {Proceedings of the 16th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {5},
numpages = {6},
keywords = {Neurodivergence, ADHD, Artificial Intelligence, Machine Learning, AI Assistant, Body Double, Accountability Partner, Privacy First},
location = {
},
series = {CHItaly '25}
}

@article{10.1007/s10462-022-10185-6,
author = {Selvachandran, Ganeshsree and Quek, Shio Gai and Paramesran, Raveendran and Ding, Weiping and Son, Le Hoang},
title = {Developments in the detection of diabetic retinopathy: a state-of-the-art review of computer-aided diagnosis and machine learning methods},
year = {2022},
issue_date = {Feb 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {56},
number = {2},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-022-10185-6},
doi = {10.1007/s10462-022-10185-6},
abstract = {The exponential increase in the number of diabetics around the world has led to an equally large increase in the number of diabetic retinopathy (DR) cases which is one of the major complications caused by diabetes. Left unattended, DR worsens the vision and would lead to partial or complete blindness. As the number of diabetics continue to increase exponentially in the coming years, the number of qualified ophthalmologists need to increase in tandem in order to meet the demand for screening of the growing number of diabetic patients. This makes it pertinent to develop ways to automate the detection process of DR. A computer aided diagnosis system has the potential to significantly reduce the burden currently placed on the ophthalmologists. Hence, this review paper is presented with the aim of summarizing, classifying, and analyzing all the recent development on automated DR detection using fundus images from 2015 up to this date. Such work offers an unprecedentedly thorough review of all the recent works on DR, which will potentially increase the understanding of all the recent studies on automated DR detection, particularly on those that deploys machine learning algorithms. Firstly, in this paper, a comprehensive state-of-the-art review of the methods that have been introduced in the detection of DR is presented, with a focus on machine learning models such as convolutional neural networks (CNN) and artificial neural networks (ANN) and various hybrid models. Each AI will then be classified according to its type (e.g. CNN, ANN, SVM), its specific task(s) in performing DR detection. In particular, the models that deploy CNN will be further analyzed and classified according to some important properties of the respective CNN architectures of each model. A total of 150 research articles related to the aforementioned areas that were published in the recent 5&nbsp;years have been utilized in this review to provide a comprehensive overview of the latest developments in the detection of DR.},
journal = {Artif. Intell. Rev.},
month = apr,
pages = {915–964},
numpages = {50},
keywords = {Diabetic retinopathy, Retina, Retinopathy, Computer-aided diagnosis, Machine learning, Fuzzy logic, Image processing, Fuzzy sets}
}

@inproceedings{10.1145/3663548.3688482,
author = {Figueira, Isabela and Cisneros, Josahandi M and Leachman, Molly and Pe\~{n}a, Elizabeth D and Branham, Stacy M},
title = {Informing Accessible Design of AI Literacy Apps through Practices of Blind Parents Reading with Sighted Children},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3688482},
doi = {10.1145/3663548.3688482},
abstract = {Being involved in teaching children to read, especially to decode (i.e., to translate written words to oral speech) is important and crucial to development. There is an increasing number of AI tools and educational apps aimed at teaching sighted children to read. However, many commercial literacy apps are not accessible, few studies of such apps exist, and studies rarely include blind parents as participants. Thus, we conducted an exploratory interview study with four blind parents about their decoding practices with their sighted children, literacy app accessibility, and AI decoding apps. We found that blind parents are motivated to teach their children literacy skills such as decoding; leverage (largely inaccessible) technology and techniques to support decoding; and want to be able to teach alongside the AI literacy apps and to make the AI teach like they do. We conclude with a discussion of design implications for AI literacy apps.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {75},
numpages = {5},
keywords = {AI, accessibility, blind, books, co-reading, literacy},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1109/CoG51982.2022.9893718,
author = {Van Nguyen, Thai and Dai, Xincheng and Khan, Ibrahim and Thawonmas, Ruck and Pham, Hai V.},
title = {A Deep Reinforcement Learning Blind AI in DareFightingICE},
year = {2022},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CoG51982.2022.9893718},
doi = {10.1109/CoG51982.2022.9893718},
abstract = {This paper presents a deep reinforcement learning agent (AI) that uses sound as the input on the DareFightingICE platform at the DareFightingICE Competition in IEEE CoG 2022. In this work, an AI that only uses sound as the input is called blind AI. While state-of-the-art AIs rely mostly on visual or structured observations provided by their environments, learning to play games from only sound is still new and thus challenging. We propose different approaches to process audio data and use the Proximal Policy optimization algorithm for our blind AI. We also propose to use our blind AI in evaluation of sound designs submitted to the competition and define two metrics for this task. The experimental results show the effectiveness of not only our blind AI but also the proposed two metrics.},
booktitle = {2022 IEEE Conference on Games (CoG)},
pages = {632–637},
numpages = {6},
location = {Beijing, China}
}

@article{10.1145/3764944.3764963,
author = {Guan, Wenkai},
title = {Green GPU: Integrating Carbon Metrics into GPU Manufacturing with Minimal Disruption},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/3764944.3764963},
doi = {10.1145/3764944.3764963},
abstract = {The increasing computational demands of artificial intelligence (AI), especially large language models (LLMs), have brought attention to their environmental impact, particularly operational carbon emissions. However, a significant yet often overlooked factor is the embodied carbon footprint resulting from the manufacturing of AI accelerators like GPUs. This ''hidden'' carbon is crucial and largely neglected in current sustainability efforts for AI hardware. Our analysis reveals that (i) this embodied carbon already constitutes a non-trivial 0.77% of GPT-3's and 2.18% of GPT- 4's operational reported emissions, highlighting a critical blind spot; (ii) while operational carbon of GPT-3 is dominated by compute-intensive kernels such as matrix multiplication and fully-connected layers, the challenge of embodied carbon lies within the complex manufacturing process itself. To effectively green AI, we argue that carbon metrics should be integrated into GPU manufacturing without disrupting existing optimized design and production workflows. We propose (i) linking carbon metrics with performance, power, and area (PPA) optimization metrics to align sustainability with efficiency, utilizing model-based and datadriven approaches; and (ii) using carbon metrics as supplementary constraints within Electronic Design Automation (EDA) tools, rather than disruptive replacements. This approach aims to harmonize sustainability with manufacturing practicality, promoting the development of environmentally responsible AI accelerators without compromising performance, thereby bridging a crucial gap in current green computing initiatives.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = aug,
pages = {87–89},
numpages = {3}
}

@inproceedings{10.24963/ijcai.2024/165,
author = {Xu, Kepeng and Xu, Li and He, Gang and Yu, Wenxin and Li, Yunsong},
title = {Beyond alignment: blind video face restoration via parsing-guided temporal-coherent transformer},
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/165},
doi = {10.24963/ijcai.2024/165},
abstract = {Multiple complex degradations are coupled in low-quality video faces in the real world. Therefore, blind video face restoration is a highly challenging ill-posed problem, requiring not only hallucinating high-fidelity details but also enhancing temporal coherence across diverse pose variations. Restoring each frame independently in a naive manner inevitably introduces temporal incoherence and artifacts from pose changes and keypoint localization errors. To address this, we propose the first blind video face restoration approach with a novel parsing-guided temporal-coherent transformer (PGTFormer) without pre-alignment. PGT-Former leverages semantic parsing guidance to select optimal face priors for generating temporally coherent artifact-free results. Specifically, we pre-train a temporal-spatial vector quantized auto-encoder on high-quality video face datasets to extract expressive context-rich priors. Then, the temporal parse-guided codebook predictor (TPCP) restores faces in different poses based on face parsing context cues without performing face pre-alignment. This strategy reduces artifacts and mitigates jitter caused by cumulative errors from face pre-alignment. Finally, the temporal fidelity regulator (TFR) enhances fidelity through temporal feature interaction and improves video temporal consistency. Extensive experiments on face videos show that our method outperforms previous face restoration baselines. The code will be released on https://github.com/kepengxu/PGTFormer.},
booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
articleno = {165},
numpages = {9},
location = {Jeju, Korea},
series = {IJCAI '24}
}

@proceedings{10.1145/3589335,
title = {WWW '24: Companion Proceedings of the ACM Web Conference 2024},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The ACM Web Conference 2024 held in person with virtual components on May 13-17, 2024, in Singapore. It is the 33rd edition of a series of yearly international conferences on the future directions of the Web. This is also the first time that the conference is held in Singapore, a sunny tropical island full of possibilities!The Web Conference, formerly known as International World Wide Web Conference and abbreviated as WWW, began the journey in 1994 at CERN. This conference has been the premier venue to present and discuss progress in research, development, standards and applications of topics related to the Web. The Web Conference has been the forum where some of the most fundamental Web technologies have been introduced, such as the Anatomy of a Large-Scale Web Search Engine in 1998, prefiguring Google, the EigenTrust algorithm in 2003, influencing reputation management distributed networks, and the YAGO knowledge base in 2007, anticipating the numerous developments over knowledge graphs.The conference has one unifying goal: to envision and create the future of the Web. In the last three decades, the scholars, researchers, practitioners, policymakers and commercial ventures came to this conference to present research results, discuss the evolution of Web, and the impact of Web technologies on society and culture. Today, the conference also embraces the elements of AI, LLM (Large Language Models) and multimodality into the Web. There will be more debate on how to integrate these elements into the Web technologies for the creation of a fair and inclusive Web development ecology.The program of this year's edition includes four distinguished keynote talks by the world-renowned experts: Bo Zhang and Jie Tang from Tsinghua University, Jon Kleinberg from Cornell University, Bin Liu from National University of Singapore, and Jeannie Marie Paterson from The University of Melbourne. The high-quality program is composed of 16 workshops, 20 tutorials, research tracks with oral presentations and poster sessions, industry tracks with oral and poster presentations, short paper poster and demo sessions, a resource track, a panel and a PhD symposium. In addition, we have two special tracks (Web4Good, History of the Web) and three special days (Online Trust and Safety Day, Large Language Model Day, and Health Day). Following the tradition, we have the Web4All as a co-located conference, highlighting the latest research results in Web accessibility. We also have a journal track built in coordination with the TWEB journal, which offers the opportunity for authors of the three selected journal papers to come and present their works at the conference.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3589334,
title = {WWW '24: Proceedings of the ACM Web Conference 2024},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The ACM Web Conference 2024 held in person with virtual components on May 13-17, 2024, in Singapore. It is the 33rd edition of a series of yearly international conferences on the future directions of the Web. This is also the first time that the conference is held in Singapore, a sunny tropical island full of possibilities!The Web Conference, formerly known as International World Wide Web Conference and abbreviated as WWW, began the journey in 1994 at CERN. This conference has been the premier venue to present and discuss progress in research, development, standards and applications of topics related to the Web. The Web Conference has been the forum where some of the most fundamental Web technologies have been introduced, such as the Anatomy of a Large-Scale Web Search Engine in 1998, prefiguring Google, the EigenTrust algorithm in 2003, influencing reputation management distributed networks, and the YAGO knowledge base in 2007, anticipating the numerous developments over knowledge graphs.The conference has one unifying goal: to envision and create the future of the Web. In the last three decades, the scholars, researchers, practitioners, policymakers and commercial ventures came to this conference to present research results, discuss the evolution of Web, and the impact of Web technologies on society and culture. Today, the conference also embraces the elements of AI, LLM (Large Language Models) and multimodality into the Web. There will be more debate on how to integrate these elements into the Web technologies for the creation of a fair and inclusive Web development ecology.The program of this year's edition includes four distinguished keynote talks by the world-renowned experts: Bo Zhang and Jie Tang from Tsinghua University, Jon Kleinberg from Cornell University, Bin Liu from National University of Singapore, and Jeannie Marie Paterson from The University of Melbourne. The high-quality program is composed of 16 workshops, 20 tutorials, research tracks with oral presentations and poster sessions, industry tracks with oral and poster presentations, short paper poster and demo sessions, a resource track, a panel and a PhD symposium. In addition, we have two special tracks (Web4Good, History of the Web) and three special days (Online Trust and Safety Day, Large Language Model Day, and Health Day). Following the tradition, we have the Web4All as a co-located conference, highlighting the latest research results in Web accessibility. We also have a journal track built in coordination with the TWEB journal, which offers the opportunity for authors of the three selected journal papers to come and present their works at the conference.},
location = {Singapore, Singapore}
}

@article{10.1016/j.jisa.2023.103547,
author = {Tang, Zongwei and Chai, Xiuli and Lu, Yang and Wang, Binjie and Tan, Yong},
title = {An end-to-end screen shooting resilient blind watermarking scheme for medical images},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {2214-2126},
url = {https://doi.org/10.1016/j.jisa.2023.103547},
doi = {10.1016/j.jisa.2023.103547},
journal = {J. Inf. Secur. Appl.},
month = aug,
numpages = {15},
keywords = {Regional medical systems, Medical data, Blind watermarking, Screen shooting, Neural network}
}

@inproceedings{10.1007/978-3-031-15086-9_32,
author = {Weerts, Sophie and Naous, Dana and Bouchikhi, Ma\'{e}va El and Clavien, Christine},
title = {AI Systems for Occupational Safety and Health: From Ethical Concerns to Limited Legal Solutions},
year = {2022},
isbn = {978-3-031-15085-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-15086-9_32},
doi = {10.1007/978-3-031-15086-9_32},
abstract = {Digital technologies in the workplace have undergone a remarkable evolution in recent years. Biosensors and wearables that enable data collection and analysis, through artificial intelligence (AI) systems became widespread in the working environment, whether private or public. These systems are heavily criticised in the media and in academia, for being used in aggressive algorithmic management contexts. However, they can also be deployed for more legitimate purposes such as occupational safety and health (OSH). Public authorities may promote them as tools for achieving public policy goals of OSH, and public employers may use them for improving employees’ health. Despite these positive aspects, we argue that the deployment of AI systems for OSH raises important issues regarding dual use, chilling effects and employment discrimination. We exemplify how these ethical concerns are raised in three realistic scenarios and elaborate on the legal responses to these issues based on European law. Our analysis highlights blind spots in which laws do not provide clear answers to relevant ethical concerns. We conclude that other avenues should be investigated to help determine whether it is legally and socially acceptable to deploy AI systems and eventually promote such tools as means to achieve the OSH public policy.},
booktitle = {Electronic Government: 21st IFIP WG 8.5 International Conference, EGOV 2022, Link\"{o}ping, Sweden, September 6–8, 2022, Proceedings},
pages = {499–514},
numpages = {16},
keywords = {AI, Occupational health and safety, Law, Ethics, Public sector},
location = {Link\"{o}ping, Sweden}
}

@article{10.1007/s11042-023-14797-0,
author = {Mohammed, Abdulhakeem O. and Hussein, Haval I. and Mstafa, Ramadhan J. and Abdulazeez, Adnan M.},
title = {A blind and robust color image watermarking scheme based on DCT and DWT domains},
year = {2023},
issue_date = {Sep 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {82},
number = {21},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-023-14797-0},
doi = {10.1007/s11042-023-14797-0},
abstract = {With the emergence of the Internet of Things (IoT) and many smart gadgets that support artificial intelligence, it is easier than ever to acquire, reproduce, and disseminate a large number of digital data. However, these great technologies have made it possible for intruders to easily violate issues related to copyright protection, identity theft, and privacy leakage. To address such issues, several approaches have been developed, among which image watermarking has been proven to be an ideal solution. In this paper, a blind watermarking approach for RGB color images based on joint Discrete Cosine Transform (DCT) and Discrete Wavelet Transform (DWT) is proposed. First, a self-adaptive color selecting strategy is used to select either the blue or green channel of the host image for embedding purpose. Subsequently, the selected color is subdivided into non-overlapping square blocks of size 4 \texttimes{} 4, and then DCT is applied to each block. Afterward, the DC values obtained from each block are decomposed into four sub-bands using DWT, and then the LH middle frequency sub-band is further decomposed into four sub-bands using DWT. Lastly, the LH1 obtained from LH is utilized for watermark embedding. To provide security to the proposed approach, the watermark image is encrypted before embedding using a chaotic sequence originated from a logistic map method. Experimental results reveal that the proposed approach not only enhances watermark invisibility but also provide excellent watermark robustness, meeting the main requirements of image watermarking.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {32855–32881},
numpages = {27},
keywords = {Watermarking, Discrete cosine transform (DCT), Discrete wavelet transform (DWT), Chaotic logistic map}
}

@inproceedings{10.1145/3706599.3721194,
author = {Killough, Daniel and Feng, Justin and Dyava, Rithvik and Ching, Zheng Xue "ZX" and Wang, Daniel and Tian, Yapeng and Zhao, Yuhang},
title = {Demonstration of VRSight: AI-Driven Real-Time Descriptions to Enhance VR Accessibility for Blind People},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3721194},
doi = {10.1145/3706599.3721194},
abstract = {Virtual Reality (VR) technologies are becoming increasingly mainstream. However, VR headsets largely rely on visuals to convey information, making them inaccessible to blind and low vision people (BLV). Therefore, we present VRSight, an end-to-end system using state-of-the-art machine learning models to empower BLV to more effectively interact with VR environments. VRSight applies to any VR application post-hoc, not needing developer implementation. VRSight uses our fine-tuned object detection model trained on VR scenes to recognize virtual elements in each frame, like interactable objects, signs, and seating areas. VRSight automatically sonifies important objects like safety information while including a keyboard-based menu system for users to query other objects. All objects are sonified corresponding to their spatial locations in VR with sound effects and text-to-speech in an application-relevant tone. VRSight increases VR accessibility for BLV, serving as a foundation for more inclusive VR design and encourages broader participation in virtual social spaces.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {724},
numpages = {5},
keywords = {Virtual Reality, Artificial Intelligence, Accessibility, Blind and Low Vision, Computer Vision},
location = {
},
series = {CHI EA '25}
}

@article{10.1613/jair.1.14630,
author = {Lamperti, Gianfranco and Trerotola, Stefano and Zanella, Marina and Zhao, Xiangfu},
title = {Sequence-Oriented Diagnosis of Discrete-Event Systems},
year = {2024},
issue_date = {Jan 2024},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {78},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.14630},
doi = {10.1613/jair.1.14630},
abstract = {Model-based diagnosis has always been conceived as set-oriented, meaning that a candidate is a set of faults, or faulty components, that explains a collection of observations. This perspective applies equally to both static and dynamical systems. Diagnosis of discrete-event systems (DESs) is no exception: a candidate is traditionally a set of faults, or faulty events, occurring in a trajectory of the DES that conforms with a given sequence of observations. As such, a candidate does not embed any temporal relationship among faults, nor does it account for multiple occurrences of the same fault. To improve diagnostic explanation and support decision making, a sequence-oriented perspective to diagnosis of DESs is presented, where a candidate is a sequence of faults occurring in a trajectory of the DES, called a fault sequence. Since a fault sequence is possibly unbounded, as the same fault may occur an unlimited number of times in the trajectory, the set of (output) candidates may be unbounded also, which contrasts with set-oriented diagnosis, where the set of candidates is bounded by the powerset of the domain of faults. Still, a possibly unbounded set of fault sequences is shown to be a regular language, which can be defined by a regular expression over the domain of faults, a property that makes sequence-oriented diagnosis feasible in practice. The task of monitoring-based diagnosis is considered, where a new candidate set is generated at the occurrence of each observation. The approach is based on three different techniques: .1/ blind diagnosis, with no compiled knowledge, .2/ greedy diagnosis, with total knowledge compilation, and .3/ lazy diagnosis, with partial knowledge compilation. By knowledge we mean a data structure slightly similar to a classical DES diagnoser, which can be generated (compiled) either entirely offline (greedy diagnosis) or incrementally online (lazy diagnosis). Experimental evidence suggests that, among these techniques, only lazy diagnosis may be viable in non-trivial application domains.},
journal = {J. Artif. Int. Res.},
month = jan,
numpages = {73}
}

@inproceedings{10.1145/3594315.3594647,
author = {Yin, Yuting and Li, Linge and Guan, Jingwei and Li, Zhiheng},
title = {DAGP-Face Restorer: Blind Face Restoration Network with Domain-aligned Generative Prior},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594315.3594647},
doi = {10.1145/3594315.3594647},
abstract = {Blind face restoration is an extremely challenging task since it often relies on suitable reference priors, which are unavailable in real-world scenarios. Generative prior encapsulated in pre-trained face generator has shown its effectiveness in providing rich and realistic facial details. However, trained on a specific face dataset, face GAN prior mainly focus on face region but limited background information and local details. In this work, we propose a framework named DAGP-Face Restorer to perform restoration procedure by fusing multi-scale generative prior features and convolutional features extracted from degraded images. We first train a domain-alignment-guided encoder to project degraded inputs into the latent space aligning with that of high quality images, which allows our network to provide generative prior with better quality. Another encoder-decoder framework with feature fusion blocks is designed to extract convolutional features and fuse them with generative priors to output the final results. Experiment results show that, with the carefully design and fusion of two features, our model can better take into account restoration of face regions as well as image backgrounds and achieves superior performance in recovering faithful details and retaining fidelity.},
booktitle = {Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
pages = {222–229},
numpages = {8},
keywords = {GAN inversion, blind face restoration, feature fusion, generative prior},
location = {Tianjin, China},
series = {ICCAI '23}
}

@inproceedings{10.1145/3653081.3653218,
author = {Fang, Xiaoya and He, Weisen and Yu, Han and Wu, Shuolei and Zhang, Ruxing and Wu, Jiajia},
title = {Guided Blind Guidance APP Based on Path Planning and Obstacle Detection},
year = {2024},
isbn = {9798400716485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653081.3653218},
doi = {10.1145/3653081.3653218},
abstract = {This article proposes a new guiding software based on advanced route planning and object detection technology that aims to increase the mobility and convenience of visually impaired users dramatically. This APP use deep learning algorithms to efficiently recognize and locate surrounding impediments, allowing users to walk more confidently and safely. In addition to its strong obstacle recognition function, the guidance APP includes a huge data analysis remote navigation function. This program can not only connect to existing navigation systems, but also design the safest and most efficient trip paths for users by analyzing a significant quantity of multidimensional travel and map data. Furthermore, users can save frequently visited locations for future travel, improving navigation stability and accuracy. This innovation not only helps visually challenged individuals more independent, but it also boosts their self-esteem. This navigation APP is a great marriage of technology and human care, delivering a more intelligent, convenient, and dependable navigation solution for visually impaired people, allowing them to better integrate into society and live more independent and independent lives.},
booktitle = {Proceedings of the 2023 5th International Conference on Internet of Things, Automation and Artificial Intelligence},
pages = {813–817},
numpages = {5},
location = {Nanchang, China},
series = {IoTAAI '23}
}

@inproceedings{10.24963/ijcai.2024/143,
author = {Suin, Maitreya and Chellappa, Rama},
title = {CLR-Face: conditional latent refinement for blind face restoration using score-based diffusion models},
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/143},
doi = {10.24963/ijcai.2024/143},
abstract = {Recent generative methods have shown promising blind face restoration performance. They usually project the degraded images to the latent space and then decode high-quality faces either by single-stage latent optimization or directly from the encoding. Generating fine-grained facial details faithful to inputs remains challenging. Most existing methods produce either overly smooth outputs or alter the identity. This could be attributed to the typical trade-off between quality and resolution in the latent space. If the latent is highly compressed, the decoded output is more robust to degradations but shows worse fidelity. On the other hand, a more flexible latent space can capture intricate details better, but is extremely difficult to optimize for highly degraded faces. We introduce a diffusion-based-prior inside a VQGAN architecture that focuses on learning the distribution over uncorrupted latent embeddings. We iteratively recover the clean embedding conditioning on the degraded counterpart. Furthermore, to ensure the reverse diffusion trajectory does not deviate from the underlying identity, we train a separate Identity Recovery Network and use its output to constrain the reverse diffusion. Specifically, using a learnable latent mask, we add gradients from a face-recognition network to a subset of latent features that correlates with the finer identity-related details in the pixel space, leaving the other features untouched. Disentanglement between perception and fidelity in the latent space allows us to achieve the best of both worlds. We perform extensive evaluations on multiple real and synthetic datasets to validate our approach.},
booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
articleno = {143},
numpages = {9},
location = {Jeju, Korea},
series = {IJCAI '24}
}

@article{10.1007/s13748-022-00292-4,
author = {Elmoufidi, Abdelali and Skouta, Ayoub and Jai-andaloussi, Said and Ouchetto, Ouail},
title = {Deep multiple instance learning for automatic glaucoma prevention and auto-annotation using color fundus photography},
year = {2022},
issue_date = {Dec 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {4},
url = {https://doi.org/10.1007/s13748-022-00292-4},
doi = {10.1007/s13748-022-00292-4},
abstract = {In the area of ophthalmology, glaucoma affects an increasing number of people. It is a major cause of blindness. Early detection prevents severe ocular complications such as glaucoma, cystoid macular edema, or diabetic proliferative retinopathy. Intelligent systems are proven to be beneficial for the assessment of glaucoma. In this paper, we describe an approach to automate the diagnosis of glaucoma disease, based on color funds photography using deep learning. The setup of the proposed framework is ordered as follows: The bidimensional empirical mode decomposition (BEMD) algorithm is applied to decompose the ROI to components (BIMFs + residue). CNN architecture VGG19 is implemented to extract features from decomposed BEMD components. The features obtained are the input parameters of the implemented classifier based on full connect layers and softmax. To train the built model, we have used the public dataset RIM-ONE DL. To test our models, we have used a part of RIM-ONE DL and REFUGE. The average obtained sensitivity, specificity, accuracy and AUC rates are, respectively, 99.14%, 99.19%, 99.13%, 99.09% and 99.17%, 99.24%, 99.20%, 99.18% in RIM-ONE DL and REFUGE dataset. The experimental results obtained from different datasets demonstrate the efficiency and robustness of the proposed approach. A comparison with some recent previous work in the literature has shown a significant advancement in our proposal.},
journal = {Prog. in Artif. Intell.},
month = dec,
pages = {397–409},
numpages = {13},
keywords = {Computer-aided diagnosis, Convolutional neural networks (CNNs), Deep learning, Glaucoma, Medical imaging, Machine learning, Ophthalmology}
}

@article{10.1007/s10209-025-01214-6,
author = {Etxabe-Antia, Amaia and Beitia-Amondarain, Amaia and Gonz\'{a}lez de Heredia-L\'{o}pez de Sabando, Arantxa and Justel-Lozano, Daniel},
title = {Characterisation of accessibility guidelines for digital technologies},
year = {2025},
issue_date = {Aug 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {3},
issn = {1615-5289},
url = {https://doi.org/10.1007/s10209-025-01214-6},
doi = {10.1007/s10209-025-01214-6},
abstract = {In contemporary society, technological advances play a crucial role in daily life. Digital inclusion is essential to ensure that the maximum number of people benefit from the opportunities these technologies offer. However, this inclusion is not universally guaranteed, as technology can either act as a barrier or a facilitator depending on its design. This study aims to identify the characteristics of accessibility recommendations for digital technologies through a systematic literature review. Analysing 32 peer-reviewed articles, we identified 596 accessibility guidelines across various technologies, including ICT, Web/apps, immersive, robotics, and artificial intelligence. The findings highlight significant differences in accessibility recommendations between established and emerging technologies. Established technologies, such as ICT and Web/apps, often rely on widely recognized guidelines like WCAG, ADA, and Sect.&nbsp;508, while emerging technologies tend to adopt user-centred design approaches. This difference underscores the need for more robust and specific accessibility guidelines tailored to the unique challenges posed by each technology. The results identify research gaps and propose future lines of study to enhance accessibility in the rapidly evolving technological landscape. We anticipate that this contribution will lead to the development of more accessible technological solutions, thereby promoting digital inclusion for everyone.},
journal = {Univers. Access Inf. Soc.},
month = apr,
pages = {2105–2125},
numpages = {21},
keywords = {Accessibility guidelines, Inclusive design, Digital technologies, Systematic literature review (SLR), Digital inclusion, User-centred design}
}

@inproceedings{10.1145/3489525.3511676,
author = {Chetoui, Sofiane and Chen, Michael and Golas, Abhinav and Hijaz, Farrukh and Belouchrani, Adel and Reda, Sherief},
title = {Alternating Blind Identification of Power Sources for Mobile SoCs},
year = {2022},
isbn = {9781450391436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489525.3511676},
doi = {10.1145/3489525.3511676},
abstract = {The need for faster Systems on Chip (SoCs) has accelerated scaling trends, leading to a considerable power density increase and raising critical power and thermal challenges. The ability to measure power consumption of different hardware units is essential for the operation and improvement of mobile SoCs, as well as the enhancement of the power efficiency of the software that runs on them. SoCs are usually enabled with embedded thermal sensors to measure the temperature at the hardware unit level; however, they lack the ability to sense the power. In this paper we introduce an Alternating Blind Identification of Power sources (Alternating-BPI), a technique that accurately estimates the power consumption of individual SoC units without the use of any design based models. The proposed technique uses a novel approach to blindly identify the sources of power consumption, by relying only on the measurements from the embedded thermal sensors and the total power consumption. The accuracy and applicability of the proposed technique was verified using simulation and experimental data. Alternating-BPI is able to estimate the power at the SoC hardware unit level with up to 98.1% accuracy. Furthermore, we demonstrate the applicability of the proposed technique on a commercial SoC and provide a fine-grain analysis of the power profiles of CPU and GPU Apps, as well as Artificial Intelligence (AI), Virtual Reality (VR) and Augmented Reality (AR) Apps. Additionally, we demonstrate that the proposed technique could be used to estimate the power consumption per-process by relying on the estimated per-unit power numbers and per-unit hardware utilization numbers. The analysis provided by the proposed technique gives useful insights about the power efficiency of the different hardware units on a state-of-the-art commercial SoC.},
booktitle = {Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering},
pages = {153–164},
numpages = {12},
keywords = {mobile socs, power characterization, power consumption, power efficiency, power modeling},
location = {Beijing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3502871.3502896,
author = {Li, Yi and Xiao, Chenfeng and Gu, Juanjuan and Peng, Rongchao},
title = {Development of a Wearable Smart Guide Device for the Blind},
year = {2022},
isbn = {9781450385077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502871.3502896},
doi = {10.1145/3502871.3502896},
abstract = {We developed a wearable smart guide device that can recognize the traffic lights and the crosswalk to help the blind to cross the road. The device uses a camera to real-time capture the road environment images and transmit them to Raspberry Pi, which runs a Python algorithm to detect the traffic lights and the crosswalk. The algorithm, on the one hand, partitions the captured image with a threshold in HSV color space, and then calculates its similarity with a pre-set template of the traffic light using template matching, and finally determines whether the traffic light exists and figures out its position if it exists; on the other hand, detects the edges in the captured image using Canny algorithm, and then finds out the straight lines in the edges using Hough Transform, and finally determines whether the crosswalk exists and figures out its position if it exists. The results of the recognized traffic lights or crosswalk are then converted to speeches through text-to-speech engines in Baidu Artificial Intelligence Cloud, and played to the user via a loudspeaker or Bluetooth earphones. After tested in different real situations, the developed device ran well and successfully recognized the traffic lights and the crosswalks. We think it is useful to bring convenience for the blind people walking outside.},
booktitle = {Proceedings of the 2021 8th International Conference on Biomedical and Bioinformatics Engineering},
pages = {157–162},
numpages = {6},
keywords = {Edge detection, OpenCV, Raspberry Pi, Smart guide device, Template matching},
location = {Kyoto, Japan},
series = {ICBBE '21}
}

@inproceedings{10.1145/3706598.3714096,
author = {Li, Chaoyu and Padmanabhuni, Sid and Cheema, Maryam S and Seifi, Hasti and Fazli, Pooyan},
title = {VideoA11y: Method and Dataset for Accessible Video Description},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714096},
doi = {10.1145/3706598.3714096},
abstract = {Video descriptions are crucial for blind and low vision (BLV) users to access visual content. However, current artificial intelligence models for generating descriptions often fall short due to limitations in the quality of human annotations within training datasets, resulting in descriptions that do not fully meet BLV users’ needs. To address this gap, we introduce VideoA11y, an approach that leverages multimodal large language models (MLLMs) and video accessibility guidelines to generate descriptions tailored for BLV individuals. Using this method, we have curated VideoA11y-40K, the largest and most comprehensive dataset of 40,000 videos described for BLV users. Rigorous experiments across 15 video categories, involving 347 sighted participants, 40 BLV participants, and seven professional describers, showed that VideoA11y descriptions outperform novice human annotations and are comparable to trained human annotations in clarity, accuracy, objectivity, descriptiveness, and user satisfaction. We evaluated models on VideoA11y-40K using both standard and custom metrics, demonstrating that MLLMs fine-tuned on this dataset produce high-quality accessible descriptions. Code and dataset are available at https://people-robots.github.io/VideoA11y/.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1055},
numpages = {29},
keywords = {Video Accessibility, Video Description, Video Understanding, Blind and Low Vision Users, Multimodal Large Language Models},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1609/aaai.v39i23.34674,
author = {Nafar, Aliakbar and Venable, Kristen Brent and Kordjamshidi, Parisa},
title = {Reasoning over uncertain text by generative large language models},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i23.34674},
doi = {10.1609/aaai.v39i23.34674},
abstract = {This paper considers the challenges Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values. This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning. To deal with this problem, we introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs. We use BLInD to find out the limitations of LLMs for tasks involving probabilistic reasoning. In addition, we present several prompting strategies that map the problem to different formal representations, including Python code, probabilistic algorithms, and probabilistic logical programming. We conclude by providing an evaluation of our methods on BLInD and an adaptation of a causal reasoning question-answering dataset. Our empirical results highlight the effectiveness of our proposed strategies for multiple LLMs. Code and Dataset — https://github.com/HLR/BLInD, Extended Version — https://arxiv.org/abs/2402.09614},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2776},
numpages = {10},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1145/3573942.3573997,
author = {Yang, Mingyu and Xu, Dongming},
title = {Study on Variable Step Size Blind Equalization Algorithm Based on CMA},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3573997},
doi = {10.1145/3573942.3573997},
abstract = {The inter-symbol interference caused by channel distortion in the communication process seriously affects the communication quality, and this problem is often solved by equalization technology. The principle of the traditional blind equalization constant modulus algorithm (CMA) with fixed step is introduced, and the problem of fast convergence speed and small steady-state error is analyzed by simulation. In order to solve this problem, a blind equalization algorithm with variable step size based on CMA is proposed. In order to solve the problem of fast convergence speed and small steady-state error, the CMA algorithm is improved and the principle of the improved algorithm is described, and the influence of the step on the performance of the algorithm is analyzed. Finally, the simulation experiment proves that the improved algorithm can speed up the convergence speed and keep a small steady-state error at the same time.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {1093–1097},
numpages = {5},
keywords = {Constant modulus algorithm, Convergence speed, Equalization technology, Inter-symbol interference, Steady-state error, Variable step size},
location = {Xiamen, China},
series = {AIPR '22}
}

@inproceedings{10.1145/3643479.3662056,
author = {Vu, Sinh Trong and Truong, Huong Thu and Do, Oanh Tien and Le, Tu Anh and Mai, Tai Tan},
title = {A ChatGPT-based approach for questions generation in higher education},
year = {2024},
isbn = {9798400705472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643479.3662056},
doi = {10.1145/3643479.3662056},
abstract = {Large language models have been widely applied in many aspects of real life, bringing significant efficiency to businesses and offering distinctive user experiences. In this paper, we focus on exploring the application of ChatGPT, a chatbot based on a large language model, to support higher educator in generating quiz questions and assessing learners. Specifically, we explore interactive prompting patterns to design an optimal AI-powered question bank creation process. The generated questions are evaluated through a "Blind test" survey sent to various stakeholders including lecturers and learners. Initial results at the Banking Academy of Vietnam are relatively promising, suggesting a potential direction to streamline the time and effort involved in assessing learners at higher education institutes.},
booktitle = {Proceedings of the 1st ACM Workshop on AI-Powered Q&amp;A Systems for Multimedia},
pages = {13–18},
numpages = {6},
keywords = {ChatGPT, Large language model, question generation},
location = {Phuket, Thailand},
series = {AIQAM '24}
}

@inproceedings{10.1145/3641554.3701841,
author = {Aljedaani, Wajdi and Eler, Marcelo Medeiros and Parthasarathy, P D},
title = {Enhancing Accessibility in Software Engineering Projects with Large Language Models (LLMs)},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701841},
doi = {10.1145/3641554.3701841},
abstract = {Digital accessibility ensures that digital products and services are usable by a diverse range of users, regardless of their physical or cognitive abilities. While numerous standards and guidelines have been established to aid developers in creating accessible content, studies reveal a persistent lack of accessibility in many web and mobile applications. This gap is often attributed to barriers such as lack of awareness, insufficient knowledge, absence of specific requirements, time constraints, and lack of executive support. In this context, we aim to address the lack of awareness and knowledge challenges by proposing a hands-on approach that leverages the capabilities of Large Language Models (LLMs) like ChatGPT to enhance students' accessibility awareness, knowledge, and practical skills. We engaged software engineering students in tasks involving website development and accessibility evaluation using checker tools, and we utilized ChatGPT 3.5 to fix identified accessibility issues. Our findings suggest that practical assignments significantly enhance learning outcomes, as interactions with LLMs allow students to develop a deeper understanding of accessibility concepts. This approach not only reinforces theoretical knowledge but also highlights the real-world impact of their work. The results indicate that combining practical assignments with AI-driven support effectively improves students' proficiency in web accessibility.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {25–31},
numpages = {7},
keywords = {chatgpt 3.5, digital accessibility, large language models, llms, project based learning, software engineering, wcag},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1609/aaai.v39i20.35392,
author = {van Delft, Bastien and Martorella, Tommaso and Alahi, Alexandre},
title = {CODE: confident ordinary differential editing},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i20.35392},
doi = {10.1609/aaai.v39i20.35392},
abstract = {Conditioning image generation facilitates seamless editing and the creation of photorealistic images. However, conditioning on noisy or Out-of-Distribution (OoD) images poses significant challenges, particularly in balancing fidelity to the input and realism of the output. We introduce Confident Ordinary Differential Editing (CODE), a novel approach for image synthesis that effectively handles OoD guidance images. Utilizing a diffusion model as a generative prior, CODE enhances images through score-based updates along the probability-flow Ordinary Differential Equation (ODE) trajectory. This method requires no task-specific training, no handcrafted modules, and no assumptions regarding the corruptions affecting the conditioning image. Our method is compatible with any diffusion model. Positioned at the intersection of conditional image generation and blind image restoration, CODE operates in a fully blind manner, relying solely on a pre-trained generative model. Our method introduces an alternative approach to blind restoration: instead of targeting a specific ground truth image based on assumptions about the underlying corruption, CODE aims to increase the likelihood of the input image while maintaining fidelity. This results in the most probable in-distribution image around the input. Our contributions are twofold. First, CODE introduces a novel editing method based on ODE, providing enhanced control, realism, and fidelity compared to its SDE-based counterpart. Second, we introduce a confidence interval-based clipping method, which improves CODE's effectiveness by allowing it to disregard certain pixels or information, thus enhancing the restoration process in a blind manner. Experimental results demonstrate CODE's effectiveness over existing methods, particularly in scenarios involving severe degradation or OoD inputs. Website — https://vita-epfl.github.io/CODE/, Code — https://github.com/vita-epfl/CODE/, Main and Appendix — https://arxiv.org/pdf/2408.12418},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2338},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1007/978-3-031-73748-0_14,
author = {Thrasher, Jacob and Amireskandari, Annahita and Gyawali, Prashnna},
title = {Enhancing Retinal Disease Classification from&nbsp;OCTA Images via&nbsp;Active Learning Techniques},
year = {2024},
isbn = {978-3-031-73747-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-73748-0_14},
doi = {10.1007/978-3-031-73748-0_14},
abstract = {Eye diseases are common in older Americans and can lead to decreased vision and blindness. Recent advancements in imaging technologies allow clinicians to capture high-quality images of the retinal blood vessels via Optical Coherence Tomography Angiography (OCTA), which contain vital information for diagnosing these diseases and expediting preventative measures. OCTA provides detailed vascular imaging as compared to the solely structural information obtained by common OCT imaging. Although there have been considerable studies on OCT imaging, there have been limited to no studies exploring the role of artificial intelligence (AI) and machine learning (ML) approaches for predictive modeling with OCTA images. In this paper, we explore the use of deep learning to identify eye disease in OCTA images. However, due to the lack of labeled data, the straightforward application of deep learning doesn’t necessarily yield good generalization. To this end, we utilize active learning to select the most valuable subset of data to train our model. We demonstrate that active learning subset selection greatly outperforms other strategies, such as inverse frequency class weighting, random undersampling, and oversampling, by up to 49% in F1 evaluation. The full code can be found here: .},
booktitle = {Data Engineering in Medical Imaging: Second MICCAI Workshop, DEMI 2024, Held in Conjunction with MICCAI 2024, Marrakesh, Morocco, October 10, 2024, Proceedings},
pages = {134–143},
numpages = {10},
keywords = {Active Learning, Optical Coherence Tomography Angiography (OCTA), Retinal Disease},
location = {Marrakesh, Morocco}
}

@inproceedings{10.1145/3613904.3642211,
author = {Gonzalez Penuela, Ricardo E and Collins, Jazmin and Bennett, Cynthia and Azenkot, Shiri},
title = {Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642211},
doi = {10.1145/3613904.3642211},
abstract = {"Scene description" applications that describe visual content in a photo are useful daily tools for blind and low vision (BLV) people. Researchers have studied their use, but they have only explored those that leverage remote sighted assistants; little is known about applications that use AI to generate their descriptions. Thus, to investigate their use cases, we conducted a two-week diary study where 16 BLV participants used an AI-powered scene description application we designed. Through their diary entries and follow-up interviews, users shared their information goals and assessments of the visual descriptions they received. We analyzed the entries and found frequent use cases, such as identifying visual features of known objects, and surprising ones, such as avoiding contact with dangerous objects. We also found users scored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for satisfaction and 2.43 out of 4 (SD=1.16) for trust, showing that descriptions still need significant improvements to deliver satisfying and trustworthy experiences. We discuss future opportunities for AI as it becomes a more powerful accessibility tool for BLV users.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {901},
numpages = {21},
keywords = {AI, Assistive Technology, Blind and Low Vision People, Computer Vision, Diary Study, Scene Description, Use cases},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1109/TLT.2022.3224121,
author = {Pereira, Filipe Dwan and Rodrigues, Luiz and Henklain, Marcelo Henrique Oliveira and Freitas, Hermino and Oliveira, David Fernandes and Cristea, Alexandra I. and Carvalho, Leandro and Isotani, Seiji and Benedict, Aileen and Dorodchi, Mohsen and de Oliveira, Elaine Harada Teixeira},
title = {Toward Human–AI Collaboration: A Recommender System to Support CS1 Instructors to Select Problems for Assignments and Exams},
year = {2023},
issue_date = {June 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {3_Part_2},
issn = {1939-1382},
url = {https://doi.org/10.1109/TLT.2022.3224121},
doi = {10.1109/TLT.2022.3224121},
abstract = {Programming online judges (POJs) have been increasingly used in CS1 classes, as they allow students to practice and get quick feedback. For instructors, it is a useful tool for creating assignments and exams. However, selecting problems in POJs is time consuming. First, problems are generally not organized based on topics covered in the CS1 syllabus. Second, assessing whether problems require similar effort to be completed and map onto the same topic is a subjective and expert-dependent task. The difficulty increases if the instructor must create variations of these assessments, e.g., to avoid plagiarism. Thus, here, we research how to support CS1 instructors in the task of selecting problems, to compose one-size-fits-all or personalized assignments/exams. Our solution is to propose a novel intelligent recommender system, based on a fine-grained data-driven analysis of the students' effort on solving problems in the integrated development environment of a POJ system, and automatic detection of topics for CS1 problems, based on problem descriptions. Data collected from 2714 students are processed to support, via our artificial intelligence (AI) method recommendations, the instructors' decision-making process. We evaluated our method against the state of the art in a simple blind experiment with CS1 instructors (&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$N =$&lt;/tex-math&gt;&lt;/inline-formula&gt; 35). Results show that our recommendations are 88% accurate, surpassing our baseline (&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$p&lt;$&lt;/tex-math&gt;&lt;/inline-formula&gt; 0.05). Finally, our work paves the way for novel POJ smart learning environments, wherein instructors define learning tasks (assignments/exams) supported by AI.},
journal = {IEEE Trans. Learn. Technol.},
month = jun,
pages = {457–472},
numpages = {16}
}

@inproceedings{10.1007/978-981-99-8850-1_18,
author = {Zhang, Shuai and Liu, Yutao},
title = {Multi-scale Transformer with&nbsp;Decoder for&nbsp;Image Quality Assessment},
year = {2023},
isbn = {978-981-99-8849-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-8850-1_18},
doi = {10.1007/978-981-99-8850-1_18},
abstract = {Blind image quality assessment (BIQA) is of great significance in image processing field. However, due to diverse image content and complex types of distortions, the issue of BIQA has not been fully resolved. To address this issue more effectively, in this paper, we propose a framework based on Vision Transformer for BIQA called MSIQT. This model aims to extract image features more effectively and achieve a more accurate representation of quality. Specifically, at the input end, we adopt a multi-scale input approach to enrich the image features and utilize ResNet-50 for feature extraction. At the output end, a decoder is introduced to interpret quality-aware vectors obtained from image features. Experiments on four image quality assessment datasets prove that the proposed method outperforms or is comparable to state-of-the-art approaches.},
booktitle = {Artificial Intelligence: Third CAAI International Conference, CICAI 2023, Fuzhou, China, July 22–23, 2023, Revised Selected Papers, Part I},
pages = {220–231},
numpages = {12},
keywords = {blind image quality assessment, vision transformer, multi-scale},
location = {Fuzhou, China}
}

@inproceedings{10.1007/978-3-031-81596-6_29,
author = {Oishe, Mahjabin Rahman and Hasan, S. M. Mahedy and Zibran, Minhaz F.},
title = {Breaking the&nbsp;Mold: ViT-CNN Fusion for&nbsp;Enhanced Glaucoma Prediction in&nbsp;OCT Images},
year = {2025},
isbn = {978-3-031-81595-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-81596-6_29},
doi = {10.1007/978-3-031-81596-6_29},
abstract = {Glaucoma, a progressive eye condition leading to potential blindness, often lacks early symptoms, necessitating timely identification. However, manual prediction systems are significantly limited by subjectivity and the inherent risk of human errors. This study addresses these challenges by introducing the ViT-CNN approach, a hybrid model that fuses features from a custom Convolutional Neural Network (CNN) and a Vision Transformer (ViT) for the analysis of Optical Coherence Tomography (OCT) images. The final classification task is executed using several Machine Learning (ML) classifiers to assess the effectiveness of the proposed ViT-CNN model. Experimental results on four datasets demonstrate that the proposed hybrid ViT-CNN model surpasses standalone CNN or ViT models. The synergy between localized feature extraction by CNN and global contextual comprehension by ViT models enhances feature complexity, as illustrated through Gradient Class Activation Map (GRAD-CAM) visualization. This research advances automated glaucoma detection with a robust and versatile model showing competitive performance across diverse datasets.},
booktitle = {Artificial Intelligence and Soft Computing: 23rd International Conference, ICAISC 2024, Zakopane, Poland, June 16–20, 2024, Proceedings, Part III},
pages = {326–344},
numpages = {19},
keywords = {Glaucoma, OCT image, Vision Transformer, Convolutional Neural Network, Machine Learning},
location = {Zakopane, Poland}
}

@inproceedings{10.1145/3706598.3713486,
author = {Mathis, Florian and Sch\"{o}ning, Johannes},
title = {LifeInsight: Design and Evaluation of an AI-Powered Assistive Wearable for Blind and Low Vision People Across Multiple Everyday Life Scenarios},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713486},
doi = {10.1145/3706598.3713486},
abstract = {Assistive technologies (ATs) have the potential to empower blind and low vision (BLV) people. Yet, they often remain underutilised due to their immobility and limited applicability across scenarios. This paper presents LifeInsight, an AI-powered assistive wearable&nbsp;for BLV people&nbsp;that uses a wearable camera, microphone and single-click interface for goal-oriented visual querying. To inform the design of LifeInsight, we first collected a corpus of BLV people’s daily experiences using video probes and interviews. Ten BLV people&nbsp;recorded their daily experiences over one week using GoPro cameras, providing empirical insights. Based on these, we report on LifeInsight&nbsp;and its evaluation with 13 BLV people&nbsp;across six scenarios. LifeInsight&nbsp;effectively responded to visual queries, such as distinguishing between jars or identifying the status of a candle. Drawing on our work, we conclude with key lessons and practical recommendations to guide future research and advance the development and evaluation of AI-powered assistive wearables.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {295},
numpages = {25},
keywords = {Assistive Technologies, Wearables, BLV People},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1609/aaai.v39i7.32828,
author = {Wang, He and Dai, Longquan and Tang, Jinhui},
title = {EMControl: adding conditional control to text-to-image diffusion models via expectation-maximization},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i7.32828},
doi = {10.1609/aaai.v39i7.32828},
abstract = {Recent advances in diffusion models focus on efficiently handling conditional generative tasks without extra training. The process involves decomposing the result into two components: 1. unconditional sample, generated in the absence of conditions; 2. condition correction, adjusting unconditional sample to include the guidance image. This adjustment is quantified by the pixel-level measure, where the latent is decoded back into a pixel image, and the forward operator translates the noisy image into the guidance domain for comparison with the guidance image. To enhance the fidelity of condition correction, we propose a learnable latent forward operator, focusing on latent-space consistency with the expectation that this latent-space consistency approximates the pixel-level fidelity measure. The encoder translates the guidance image into the latent space, and a correctional operator is proposed to rectify model mismatching in the latent guidance model. The determination of the condition term and the correction estimation is akin to solving a blind inverse problem. Our EMControl employs the Expectation-Maximization (EM) algorithm to solve the blind inverse problem during the reverse sampling process. This technique ensures that samples, once consistent with the guidance, are accurately mapped back onto the noisy data manifold, adhering to the data's inherent distribution. The EMControl has proven its effectiveness by delivering superior performance in conditional diffusion generation tasks compared to previous approaches. Moreover, its application to multiple-condition scenarios underscores its versatility and robustness across a range of generative tasks.},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {855},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@article{10.1016/j.eswa.2024.124888,
author = {Ashtari-Majlan, Mona and Dehshibi, Mohammad Mahdi and Masip, David},
title = {Glaucoma diagnosis in the era of deep learning: A survey},
year = {2024},
issue_date = {Dec 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {256},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2024.124888},
doi = {10.1016/j.eswa.2024.124888},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {19},
keywords = {Glaucoma, Deep learning, Computer vision, Machine learning}
}

@inproceedings{10.1145/3641584.3641688,
author = {Liu, Xing and Yang, Guang and Nie, Min},
title = {Research on non-linear blind source separation multi-interference recognition scheme based on GRU neural network},
year = {2024},
isbn = {9798400707674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641584.3641688},
doi = {10.1145/3641584.3641688},
abstract = {In an increasingly complex electromagnetic environment, GNSS receivers are subject to mixed signal interference and the non-linear distortion present in the receiver itself, which seriously affects the true satellite signal. By achieving the identification of the mixed interfering signals, the impact of interfering signals can be effectively reduced or avoided. In this paper, based on an in-depth study of the problems of the target signal and interference signal mixing model selection, feature extraction methods, classifier selection and training, and performance evaluation, the problem is decomposed into two sequential and progressive steps: separation of mixed interference signals and classification and identification of interference signals. Incorporating the ideas of feature engineering and deep learning, the use of Gated Recurrent Unit (GRU) neural networks improves the recognition of interfering signals compared to conventional neural networks, achieving 96% accuracy at a 10dB ratio of interference to signal.},
booktitle = {Proceedings of the 2023 6th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {707–713},
numpages = {7},
keywords = {Blind source separation, Deep learning, Interference signals, Signal recognition;},
location = {Xiamen, China},
series = {AIPR '23}
}

@article{10.1007/s11042-022-13443-5,
author = {Tiwary, Tejal and Mahapatra, Rajendra Prasad},
title = {An accurate generation of image captions for blind people using extended convolutional atom neural network},
year = {2022},
issue_date = {Jan 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {82},
number = {3},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-022-13443-5},
doi = {10.1007/s11042-022-13443-5},
abstract = {Recently, the progress on image understanding and AIC (Automatic Image Captioning) has attracted lots of researchers to make use of AI (Artificial Intelligence) models to assist the blind people. AIC integrates the principle of both computer vision and NLP (Natural Language Processing) to generate automatic language descriptions in relation to the image observed. This work presents a new assistive technology based on deep learning which helps the blind people to distinguish the food items in online grocery shopping. The proposed AIC model involves the following steps such as Data Collection, Non-captioned image selection, Extraction of appearance, texture features and Generation of automatic image captions. Initially, the data is collected from two public sources and the selection of non-captioned images are done using the ARO (Adaptive Rain Optimization). Next, the appearance feature is extracted using SDM (Spatial Derivative and Multi-scale) approach and WPLBP (Weighted Patch Local Binary Pattern) is used in the extraction of texture features. Finally, the captions are automatically generated using ECANN (Extended Convolutional Atom Neural Network). ECANN model combines the CNN (Convolutional Neural Network) and LSTM (Long Short-Term Memory) architectures to perform the caption reusable system to select the most accurate caption. The loss in the ECANN architecture is minimized using AAS (Adaptive Atom Search) Optimization algorithm. The implementation tool used is PYTHON and the dataset used for the analysis are Grocery datasets (Freiburg Groceries and Grocery Store Dataset). The proposed ECANN model acquired accuracy (99.46%) on Grocery Store Dataset and (99.32%) accuracy on Freiburg Groceries dataset. Thus, the performance of the proposed ECANN model is compared with other existing models to verify the supremacy of the proposed work over the other existing works.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {3801–3830},
numpages = {30},
keywords = {Image captioning, Blind people, Alternative text, Natural language processing, Deep learning, Extended convolutional atom neural network (ECANN)}
}

@inproceedings{10.1007/978-981-96-0695-5_33,
author = {Zhai, Yingbo and Piao, Yanji},
title = {New Energy Vehicle Sales Prediction Based on Data Mining: A Case Study of BYD},
year = {2025},
isbn = {978-981-96-0694-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-96-0695-5_33},
doi = {10.1007/978-981-96-0695-5_33},
abstract = {With the rapid development and transformation of the vehicle industry, the production intensity of vehicle manufacturers is increasing year by year. Blind production planning and increasingly fierce competition between enterprises make enterprises prone to the problem of unbalanced production and sales. Due to the development of the Internet, customers tend to obtain and share product information through online media before and after purchasing products, which provides a new idea for the sales prediction of new energy vehicles. Based on this, this paper uses text mining and neural network method to construct a multi-feature time series prediction model. Taking BYD enterprise as an example, this paper provides decision-making reference for the production and marketing plan of new energy enterprises. It is found that the Attention-Seq2Seq model has higher prediction accuracy than GRU network and Seq2Seq model. This paper proposes a new energy vehicle sales prediction model based on data mining, which helps enterprises to identify changes in new energy vehicle sales in time and optimize the inventory of production enterprises.},
booktitle = {Multi-Disciplinary Trends in Artificial Intelligence: 17th International Conference, MIWAI 2024, Pattaya, Thailand, November 11–15, 2024, Proceedings, Part II},
pages = {411–422},
numpages = {12},
keywords = {Seq2Seq Model, Attention Mechanism, Online Comments, Sales Prediction},
location = {Pattaya, Thailand}
}

@article{10.1016/j.artmed.2022.102259,
author = {Li, Xiang and Jiang, Yuchen and Zhang, Jiusi and Li, Minglei and Luo, Hao and Yin, Shen},
title = {Lesion-attention pyramid network for diabetic retinopathy grading},
year = {2022},
issue_date = {Apr 2022},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {126},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2022.102259},
doi = {10.1016/j.artmed.2022.102259},
journal = {Artif. Intell. Med.},
month = apr,
numpages = {10},
keywords = {Diabetic retinopathy, Pyramid network, Attention mechanism, Convolutional neural network}
}

@inproceedings{10.1145/3641512.3686393,
author = {Tong, Jingyu and An, Zhenlin and Zhao, Xiaopeng and Liao, Sicong and Yang, Lei},
title = {In-Sensor Machine Learning: Radio Frequency Neural Networks for Wireless Sensing},
year = {2024},
isbn = {9798400705212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641512.3686393},
doi = {10.1145/3641512.3686393},
abstract = {Growing interest in wireless sensing, a cornerstone of the Artificial Intelligence of Things (AIoT), stems from its ability to gauge target states through nearby wireless signals. However, the escalating count of AIoT nodes escalates redundant data flow and exacerbates energy usage in AI cloud infrastructures. This amplifies the urgency for machine learning techniques that function in proximity to, or directly within, sensors. In light of this, we present the Radio-Frequency Neural Network (RFNN), a novel architecture that uses cost-effective transmissive intelligent surfaces to mimic the functions of a traditional neural network near (or in) sensors, transforming sensory nodes into intelligent terminals primed for machine learning. We first devised a unique training algorithm to mitigate the issues arising from unmodelable error-backward propagation; secondly, we incorporated contrastive learning to address the issue of blind labels stemming from environmental uncertainties. Our RFNN prototype, resonating at a 5 GHz WiFi bandwidth, has been honed across nine varied sensing tasks. The rigorous evaluation shows that it achieves a mean accuracy of 91.5% while consuming only 67.2 μJ of energy. This positions RFNN as a match in inferencing prowess to its electronic neural network counterparts but with significantly diminished energy demands.},
booktitle = {Proceedings of the Twenty-Fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {261–270},
numpages = {10},
keywords = {in-sensor machine learning, physical neural network},
location = {Athens, Greece},
series = {MobiHoc '24}
}

@inproceedings{10.1007/978-981-99-8850-1_11,
author = {Wang, Jianan and Li, Hesong and Wang, Xiaoyong and Fu, Ying},
title = {3D-B2U: Self-supervised Fluorescent Image Sequences Denoising},
year = {2023},
isbn = {978-981-99-8849-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-8850-1_11},
doi = {10.1007/978-981-99-8850-1_11},
abstract = {Fluorescence imaging can reveal the spatiotemporal dynamics of life activities. However, fluorescence image data suffers from photon shot noise due to a limited photon budget. Therefore, denoising fluorescence image sequences is an important task. Existing self-supervised methods solve the problem of complex parameter tuning of non-learning methods and the problem of requiring a large number of noisy-clean image pairs for supervised learning and become state-of-the-art methods for fluorescent image sequences denoising. However, they aim at 2D data, which cannot make good use of the increased time dimension information of fluorescence data compared with single image data. Besides, they still use paired noisy data to train models, and the strong prior information brought by paired data may lead to the overfitting of the model. In this work, we extend existing self-supervised methods to 3D and propose a 3D global masker that introduces a visible blind-spot structure based on 3D convolutions to avoid identity mapping while fully utilizing the input data information. Our method makes reasonable use of time dimension information and enables the task of self-supervised denoising on fluorescent images to mine information from the input data itself. Experimental results show that our method achieves a better denoising effect for fluorescent image sequences.},
booktitle = {Artificial Intelligence: Third CAAI International Conference, CICAI 2023, Fuzhou, China, July 22–23, 2023, Revised Selected Papers, Part I},
pages = {130–142},
numpages = {13},
keywords = {Self-Supervised Learning, Fluorescence Image Sequences Denoising, 3D Global Masker},
location = {Fuzhou, China}
}

@inproceedings{10.1145/3766882.3767169,
author = {Zheng, Yusheng and Hu, Yanpeng and Yu, Tong and Quinn, Andi},
title = {AgentSight: System-Level Observability for AI Agents Using eBPF},
year = {2025},
isbn = {9798400722059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3766882.3767169},
doi = {10.1145/3766882.3767169},
abstract = {Modern software infrastructure increasingly relies on LLM agents for development and maintenance, such as Claude Code and Gemini-cli. However, these AI agents differ fundamentally from traditional deterministic software, posing a significant challenge to conventional monitoring and debugging. This creates a critical semantic gap: existing tools observe either an agent's high-level intent (via LLM prompts) or its low-level actions (e.g., system calls), but cannot correlate these two views. This blindness makes it difficult to distinguish between benign operations, malicious attacks, and costly failures. We introduce AgentSight, an AgentOps observability framework that bridges this semantic gap using a hybrid approach. Our approach, boundary tracing, monitors agents from outside their application code at stable system interfaces using eBPF. AgentSight intercepts TLS-encrypted LLM traffic to extract semantic intent, monitors kernel events to observe system-wide effects, and causally correlates these two streams across process boundaries using a real-time engine and secondary LLM analysis. This instrumentation-free technique is framework-agnostic, resilient to rapid API changes, and incurs less than 3% performance overhead. Our evaluation shows AgentSight detects prompt injection attacks, identifies resource-wasting reasoning loops, and reveals hidden coordination bottlenecks in multi-agent systems. AgentSight is released as an open-source project at https://github.com/eunomia-bpf/agentsight.},
booktitle = {Proceedings of the 4th Workshop on Practical Adoption Challenges of ML for Systems},
pages = {110–115},
numpages = {6},
keywords = {AI Agents, LLM, eBPF, observability},
location = {Seoul, Republic of Korea},
series = {PACMI '25}
}

@inproceedings{10.1609/aaai.v39i5.32500,
author = {Li, Huaqiu and Zhang, Wang and Hu, Xiaowan and Jiang, Tao and Chen, Zikang and Wang, Haoqian},
title = {Prompt-SID: learning structural representation prompt via latent diffusion for single-image denoising},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i5.32500},
doi = {10.1609/aaai.v39i5.32500},
abstract = {Many studies have concentrated on constructing supervised models utilizing paired datasets for image denoising, which proves to be expensive and time-consuming. Current self-supervised and unsupervised approaches typically rely on blind-spot networks or sub-image pairs sampling, resulting in pixel information loss and destruction of detailed structural information, thereby significantly constraining the efficacy of such methods. In this paper, we introduce Prompt-SID, a prompt-learning-based single image denoising framework that emphasizes preserving of structural details. This approach is trained in a self-supervised manner using down-sampled image pairs. It captures original-scale image information through structural encoding and integrates this prompt into the denoiser. To achieve this, we propose a structural representation generation model based on the latent diffusion process and design a structural attention module within the transformer-based denoiser architecture to decode the prompt. Additionally, we introduce a scale replay training mechanism, which effectively mitigates the scale gap from images of different resolutions. We conduct comprehensive experiments on synthetic, real-world, and fluorescence imaging datasets, showcasing the remarkable effectiveness of Prompt-SID. Code — https://github.com/huaqlili/Prompt-SID.},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {527},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1145/3643832.3661420,
author = {Ryu, Junhyeong and Paek, Jeongyeup},
title = {Poster: Fast Field-of-View Expansion for Collaborative Object Detection},
year = {2024},
isbn = {9798400705816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643832.3661420},
doi = {10.1145/3643832.3661420},
abstract = {As interest in autonomous driving and advanced driver-assistance systems (ADAS) has grown, various sensing technologies have been developed to accurately determine the position and situation of surrounding vehicles and objects. In particular, light detection and ranging (LiDAR) sensors have attracted attention and are widely used in autonomous driving and ADAS because of their accuracy and reliability. However, when LiDAR sensors are used on a single vehicle, they can encounter blind spots caused by obstacles, which limits the detection of the environment. To overcome this issue, a method that can register and identify objects using LiDAR data from multiple vehicles in real-time is needed. Conventional artificial intelligence and iterative closest point (ICP) approaches need faster processing speed for practical use. Therefore, this work proposes an object-based single-point ICP (SP-ICP) which enables faster processing while maintaining accuracy using only a single point centered on each of the objects.},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services},
pages = {684–685},
numpages = {2},
keywords = {object detection, field-of-view expansion, collaborative sensing},
location = {Minato-ku, Tokyo, Japan},
series = {MOBISYS '24}
}

@article{10.1007/s10209-024-01170-7,
author = {Boubakri, Meryem and Nafil, Khalid},
title = {Gamification solutions for persons with disabilities: a systematic literature review},
year = {2024},
issue_date = {Jun 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {2},
issn = {1615-5289},
url = {https://doi.org/10.1007/s10209-024-01170-7},
doi = {10.1007/s10209-024-01170-7},
journal = {Univers. Access Inf. Soc.},
month = nov,
pages = {1009–1035},
numpages = {27},
keywords = {Gamification, Persons with disabilities, Game design elements, Gamified solutions, Systematic literature review}
}

@inproceedings{10.1145/3744257.3744266,
author = {Gurita, Alexandra-Elena and Vatavu, Radu-Daniel},
title = {When LLM-Generated Code Perpetuates User Interface Accessibility Barriers, How Can We Break the Cycle?},
year = {2025},
isbn = {9798400718823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744257.3744266},
doi = {10.1145/3744257.3744266},
abstract = {The integration of Large Language Models (LLMs) into web development workflows has the potential to revolutionize user interface design, yet their ability to produce accessible interfaces still remains underexplored. In this paper, we present an evaluation of LLM-generated user interfaces against the accessibility criteria from the Web Content Accessibility Guidelines (WCAG 2.1), comparing the output of ChatGPT and Claude with two distinct prompt types—accessibility-agnostic and accessibility-oriented. Our evaluation approach, consisting of automated testing, expert evaluation, and LLM self-reflection, reveals that accessibility-oriented prompts increase success counts and reduce violation rates in WCAG criteria, but persistent barriers remain, particularly in semantic structure. We argue that advancing accessible user interface development through LLM-generated code requires not just enhanced prompting but deeper semantic understanding and context awareness in these systems. We use our findings to suggest future work opportunities on the development of next-generation LLM-based accessibility tools as well as practical implications for designers.},
booktitle = {Proceedings of the 22nd International Web for All Conference},
pages = {124–134},
numpages = {11},
keywords = {Accessibility, WCAG, AI, Large Language Models, LLMs, User interfaces, Generative AI, Prompt engineering, Accessibility evaluation},
location = {
},
series = {W4A '25}
}

@inproceedings{10.1145/3711896.3737184,
author = {Lin, Hongyu and Zhong, Shuxin and Fang, Yan and Hong, Zhiqing and Lyu, Wenjun and Xie, Qipeng and Wang, Haotian and Wang, Lu and Wu, Kaishun},
title = {A Fraudulent Blind Shipment Detection Framework in Logistics},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737184},
doi = {10.1145/3711896.3737184},
abstract = {An emerging type of fraud involves malicious senders exploiting the blind shipment and cash-on-delivery (COD) mechanisms by dispatching large volumes of unsolicited, low-cost parcels. If unsuspecting receivers accept these parcels, they pay for both shipping and goods; otherwise, logistics providers bear the round-trip shipping costs. Existing detection techniques, which rely on extensive labeled cases, struggle with this emerging fraud because receivers' unawareness and low transaction values discourage complaints, resulting in few confirmed cases. Therefore, we propose leveraging receivers' complaints, though not initially collected for fraud detection, to uncover subtle indicators of fraud patterns, while addressing three challenges: (C1) noise-rich dialogues(C2) data privacy concerns, and (C3) ever-evolving fraud patterns. To address them, we design BLOFF, a Blind shipment detection Framework for LO gistics Fraud powered by large language models (LLMs). Specifically, BLOFF includes three components: i) Sensitivity Anonymization to protect sensitive user information; ii) Dialogue Profile Distillation to transform informal dialogues into structured representation, addressing C1, and distill knowledge from a teacher LLM (GPT-4o) to a lightweight student LLM (ChatGLM4-9B), addressing C2; ii) Multi-faceted Context Augmentation to enhance the interpretation of fraud signatures and adaptation of evolving patterns, addressing C3. We evaluate BLOFF on about 56,000 complaints records collected from JD Logistics between January and November 2024. Results show that BLOFF outperforms state-of-the-art methods, achieving a 10.19% improvement in precision. Furthermore, during its real-world deployment in December 2024, BLOFF identified over 90 fraudulent parcels with a 91.4% precision.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {4590–4598},
numpages = {9},
keywords = {blind shipment detection, large language model, multi-modal data aggregation},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1609/aaai.v39i25.34859,
author = {Farciola, Alessandro La and Valentini, Alessandro and Micheli, Andrea},
title = {Automatic selection of macro-events for heuristic-search temporal planning},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i25.34859},
doi = {10.1609/aaai.v39i25.34859},
abstract = {One of the major techniques to tackle temporal planning problems is heuristic search augmented with a symbolic representation of time in the states. Adding composite actions (macro-actions) to the problem is a simple and powerful approach to create "shortcuts" in the search space, at the cost of increasing the branching factor of the problem and thus the execution time of a heuristic search planner. Hence, it is of paramount importance to select the right macro-actions and minimize their number to optimize the planner performance.In this paper, we introduce "macro-events": a simple, yet powerful, "shortcut" model similar to macro-actions for the case of temporal planning. Then, we present a novel ranking function to extract and select a suitable set of macro-events from a dataset of valid plans. In our ranking approach, we consider an estimation of the hypothetical search space for a blind search under four different exploitation schemata. Finally, we experimentally demonstrate that the proposed approach yields a substantial performance improvement for a state-of-the-art temporal planner.},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2961},
numpages = {8},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.24963/ijcai.2023/89,
author = {Han, Young-Joo and Yu, Ha-Jin},
title = {SS-BSN: attentive blind-spot network for self-supervised denoising with nonlocal self-similarity},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/89},
doi = {10.24963/ijcai.2023/89},
abstract = {Recently, numerous studies have been conducted on supervised learning-based image denoising methods. However, these methods rely on large-scale noisy-clean image pairs, which are difficult to obtain in practice. Denoising methods with self-supervised training that can be trained with only noisy images have been proposed to address the limitation. These methods are based on the convolutional neural network (CNN) and have shown promising performance. However, CNN-based methods do not consider using nonlocal self-similarities essential in the traditional method, which can cause performance limitations. This paper presents self-similarity attention (SS-Attention), a novel self-attention module that can capture nonlocal self-similarities to solve the problem. We focus on designing a lightweight self-attention module in a pixel-wise manner, which is nearly impossible to implement using the classic self-attention module due to the quadratically increasing complexity with spatial resolution. Furthermore, we integrate SS-Attention into the blind-spot network called self-similarity-based blind-spot network (SS-BSN). We conduct the experiments on real-world image denoising tasks. The proposed method quantitatively and qualitatively outperforms state-of-the-art methods in self-supervised denoising on the Smartphone Image Denoising Dataset (SIDD) and Darmstadt Noise Dataset (DND) benchmark datasets.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {89},
numpages = {10},
location = {Macao, P.R.China},
series = {IJCAI '23}
}

@inproceedings{10.1609/aaai.v39i12.33349,
author = {Mao, Kai and Lian, Yiyang and Wang, Yangyang and Liu, Meiqin and Zheng, Nanning and Wei, Ping},
title = {Unveiling multi-view anomaly detection: intra-view decoupling and inter-view fusion},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i12.33349},
doi = {10.1609/aaai.v39i12.33349},
abstract = {Anomaly detection has garnered significant attention for its extensive industrial application value. Most existing methods focus on single-view scenarios and fail to detect anomalies hidden in blind spots, leaving a gap in addressing the demands of multi-view detection in practical applications. Ensemble of multiple single-view models is a typical way to tackle the multi-view situation, but it overlooks the correlations between different views. In this paper, we propose a novel multi-view anomaly detection framework, Intra-view Decoupling and Inter-view Fusion (IDIF), to explore correlations among views. Our method contains three key components: 1) a proposed Consistency Bottleneck module extracting the common features of different views through information compression and mutual information maximization; 2) an Implicit Voxel Construction module fusing features of different views with prior knowledge represented in the form of voxels; and 3) a View-wise Dropout training strategy enabling the model to learn how to cope with missing views during test. The proposed IDIF achieves state-of-the-art performance on three datasets. Extensive ablation studies also demonstrate the superiority of our methods. Code — https://github.com/Kerio99/IDIF},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1376},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1007/978-3-031-70893-0_2,
author = {Bertram, Timo and F\"{u}rnkranz, Johannes and M\"{u}ller, Martin},
title = {Efficiently Training Neural Networks for Imperfect Information Games by Sampling Information Sets},
year = {2024},
isbn = {978-3-031-70892-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-70893-0_2},
doi = {10.1007/978-3-031-70893-0_2},
abstract = {In imperfect information games, the evaluation of a game state not only depends on the observable world but also relies on hidden parts of the environment. As accessing the obstructed information trivialises state evaluations, one approach to tackle such problems is to estimate the value of the imperfect state as a combination of all states in the information set, i.e., all possible states that are consistent with the current imperfect information. In this work, the goal is to learn a function that maps from the imperfect game information state to its expected value. However, constructing a perfect training set, i.e. an enumeration of the whole information set for numerous imperfect states, is often infeasible. To compute the expected values for an imperfect information game like Reconnaissance Blind Chess, one would need to evaluate thousands of chess positions just to obtain the training target for a single state. Still, the expected value of a state can already be approximated with appropriate accuracy from a much smaller set of evaluations. Thus, in this paper, we empirically investigate how a budget of perfect information game evaluations should be distributed among training samples to maximise the return. Our results show that sampling a small number of states, in our experiments roughly 3, for a larger number of separate positions is preferable over repeatedly sampling a smaller quantity of states. Thus, we find that in our case, the quantity of different samples seems to be more important than higher target quality.},
booktitle = {KI 2024: Advances in Artificial Intelligence: 47th German Conference on AI, W\"{u}rzburg, Germany, September 25–27, 2024, Proceedings},
pages = {17–29},
numpages = {13},
keywords = {neural networks, imperfect information games},
location = {W\"{u}rzburg, Germany}
}

@inproceedings{10.1145/3587281.3587294,
author = {Dai, Jiamin and Miedema, John and Hernandez, Sebastian and Sutton-Lalani, Alexandra and Moffatt, Karyn},
title = {Cognitive Accessibility of Digital Payments: A Literature Review},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587281.3587294},
doi = {10.1145/3587281.3587294},
abstract = {Given its current ubiquity in banking and other services, digital payments should be accessible to all, but neurodiverse populations encounter barriers often understudied in research and practice. A greater understanding of user needs across the neurodivergent spectrum will thus improve universal access. To characterize the cognitive accessibility of digital payments, this literature review examines 30 scholarly publications, nuancing the challenges of online banking for older adults and people with neurodiverse needs. Our findings uncover a range of potential design and support strategies, including simplifying interfaces with diversified cues, raising designer awareness and participant involvement, extending third-party support, and leveraging new technological aids. We further discuss implications for digital currency design through support for user agency, collaborative payments, contextualized inclusive approaches, and AI-powered accessible design, hopefully inspiring future research on improving web accessibility.},
booktitle = {Proceedings of the 20th International Web for All Conference},
pages = {116–121},
numpages = {6},
keywords = {Web accessibility, central bank digital currency (CBDC), cognitive impairment, digital currency, financial technology (fintech), mental health, neurodiversity, older adults, online banking, universal access},
location = {Austin, TX, USA},
series = {W4A '23}
}

@inproceedings{10.1609/aaai.v39i4.32447,
author = {Kim, Hyunjun and Cho, Nam Ik},
title = {APR-RD: complemental two steps for self-supervised real image denoising},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i4.32447},
doi = {10.1609/aaai.v39i4.32447},
abstract = {Recent advancements in self-supervised denoising have made it possible to train models without needing a large amount of noisy-clean image pairs. A significant development in this area is the use of blind-spot networks (BSNs), which use single noisy images as training pairs by masking some input information to prevent noise transmission to the network output. Researchers have shown that BSNs are capable of reconstructing clean pixels from various types of independent pixel-wise degradations, such as synthetic additive white Gaussian noise (AWGN). However, unlike synthetic noise, real noise often contains highly correlated components which can induce noise transmission and reduce the performance of BSNs. To address the spatial correlation of real noise, we propose the Adjacent Pixel Replacer (APR), which decorrelates noise without a downsampling process that is widely adopted in previous research. The dissimilarity in our APR-generated pairs serves as relatively different noise components during training. Hence, it enables the BSN to block noise transmission while utilizing clean information effectively. As a result, BSN can utilize denser information to reconstruct the corresponding center pixel. We also propose Recharged Distillation (RD) to enhance high-frequency textures without additional network modifications. This method selectively refines clean information from recharged noisy pixels during distillation. Extensive experimental results demonstrate that our proposed method outperforms the existing state-of-the-art self-supervised denoising methods in real sRGB space. Project Page — https://github.com/HYK2017/APRRD},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {474},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1145/3488933.3488949,
author = {Wang, Shuai and Wang, Qian and Xie, Xihai},
title = {A Variable Step Size MSEI-DDLMS Double Mode Blind Equalization Algorithm},
year = {2022},
isbn = {9781450384087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488933.3488949},
doi = {10.1145/3488933.3488949},
abstract = {The quality of communication is seriously affected by ICI in communication. Blind equalization can solve this problem well. In this paper, the fixed step length modified super-exponential iterative blind equalization (MSEI) algorithm is introduced. The convergence speed and steady-state error of the fixed step length MSEI-DDLMS double mode blind equalization algorithm is restricted, which leads to the poor convergence effect of the algorithm and some probability of failure to converge. To solve this problem, a variable step-size MSEI-DDLMS algorithm is proposed. In the initial stage of the algorithm, the step size of the MSEI algorithm is adjusted automatically according to the change of the error to ensure rapid convergence in the early stage. After stabilization, it is switched to DDLMS algorithm with smaller residual mean square error, and the step size of DDLMS algorithm is further adjusted to reduce the steady-state error. The algorithm analysis and simulation results show that the improved algorithm has faster convergence speed and higher convergence efficiency.},
booktitle = {Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {435–440},
numpages = {6},
keywords = {Adaptive algorithm, Blind equalization, Decision directed, Super-exponential, The rate of convergence, Variable step-size},
location = {Xiamen, China},
series = {AIPR '21}
}

@inproceedings{10.24963/ijcai.2024/931,
author = {Fr\"{o}be, Maik and Reimer, Jan Heinrich and MacAvaney, Sean and Deckers, Niklas and Reich, Simon and Bevendorff, Janek and Stein, Benno and Hagen, Matthias and Potthast, Martin},
title = {The information retrieval experiment platform (extended abstract)},
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/931},
doi = {10.24963/ijcai.2024/931},
abstract = {We have built TIREx, the information retrieval experiment platform, to promote standardized, reproducible, scalable, and blinded retrieval experiments. Standardization is achieved through integration with PyTerrier's interfaces and compatibility with ir_datasets and ir_measures. Reproducibility and scalability are based on the underlying TIRA framework, which runs dockerized software in a cloud-native execution environment. Using Docker images of 50 standard retrieval approaches, we evaluated all of them on 32 tasks (i.e., 1,600 runs) in less than a week on a midsize cluster (1,620 CPU cores and 24 GPUs), demonstrating multi-task scalability. Importantly, TIRA also enables blind evaluation of AI experiments, as the test data can be hidden from public access and the tested approaches run in a sandbox that prevents data leaks. Keeping the test data hidden from public access ensures that it cannot be used by third parties for LLM training, preventing future training-test leaks.},
booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
articleno = {931},
numpages = {6},
location = {Jeju, Korea},
series = {IJCAI '24}
}

@inproceedings{10.1145/3586183.3606735,
author = {Huh, Mina and Peng, Yi-Hao and Pavel, Amy},
title = {GenAssist: Making Image Generation Accessible},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606735},
doi = {10.1145/3586183.3606735},
abstract = {Blind and low vision (BLV) creators use images to communicate with sighted audiences. However, creating or retrieving images is challenging for BLV creators as it is difficult to use authoring tools or assess image search results. Thus, creators limit the types of images they create or recruit sighted collaborators. While text-to-image generation models let creators generate high-fidelity images based on a text description (i.e. prompt), it is difficult to assess the content and quality of generated images. We present GenAssist, a system to make text-to-image generation accessible. Using our interface, creators can verify whether generated image candidates followed the prompt, access additional details in the image not specified in the prompt, and skim a summary of similarities and differences between image candidates. To power the interface, GenAssist uses a large language model to generate visual questions, vision-language models to extract answers, and a large language model to summarize the results. Our study with 12 BLV creators demonstrated that GenAssist enables and simplifies the process of image selection and generation, making visual authoring more accessible to all.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {38},
numpages = {17},
keywords = {Accessibility, Creativity Support Tools, Generative AI, Image Generation},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1007/978-3-032-07845-2_7,
author = {Li, Chunlei and Shi, Yilei and Hu, Haoxi and Hu, Jingliang and Xiang Zhu, Xiao and Mou, Lichao},
title = {Taming Stable Diffusion for&nbsp;Computed Tomography Blind Super-Resolution},
year = {2025},
isbn = {978-3-032-07844-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-07845-2_7},
doi = {10.1007/978-3-032-07845-2_7},
abstract = {High-resolution computed tomography (CT) imaging is essential for medical diagnosis but requires increased radiation exposure, creating a critical trade-off between image quality and patient safety. While deep learning methods have shown promise in CT super-resolution, they face challenges with complex degradations and limited medical training data. Meanwhile, large-scale pre-trained diffusion models, particularly Stable Diffusion, have demonstrated remarkable capabilities in synthesizing fine details across various vision tasks. Motivated by this, we propose a novel framework that adapts Stable Diffusion for CT blind super-resolution. We employ a practical degradation model to synthesize realistic low-quality images and leverage a pre-trained vision-language model to generate corresponding descriptions. Subsequently, we perform super-resolution using Stable Diffusion with a specialized controlling strategy, conditioned on both low-resolution inputs and the generated text descriptions. Extensive experiments show that our method outperforms existing approaches, demonstrating its potential for achieving high-quality CT imaging at reduced radiation doses. Code is available at .},
booktitle = {Foundation Models for General Medical AI: Third International Workshop, MedAGI 2025, Held in Conjunction with MICCAI 2025, Daejeon, South Korea, September 27, 2025, Proceedings},
pages = {65–75},
numpages = {11},
keywords = {CT super-resolution, Stable Diffusion, fine-tuning},
location = {Daejeon, Korea (Republic of)}
}

@article{10.1016/j.engappai.2022.105380,
author = {Ilango, Harun Surej and Ma, Maode and Su, Rong},
title = {A misbehavior detection system to detect novel position falsification attacks in the Internet of Vehicles},
year = {2022},
issue_date = {Nov 2022},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2022.105380},
doi = {10.1016/j.engappai.2022.105380},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {13},
keywords = {AE, AMF, AUC, BSM, CA, CCR, DSRC, FN-BSMD, IoV, MCR, MDS, MSE, MVD, NADM, NASEA, NPFADS for the IoV, OBU, OBU-BSMD, RF, ROC, RSSI, RSU, V2I, V2V, VeReMi, Internet of Vehicles, Position falsification attacks, Machine learning, Novel attack detection, Basic Safety Messages (BSMs), VeReMi dataset, Vehicle-to-Vehicle (V2V) communication, Anomaly detection}
}

@article{10.1002/aaai.12191,
author = {Zhao, Han},
title = {Fair and optimal prediction via post‐processing},
year = {2024},
issue_date = {Fall 2024},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {45},
number = {3},
issn = {0738-4602},
url = {https://doi.org/10.1002/aaai.12191},
doi = {10.1002/aaai.12191},
abstract = {With the development of machine learning algorithms and the increasing computational resources available, artificial intelligence has achieved great success in many application domains. However, the success of machine learning has also raised concerns about the fairness of the learned models. For instance, the learned models can perpetuate and even exacerbate the potential bias and discrimination in the training data. This issue has become a major obstacle to the deployment of machine learning systems in high‐stakes domains, for example, criminal judgment, medical testing, online advertising, hiring process, and so forth.&nbsp;To mitigate the potential bias exhibited by machine learning models, fairness criteria can be integrated into the training process to ensure fair treatment across all demographics, but it often comes at the expense of model performance. Understanding such tradeoffs, therefore, is crucial to the design of optimal and fair algorithms. My research focuses on characterizing the inherent tradeoff between fairness and accuracy in machine learning, and developing algorithms that can achieve both fairness and optimality. In this article, I will discuss our recent work on designing post‐processing algorithms for fair classification, which can be applied to a wide range of fairness criteria, including statistical parity, equal opportunity, and equalized odds, under both attribute‐aware and attribute‐blind settings, and is particularly suited to large‐scale foundation models where retraining is expensive or even infeasible. I will also discuss the connections between our work and other related research on trustworthy machine learning, including the connections between algorithmic fairness and differential privacy as well as adversarial&nbsp;robustness.},
journal = {AI Mag.},
month = sep,
pages = {411–418},
numpages = {8}
}

@inproceedings{10.1145/3677846.3677860,
author = {Kamikubo, Rie and Kacorri, Hernisa and Asakawa, Chieko},
title = {"We are at the mercy of others' opinion": Supporting Blind People in Recreational Window Shopping with AI-infused Technology},
year = {2024},
isbn = {9798400710308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677846.3677860},
doi = {10.1145/3677846.3677860},
abstract = {Engaging in recreational activities in public spaces poses challenges for blind people, often involving dependency on sighted help. Window shopping is a key recreational activity that remains inaccessible. In this paper, we investigate the information needs, challenges, and current approaches blind people have to recreational window shopping to inform the design of existing wayfinding and navigation technology for supporting blind shoppers in exploration and serendipitous discovery. We conduct a formative study with a total of 18 blind participants that include both focus groups (N=8) and interviews for requirements analysis (N=10). We find that there is a desire for push notifications of promotional information and pull notifications about shops of interest such as the targeted audience of a brand. Information about obstacles and points-of-interest required customization depending on one’s mobility aid as well as presence of a crowd, children, and wheelchair users. We translate these findings into specific information modalities and rendering in the context of two existing AI-infused assistive applications: NavCog (a turn-by-turn navigation app) and Cabot (a navigation robot).},
booktitle = {Proceedings of the 21st International Web for All Conference},
pages = {45–58},
numpages = {14},
keywords = {Blind people, navigation, guide robots, AI, information requirements},
location = {Singapore, Singapore},
series = {W4A '24}
}

@inproceedings{10.1007/978-3-031-35894-4_28,
author = {Raglin, Adrienne and Newcomb, Allison and Scott, Lisa},
title = {What Color is Your Swan? Uncertainty of Information Across Data},
year = {2023},
isbn = {978-3-031-35893-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-35894-4_28},
doi = {10.1007/978-3-031-35894-4_28},
abstract = {In 2007, Taleb coined the term “Black Swan” to describe those events that are extremely rare but unpredictable. However, if as Taleb states it is “our blindness with respect to randomness particularly large deviations” that is the issue, then how do we continue to understand these potential deviations and how do we adapt when they occur? Researchers continue to explore the area of uncertainty, how it is quantified, represented, and communicated. This paper is inspired by the concept of uncertainty and the association with the swans’ color as a means of expressing the uncertainty. In addition, this paper will discuss the concept of uncertainty of information, utilizing a taxonomy based on Gershon’s nature of imperfect information with a value. Firstly, we will explore existing associations made to different swan colors. Secondly, we will discuss how this concept can be modified relevant to uncertainty of information, and the challenges this idea presents such as risk and trust. Thirdly, we will explore applying a modified version of “uncertainty of information swan color” to a specific application.},
booktitle = {Artificial Intelligence in HCI: 4th International Conference, AI-HCI 2023, Held as Part of the 25th HCI International Conference, HCII 2023, Copenhagen, Denmark, July 23–28, 2023, Proceedings, Part II},
pages = {383–388},
numpages = {6},
keywords = {Uncertainty of Information, Black Swan, Data},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1007/978-3-031-20500-2_15,
author = {Shang, Kejun and Li, Xixi and Liu, Chongliang and Ming, Li and Hu, Guangfeng},
title = {An Integrated Navigation Method for UAV Autonomous Landing Based on Inertial and Vision Sensors},
year = {2022},
isbn = {978-3-031-20499-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-20500-2_15},
doi = {10.1007/978-3-031-20500-2_15},
abstract = {In the process of autonomous landing of unmanned aerial vehicles (UAV), the vision sensor is restricted by the field of view and UAV maneuvering process, which may make the acquired relative position/attitude parameters unstable or even odd (not unique), and there is a ‘blind area’ of vision measurement in the UAV rollout stage, which loses the navigation ability and seriously affects the safety of landing. In this paper, an autonomous landing navigation method based on inertial/visual sensor information fusion is proposed. When the UAV is far away from the airport and the runway imaging is complete, landing navigation parameters are determined by vision sensor based on the object image conjugate relationship of the runway sideline, and fuses with the inertial information to improve the measure performance. When the UAV is close to the airport and the runway imaging is incomplete, the measurement information of the vision sensor appears singular. The estimation of the landing navigation parameters is realized by inertial information in the aid of vision. When the UAV rollouts, the vision sensor enters the ‘blind area’, judges the UAV’s motion state through the imaging features of two adjacent frames, and suppresses the inertial sensor error by using the UAV’s motion state constraint, so as to achieve the high-precision maintenance of landing navigation parameters. The flight test shows that the lateral relative position error is less than 10m when the inertial with low accuracy and visual sensor are used, which can meet the requirement of UAV landing safely.},
booktitle = {Artificial Intelligence: Second CAAI International Conference, CICAI 2022, Beijing, China, August 27–28, 2022, Revised Selected Papers, Part II},
pages = {182–193},
numpages = {12},
keywords = {Autonomous landing navigation, Deep learning semantic segmentation, Inertial/Vision data fusion},
location = {Beijing, China}
}

@article{10.1109/TCSVT.2024.3507383,
author = {Cai, Zhefei and Fan, Yingle and Zhu, Minwei and Fang, Tao},
title = {Ultra-Lightweight Network for Medical Image Segmentation Inspired by Bio-Visual Interaction},
year = {2024},
issue_date = {April 2025},
publisher = {IEEE Press},
volume = {35},
number = {4},
issn = {1051-8215},
url = {https://doi.org/10.1109/TCSVT.2024.3507383},
doi = {10.1109/TCSVT.2024.3507383},
abstract = {Computer-aided medical image segmentation helps to assist physicians in locating lesion area for the subsequent diagnosis and treatment. Due to the irregular shape of the target and the uneven sample size between the target and the background area, automatic segmentation of medical images is a challenging task. Many CNN-Based, Transformer-Based models deepen the number of network layers or introduce complex modules in order to improve the segmentation accuracy. Limited by the computational resources, these types of large models are not suitable for the actual clinical environment. Inspired by the rapidity, accuracy, and low consumption characteristics of bio-visual processing, the Ultra-Lightweight Network Inspired by Bio-Visual Interaction (BVI-Net) is constructed in this paper. The Global Pathway is constructed by simulating the dorsal stream, in order to extract global features rapidly, and the Local Pathway is constructed by simulating the ventral stream, in order to process local features finely. At the same time, the skip connection module integrating Graph Convolutional Network (GCN) attention mechanism is constructed to simulate the synchronous integration ability of the visual pathway for multi-level features. The International Skin Imaging Collaboration (ISIC) dataset, the Liver Tumor Segmentation (LiTS) dataset, and the Brain Tumor Segmentation Challenge (BraTS) dataset are used for experiments. The BVI-Net proposed in this paper requires only 0.026M parameters to achieve the excellent performance in three representative medical image segmentation datasets, which has certain advantages over state-of-the-art (SOTA) methods. The biological vision mechanism and the artificial intelligence algorithm are integrated in this paper, which provides new ideas for the construction of biological vision-guided deep learning models and promotes the development of biomimetic computational vision.},
journal = {IEEE Trans. Cir. and Sys. for Video Technol.},
month = nov,
pages = {3486–3497},
numpages = {12}
}

@inproceedings{10.1145/3677846.3677863,
author = {Leotta, Maurizio and Ribaudo, Marina},
title = {Evaluating the Effectiveness of STEM Images Captioning},
year = {2024},
isbn = {9798400710308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677846.3677863},
doi = {10.1145/3677846.3677863},
abstract = {Despite the advent of the World Wide Web over three decades ago and the prevalence of online activities in contemporary society, barriers for users with disabilities to access online content still exist. With the proliferation of smartphones and easy access to cameras, the number of images shared daily on social media platforms is substantial and continuously growing. For visually impaired users, this abundance of images can be perceived as a significant obstacle to the understanding of online content.This study presents the initial findings of a long-term experiment focused on exploring the challenges in creating alternative texts for complex images, particularly those pertinent to disciplines encompassing mathematics, physics, technology, and the natural sciences. Our investigation is focused on two main aspects. Firstly, we explore the feasibility of using AI engines to generate alternative descriptions for this type of images. Secondly, we assess how basic training on STEM accessibility impacts undergraduate students’ awareness.The results indicate that conveying the meaning of complex images to visually impaired users through descriptions remains challenging, both for students and AI engines. However, it is encouraging to note that even basic training has empowered students to write improved descriptions and to critically assess those composed by their peers. This underscores the significance of education in promoting a foundational awareness among future software developers concerning the crucial yet frequently overlooked aspect of web accessibility.},
booktitle = {Proceedings of the 21st International Web for All Conference},
pages = {150–159},
numpages = {10},
keywords = {Accessibility, Computer Science Education, Alt-Text Generation, STEM, AI Engine, Visual Language Model, IDEFICS},
location = {Singapore, Singapore},
series = {W4A '24}
}

@inproceedings{10.1007/978-3-031-49601-1_9,
author = {Yadav, Shweta},
title = {Assessing the&nbsp;Efficacy of&nbsp;Synthetic Data for&nbsp;Enhancing Machine Translation Models in&nbsp;Low Resource Domains},
year = {2023},
isbn = {978-3-031-49600-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49601-1_9},
doi = {10.1007/978-3-031-49601-1_9},
abstract = {An artificially generated dataset mimics real-world data in terms of its statistical properties, but it contains no real information. Data around rare occurrences like Covid-19 pandemic is difficult to capture in real-world data due to their infrequent nature. Additionally, cost involved and time-consumption to gather real world data is a big challenge. In such cases, synthetic data can help create more balanced datasets for model training. This project investigates the effectiveness of using synthetic data for tuning machine translation models when training data is limited. The Covid-19 domain is chosen considering the urgency and importance of the global accessibility of information related to the pandemic. TICO-19, a publically available dataset was effectively formulated to cater to this need. The medical terminologies were extracted and passed to OpenAI API to generate training language pair data. The fine-tuned davinci model is then verified with blind test data provided under TICO-19 for translation from English to French. SacreBLEU score is used to compute the translation quality, the fine-tuned model has a significantly higher BLEU score of 19.54 in comparison to the base model with a BLEU score of 0.44. The adapted model also has a comparable score to the next-generation version of davinci with a BLEU score of 22.29.},
booktitle = {Big Data and Artificial Intelligence: 11th International Conference, BDA 2023, Delhi, India, December 7–9, 2023, Proceedings},
pages = {122–132},
numpages = {11},
keywords = {OpenAI, davinci, TICO-19, low resource domain, machine translation, Covid-19},
location = {Delhi, India}
}

@article{10.1007/s11042-022-13540-5,
author = {Singh, Law Kumar and Pooja and Garg, Hitendra and Khanna, Munish},
title = {RETRACTED ARTICLE: An IoT based predictive modeling for Glaucoma detection in optical coherence tomography images using hybrid genetic algorithm},
year = {2022},
issue_date = {Nov 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {81},
number = {26},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-022-13540-5},
doi = {10.1007/s11042-022-13540-5},
abstract = {The primary cause of irreversible blindness due to glaucoma is a silent, progressive disease with no noticeable symptoms. This eye disease gradually and rapidly damages the optic nerve, resulting in visual field defects. Glaucoma can cause substantial vision loss if left untreated. Early detection and proper treatment help limit those severe consequences. The benefits of screening for glaucoma while reducing the workload on eye specialists outweigh the extra effort required for screening. With unmanned aircraft, self-driving cars, facial recognition, and language processing, artificial intelligence (AI) has altered our way of life. AI is capable of outperforming humans in tasks like image recognition. Data analysis and processing are critical, as the volume of image data generated by ophthalmic imaging centers continues to grow at a breakneck pace. Glaucoma has been predicted using OCT and fundus images of prospective patients and for this prediction; AI can be employed to help medical practitioners to come out of these problems. In this paper, a novel AI and Internet of Things (IoT) based predictive modeling is proposed in which a bio-inspired and artificial intelligence based computing approach is employed for classification and prediction of glaucoma disease from Optical coherence tomography (OCT) images through continuous monitoring. That ultimately results in an improvement in healthcare by providing necessary medical instructions. We are the frontrunners to present a unique IoT embedded with artificial intelligence that supports Glaucoma screening, an automated and timely system based on the fusion of machine learning and bio-inspired computing approaches, in the form of this study.150 OCT pictures were utilized in the experiment, which were derived from a mixture of MENDELEY and a private dataset by a renowned eye physicians. This work presents a solution to the question of how to diagnose this condition at an early stage utilizing 45 critical characteristics retrieved using the ORB feature extractor and custom algorithms. Our suggested model has four dimensions; originally, these 45 features were reduced to 20% (i.e., 9) utilizing a statistically based univariate selection procedure. Following that, a Genetic Algorithm (GA) is used to discover an optimum subset of characteristics. These optimized characteristics are routed sequentially to state-of-the-art machine learning models (K-Nearest Neighbor (KNN), XGBoost, Random Forest, and Support Vector Machine (SVM)) for classification. Additionally to the above, the technique is fully integrated into an IoT framework and can be accessed remotely to aid ophthalmologists in diagnosing and treating glaucoma. Additionally, the proposed model facilitates the collection of health data from patients through IoT devices. It is concluded that out of four possible sets of results, GA-KNN based combination input of 9 features, enhanced the computed results with 99% accuracy for glaucoma recognition. The accuracy is obtained through a fivefold cross-validation technique. Because the proposed system has a brilliant ability to differentiate between healthy and glaucomatous eyes, this study will help to achieve high standards of glaucoma identification.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {37203–37242},
numpages = {40},
keywords = {Glaucomatous, Genetic algorithm, SVM, KNN, Random forest, XGboost, Internet of things}
}

@article{10.1016/j.ijinfomgt.2022.102507,
author = {Werle, Marcel and Laumer, Sven},
title = {Competitor identification: A review of use cases, data sources, and algorithms},
year = {2022},
issue_date = {Aug 2022},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {65},
number = {C},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2022.102507},
doi = {10.1016/j.ijinfomgt.2022.102507},
journal = {Int. J. Inf. Manag.},
month = aug,
numpages = {15},
keywords = {Competitor Identification, Decision Support Systems, Strategic Management, Data mining, Artificial Intelligence}
}

@inproceedings{10.1145/3704637.3734728,
author = {Fleet, Chancey},
title = {AI and Accessibility: A Peer Educator's Perspective},
year = {2025},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704637.3734728},
doi = {10.1145/3704637.3734728},
abstract = {Blind people find ourselves interacting with a proliferation of AI systems that solve accessibility problems: apps describe images, read traffic signals, create graphics from text prompts and interpret signage, physical surroundings and even one's own face in the mirror. What does it mean to be part of a community that increasingly exchanges highly personal data for accessibility? How can technologists and educators balance the need for accessibility with a user's need for privacy? How do users experience AI ''hallucinations'' in the context of everyday decision-making? And, in an age of accessibility wonders, what can we learn from the fact that large language models still can't read or write basic Braille? Chancey will explore an exciting, turbulent and complex moment in AI and accessibility that has implications for everyone as AI tools become ever more convenient, personal and enmeshed in our daily lives.},
booktitle = {Proceedings of the 2025 Conference on Research on Equitable and Sustained Participation in Engineering, Computing, and Technology},
pages = {1},
numpages = {1},
keywords = {accessibility, artificial intelligence, blindness},
location = {Newark, NJ, USA},
series = {RESPECT 2025}
}

@phdthesis{10.5555/AAI29253018,
author = {Zhang, Mingrui and Alexis, Hiniker, and Leah, Findlater, and Shumin, Zhai,},
advisor = {Jacob, Wobbrock,},
title = {Towards More Intelligent, Intuitive, and Inclusive Communication with Computers in Text and Images},
year = {2022},
isbn = {9798837534386},
publisher = {University of Washington},
abstract = {Communication is fundamental to human experience and our interaction with computers. The efficiency of human communication relies largely on the quality of the medium. Modern computing devices offer mediums such as keyboards to interact with information, yet they are still primitive (e.g., one has to press every character of a phrase) and often fail to support different user needs (e.g., limited emoji support for visually-impaired users). Thus an important question is, how to make the keyboard understand our languages like a human being, so that communication with computers can be intelligent, intuitive and inclusive?In this dissertation, I demonstrate how to design, build and evaluate communication interactions using the power of artificial intelligence. My dissertation addresses the above question in three different strands of work: (1) Intelligent text entry and editing interactions that understand the user intention; (2) Assistive systems that help blind or low vision users to communicate with pictorial information such as emojis and GIFs; (3) Models and metrics that evaluate intelligent input systems on their performance and impact on human behaviors. Together, the work demonstrates the thesis statement: Artificial intelligence can enable and improve advanced text production and accessible interactions with pictures; in addition, new metrics for text entry enable the evaluation of advanced capabilities.},
note = {AAI29253018}
}

@article{10.1007/s42979-025-03726-7,
author = {Afsal, C. P. and Kuppusamy, K. S.},
title = {WEBSumm: A Chrome Extension for Summarizing Web Content Using LLMs for Visually Impaired Users},
year = {2025},
issue_date = {Feb 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {6},
number = {2},
url = {https://doi.org/10.1007/s42979-025-03726-7},
doi = {10.1007/s42979-025-03726-7},
abstract = {Summarization of web content plays a crucial role in making vast amounts of online information more accessible and digestible for users, particularly individuals with visual impairments. It helps condense lengthy articles into concise summaries, enabling quicker understanding and easier navigation through digital content. In this paper, we present a Chrome extension web application designed to summarize web content for individuals with visual impairments. The system utilizes four locally deployable LLMs: Llama 3.1, Phi 3, Gemma 2, and Mistral, which generate summaries from web pages. To ensure that the best summary is provided, a weighted sum approach was implemented to evaluate summaries across 42 news articles from seven BBC News domains. Gemma 2 emerged as the top model, achieving the highest weighted sum score of 0.585, delivering the best summaries in 17 out of 42 cases, and excelling in readability with the highest FRE score of 52.53 and lowest FKGL score of 10.08. It produced concise summaries with an average word count of 70.19, outperforming other models. A usability study with visually impaired users highlighted Gemma 2 as the preferred model (35.2%), followed by Llama 3.1 (28.7%) and Mistral (26.2%). The system received an effectiveness rating of 4.18/5, with 81.8% of users rating it 4 or higher, reflecting high satisfaction. A comparison with tools like ’TLDR This’ and ’AI Summarizer’ showed WEBSumm’s superior accessibility and readability, making it a more inclusive solution. The results highlight the effectiveness of the weighted sum method in delivering high-quality, accessible summaries with enhanced usability. The proposed model outperforms existing tools in readability, coherence, and accessibility, making it more suitable for diverse user needs. This work uniquely contributes to web accessibility, enabling persons with visual impairments to navigate and understand web content with ease.},
journal = {SN Comput. Sci.},
month = feb,
numpages = {15},
keywords = {Web accessibility, Large Language Models (LLMs), Visual impairment, Summarization, Chrome extension}
}

@inproceedings{10.1609/aaai.v38i6.28453,
author = {Xu, QiHao and Luo, Xiaoling and Huang, Chao and Liu, Chengliang and Wen, Jie and Wang, Jialei and Xu, Yong},
title = {HACDR-Net: heterogeneous-aware convolutional network for diabetic retinopathy multi-lesion segmentation},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i6.28453},
doi = {10.1609/aaai.v38i6.28453},
abstract = {Diabetic Retinopathy (DR), the leading cause of blindness in diabetic patients, is diagnosed by the condition of retinal multiple lesions. As a difficult task in medical image segmentation, DR multi-lesion segmentation faces the main concerns as follows. On the one hand, retinal lesions vary in location, shape, and size. On the other hand, because some lesions occupy only a very small part of the entire fundus image, the high proportion of background leads to difficulties in lesion segmentation. To solve the above problems, we propose a heterogeneous-aware convolutional network (HACDR-Net) that composes heterogeneous cross-convolution, heterogeneous modulated deformable convolution, and optional near-far-aware convolution. Our network introduces an adaptive aggregation module to summarize the heterogeneous feature maps and get diverse lesion areas in the heterogeneous receptive field along the channels and space. In addition, to solve the problem of the highly imbalanced proportion of focal areas, we design a new medical image segmentation loss function, Noise Adjusted Loss (NALoss). NALoss balances the predictive feature distribution of background and lesion by jointing Gaussian noise and hard example mining, thus enhancing awareness of lesions. We conduct the experiments on the public datasets IDRiD and DDR, and the experimental results show that the proposed method achieves better performance than other state-of-the-art methods. The code is open-sourced on github.com/xqh180110910537/HACDR-Net.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {705},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inbook{10.1145/3756423.3756569,
author = {Jia, Xinjie and Xia, Xianyu and Duan, Yi},
title = {A Method for Optimal Frequency Selection in HF Ultra-Long-Distance Communication},
year = {2025},
isbn = {9798400714351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3756423.3756569},
abstract = {NVIS (Near-Vertical Incidence Sky-wave) is currently an effective method to solve the blind zone effect of high-frequency communication, but the effectiveness of its operation is closely related to the frequency selection. Aiming at the existing problems in frequency selection of the current NVIS system, this paper proposes a rapid frequency selection method for the NVIS system based on passive monitoring. This method can achieve both the accuracy and timeliness of frequency selection without sending any detection signals. It can realize the rapid tracking of the ionospheric parameter f0F2 and the capture of the spectrum "holes" during the communication gap, and dynamically adjust the communication frequency in real time. This paper measures and analyzes the experimental data of different antennas. The results show that this method is applicable to a variety of NVIS antennas, and has the characteristics of high frequency selection accuracy, fast speed, strong concealment, etc., which has important reference value for improving the communication effect of high-frequency near-vertical incidence skywave propagation.},
booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Smart Manufacturing},
pages = {883–888},
numpages = {6}
}

@inproceedings{10.1609/aaai.v38i4.28125,
author = {Lin, Huangxing and Dong, Yuhang and Ding, Xinghao and Liu, Tianpeng and Liu, Yongxiang},
title = {Unsupervised pan-sharpening via mutually guided detail restoration},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i4.28125},
doi = {10.1609/aaai.v38i4.28125},
abstract = {Pan-sharpening is a task that aims to super-resolve the low-resolution multispectral (LRMS) image with the guidance of a corresponding high-resolution panchromatic (PAN) image. The key challenge in pan-sharpening is to accurately modeling the relationship between the MS and PAN images. While supervised deep learning methods are commonly employed to address this task, the unavailability of ground-truth severely limits their effectiveness. In this paper, we propose a mutually guided detail restoration method for unsupervised pan-sharpening. Specifically, we treat pan-sharpening as a blind image deblurring task, in which the blur kernel can be estimated by a CNN. Constrained by the blur kernel, the pan-sharpened image retains spectral information consistent with the LRMS image. Once the pan-sharpened image is obtained, the PAN image is blurred using a pre-defined blur operator. The pan-sharpened image, in turn, is used to guide the detail restoration of the blurred PAN image. By leveraging the mutual guidance between MS and PAN images, the pan-sharpening network can implicitly learn the spatial relationship between the two modalities. Extensive experiments show that the proposed method significantly outperforms existing unsupervised pan-sharpening methods.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {377},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@article{10.1109/MCOM.001.2400048,
author = {Yang, Hui and Yu, Tiankuo and Liu, Wenxin and Yao, Qiuyan and Meng, Daqing and Vasilakos, Athanasios V. and Cheriet, Mohamed},
title = {PAINet: An Integrated Passive and Active Intent Network for Digital Twins in Automatic Driving},
year = {2024},
issue_date = {March 2025},
publisher = {IEEE Press},
volume = {63},
number = {3},
issn = {0163-6804},
url = {https://doi.org/10.1109/MCOM.001.2400048},
doi = {10.1109/MCOM.001.2400048},
abstract = {In recent years, the rapid development of artificial intelligence (AI) has accelerated the convergence of digital twin technology with the landscape of 6G automatic driving which is poised to exert profound and far-reaching effects on various aspects of human life. For example, it can offer solutions to prevent traffic accidents caused by blind spots, sudden malfunctions, and other factors. However, current digital twins used in automatic driving fail to meet the requirements for effective operation. This is mainly because the single view of vehicles is limited due to visual blind spots for object detection, while the information sensed by the network requires additional communication time, resulting in a lack of real-time updates and accurate labeling. To advance the level of digital twinning, this article proposes an integrated passive and active intent network (PAINet). This pioneering approach for communication amalgamates passive information sensed by vehicles and their behavioral data within vehicular networks. Through the extraction and detection of pertinent data at the intent layer, informed driving decisions are facilitated, particularly in response to unforeseen emergencies. Therefore, PAINet engenders a unified intelligent entity that seamlessly integrates communication, perception, computation, and control, ushering in a revolutionary safety paradigm for 6G automatic driving. We thoroughly constructed a vehicle intent dataset derived from conventional object detection datasets. In the context of our use case scenario, experimental findings strongly confirm the effectiveness of this methodology, revealing an impressive 96 percent precision in vehicle intent parse, along with a significant reduction of over 50 percent in processing delay. This innovative approach shows great potential for making significant strides in tackling the challenges of perception and decision-making in the realm of 6G automatic driving. We have uploaded the code to github.com/YU-spec-arch/PAINET.},
journal = {Comm. Mag.},
month = nov,
pages = {32–38},
numpages = {7}
}

@inproceedings{10.1609/aaai.v38i2.27875,
author = {Chen, Lianggangxu and Song, Youqi and Lin, Shaohui and Wang, Changbo and He, Gaoqi},
title = {Kumaraswamy wavelet for heterophilic scene graph generation},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i2.27875},
doi = {10.1609/aaai.v38i2.27875},
abstract = {Graph neural networks (GNNs) has demonstrated its capabilities in the field of scene graph generation (SGG) by updating node representations from neighboring nodes. Actually it can be viewed as a form of low-pass filter in the spatial domain, which smooths node feature representation and retains commonalities among nodes. However, spatial GNNs does not work well in the case of heterophilic SGG in which finegrained predicates are always connected to a large number of coarse-grained predicates. Blind smoothing undermines the discriminative information of the fine-grained predicates, resulting in failure to predict them accurately. To address the heterophily, our key idea is to design tailored filters by wavelet transform from the spectral domain. First, we prove rigorously that when the heterophily on the scene graph increases, the spectral energy gradually shifts towards the high-frequency part. Inspired by this observation, we subsequently propose the Kumaraswamy Wavelet Graph Neural Network (KWGNN). KWGNN leverages complementary multi-group Kumaraswamy wavelets to cover all frequency bands. Finally, KWGNN adaptively generates band-pass filters and then integrates the filtering results to better accommodate varying levels of smoothness on the graph. Comprehensive experiments on the Visual Genome and Open Images datasets show that our method achieves state-of-the-art performance.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {127},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.1145/3689932.3694757,
author = {Apruzzese, Giovanni and Fass, Aurore and Pierazzi, Fabio},
title = {When Adversarial Perturbations meet Concept Drift: An Exploratory Analysis on ML-NIDS},
year = {2024},
isbn = {9798400712289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689932.3694757},
doi = {10.1145/3689932.3694757},
abstract = {We scrutinize the effects of "blind'' adversarial perturbations against machine learning (ML)-based network intrusion detection systems (NIDS) affected by concept drift. There may be cases in which a real attacker -- unable to access and hence unaware that the ML-NIDS is weakened by concept drift -- attempts to evade the ML-NIDS with data perturbations. It is currently unknown if the cumulative effect of such adversarial perturbations and concept drift leads to a greater or lower impact on ML-NIDS. In this "open problem'' paper, we seek to investigate this unusual, but realistic, setting---we are not interested in perfect knowledge attackers.We begin by retrieving a publicly available dataset of documented network traces captured in a real, large (&gt; 300 hosts) organization. Overall, these traces include several years of raw traffic packets---both benign and malicious. Then, we adversarially manipulate malicious packets with problem-space perturbations, representing a physically realizable attack. Finally, we carry out the first exploratory analysis focused on comparing the effects of our "adversarial examples'' with their respective unperturbed malicious variants in concept-drift scenarios. Through two case studies (a "short-term'' one of 8 days; and a "long-term'' one of 4 years) encompassing 48 detector variants, we find that, although our perturbations induce a lower detection rate in concept-drift scenarios, some perturbations yield adverse effects for the attacker in intriguing use cases. Overall, our study shows that the topics we covered are still an open problem which require a re-assessment from future research.},
booktitle = {Proceedings of the 2024 Workshop on Artificial Intelligence and Security},
pages = {149–160},
numpages = {12},
keywords = {adversarial example, ctu13, data drift, distribution shift, machine learning, mcfp, network intrusion detection, temporal evaluation},
location = {Salt Lake City, UT, USA},
series = {AISec '24}
}

@inproceedings{10.1145/3663548.3675612,
author = {Kamikubo, Rie and Zamiri Zeraati, Farnaz and Lee, Kyungjun and Kacorri, Hernisa},
title = {AccessShare: Co-designing Data Access and Sharing with Blind People},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675612},
doi = {10.1145/3663548.3675612},
abstract = {Blind people are often called to contribute image data to datasets for AI innovation with the hope for future accessibility and inclusion. Yet, the visual inspection of the contributed images is inaccessible. To this day, we lack mechanisms for data inspection and control that are accessible to the blind community. To address this gap, we engage 10 blind participants in a scenario where they wear smartglasses and collect image data using an AI-infused application in their homes. We also engineer a design probe, a novel data access interface called AccessShare, and conduct a co-design study to discuss participants’ needs, preferences, and ideas on consent, data inspection, and control. Our findings reveal the impact of interactive informed consent and the complementary role of data inspection systems such as AccessShare in facilitating communication between data stewards and blind data contributors. We discuss how key insights can guide future informed consent and data control to promote inclusive and responsible data practices in AI.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {60},
numpages = {16},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1609/aaai.v38i18.30046,
author = {Yu, Shenbao and Zeng, Yifeng and Yang, Fan and Pan, Yinghui},
title = {Causal-driven skill prerequisite structure discovery},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i18.30046},
doi = {10.1609/aaai.v38i18.30046},
abstract = {Knowing a prerequisite structure among skills in a subject domain effectively enables several educational applications, including intelligent tutoring systems and curriculum planning. Traditionally, educators or domain experts use intuition to determine the skills' prerequisite relationships, which is time-consuming and prone to fall into the trap of blind spots. In this paper, we focus on inferring the prerequisite structure given access to students' performance on exercises in a subject. Nevertheless, it is challenging since students' mastery of skills can not be directly observed, but can only be estimated, i.e., its latency in nature. To tackle this problem, we propose a causal-driven skill prerequisite structure discovery (CSPS) method in a two-stage learning framework. In the first stage, we learn the skills' correlation relationships presented in the covariance matrix from the student performance data while, through the predicted covariance matrix in the second stage, we consider a heuristic method based on conditional independence tests and standardized partial variance to discover the prerequisite structure. We demonstrate the performance of the new approach with both simulated and real-world data. The experimental results show the effectiveness of the proposed model for identifying the skills' prerequisite structure.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2298},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.1609/aaai.v38i12.29311,
author = {Liu, Xinhui and Chen, Zhenghao and Zhou, Luping and Xu, Dong and Xi, Wei and Bai, Gairui and Zhao, Yihan and Zhao, Jizhong},
title = {UFDA: universal federated domain adaptation with practical assumptions},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i12.29311},
doi = {10.1609/aaai.v38i12.29311},
abstract = {Conventional Federated Domain Adaptation (FDA) approaches usually demand an abundance of assumptions, which makes them significantly less feasible for real-world situations and introduces security hazards. This paper relaxes the assumptions from previous FDAs and studies a more practical scenario named Universal Federated Domain Adaptation (UFDA). It only requires the black-box model and the label set information of each source domain, while the label sets of different source domains could be inconsistent, and the target-domain label set is totally blind. Towards a more effective solution for our newly proposed UFDA scenario, we propose a corresponding methodology called Hot-Learning with Contrastive Label Disambiguation (HCLD). It particularly tackles UFDA's domain shifts and category gaps problems by using one-hot outputs from the black-box models of various source domains. Moreover, to better distinguish the shared and unknown classes, we further present a cluster-level strategy named Mutual-Voting Decision (MVD) to extract robust consensus knowledge across peer classes from both source and target domains. Extensive experiments on three benchmark datasets demonstrate that our method achieves comparable performance for our UFDA scenario with much fewer assumptions, compared to previous methodologies with comprehensive additional assumptions.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1563},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.1145/3746059.3747756,
author = {Froehlich, Jon E. and Fiannaca, Alexander J. and Jaber, Nimer M and Tsaran, Victor and Kane, Shaun K.},
title = {StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747756},
doi = {10.1145/3746059.3747756},
abstract = {Interactive streetscape mapping tools such as Google Street View (GSV) and Meta Mapillary enable users to virtually navigate and experience real-world environments via immersive 360° imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal AI, accessible navigation controls, and conversational speech. With StreetViewAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetViewAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {43},
numpages = {22},
keywords = {Accessible maps, Multimodal LLMs, AI Chat, Street View},
location = {
},
series = {UIST '25}
}

@inbook{10.1145/3730436.3730499,
author = {Cai, Erhan and Lin, Xingjun},
title = {Application of Ant Colony Algorithm in several fields},
year = {2025},
isbn = {9798400713637},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3730436.3730499},
abstract = {This paper reviews the development of Ant Colony Optimization (ACO) algorithm and its applications in several fields since it was proposed in the 1990s. ACO is an optimization algorithm that mimics the foraging behavior of ants in nature, and solves complex combinatorial optimization problems by simulating pheromone mechanisms. The paper discusses in detail the application examples of the ACO algorithm in blind signal separation, edge computing, rural revitalization, and sign language recognition, etc., and introduces the research progress in improving the performance of the algorithm in recent years. The research results show that the ant colony algorithm not only improves the efficiency of various industries, but also promotes technological progress and interdisciplinary cooperation, contributing an intelligent optimization tool for sustainable development.},
booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence},
pages = {372–375},
numpages = {4}
}

@inproceedings{10.1145/3607720.3607727,
author = {Oukrich, Nadia and Tamega, Bougary and Laaz, Naziha},
title = {Matia Application: An AI Multi-Lingual Assistant For Visually Impaired And Blind People},
year = {2023},
isbn = {9798400700194},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607720.3607727},
doi = {10.1145/3607720.3607727},
abstract = {The World Health Organization (WHO) estimates that at least 2.2 billion people suffer from some form of vision impairment. This widespread problem has resulted in a staggering $411 billion loss of productivity per year. To tackle this issue, researchers have employed IoT devices to aid visually impaired individuals in navigating their environments, but these solutions can be both inconvenient and costly. Mobile technology, on the other hand, has the potential to provide a more cost-effective and user-friendly solution. However, current mobile applications for the visually impaired only support English or French and require an internet connection to use. To address these limitations, a new mobile application named "Matia" has been developed to offer a solution in three languages: Arabic, French, and English, without the need for IoT devices or an internet connection. This paper describes the design and implementation of Matia application. The application's accuracy in recognizing five datasets (clothes, fruits, house equipment, pets, and banknotes) ranges from 74% to 90%, which is higher than existing literature. Further functionalities, such as guidance assistance and danger recognition, are under development to make the application even more useful in various situations.},
booktitle = {Proceedings of the 6th International Conference on Networking, Intelligent Systems &amp; Security},
articleno = {6},
numpages = {7},
location = {Larache, Morocco},
series = {NISS '23}
}

@inproceedings{10.1145/3555776.3577648,
author = {Sliman, Heithem and Megdiche, Imen and Yangui, Sami and Drira, Aida and Drira, Ines and Lamine, Elyes},
title = {A Synthetic Dataset Generation for the Uveitis Pathology Based on MedWGAN Model},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577648},
doi = {10.1145/3555776.3577648},
abstract = {Artificial Intelligence (AI) has undergone considerable development in recent years in the field of medicine and in particular in decision support diagnostic. However, the development of such algorithms depends on the presence of a sufficiently large amount of data to provide reliable results. Unfortunately in medicine, it is not always possible to provide so much data on all pathologies. This problem is particularly true for rare diseases. In this paper we focus on uveitis, a rare disease in ophthalmology which is the third cause of blindness worldwide. This pathology is difficult to diagnose because of the disparity in prevalence of its etiologies. In order to provide physicians with a diagnostic aid system, it would be necessary to have a representative dataset of epidemiological profiles that have been studied for a long time in this domain. This work proposes a breakthrough in this field by suggesting a methodological framework for the generation of an open source dataset based on the crossing of several epidemiological profiles and using data augmentation techniques. The results of these generated synthetic data have been qualitatively validated by specialist physicians in ophthalmology. Our results are very promising and consist in a first brick to promote research in AI on Uveitis disease.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {559–566},
numpages = {8},
keywords = {synthetic datasets, data augmentation, decision support systems},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3746058.3758460,
author = {Jiang, Chutian},
title = {Electrotactile Assistive Tools to Support Blind and Low Vision People’s Data Comprehension},
year = {2025},
isbn = {9798400720369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746058.3758460},
doi = {10.1145/3746058.3758460},
abstract = {Being able to access and interpret data is a critical skill in today’s world, but it remains challenging for blind and low vision (BLV) people. While the accessibility community has explored various solutions to make data more accessible, little is known about BLV people’s daily data analysis (DDA) workflows and how haptic assistive tools, especially electrotactile devices, can support their data comprehension. To address these gaps, my dissertation investigates how BLV people engage in DDA and develops electrotactile assistive tools to support common scenarios such as chart and map understanding, and trip preview based on the study results. Through a series of user studies, I contribute empirical insights into five key strategies used by BLV people for DDA, and two electrotactile systems, providing evidence that these systems are effective when optimized for different scenarios. I plan to conduct further research on optimizing electrotactile feedback through a deeper understanding of its perceptual mechanisms, supported by AI-driven adaptation and personalization.},
booktitle = {Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {6},
numpages = {6},
keywords = {blind and low vision, daily data analysis, electrotactile feedback, charts understanding, trip planning},
location = {
},
series = {UIST Adjunct '25}
}

@inproceedings{10.24963/ijcai.2024/405,
author = {Atienza, Adrian and Bardram, Jakob and Puthusserypady, Sadasivan},
title = {Contrastive learning is not optimal for quasiperiodic time series},
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/405},
doi = {10.24963/ijcai.2024/405},
abstract = {Despite recent advancements in Self-Supervised Learning (SSL) for time series analysis, a noticeable gap persists between the anticipated achievements and actual performance. While these methods have demonstrated formidable generalization capabilities with minimal labels in various domains, their effectiveness in distinguishing between different classes based on a limited number of annotated records is notably lacking. Our hypothesis attributes this bottleneck to the prevalent use of Contrastive Learning, a shared training objective in previous state-of-the-art (SOTA) methods. By mandating distinctiveness between representations for negative pairs drawn from separate records, this approach compels the model to encode unique record-based patterns but simultaneously neglects changes occurring across the entire record. To overcome this challenge, we introduce Distilled Embedding for Almost-Periodic Time Series (DEAPS) in this paper, offering a noncontrastive method tailored for quasiperiodic time series, such as electrocardiogram (ECG) data. By avoiding the use of negative pairs, we not only mitigate the model's blindness to temporal changes but also enable the integration of a "Gradual Loss (Lgra)" function. This function guides the model to effectively capture dynamic patterns evolving throughout the record. The outcomes are promising, as DEAPS demonstrates a notable improvement of +10% over existing SOTA methods when just a few annotated records are presented to fit a Machine Learning (ML) model based on the learned representation.},
booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
articleno = {405},
numpages = {8},
location = {Jeju, Korea},
series = {IJCAI '24}
}

@inproceedings{10.5555/3702676.3702874,
author = {Zhao, Aidong and Zhao, Xuyang and Gu, Tianchen and Bi, Zhaori and Sun, Xinwei and Yan, Changhao and Yang, Fan and Zhou, Dian and Zeng, Xuan},
title = {Exploring high-dimensional search space via voronoi graph traversing},
year = {2024},
publisher = {JMLR.org},
abstract = {Bayesian optimization (BO) is a well-established methodology for optimizing costly black-box functions. However, the sparse observations in the high-dimensional search space pose challenges in constructing reliable Gaussian Process (GP) models, which leads to blind exploration of the search space. We propose a novel Voronoi Graph Traversing (VGT) algorithm to extend BO to ultra high-dimensional problems. VGT employs a Voronoi diagram to mesh the design space and transform it into an undirected Voronoi graph. VGT explores the search space by iteratively performing path selection, promising cell sampling, and graph expansion operations. We introduce a UCB-based global traversal strategy to select the path towards promising Voronoi cells. Then we perform local BO within the promising cell and train local GP with a neighboring subset. The intrinsic geometric boundaries and adjacency of the Voronoi graph assist in fine-tuning the trajectory of local BO sampling. We also present a subspace enhancement approach for the intrinsic low-dimensional problems. Experimental results, including both synthetic benchmarks and real-world applications, demonstrate the proposed approach's state-of-the-art performance for tackling ultra high-dimensional problems ranging from hundreds to one thousand dimensions.},
booktitle = {Proceedings of the Fortieth Conference on Uncertainty in Artificial Intelligence},
articleno = {198},
numpages = {18},
location = {Barcelona, Spain},
series = {UAI '24}
}

@article{10.1007/s42979-023-02565-8,
author = {Poranki, Venkata Kotam Raju and Srinivasarao, B.},
title = {Computer-Aided Diagnosis-Based Grading Classification of Diabetic Retinopathy Using Deep Graph Correlation Network with IRF},
year = {2024},
issue_date = {Jan 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {5},
number = {2},
url = {https://doi.org/10.1007/s42979-023-02565-8},
doi = {10.1007/s42979-023-02565-8},
abstract = {Diabetic retinopathy (DR) is a leading cause of blindness in different age groups. So, the early detection of DR can save millions of people from blindness issues. Further, the manual analysis of DR requires much processing time and experienced doctors. So, computer-aided diagnosis (CAD)-based artificial intelligence models were developed for early DR prediction. These traditional methods, however, needed to extract balanced deep features, resulting in poor classification performance. This study creates a CAD-based grading classification of DR using graph learning and optimal feature selection properties, which is named as CEFNet. The EyePACS and Messidor datasets are first put through the edited nearest neighbor oversampling (ENNOS) method, ensuring that each class's instances are equal. Then, a deep graph correlation network (DGCN) is used to get the unique features of each class by finding the connections in color eye fundus (CEF) images. In addition, an iterative random forest (IRF) is performed for feature selection to select the highly relevant DGCN features. Finally, these balanced optimal features are used to train a decision tree-based extreme gradient boosting (DTEGB) classifier, which then uses test CEF image data to make the prediction. The simulations done on the EyePACS and Messidor datasets show that the proposed CEFNet performed better than the current best DR grading classification methods. The EyePACS dataset had an accuracy of 99.50%, a sensitivity of 99.20%, and a specificity of 100%. The Messidor dataset had an accuracy of 99.80%, a sensitivity of 99.67%, and a specificity of 100%.},
journal = {SN Comput. Sci.},
month = jan,
numpages = {17},
keywords = {Diabetic retinopathy, Nearest neighbor oversampling, Iterative random forest, Graph convolutional neural network, Extreme gradient boosting}
}

@inproceedings{10.1007/978-3-031-44013-7_7,
author = {Tian, Ye and Zang, Mingyang and Sharma, Anurag and Gu, Sophie Z. and Leshno, Ari and Thakoor, Kaveri A.},
title = {Glaucoma Progression Detection and&nbsp;Humphrey Visual Field Prediction Using Discriminative and&nbsp;Generative Vision Transformers},
year = {2023},
isbn = {978-3-031-44012-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44013-7_7},
doi = {10.1007/978-3-031-44013-7_7},
abstract = {Glaucoma is one of the top causes of blindness worldwide. Assessing its progression is critical to determine potential visual impairment and to design sound treatment plans. Standard automated perimetry tests, commonly known as visual field (VF) tests, are clinically used to evaluate the state of functional vision. To provide an accurate and automatic diagnostic tool for clinical decision making in glaucoma progression, we utilize the predictive power of artificial intelligence (AI) and propose two vision transformer (ViT)-based deep learning (DL) networks. First, we optimize a spatiotemporal ViT to classify a subject’s rate of glaucoma progression (GP) using only 3 baseline VFs; we explore threshold mean deviation (MD) rate of change from −0.3 to −1.5&nbsp;dB/year and achieve up to 89% GP detection accuracy. Second, we develop a VF-to-VF generation architecture via a diffusion model with a ViT backbone. The model predicts future VFs with Pointwise Mean Absolute Error (PMAE) as low as 2.15 dB for mild VF deficits and is the first to extend VF prediction up to 10 years into the future. Our models are trained and validated on our ‘62K+’ dataset, the largest available of VFs to-date including at-risk, minority populations, thus ensuring our models’ generalizability. We establish our computational methods and compare testing results on the publicly available UWHVF dataset. In short, our study utilizes novel AI methods for predicting future rates and patterns of glaucoma progression in order to expedite timely treatment for better patient quality of life. The code is available at},
booktitle = {Ophthalmic Medical Image Analysis: 10th International Workshop, OMIA 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 12, 2023, Proceedings},
pages = {62–71},
numpages = {10},
keywords = {Glaucoma, Visual Field, Vision Transformer},
location = {Vancouver, BC, Canada}
}

@article{10.1504/ijnvo.2022.127584,
author = {Padmapriya, M. and Pasupathy, S. and Sumathi, R. and Punitha, V.},
title = {A deep learning model framework for diabetic retinopathy detection},
year = {2022},
issue_date = {2022},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {27},
number = {2},
issn = {1470-9503},
url = {https://doi.org/10.1504/ijnvo.2022.127584},
doi = {10.1504/ijnvo.2022.127584},
abstract = {Diabetic retinopathy (DR) is the typical diabetic eye issue and a main reason of blindness around the world. As per the International Diabetes Federation (IDF), the rates of diabetes would rise to 552 million by 2034. Breakthroughs in computer science techniques inclusive of artificial intelligence (AI) and deep learning (DL) have multiplied opportunities for early detection of DR. This indicates that the likelihood of a patient's healing will improve, and the risk of eyesight loss could be minimised in due course. A deep learning model (ResNet) for medical DR detection was examined in this article. The dataset of Asia Pacific Tele-Ophthalmology Society (APTOS) 2019 was used to train and test the DL model. To demonstrate the vitality of the chosen ResNet model, performance measures and testing accuracy like recall, precision, and F1 score were determined. The modified ResNet model attained a testing accuracy of around 84% even with only a few dataset images. Also, training time and computational complexity were reduced with this simpler model.},
journal = {Int. J. Netw. Virtual Organ.},
month = jan,
pages = {107–124},
numpages = {17},
keywords = {diabetic retinopathy, convolutional neural network, CNN, machine learning, deep learning, ResNet model}
}

@inproceedings{10.1145/3715336.3735685,
author = {Cheema, Maryam and Seifi, Hasti and Fazli, Pooyan},
title = {Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735685},
doi = {10.1145/3715336.3735685},
abstract = {Audio descriptions (AD) make videos accessible for blind and low vision (BLV) users by describing visual elements that cannot be understood from the main audio track. AD created by professionals or novice describers is time-consuming and offers little customization or control to BLV viewers on description length and content and when they receive it. To address this gap, we explore user-driven AI-generated descriptions, enabling BLV viewers to control both the timing and level of detail of the descriptions they receive. In a study, 20 BLV participants activated audio descriptions for seven different video genres with two levels of detail: concise and detailed. Our findings reveal differences in the preferred frequency and level of detail of ADs for different videos, participants’ sense of control with this style of AD delivery, and its limitations. We discuss the implications of these findings for the development of future AD tools for BLV users.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {458–474},
numpages = {17},
keywords = {audio description, online videos, accessibility, multimodal large language models, blind and low vision},
location = {
},
series = {DIS '25}
}

@article{10.1007/s42979-022-01240-8,
author = {Menaouer, Brahami and Dermane, Zoulikha and El Houda Kebir, Nour and Matta, Nada},
title = {Diabetic Retinopathy Classification Using Hybrid Deep Learning Approach},
year = {2022},
issue_date = {Jul 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {3},
number = {5},
url = {https://doi.org/10.1007/s42979-022-01240-8},
doi = {10.1007/s42979-022-01240-8},
abstract = {During the recent years, diabetic retinopathy (DR) has been one of the most threatening complications of diabetes that leads to permanent blindness. Further, DR mutilates the retinal blood vessels of a patient having diabetes. Accordingly, various artificial intelligence techniques and deep learning have been proposed to automatically detect abnormalities in DR and its different stages from retina images. In this paper, we propose a hybrid deep learning approach using deep convolutional neural network (CNN) method and two VGG network models (VGG16 and VGG19) to diabetic retinopathy detection and classification according to the visual risk linked to the severity of retinal ischemia. Indeed, the classification of DR deals with understanding the images and their context with respect to the categories. The experimental results, performed on 5584 images, which are an ensemble of online datasets, yielded an accuracy of 90.60%, recall of 95% and F1 score of 94%. The main aim of this work is to develop a robust system for detecting and classifying DR automatically.},
journal = {SN Comput. Sci.},
month = jul,
numpages = {15},
keywords = {Knowledge management, Deep learning, Convolutional neural networks (CNNs), VGGNet, Diabetic retinopathy, Image processing, Image classification, Healthcare decision support systems}
}

@inproceedings{10.1145/3701716.3715552,
author = {Guriundefined\u{a}, Alexandra-Elena and Vatavu, Radu-Daniel},
title = {Insights and Implications of Evaluating Accessibility Compliance in AI-Generated Web Interfaces},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715552},
doi = {10.1145/3701716.3715552},
abstract = {The recent availability of AI-driven user interface generation tools necessitates a proper understanding of their capabilities to produce accessible designs in accordance to established standards. To this end, this paper reports results from an evaluation of fifty user interfaces, generated using five publicly available AI design tools, against WCAG 2.1 criteria (e.g., text contrast and target size), comparing both accessibility-agnostic and accessibility-oriented text prompts. Our analysis reveals moderate violation severity (M=0.47) on a scale from 0, none to 4, critical), with text contrast (M=1.08) and target size (M=0.86) emerging as primary yet fixable violations. Contrary to our expectations, the accessibility-oriented prompt did not improve compliance in our dataset, but actually reduced it (M=0.54 vs. M=0.39). However, three of the examined AI tools showed improved results through dialogue and iterative refinement.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {996–1000},
numpages = {5},
keywords = {accessibility, ai-generated interfaces, design tools, generative ai, prompt engineering, user interface design, wcag},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1016/j.knosys.2024.112521,
author = {Purves, Tom and Kyriakopoulos, Konstantinos G. and Jenkins, Si\^{a}n and Phillips, Iain and Dudman, Tim},
title = {Causally aware reinforcement learning agents for autonomous cyber defence},
year = {2024},
issue_date = {Nov 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {304},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2024.112521},
doi = {10.1016/j.knosys.2024.112521},
journal = {Know.-Based Syst.},
month = nov,
numpages = {18},
keywords = {Autonomous cyber defence, Reinforcement learning, Causal inference, Structural Causal Model}
}

@inproceedings{10.1007/978-3-032-05825-6_9,
author = {M\"{u}ller, Johanna P. and Knupfer, Anika and Bl\"{o}ss, Pedro and Vittur, Edoardo Berardi and Kainz, Bernhard and Hutter, Jana},
title = {Diffusing the&nbsp;Blind Spot: Uterine MRI Synthesis with&nbsp;Diffusion Models},
year = {2025},
isbn = {978-3-032-05824-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-05825-6_9},
doi = {10.1007/978-3-032-05825-6_9},
abstract = {Despite significant progress in generative modelling, existing diffusion models often struggle to produce anatomically precise female pelvic images, limiting their application in gynaecological imaging, where data scarcity and patient privacy concerns are critical. To overcome these barriers, we introduce a novel diffusion-based framework for uterine MRI synthesis, integrating both unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs) and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates anatomically coherent, high fidelity synthetic images that closely mimic real scans and provide valuable resources for training robust diagnostic models. We evaluate generative quality using advanced perceptual and distributional metrics, benchmarking against standard reconstruction methods, and demonstrate substantial gains in diagnostic accuracy on a key classification task. A blinded expert evaluation further validates the clinical realism of our synthetic images. We release our models with privacy safeguards and a comprehensive synthetic uterine MRI dataset to support reproducible research and advance equitable AI in gynaecology. The code and data are available at .},
booktitle = {Skin Image Analysis, and Computer-Aided Pelvic Imaging for Female Health: 10th International Workshop, ISIC 2025, and First International Workshop, CAPI 2025, Held in Conjunction with MICCAI 2025, Daejeon, South Korea, September 23, 2025, Proceedings},
pages = {93–102},
numpages = {10},
keywords = {Uterus, Diffusion Models, Image Generation, MRI},
location = {Daejeon, Korea (Republic of)}
}

@article{10.1016/j.eswa.2023.122230,
author = {Liu, Jinhui and Tang, Bo and Dong, Guishan and Yu, Yong},
title = {iPMRSS: An Improved privacy-preserving medical record searching scheme for intelligent diagnosis in IoMT},
year = {2024},
issue_date = {Apr 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {239},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2023.122230},
doi = {10.1016/j.eswa.2023.122230},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {10},
keywords = {Internet of medical things, Data security, Medical record searching, Blind signature, ECDLP}
}

@inproceedings{10.1145/3510858.3511377,
author = {Chen, Jundong and Cao, Dezhuang and Wang, Zhaoxi and Lu, Qingru},
title = {Visual Assistance System for the Visually Impaired people based on Image Recognition Technology},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3511377},
doi = {10.1145/3510858.3511377},
abstract = {In recent years, with the development of Artificial Intelligence and Image Recognition Technology, the exploitation of assistive equipment for the disabled has become one of its popular application directions. In this research, we took STM32 as the core, build a convolutional neural network models for the living environment of visually impaired people, and designed an intelligent hardware device. The equipment also includes image acquisition, distance measurement, information broadcast and other modules, which finally realizes the effect of playing the recognition object name, distance and other information in real time. This paper introduces the working principle of various modules in the equipment, and provides a feasible idea for the application of image recognition technology in the auxiliary equipment for the blind.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {747–752},
numpages = {6},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1609/aaai.v37i7.26080,
author = {Luo, Xiaoling and Liu, Chengliang and Wong, Waikeung and Wen, Jie and Jin, Xiaopeng and Xu, Yong},
title = {MVCINN: multi-view diabetic retinopathy detection using a deep cross-interaction neural network},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i7.26080},
doi = {10.1609/aaai.v37i7.26080},
abstract = {Diabetic retinopathy (DR) is the main cause of irreversible blindness for working-age adults. The previous models for DR detection have difficulties in clinical application. The main reason is that most of the previous methods only use single-view data, and the single field of view (FOV) only accounts for about 13% of the FOV of the retina, resulting in the loss of most lesion features. To alleviate this problem, we propose a multi-view model for DR detection, which takes full advantage of multi-view images covering almost all of the retinal field. To be specific, we design a cross-interaction self-attention based module (CISAM) that interfuses local features extracted from convolutional blocks with long-range global features learned from transformer blocks. Furthermore, considering the pathological association in different views, we use the feature jigsaw to assemble and learn the features of multiple views. Extensive experiments on the latest public multi-view MFIDDR dataset with 34,452 images demonstrate the superiority of our method, which performs favorably against state-of-the-art models. To the best of our knowledge, this work is the first study on the public large-scale multi-view fundus images dataset for DR detection.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1011},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@phdthesis{10.5555/AAI30338628,
author = {Shi, Chuying},
advisor = {Benny, Zee, Chung Ying},
title = {Early Detection of Glaucoma Using Machine-Learning Methods for Colour Fundus Photographs with Image Quality Assessments},
year = {2022},
isbn = {9798368452265},
publisher = {The Chinese University of Hong Kong (Hong Kong)},
abstract = {BackgroundGlaucoma is one of the leading causes of irreversible blindness worldwide, which is a huge public health problem globally. The non-mydriatic retinal images are low-cost, portable, quick and straightforward to operate and interpret , which is an essential tool in tele-glaucoma. With the application of the artificial intelligence ( AI ) approach , it may outperform glaucoma specialists potential to be an automatic tool in telemedicine.ObjectivesObjective 1: Train, test, and validate the automatic retinal image analysis (ARIA) method to access image quality as well as to differentiate the eye-abnormality-associated-poor-quality from the artefact-associated-poor-quality on colour fundus retinal images.Objective 2: Train and test the ARIA method to detect glaucomatous optic neuropathy (GON) on non-mydriatic retinal images labelled with the additional results of OCT.&nbsp;Objective 3: Train, test, and validate the ARIA method to estimate continuous glaucomatous parameters, including optic nerve head (ONH) parameters, retinal nerve fibre layer (RNFL) thickness, and ganglion cell-inner plexiform layer (GCIPL) thickness, measured by OCT on non-mydriatic retinal images.&nbsp;},
note = {AAI30338628}
}

@article{10.1007/s00146-024-01862-x,
author = {Bown, Oliver},
title = {Blind search and flexible product visions: the sociotechnical shaping of generative music engines},
year = {2024},
issue_date = {Feb 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {40},
number = {2},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-024-01862-x},
doi = {10.1007/s00146-024-01862-x},
abstract = {Amidst the surge in AI-oriented commercial ventures, music is a site of intensive efforts to innovate. A number of companies are seeking to apply AI to music production and consumption, and amongst them several are seeking to reinvent the music listening experience as adaptive, interactive, functional and infinitely generative. These are bold objectives, having no clear roadmap for what designs, technologies and use cases, if any, will be successful. Thus each company relies on speculative product visions. Through four case studies of such companies, I consider how product visions must carefully provide a clear plan for developers and investors, whilst also remaining open to agile user-centred product development strategies, which I discuss in terms of the ‘blind search’ nature of innovation. I suggest that innovation in this area needs to be understood in terms of technological emergence, which is neither technologically determinist nor driven entirely by the visions of founders, but through a complex of interacting forces. I also consider, through these cases, how, through the accumulation of residual value, all such start-up work risks being exapted for more familiar extractive capitalist agendas under the general process that Doctorow calls “enshittification”. Lastly, I consider a number of other more specific ways in which these projects, if their growth is achieved, could influence music culture more broadly.},
journal = {AI Soc.},
month = mar,
pages = {585–603},
numpages = {19},
keywords = {Creative AI, AI music, AI start-ups, Generative music engines}
}

@inproceedings{10.1609/aaai.v37i4.25561,
author = {Li, Dong and Jin, Ruoming and Liu, Zhenming and Ren, Bin and Gao, Jing and Liu, Zhi},
title = {Towards reliable item sampling for recommendation evaluation},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i4.25561},
doi = {10.1609/aaai.v37i4.25561},
abstract = {Since Rendle and Krichene argued that commonly used sampling-based evaluation metrics are "inconsistent" with respect to the global metrics (even in expectation), there have been a few studies on the sampling-based recommender system evaluation. Existing methods try either mapping the sampling-based metrics to their global counterparts or more generally, learning the empirical rank distribution to estimate the top-K metrics. However, despite existing efforts, there is still a lack of rigorous theoretical understanding of the proposed metric estimators, and the basic item sampling also suffers from the "blind spot" issue, i.e., estimation accuracy to recover the top-K metrics when K is small can still be rather substantial. In this paper, we provide an in-depth investigation into these problems and make two innovative contributions. First, we propose a new item-sampling estimator that explicitly optimizes the error with respect to the ground truth, and theoretically highlights its subtle difference against prior work. Second, we propose a new adaptive sampling method that aims to deal with the "blind spot" problem and also demonstrate the expectation-maximization (EM) algorithm can be generalized for such a setting. Our experimental results confirm our statistical analysis and the superiority of the proposed works. This study helps lay the theoretical foundation for adopting item sampling metrics for recommendation evaluation and provides strong evidence for making item sampling a powerful and reliable tool for recommendation evaluation.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {492},
numpages = {8},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@inproceedings{10.1609/aaai.v37i4.25636,
author = {He, Jialing and Liu, Jiamou and Zhang, Zijian and Chen, Yang and Liu, Yiwei and Khoussainov, Bakh and Zhu, Liehuang},
title = {MSDC: exploiting multi-state power consumption in non-intrusive load monitoring based on a dual-CNN model},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i4.25636},
doi = {10.1609/aaai.v37i4.25636},
abstract = {Non-intrusive load monitoring (NILM) aims to decompose aggregated electrical usage signal into appliance-specific power consumption and it amounts to a classical example of blind source separation tasks. Leveraging recent progress on deep learning techniques, we design a new neural NILM model Multi-State Dual CNN (MSDC). Different from previous models, MSDC explicitly extracts information about the appliance's multiple states and state transitions, which in turn regulates the prediction of signals for appliances. More specifically, we employ a dual-CNN architecture: one CNN for outputting state distributions and the other for predicting the power of each state. A new technique is invented that utilizes conditional random fields (CRF) to capture state transitions. Experiments on two real-world datasets REDD and UK-DALE demonstrate that our model significantly outperform state-of-the-art models while having good generalization capacity, achieving 6%-10% MAE gain and 33%-51% SAE gain to unseen appliances.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {567},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@article{10.1007/s10639-024-12633-y,
author = {Cingillioglu, Ilker and Gal, Uri and Prokhorov, Artem},
title = {AI-experiments in education: An AI-driven randomized controlled trial for higher education research},
year = {2024},
issue_date = {Oct 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {15},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-024-12633-y},
doi = {10.1007/s10639-024-12633-y},
abstract = {This study presents a novel approach contributing to our understanding of the design, development, and implementation AI-based systems for conducting double-blind online randomized controlled trials (RCTs) for higher education research. The process of the entire interaction with the participants (n = 1193) and their allocation to test and control groups was executed seamlessly by our AI system, without human intervention. In this fully automated experiment, we systematically examined eight hypotheses. The AI-experiment strengthened five of these hypotheses, while not accepting three of the factors previously acknowledged in the literature as influential in students’ choices of universities. We showcased how AI can efficiently interview participants and collect their input, offering robust evidence through an RCT (Gold standard) to establish causal relationships between interventions and their outcomes. This approach may enable researchers and industry practitioners to collect data from large samples on which such experiments can be conducted with and by AI to produce statistically reproducible, reliable, and generalizable results in an efficient, rigorous and ethical way.},
journal = {Education and Information Technologies},
month = mar,
pages = {19649–19677},
numpages = {29},
keywords = {AI-based chatbots, AI experiments, AI-led RCT, Social online experiments}
}

@article{10.1016/j.patcog.2025.111693,
author = {Zhou, Tianwei and Tan, Songbai and Li, Leida and Zhao, Baoquan and Jiang, Qiuping and Yue, Guanghui},
title = {Cross-Modality Interactive Attention Network for AI-generated image quality assessment},
year = {2025},
issue_date = {Nov 2025},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2025.111693},
doi = {10.1016/j.patcog.2025.111693},
journal = {Pattern Recogn.},
month = aug,
numpages = {11},
keywords = {AI-generated images, Cross-modality interactive attention, Multi-task learning, Blind image quality assessment}
}

@article{10.1145/3609468.3609471,
author = {Wijmans, Erik and Savva, Manolis and Essa, Irfan and Lee, Stefan and Morcos, Ari S. and Batra, Dhruv},
title = {Emergence of Maps in the Memories of Blind Navigation Agents},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3609468.3609471},
doi = {10.1145/3609468.3609471},
abstract = {Decades of research into intelligent animal navigation posits that organisms build and maintain internal spatial representations (or maps)1 of their environment, that enables the organism to determine and follow task-appropriate paths (Epstein, Patai, Julian, &amp; Spiers, 2017; O'keefe &amp; Nadel, 1978; Tollman, 1948). Hamsters, wolves, chimpanzees, and bats leverage prior exploration to determine and follow shortcuts they may never have taken before (Chapuis &amp; Scardigli, 1993; Harten, Katz, Goldshtein, Handel, &amp; Yovel, 2020; Menzel, 1973; Peters, 1976; Toledo et al., 2020). Even blind mole rats and animals rendered situationally-blind in dark environments demonstrate shortcut behaviors (Avni, Tzvaigrach, &amp; Eilam, 2008; Kimchi, Etienne, &amp; Terkel, 2004; Maaswinkel &amp; Whishaw, 1999). Ants forage for food along meandering paths but take near-optimal return trips (M\"{u}ller &amp; Wehner, 1988), though there is some controversy about whether insects like ants and bees are capable of forming maps (Cheung et al., 2014; Cruse &amp; Wehner, 2011).},
journal = {AI Matters},
month = oct,
pages = {8–14},
numpages = {7}
}

@inproceedings{10.1609/aaai.v37i4.25655,
author = {Sun, Ling and Rao, Yuan and Lan, Yuqian and Xia, Bingcan and Li, Yangyang},
title = {HG-SL: jointly learning of global and local user spreading behavior for fake news early detection},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i4.25655},
doi = {10.1609/aaai.v37i4.25655},
abstract = {Recently, fake news forgery technology has become more and more sophisticated, and even the profiles of participants may be faked, which challenges the robustness and effectiveness of traditional detection methods involving text or user identity. Most propagation-only approaches mainly rely on neural networks to learn the diffusion pattern of individual news, but this is insufficient to describe the differences in news spread ability, and also ignores the valuable global connections of news and users, limiting the performance of detection. Therefore, we propose a joint learning model named HG-SL, which is blind to news content and user identity, but capable of catching the differences between true and fake news in the early stages of propagation through global and local user spreading behavior. Specifically, we innovatively design a Hypergraph-based Global interaction learning module to capture the global preferences of users from their co-spreading behaviors, and introduce node centrality encoding to complement user influence in hypergraph learning. Moreover, the designed Self-attention-based Local context learning module first introduce spread status in behavior learning process to highlight the propagation ability of news and users, thus providing additional signals for verifying news authenticity. Experiments on real-world datasets indicate that our HG-SL, which solely relies on user behavior, outperforms SOTA baselines utilizing multidimensional features in both fake news detection and early detection task.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {586},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@article{10.1016/j.ijhcs.2025.103525,
author = {Wu, Hantian and Yang, Hongyi and Chang, Fangyuan and Zhu, Dian and Liu, Zhao},
title = {AI-generated tactile graphics for visually impaired children: A usability study of a multimodal educational product},
year = {2025},
issue_date = {May 2025},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {200},
number = {C},
issn = {1071-5819},
url = {https://doi.org/10.1016/j.ijhcs.2025.103525},
doi = {10.1016/j.ijhcs.2025.103525},
journal = {Int. J. Hum.-Comput. Stud.},
month = jun,
numpages = {14},
keywords = {Human computer interaction (HCL), Interaction design, Artificial intelligence, Accessibility}
}

@article{10.1016/j.procs.2021.12.036,
author = {Leite, Danilo and Campelos, Maria and Fernandes, Ana and Batista, Pedro and Beir\~{a}o, Jo\~{a}o and Men\'{e}res, Pedro and Cunha, Ant\'{o}nio},
title = {Machine Learning automatic assessment for glaucoma and myopia based on Corvis ST data},
year = {2022},
issue_date = {2022},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {196},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.12.036},
doi = {10.1016/j.procs.2021.12.036},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {454–460},
numpages = {7},
keywords = {Corvis ST, Machine Learning, Glaucoma, Cornea dynamics}
}

@article{10.1016/j.ins.2022.07.094,
author = {Xia, Xuan and He, Xing and Feng, Lu and Pan, Xizhou and Li, Nan and Zhang, Jingfei and Pang, Xufang and Yu, Fengqi and Ding, Ning},
title = {Semantic translation of face image with limited pixels for simulated prosthetic vision},
year = {2022},
issue_date = {Sep 2022},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {609},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2022.07.094},
doi = {10.1016/j.ins.2022.07.094},
journal = {Inf. Sci.},
month = sep,
pages = {507–532},
numpages = {26},
keywords = {Simulated prosthetic vision, Image-to-image translation, Generative adversarial network, Pixel face}
}

@inproceedings{10.1609/aaai.v37i2.25302,
author = {Qin, Guanyi and Hu, Runze and Liu, Yutao and Zheng, Xiawu and Liu, Haotian and Li, Xiu and Zhang, Yan},
title = {Data-efficient image quality assessment with attention-panel decoder},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i2.25302},
doi = {10.1609/aaai.v37i2.25302},
abstract = {Blind Image Quality Assessment (BIQA) is a fundamental task in computer vision, which however remains unresolved due to the complex distortion conditions and diversified image contents. To confront this challenge, we in this paper propose a novel BIQA pipeline based on the Transformer architecture, which achieves an efficient quality-aware feature representation with much fewer data. More specifically, we consider the traditional fine-tuning in BIQA as an interpretation of the pre-trained model. In this way, we further introduce a Transformer decoder to refine the perceptual information of the CLS token from different perspectives. This enables our model to establish the quality-aware feature manifold efficiently while attaining a strong generalization capability. Meanwhile, inspired by the subjective evaluation behaviors of human, we introduce a novel attention panel mechanism, which improves the model performance and reduces the prediction uncertainty simultaneously. The proposed BIQA method maintains a lightweight design with only one layer of the decoder, yet extensive experiments on eight standard BIQA datasets (both synthetic and authentic) demonstrate its superior performance to the state-of-the-art BIQA methods, i.e., achieving the SRCC values of 0.875 (vs. 0.859 in LIVEC) and 0.980 (vs. 0.969 in LIVE).},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {233},
numpages = {10},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@inproceedings{10.1145/3674225.3674348,
author = {Xie, Chen and Yang, Ling and Zhu, Difan and Li, Jiewen and Hu, Wenbo},
title = {A Short-term Power Load Forecasting Based on CSWOA-TPA-BiGRU},
year = {2024},
isbn = {9798400716638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674225.3674348},
doi = {10.1145/3674225.3674348},
abstract = {To address the issues of existing short-term load forecasting models that cannot effectively mine key information for load prediction, lack of extraction of time series patterns, and insufficient prediction accuracy, a short-term electric load forecasting model is proposed that combines Bidirectional Gated Recurrent Unit (BiGRU), Temporal Pattern Attention (TPA)and Crisscross Whale Optimization Algorithm (CSWOA). Firstly, BiGRU is used to obtain time series features in the raw data and effectively capture the changing patterns of key feature vectors. Secondly, TPA adaptively weights the state vectors output by BiGRU to further mine the hidden relationships among different variables at different time steps and solve the blindness of artificially selecting the influencing factors of load forecasting. Finally, the CSWOA algorithm is used to optimize the weight coefficients and bias parameters of the fully connected layer in the TPA-BiGRU model, solving the problem that neural networks using gradient descent are prone to getting stuck in local optima when updating parameters. The model was tested on electric load data provided by the 9th National University Student Electricity and Mathematics Modeling Competition. The experimental results showed that the model had lower percentage error (MAPE), root mean square error (RMSE), and higher determination coefficient (R2).},
booktitle = {Proceedings of the 2024 International Conference on Power Electronics and Artificial Intelligence},
pages = {677–681},
numpages = {5},
location = {Xiamen, China},
series = {PEAI '24}
}

@inproceedings{10.1609/aaai.v37i8.26106,
author = {Amiri, Mohammad Mohammadi and Berdoz, Fr\'{e}d\'{e}ric and Raskar, Ramesh},
title = {Fundamentals of task-agnostic data valuation},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i8.26106},
doi = {10.1609/aaai.v37i8.26106},
abstract = {We study valuing the data of a data owner/seller for a data seeker/buyer. Data valuation is often carried out for a specific task assuming a particular utility metric, such as test accuracy on a validation set, that may not exist in practice. In this work, we focus on task-agnostic data valuation without any validation requirements. The data buyer has access to a limited amount of data (which could be publicly available) and seeks more data samples from a data seller. We formulate the problem as estimating the differences in the statistical properties of the data at the seller with respect to the baseline data available at the buyer. We capture these statistical differences through second moment by measuring diversity and relevance of the seller's data for the buyer; we estimate these measures through queries to the seller without requesting the raw data. We design the queries with the proposed approach so that the seller is blind to the buyer's raw data and has no knowledge to fabricate responses to the queries to obtain a desired outcome of the diversity and relevance trade-off. We will show through extensive experiments on real tabular and image datasets that the proposed estimates capture the diversity and relevance of the seller's data for the buyer.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1037},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@article{10.1016/j.compedu.2022.104657,
author = {Hsu, Ting-Chia and Chang, Ching and Lin, Yi-Wei},
title = {Effects of voice assistant creation using different learning approaches on performance of computational thinking},
year = {2023},
issue_date = {Jan 2023},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {192},
number = {C},
issn = {0360-1315},
url = {https://doi.org/10.1016/j.compedu.2022.104657},
doi = {10.1016/j.compedu.2022.104657},
journal = {Comput. Educ.},
month = jan,
numpages = {13},
keywords = {Artificial intelligence, Computational thinking, Experiential learning, Student learning behaviour, Voice assistant}
}

@inproceedings{10.1145/3708359.3712082,
author = {Chheda-Kothary, Arnavi and Kanchi, Ritesh and Sanders, Chris and Xiao, Kevin and Sengupta, Aditya and Kneitmix, Melanie and Wobbrock, Jacob O. and Froehlich, Jon E.},
title = {ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed Visual-Ability Families},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712082},
doi = {10.1145/3708359.3712082},
abstract = {We introduce ArtInsight, a novel AI-powered system to facilitate deeper engagement with child-created artwork in mixed visual-ability families. ArtInsight leverages large language models (LLMs) to craft a respectful and thorough initial description of a child’s artwork, and provides: creative AI-generated descriptions for a vivid overview, audio recording to capture the child’s own description of their artwork, and a set of AI-generated questions to facilitate discussion between blind or low-vision (BLV) family members and their children. Alongside ArtInsight, we also contribute a new rubric to score AI-generated descriptions of child-created artwork and an assessment of state-of-the-art LLMs. We evaluated ArtInsight with five groups of BLV family members and their children, and as a case study with one BLV child therapist. Our findings highlight a preference for ArtInsight’s longer, artistically-tailored descriptions over those generated by existing BLV AI tools. Participants highlighted the creative description and audio recording components as most beneficial, with the former helping “bring a picture to life” and the latter centering the child’s narrative to generate context-aware AI responses. Our findings reveal different ways that AI can be used to support art engagement, including before, during, and after interaction with the child artist, as well as expectations that BLV adults and their sighted children have about AI-powered tools.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {190–210},
numpages = {21},
keywords = {Accessibility, blind or low-vision, mixed-ability families, children’s artwork, AI},
location = {
},
series = {IUI '25}
}

@proceedings{10.5555/3721488,
title = {HRI '25: Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
year = {2025},
publisher = {IEEE Press},
abstract = {Welcome to the 20th Annual IEEE/ACM International Conference on Human-Robot Interaction: we are so pleased to welcome the HRI community to Melbourne, Australia, to celebrate together the 20th anniversary of our conference.The HRI community is a living and evolving community, and we want our conference to reflect these evolutions as well. Indeed, HRI'25 is introducing several important changes: the most visible one is certainly the new dual-track format, which reflects the growth of the community. We also acknowledge the growing interest of the industry in HRI and we want the HRI conference to be a place to foster discussions and networking between academia and industry. To this end, we introduce this year a full-day industry forum, the Industry Day, which takes place in parallel to the workshops.But maybe more important to us, General and Program Chairs, is the focus we are putting on our social and environmental responsibility. The theme of HRI'25 is 'Robots for a Sustainable World,' and we want the conference itself to live up to this objective. Not simply with reusable cups and low-CO2 menus, but also by asking what sustainable research might mean in the age of Generative AI and climate crises, and more specifically what sustainable human-robot interactions might look like.On the conference website, we have published a call for action1: we encourage all of you to read it and discuss it during the conference. We certainly do not hold all the keys, but we can and should play our part. The (somewhat controversial) decision to hold the conference in-persononly, with no hybrid option, is an example of our reflection towards more sustainable science: we believe that the purpose of a scientific conference should be first and foremost to meet people, network, throw ideas at each other, and challenge them. Remote attendance does not enable this kind of interaction, so instead, we have built plenty of time for panels and interactive sessions in our program, and we went the extra mile to request and obtain funding for travel grants, ensuring as many people as possible can join us in Melbourne. In that sense, we hope that HRI'25 will be 'worth its carbon footprint' in fostering new collaboration, ideas, and projects.The call for papers attracted a record-breaking 400 Full Paper submissions-a 13.6% increase from last year-with excellent contributions spanning a wide geographical distribution. Full papers underwent a double-blind review process with three external reviews, a rebuttal phase, and selective shepherding by the HRI program committee. Ultimately, 100 outstanding submissions were accepted, for a total of 346 scientific artifacts to be presented (all categories included) during the conference: HRI'25 is by a solid margin the largest HRI conference ever organized. The strong interest in our research field extends as well beyond the academic sphere, with a record number of organizations sponsoring the event, from small start-ups to large companies. See the table below for the full submission and acceptance statistics.},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/3616855.3635818,
author = {Wallat, Jonas and Jatowt, Adam and Anand, Avishek},
title = {Temporal Blind Spots in Large Language Models},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635818},
doi = {10.1145/3616855.3635818},
abstract = {Large language models (LLMs) have recently gained significant attention due to their unparalleled zero-shot performance on various natural language processing tasks. However, the pre-training data utilized in LLMs is often confined to a specific corpus, resulting in inherent freshness and temporal scope limitations. Consequently, this raises concerns regarding the effectiveness of LLMs for tasks involving temporal intents. In this study, we aim to investigate the underlying limitations of general-purpose LLMs when deployed for tasks that require a temporal understanding. We pay particular attention to handling factual temporal knowledge through three popular temporal QA datasets. Specifically, we observe low performance on detailed questions about the past and, surprisingly, for rather new information. In manual and automatic testing, we find multiple temporal errors and characterize the conditions under which QA performance deteriorates. Our analysis contributes to understanding LLM limitations and offers valuable insights into developing future models that can better cater to the demands of temporally-oriented tasks. The code is available https://github.com/jwallat/temporalblindspots.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {683–692},
numpages = {10},
keywords = {large language models, question answering, temporal information retrieval, temporal query intents},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3663548.3675663,
author = {Jung, Crescentia and Collins, Jazmin and Gonzalez Penuela, Ricardo E. and Segal, Jonathan Isaac and Won, Andrea Stevenson and Azenkot, Shiri},
title = {Accessible Nonverbal Cues to Support Conversations in VR for Blind and Low Vision People},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675663},
doi = {10.1145/3663548.3675663},
abstract = {Social VR has increased in popularity due to its affordances for rich, embodied, and nonverbal communication. However, nonverbal communication remains inaccessible for blind and low vision people in social VR. We designed accessible cues with audio and haptics to represent three nonverbal behaviors: eye contact, head shaking, and head nodding. We evaluated these cues in real-time conversation tasks where 16 blind and low vision participants conversed with two other users in VR. We found that the cues were effective in supporting conversations in VR. Participants had statistically significantly higher scores for accuracy and confidence in detecting attention during conversations with the cues than without. We also found that participants had a range of preferences and uses for the cues, such as learning social norms. We present design implications for handling additional cues in the future, such as the challenges of incorporating AI. Through this work, we take a step towards making interpersonal embodied interactions in VR fully accessible for blind and low vision people.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {20},
numpages = {13},
keywords = {VR, accessibility, blind, low vision},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3650215.3650369,
author = {Lin, Jie and Lei, Sida and Huang, Silu and Li, Mingpeng and Xiao, Qiang and Huang, Yubing},
title = {Research on Intelligent Detection Technology of Bridge Surface Defects Based on UAVs},
year = {2024},
isbn = {9798400709449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650215.3650369},
doi = {10.1145/3650215.3650369},
abstract = {In order to address the issue of blind spots in traditional bridge inspections, achieve comprehensive coverage of bridge inspections, and enhance the safety and effectiveness of bridge health assessments, this paper proposes an intelligent detection method for bridge surface defects based on unmanned aerial vehicles (UAVs) as carriers and artificial intelligence as the core. This method initially combines GPS positioning information with Ultra-Wideband (UWB) wireless communication technology positioning information through smoothing processing, enabling seamless switching between the two positioning methods. Subsequently, a UAV equipped with a telephoto camera is employed for autonomous omnidirectional flight and data collection over the bridge. Finally, the collected data is processed using the StyleGAN2 (Style-Based Generative Adversarial Network) model to generate a larger quantity of finely distributed images of small cracks, which are then recognized in real time using the YOLOv3 neural network algorithm. The proposed method is tested on the Longtanhe Extra-Large Bridge segment of the G50 Shanghai-Chongqing Expressway mainline in Hubei, China. Experimental results demonstrate that the proposed method not only achieves accurate UAV positioning in situations where GPS signals are obstructed, but also effectively identifies small cracks on the bridge surface in complex backgrounds, with a crack recall rate of 90% and an improved crack recognition rate of 95%. The methodology presented in this paper provides a scientific and precise basis for the safe operation and maintenance of bridges, effectively addressing the shortcomings of traditional bridge inspection methods. It offers a new direction for the development of intelligent bridge inspection techniques, bridging the gap between conventional methods and intelligent approaches.},
booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application},
pages = {878–887},
numpages = {10},
location = {Hangzhou, China},
series = {ICMLCA '23}
}

@inproceedings{10.5555/3600270.3600930,
author = {Wang, Yipei and Wang, Xiaoqian},
title = {"Why not other classes?": towards class-contrastive back-propagation explanations},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Numerous methods have been developed to explain the inner mechanism of deep neural network (DNN) based classifiers. Existing explanation methods are often limited to explaining predictions of a pre-specified class, which answers the question "why is the input classified into this class?" However, such explanations with respect to a single class are inherently insufficient because they do not capture features with class-discriminative power. That is, features that are important for predicting one class may also be important for other classes. To capture features with true class-discriminative power, we should instead ask "why is the input classified into this class, but not others?" To answer this question, we propose a weighted contrastive framework for explaining DNNs. Our framework can easily convert any existing back-propagation explanation methods to build class-contrastive explanations. We theoretically validate our weighted contrast explanation in general back-propagation explanations, and show that our framework enables class-contrastive explanations with significant improvements in both qualitative and quantitative experiments. Based on the results, we point out an important blind spot in the current explainable artificial intelligence (XAI) study, where explanations towards the predicted logits and the probabilities are obfuscated. We suggest that these two aspects should be distinguished explicitly any time explanation methods are applied.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {660},
numpages = {13},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@inproceedings{10.5555/3625834.3625988,
author = {Pandeva, Teodora and Forr\'{e}, Patrick},
title = {Multi-view independent component analysis with shared and individual sources},
year = {2023},
publisher = {JMLR.org},
abstract = {Independent component analysis (ICA) is a blind source separation method for linear disentanglement of independent latent sources from observed data. We investigate the special setting of noisy linear ICA, referred to as ShIndICA, where the observations are split among different views, each receiving a mixture of shared and individual sources. We prove that the corresponding linear structure is identifiable and the sources distribution can be recovered. To computationally estimate the sources, we optimize a constrained form of the joint log-likelihood of the observed data among all views. Furthermore, we propose a model selection procedure for recovering the number of shared sources. Finally, we empirically demonstrate the advantages of our model over baselines. We apply ShIndICA in a challenging real-life task, using three transcriptome datasets provided by three different labs (three different views). The recovered sources were used for a downstream graph inference task, facilitating the discovery of a plausible representation of the data's underlying graph structure.},
booktitle = {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
articleno = {154},
numpages = {12},
location = {Pittsburgh, PA, USA},
series = {UAI '23}
}

@inproceedings{10.1145/3661638.3661705,
author = {Liu, Shuai and Zuo, Yuanjun and Chen, Zhuo and Zhang, Wenjing and Li, Xiaoqing},
title = {Non-invasive event monitoring in digital twin system based on random matrix theory and independent sub-vectors},
year = {2024},
isbn = {9798400716966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661638.3661705},
doi = {10.1145/3661638.3661705},
abstract = {In this paper, a new method of independent event monitoring and decomposition is proposed, which introduces random matrix theory (RMS) and improved fast independent component analysis (FastICA) into non-intrusive event monitoring. Firstly, the M-P law of random matrix theory is used to identify the independent events in the digital twin system and obtain the number of independent source signals. Then, the independent sub-vectors of the original measured signals are calculated. Finally, the blind source separation of independent events is realized based on FastICA. The effectiveness and feasibility of the proposed method are verified by the analysis in an actual integrated energy digital twin system.},
booktitle = {Proceedings of the 2023 International Conference on Artificial Intelligence, Systems and Network Security},
pages = {355–359},
numpages = {5},
location = {Mianyang, China},
series = {AISNS '23}
}

@inproceedings{10.24963/ijcai.2023/170,
author = {Wu, Wanyu and Wang, Wei and Wang, Zheng and Jiang, Kui and Xu, Xin},
title = {From generation to suppression: towards effective irregular glow removal for nighttime visibility enhancement},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/170},
doi = {10.24963/ijcai.2023/170},
abstract = {Most existing Low-Light Image Enhancement (LLIE) methods are primarily designed to improve brightness in dark regions, which suffer from severe degradation in nighttime images. However, these methods have limited exploration in another major visibility damage, the glow effects in real night scenes. Glow effects are inevitable in the presence of artificial light sources and cause further diffused blurring when directly enhanced. To settle this issue, we innovatively consider the glow suppression task as learning physical glow generation via multiple scattering estimation according to the Atmospheric Point Spread Function (APSF). In response to the challenges posed by uneven glow intensity and varying source shapes, an APSF-based Nighttime Imaging Model with Near-field Light Sources (NIM-NLS) is specifically derived to design a scalable Light-aware Blind Deconvolution Network (LBDN). The glow-suppressed result is then brightened via a Retinex-based Enhancement Module (REM). Remarkably, the proposed glow suppression method is based on zero-shot learning and does not rely on any paired or unpaired training data. Empirical evaluations demonstrate the effectiveness of the proposed method in both glow suppression and low-light enhancement tasks.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {170},
numpages = {9},
location = {Macao, P.R.China},
series = {IJCAI '23}
}

@inproceedings{10.1145/3632971.3632997,
author = {Zheng, Lin Jin and Meng, Ming De and Yu, Bo Hai and Hao, Ran Ze and Niu, Tian Da},
title = {Bi-Unet: Encoder-decoder Based Network for Lesion Segmentation of Diabetic Retinopathy Images},
year = {2024},
isbn = {9798400707704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632971.3632997},
doi = {10.1145/3632971.3632997},
abstract = {Diabetic Retinopathy (DR) is a prevalent complication of diabetes mellitus and a leading cause of adult blindness worldwide. In this study, we propose a multiclass segmentation method for DR images using a codec structure. Specifically, we use an encoder to down-sample the image, followed by feature extraction using a quadruple Multi-scale Receptive Field (MSRF) module to address uneven lesion distribution. We then apply the Bi-layer Feature Fusion (BiLFF) module mechanism to fuse feature maps obtained from down-sampling. This mechanism adaptively learns important foci parts of the features while reducing interference from non-important healthy eye information. Finally, we up-sample the fused features and cascade them with feature maps obtained from corresponding down-sampling. We perform feature extraction on cascaded feature maps to obtain final predicted segmentation maps. Our method achieves state-of-the-art results on the DDR dataset.},
booktitle = {Proceedings of the 2023 International Joint Conference on Robotics and Artificial Intelligence},
pages = {59–64},
numpages = {6},
location = {Shanghai, China},
series = {JCRAI '23}
}

@inproceedings{10.24963/ijcai.2023/697,
author = {Yu, Lu and Nikandrou, Malvina and Jin, Jiali and Rieser, Verena},
title = {Quality-agnostic image captioning to safely assist people with vision impairment},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/697},
doi = {10.24963/ijcai.2023/697},
abstract = {Automated image captioning has the potential to be a useful tool for people with vision impairments. Images taken by this user group are often noisy, which leads to incorrect and even unsafe model predictions. In this paper, we propose a quality-agnostic framework to improve the performance and robustness of image captioning models for visually impaired people. We address this problem from three angles: data, model, and evaluation. First, we show how data augmentation techniques for generating synthetic noise can address data sparsity in this domain. Second, we enhance the robustness of the model by expanding a state-of-the-art model to a dual network architecture, using the augmented data and leveraging different consistency losses. Our results demonstrate increased performance, e.g. an absolute improvement of 2.15 on CIDEr, compared to state-of-the-art image captioning networks, as well as increased robustness to noise with up to 3 points improvement on CIDEr in more noisy settings. Finally, we evaluate the prediction reliability using confidence calibration on images with different difficulty / noise levels, showing that our models perform more reliably in safety-critical situations. The improved model is part of an assisted living application, which we develop in partnership with the Royal National Institute of Blind People.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {697},
numpages = {9},
location = {Macao, P.R.China},
series = {IJCAI '23}
}

@inproceedings{10.1145/3640872.3640873,
author = {Ma, Jianqiao and Gao, Chao and Li, Ting},
title = {Intelligent Signal Control System for Abnormal Traffic Conditions in Slow-Moving Traffic},
year = {2024},
isbn = {9798400708695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640872.3640873},
doi = {10.1145/3640872.3640873},
abstract = {Current intersection signal control studies mainly consider the passage of motor vehicles, with too little consideration given to pedestrians and non-motorized factors. This has led to many unsafe hazards in slow-moving traffic at intersections, such as travel conflicts caused by pedestrians and right-turning vehicles being released at the same time; the red light comes on while vulnerable groups such as the elderly do not finish crossing the street, etc. The resulting accident disputes are common. This project introduces computer vision technology and artificial intelligence technology into the intersection signal control system to improve the traditional signal control strategy in terms of hardware and software, respectively. In terms of hardware, an "emergency stop" signal is proposed to alert motorists to stop in case of a possible collision with pedestrians or non-motorized vehicles. At the same time, a "go" signal and face capture are activated at crosswalks to remind pedestrians to leave the intersection quickly. In the software, firstly, the surveillance video of the crosswalk is transmitted to the backend server; then, the YOLOv6 architecture is used as the basis for image detection of pedestrians, non-motorized vehicles and motor vehicles; then, the distance between pedestrians, non-motorized vehicles and motor vehicles is calculated; then, based on the distance and location, it determines whether the "emergency stop" signal is activated in each lane. Finally, the traditional intersection signal timing method is improved by considering the delay caused by "emergency stop". This technology can effectively protect the safety of pedestrians and non-motorized travelers, especially disadvantaged groups, and effectively solve the safety hazards caused by vehicle blind spots and reduce the occurrence of traffic accidents.},
booktitle = {Proceedings of the 2023 5th International Conference on Big Data Engineering},
pages = {1–5},
numpages = {5},
keywords = {VISSIM, artificial intelligence, computer vision technology, emergency parking, intersection traffic accidents, motor vehicles, signal timing, slow-moving traffic},
location = {Zhuhai, China},
series = {BDE '23}
}

@inproceedings{10.1145/3715070.3749268,
author = {Jiang, Ziyi and Jian, Mengjie and Hu, Jiajia and Zhao, Hongze and Qu, Huamin and Xu, Shuchang and Liu, Guanhong},
title = {From Audio Description to Movement: Challenges and Design Principles in Video-based Body Exercise for Blind and Low Vision Users},
year = {2025},
isbn = {9798400714801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715070.3749268},
doi = {10.1145/3715070.3749268},
abstract = {The proliferation of online workout videos has revolutionized fitness practices, yet their visual-centric design creates significant accessibility barriers for blind and low vision (BLV) users. Through interviews and co-watching sessions with eight BLV participants, we identify three key challenges in video-based exercise: (1) unclear motion instructions, (2) misaligned pacing, and (3) lack of performance feedback. To address these challenges, we propose five design principles for accessible video-based exercise: clear movement descriptions, step-by-step breakdown, rhythm guidance, personalized adaptation, and trust-building feedback. We operationalize these principles through an AI-driven audio guidance system featuring: (1) standardized motion descriptions, (2) personalized pacing control, and (3) real-time corrective feedback. Our work provides insights for designing inclusive fitness experiences for BLV users.},
booktitle = {Companion Publication of the 2025 Conference on Computer-Supported Cooperative Work and Social Computing},
pages = {446–451},
numpages = {6},
keywords = {Workout Video, Body Exercise, Audio Description, Blind, Low Vision, Visual Impairment},
location = {
},
series = {CSCW Companion '25}
}

@inproceedings{10.1145/3661638.3661691,
author = {Cong, Guotao and Wang, Sen and Li, Henan},
title = {Implementation of Image Watermark Algorithm Based on FPGAImage watermark embedding circuit},
year = {2024},
isbn = {9798400716966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661638.3661691},
doi = {10.1145/3661638.3661691},
abstract = {With the popularization of the Internet and the widespread application of digital technology, there are more and more digital products that are easy to spread, and at the same time, copyright protection issues are becoming increasingly prominent. Digital watermarking technology has been continuously developed in this context. This system is based on the FPGA platform to embed image watermarks and achieve hardware acceleration processing. Receive the data of the original image and watermark image through the high-speed transmission interface, perform wavelet transform on the original image, embed the watermark image, and then output the synthesized watermark image data through the high-speed transmission interface. This system selects the lifting wavelet transform algorithm as the watermark embedding algorithm to achieve blind watermarking, which has the characteristics of easy hardware implementation, high robustness and security of the watermark. The system has been verified through simulation and physical testing, with correct functionality and reliable performance.},
booktitle = {Proceedings of the 2023 International Conference on Artificial Intelligence, Systems and Network Security},
pages = {277–282},
numpages = {6},
keywords = {Field Programmable Gate Array, Hardware Acceleration, Image Watermarks, Wavelet Transform},
location = {Mianyang, China},
series = {AISNS '23}
}

@phdthesis{10.5555/AAI29994282,
author = {Lee, Kyungjun and Abhinav, Shrivastava, and Hal, Daum\'{e} III, and Michelle, Mazurek, and Gregg, Vanderheiden,},
advisor = {Hernisa, Kacorri,},
title = {Egocentric Vision in Assistive Technologies for and by the Blind},
year = {2022},
isbn = {9798368485331},
publisher = {University of Maryland, College Park},
abstract = {Visual information in our surroundings, such as everyday objects and passersby, is often inaccessible to people who are blind. Cameras that leverage egocentric vision, in an attempt to approximate the visual field of the camera wearer, hold great promise for making the visual world more accessible for this population. Typically, such applications rely on pre-trained computer vision models and thus are limited. Moreover, as with any AI system that augments sensory abilities, conversations around ethical implications and privacy concerns lie at the core of their design and regulation. However, early efforts tend to decouple perspectives, considering only either those of the blind users or potential bystanders.In this dissertation, we revisit egocentric vision for the blind. Through a holistic approach, we examine the following dimensions: type of application (objects and passersby), camera form factor (handheld and wearable), user's role (a passive consumer and an active director of technology), and privacy concerns (from both end-users and bystanders). Specifically, we propose to design egocentric vision models that capture blind users' intent and are fine-tuned by the user in the context of object recognition. We seek to explore societal issues that AI-powered cameras may lead to, considering perspectives from both blind users and nearby people whose faces or objects might be captured by the cameras. Last, we investigate interactions and perceptions across different camera form factors to reveal design implications for future work.},
note = {AAI29994282}
}

@inproceedings{10.1145/3644116.3644196,
author = {Gao, Jianyun and Xiang, Rongwu and Jiang, Zongbo and Ran, Lin and Li, Shu},
title = {Combining ResNet and RBF Networks to Enhance the Robustness of Diabetic Retinopathy Detection Against FGSM Attacks},
year = {2024},
isbn = {9798400708138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644116.3644196},
doi = {10.1145/3644116.3644196},
abstract = {Diabetic Retinopathy (DR) is a common complication of diabetes that can lead to vision loss if not detected early. Deep learning models, particularly Convolutional Neural Networks (CNN), have shown impressive results in DR detection. However, these models are susceptible to adversarial attacks, such as the Fast Gradient Sign Method (FGSM), leading to misclassification of detection data. In order to improve the robustness of the Diabetic Retinopathy (DR) detection model against FGSM attacks, we propose a new neural network in this paper that combines the Radial Basis Function (RBF) network with the modern CNN ResNet. We first trained ResNet-101 and the proposed ResNet-101-RBF on the aptos2019-blindness-detection dataset. After training, we evaluated the performance of the two models on normal DR data, and next, we evaluated the performance of the two models on adversarial data generated by FGSM attacks. Our results show that the accuracy of ResNet-101 in normal data is 78.26%, and the accuracy of ResNet-101-RBF is 75.26%, with the accuracy of ResNet-101 being 3% higher than that of ResNet-101-RBF in normal data. However, on adversarial samples generated with perturbation rates of 0.01, 0.05, and 0.1, the accuracy of ResNet-101-RBF is 6%, 5.5%, and 4.75% higher than that of ResNet-101, respectively. The ResNet-101-RBF model can maintain a high detection accuracy in normal data while having higher robustness against FGSM attacks compared to the ResNet-101 model.},
booktitle = {Proceedings of the 2023 4th International Symposium on Artificial Intelligence for Medicine Science},
pages = {488–495},
numpages = {8},
location = {Chengdu, China},
series = {ISAIMS '23}
}

@inproceedings{10.1007/978-3-031-06242-1_51,
author = {Grani, Fabrizio and Soto-Sanchez, Cristina and Rodil Doblado, Alfonso and Grima, Maria Dolores and Farfan, Fernando and Val Calvo, Mikel and Soo, Leili and Waclawczyk, Dorota and Ferrandez, JoseManuel and Gonzalez, Pablo and Coves, Mar\'{\i}a Dolores and Alfaro, Arantxa and Fern\'{a}ndez, Eduardo},
title = {Performance Evaluation of&nbsp;a&nbsp;Real-Time Phase Estimation Algorithm Applied to&nbsp;Intracortical Signals from&nbsp;Human Visual Cortex},
year = {2022},
isbn = {978-3-031-06241-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-06242-1_51},
doi = {10.1007/978-3-031-06242-1_51},
abstract = {Cortical visual prostheses are a subgroup of visual prostheses which use electrical stimulation of the occipital cortex to evoke visual percepts in profoundly blind people. The stimulation approaches are usually open-loop, meaning that the stimulation is not controlled by any other factor. However, closed-loop approaches have shown advantages in many neural prosthesis. In the case of cortical visual prosthesis, the closed-loop approach can be based on the phase of local field potentials recorded by the electrodes. Indeed, previous studies have shown that it is easier to induce perception through stimulation at certain phases of brain oscillations.Here, we evaluated the performance of a real-time phase estimator algorithm applied to local field potentials recorded with intracortical microelectrodes inserted in the occipital cortex of a blind human volunteer. Phase estimation was more accurate at certain phases than others. The error of the estimated phase was in the range ±20∘.These results should be taken into account when implementing phase-locked stimulation approaches in cortical visual prosthesis. Indeed, the phase estimation accuracy represents the limitation of the closed-loop stimulation approach.},
booktitle = {Artificial Intelligence in Neuroscience: Affective Analysis and Health Applications: 9th International Work-Conference on the Interplay Between Natural and Artificial Computation, IWINAC 2022, Puerto de La Cruz, Tenerife, Spain, May 31 – June 3, 2022, Proceedings, Part I},
pages = {516–525},
numpages = {10},
keywords = {Visual prosthesis, Multielectrode recordings, LFPs, Closed-loop stimulation},
location = {Puerto de la Cruz, Tenerife, Spain}
}

@inproceedings{10.1145/3644116.3644185,
author = {Sun, Chaoran},
title = {A four-class classification of ocular diseases based on multi-model comparative training},
year = {2024},
isbn = {9798400708138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644116.3644185},
doi = {10.1145/3644116.3644185},
abstract = {Retinal fundus diseases exhibit substantial heterogeneity and pose a significant threat to visual functionality. Among the common retinal fundus diseases, glaucoma, cataracts and age-related macular degeneration stand out for their ability to inflict retinal damage, resulting in vision impairment and even blindness. Therefore, early detection of these diseases holds immense importance for subsequent treatment interventions. While numerous studies have explored the classification of fundus image diseases based on Convolutional Neural Networks (CNN), the majority have primarily focused on the two types of ophthalmic diseases in the top two prevalences, namely age-related macular degeneration (196 million patients worldwide) and diabetic retinopathy (146 million patients worldwide), with limited attention given to other fundus disease classifications. To address the aforementioned gaps, this research paper aims to employ deep learning technology in conjunction with computer vision techniques to classify and diagnose fundus diseases. The specific research objectives are as follows: (1) Investigation of classical convolutional network models, such as VGGNet19, ResNet50, DenseNet121, MobileNetV2, and Swin-Transformer, training them in three stages. Stage 1 uses network model framework training, stage 2 entails transfer learning training, and stage 3 encompasses training after data enhancement of the dataset. Each stage involves the selection of models exhibiting exceptional performance, ultimately leading to the identification of the optimal model. (2) Leveraging the foundation of convolutional neural networks, a transfer learning-based classification method is proposed for fundus images using a small dataset. This involves utilizing the pre-trained deep learning model screened out in Phase 1, reconfiguring its fully connected layer, and adapting it to the four-classification task of fundus disease. (3) The experimental data is sourced from the Eye Disease Intelligent Recognition (ODIR) database, and the data on the test set should be validated using cross-validation. Additionally, data augmentation is implemented using six techniques facilitated by the OpenCV library. The experimental results show that DenseNet121, based on convolutional neural networks, exhibits superior performance in distinguishing between the four types of fundus diseases, and achieving the optimal classification accuracy.},
booktitle = {Proceedings of the 2023 4th International Symposium on Artificial Intelligence for Medicine Science},
pages = {410–416},
numpages = {7},
location = {Chengdu, China},
series = {ISAIMS '23}
}

@inproceedings{10.1007/978-3-031-06242-1_38,
author = {Val Calvo, Mikel and Moroll\'{o}n Ruiz, Roberto and Soo, Leili and Wac\l{}awczyk, Dorota and Grani, Fabrizio and Ferr\'{a}ndez, Jos\'{e} Manuel and Jover, Eduardo Fern\'{a}ndez},
title = {Horizon Cyber-Vision: A Cybernetic Approach for&nbsp;a&nbsp;Cortical Visual Prosthesis},
year = {2022},
isbn = {978-3-031-06241-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-06242-1_38},
doi = {10.1007/978-3-031-06242-1_38},
abstract = {The way towards the next generation of visual cortical prosthesis is visualised through an engineering cycle based on a cybernetic paradigm. Our proposal is to develop a configurable and wearable system that will generate simulated prosthetic vision, while on the other hand, perform intracortical stimulation when applied to blind patients, so that it is expected that improvements with sighted volunteers, in combination with transformed reality strategies, will correlate with similar improvements in blind patients. The resulting cybernetic model involves modelling from stimuli to visual percepts, and in parallel, developing the best suited transformed reality strategy leading to a better perception of the environment. Deep learning approaches for object detection, monocular depth estimation, or structural edge detection, in combination with the use of an eye-tracking system, will lead to an integrated system that has proved to be wearable, optimised, modular, and computationally lightweight. To assess the cybernetic approach, behavioural experiments are proposed using two different scenarios. Firstly, a corridor with a series of obstacles and a controlled but more complex environment that resembles a city square, called StreetLab.},
booktitle = {Artificial Intelligence in Neuroscience: Affective Analysis and Health Applications: 9th International Work-Conference on the Interplay Between Natural and Artificial Computation, IWINAC 2022, Puerto de La Cruz, Tenerife, Spain, May 31 – June 3, 2022, Proceedings, Part I},
pages = {380–394},
numpages = {15},
keywords = {Cybernetic, Bio-inspired, Prosthesis, Cortical, Deep learning},
location = {Puerto de la Cruz, Tenerife, Spain}
}

@inproceedings{10.1145/3686169.3686185,
author = {Ehsan, Upol and Riedl, Mark},
title = {Explainable AI Reloaded: Challenging the XAI Status Quo in the Era of Large Language Models},
year = {2024},
isbn = {9798400710421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686169.3686185},
doi = {10.1145/3686169.3686185},
abstract = {When the initial vision of Explainable (XAI) was articulated, the most popular framing was to open the (proverbial) “black-box” of AI so that we could understand the inner workings. With the advent of Large Language Models (LLMs), the very ability to open the black-box is increasingly limited. Especially when it comes to non-technical end-users. In this paper, we challenge the assumption of “opening” the black-box in the LLM era and argue for a shift in our XAI expectations. Highlighting the epistemic blind spots of an algorithm-centered XAI view, we argue that a human-centered perspective can be a path forward. We operationalize the argument by synthesizing XAI research along three dimensions: explainability outside the black-box, explainability around the edges of the black box, and explainability that leverages infrastructural seams. We conclude with takeaways that reflexively inform XAI as a domain.},
booktitle = {Proceedings of the Halfway to the Future Symposium},
articleno = {8},
numpages = {8},
keywords = {Explainable AI, Generative AI, Large Language Models},
location = {Santa Cruz, CA, USA},
series = {HttF '24}
}

@inproceedings{10.1145/3625343.3625360,
author = {Xu, Zhihao and Li, Cong and Li, Yan},
title = {Channel Estimation for IRS-Aided Single-User MIMO Systems Based on Core Tensor},
year = {2023},
isbn = {9798400708312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625343.3625360},
doi = {10.1145/3625343.3625360},
abstract = {Intelligent reflective surface (IRS), comprised of numerous low-cost passive components, is a vital technology for the future sixth-generation wireless system. However, the inability of the IRS to handle signals independently poses challenges in estimating channels in fully passive IRS-aided systems. This paper investigates the channel estimation for an IRS-aided single-user multiple-input multiple-output (SU-MIMO) system and specifically focuses on the scenario where the large-dimensional received signals processed at the base station prove inconvenient for memory-based processing. To tackle this issue, this paper suggests a non-blind receiver that relies on a core tensor of the signals received, while also considering the quasi-static channel characteristic between the IRS and BS. Simulation results demonstrate the proposed receiver has superior performance in high-dimensional data processing, i.e., a lower normalized mean square error (NMSE) and computational complexity over the uncompressed received signal tensor-assisted receiver.},
booktitle = {Proceedings of the 2023 Asia Conference on Artificial Intelligence, Machine Learning and Robotics},
articleno = {15},
numpages = {5},
location = {Bangkok, Thailand},
series = {AIMLR '23}
}

@inproceedings{10.1145/3745238.3745409,
author = {Wu, Bo and Li, Jiancai},
title = {The Impact of Diversification Strategy on Service Innovation Performance of Community Service Platform Companies in the Context of Economic Recession- a Further Exploration of the Computer Simulation of the Impact Mechanism},
year = {2025},
isbn = {9798400712791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745238.3745409},
doi = {10.1145/3745238.3745409},
abstract = {During the market downturn, enterprises focus on cost reduction and efficiency improvement. However, some platform - based service enterprises paradoxically and actively expand their business scope during this period, accompanied by more active resource integration and organizational change. According to the computer - simulation research on enterprise operations, different combinations of organizational change, diversification strategies, and resource integration have complex impact mechanisms on enterprise innovation, especially during complex market environments. This paper conducts an empirical research through enterprise surveys. It studies, through data&nbsp;analysis and&nbsp;statistical&nbsp;computing methods such as mediating effect analysis and path analysis based on data modeling, as well as quadratic fitting for key impact relationships, the impact mechanism of diversification expansion on the service innovation performance of such enterprises in special environments, and observes the influence of the two behaviors of active resource integration and organizational change, so as to verify and to make a further exploration of the conclusions of the impact mechanism in computer - simulation research. According to the research results, the diversification expansion of platform - based service enterprises during this period can indeed effectively promote service innovation performance and thus enhance enterprise competitiveness (but there are problems). The two factors of resource integration and organizational change play a mediating role therein. Through further in - depth analysis of independent variables and mediating variables based on computer - simulation research results, it is found that organizational change has an inverted U - shaped impact on innovation performance, indicating that some enterprises have relatively obvious over - change. In addition, diversification expansion also shows an inverted U - shaped impact on innovation performance, that is, the phenomenon of diminishing marginal returns brought about by blind expansion, suggesting that enterprises need to improve the success rate of diversification expansion at the strategic management level.},
booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
pages = {1090–1097},
numpages = {8},
keywords = {Computer Simulation, Diversification Strategy, Organizational Change, Resource Integration, Service Innovation},
location = {
},
series = {DEAI '25}
}

@inproceedings{10.1007/978-3-031-92823-9_17,
author = {Zhang, Yuntian and Chen, Keyi and Xiong, Yuxuan and Hua, Min and Fan, Yunxuan and Zhong, Shangyun},
title = {Seeing Through the Blind Box: Enhancing POP Toy Satisfaction and Word-of-Mouth via AIGC Design and Innovative Sales Mechanisms},
year = {2025},
isbn = {978-3-031-92822-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-92823-9_17},
doi = {10.1007/978-3-031-92823-9_17},
abstract = {The blind box sales mechanism has catalyzed the widespread adoption of POP toys, transitioning from niche subcultures to mass consumer markets. However, challenges such as diminishing novelty and consumer satisfaction have emerged, posing obstacles to cultivating brand word-of-mouth. In creative industry, the competitive edge of POP toys relies heavily on innovative product design and sales strategies. This study investigates two primary facets: (1) The impact of production methods—AI-generated (AIGC) versus designer-original—based on the “Cuteness” design principle on consumer purchase intentions. (2) Variations in consumer purchase intentions, satisfaction, and word-of-mouth across different sales mechanisms (self-selection, blind box, face recognition). A controlled experimental platform aligned with three distinct sales mechanisms were designed. The face recognition mechanism was assigned as the experimental group, while the other two mechanisms served as control groups, with each group comprising 100 participants. Findings indicate that emphasizing Cuteness in POP toy design significantly enhances consumer positive attitude when utilizing AIGC. Conversely, blind box sales demonstrate lower purchase intentions, satisfaction levels, and word-of-mouth, highlighting inherent drawbacks. Face recognition mechanisms foster emotional connections between consumers and brands, thereby enhancing brand Word-of-Mouth (WOM). This study advocates for a personalized sales approach that maintains a balance between predictability and surprise, supporting the POP toys industry’s shift towards user-centric strategies over traditional traffic-driven approaches.},
booktitle = {HCI in Business, Government and Organizations: 12th International Conference, HCIBGO 2025, Held as Part of the 27th HCI International Conference, HCII 2025, Gothenburg, Sweden, June 22–27, 2025, Proceedings, Part I},
pages = {256–278},
numpages = {23},
keywords = {POP Toy, Self-Image Congruence, AIGC, Blind Box, Word-of-Mouth (WOM)},
location = {Gothenburg, Sweden}
}

@inproceedings{10.1145/3706599.3719714,
author = {Gonzalez Penuela, Ricardo E. and Hu, Ruiying and Lin, Sharon and Shende, Tanisha and Azenkot, Shiri},
title = {Towards Understanding the Use of MLLM-Enabled Applications for Visual Interpretation by Blind and Low Vision People},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719714},
doi = {10.1145/3706599.3719714},
abstract = {Blind and Low Vision (BLV) people have adopted AI-powered visual interpretation applications to address their daily needs. While these applications have been helpful, prior work has found that users remain unsatisfied by their frequent errors. Recently, multimodal large language models (MLLMs) have been integrated into visual interpretation applications, and they show promise for more descriptive visual interpretations. However, it is still unknown how this advancement has changed people’s use of these applications. To address this gap, we conducted a two-week diary study in which 20 BLV people used an MLLM-enabled visual interpretation application we developed, and we collected 553 entries. In this paper, we report a preliminary analysis of 60 diary entries from 6 participants. We found that participants considered the application’s visual interpretations trustworthy (mean 3.75 out of 5) and satisfying (mean 4.15 out of 5). Moreover, participants trusted our application in high-stakes scenarios, such as receiving medical dosage advice. We discuss our plan to complete our analysis to inform the design of future MLLM-enabled visual interpretation systems.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {536},
numpages = {8},
keywords = {Accessibility, BLV, Blind People, Low Vision People, AI, Artificial Intelligence, LLM, Large language models, Diary Study, Conversations, MLLM, Multimodal models, VQA, Visual Question Answering},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3632314.3632359,
author = {Zhao, Lin and Zhang, Lihang},
title = {A Blind Navigation Algorithm Based on The PPYOLO Model},
year = {2023},
isbn = {9798400709401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632314.3632359},
doi = {10.1145/3632314.3632359},
abstract = {Abstract: This study aims to address challenges faced by visually impaired individuals in daily travel, such as inability to perceive traffic lights, navigate obstacles, and locate tactile pavings. The goal is to develop a specialized travel assistance algorithm for visually impaired individuals. After thorough research on existing technologies including GPS, IrDA, AI-assisted technology, mobile applications, and object recognition, and analyzing their limitations, we proposed a blind navigation algorithm based on the PP-YOLO model. This algorithm combines traffic sign recognition, object recognition, and tactile paving recognition to achieve multi-task image recognition. Through dataset preparation, data annotation, and parameter adjustment, the algorithm was implemented on portable devices like mobile phones and wearable devices. Laboratory verification and field testing confirmed that the algorithm provides accurate navigation guidance and ensures safety for visually impaired individuals during travel. With a frame rate of 8fps, the algorithm achieved a recognition rate of 96.3% within an 8m range on portable devices; in low-light environments, the object recognition accuracy within the same range reached 94%. Even at complex intersections with a width of 30m, the recognition rate was 85%. This research outcome presents an innovative technical solution for travel assistance technology for visually impaired individuals and offers new research ideas for similar studies. The PP-YOLO model was chosen for its superior speed and accuracy in object detection tasks, crucial for real-time navigation. The model was streamlined for use on portable devices through lightweight design and hardware acceleration features such as NPU and GPU. The challenge with this approach involves some risk of recognition errors, similar to those in autonomous driving technology. However, considering the slower walking speed of visually impaired individuals, the consequences of these errors are mitigated. Thus, the project provides an additional mobility option for visually impaired individuals, providing more freedom than without it.},
booktitle = {Proceedings of the 2023 International Conference on Intelligent Sensing and Industrial Automation},
articleno = {35},
numpages = {5},
keywords = {Artificial Intelligence, Blind Path Obstacle Detection, Deep Learning, Environmental Perception, Object Detection, Visual Processing},
location = {Virtual Event, China},
series = {ISIA '23}
}

@inproceedings{10.5555/3716662.3716804,
author = {Wolfe, Robert and Dangol, Aayushi and Hiniker, Alexis and Howe, Bill},
title = {Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI},
year = {2025},
publisher = {AAAI Press},
abstract = {Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about harmful bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we use a hierarchical clustering approach to show that dataset size predicts the extent to which the underlying structure of facial impression bias resembles that of facial impression bias in humans. Finally, we show that Stable Diffusion models employing CLIP as a text encoder learn facial impression biases, and that these biases intersect with racial biases in Stable Diffusion XL-Turbo. While pretrained CLIP models may prove useful for scientific studies of bias, they will also require significant dataset curation when intended for use as general-purpose models in a zero-shot setting.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1635–1647},
numpages = {13},
location = {San Jose, California, USA},
series = {AIES '24}
}

@article{10.1016/j.eswa.2022.118722,
author = {Toma, Filip-Mihai},
title = {A hybrid neuro-experimental decision support system to classify overconfidence and performance in a simulated bubble using a passive BCI},
year = {2023},
issue_date = {Feb 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {212},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2022.118722},
doi = {10.1016/j.eswa.2022.118722},
journal = {Expert Syst. Appl.},
month = feb,
numpages = {12},
keywords = {AMICA, ANN, BCI, DWT, EMD, EEG, fMRI, FRN, ML, MLP, mPFC, NAcc, IC, ICA, RON, SVM, EEG, Asset bubble, Decision-making, Brain-computer interface, Machine learning, Decision support system}
}

@inproceedings{10.1145/3613904.3642817,
author = {Han, Chaeeun and Mitra, Prasenjit and Billah, Syed Masum},
title = {Uncovering Human Traits in Determining Real and Spoofed Audio: Insights from Blind and Sighted Individuals},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642817},
doi = {10.1145/3613904.3642817},
abstract = {This paper explores how blind and sighted individuals perceive real and spoofed audio, highlighting differences and similarities between the groups. Through two studies, we find that both groups focus on specific human traits in audio–such as accents, vocal inflections, breathing patterns, and emotions–to assess audio authenticity. We further reveal that humans, irrespective of visual ability, can still outperform current state-of-the-art machine learning models in discerning audio authenticity; however, the task proves psychologically demanding. Moreover, detection accuracy scores between blind and sighted individuals are comparable, but each group exhibits unique strengths: the sighted group excels at detecting deepfake-generated audio, while the blind group excels at detecting text-to-speech (TTS) generated audio. These findings not only deepen our understanding of machine-manipulated and neural-renderer audio but also have implications for developing countermeasures, such as perceptible watermarks and human-AI collaboration strategies for spoofing detection.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {949},
numpages = {14},
keywords = {Audio perception, and audio watermarking, and human-AI collaboration., audio, blind, bona fide audio, deep fake audio, generative AI, neural speech, replay attack, sighted, speech, spoofed audio, text-to-speech (TTS), vision impairments, voice},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3746059.3747650,
author = {Ning, Zheng and Li, Leyang and Killough, Daniel and Seo, JooYoung and Carrington, Patrick and Tian, Yapeng and Zhao, Yuhang and Li, Franklin Mingzhe and Li, Toby Jia-Jun},
title = {AROMA: Mixed-Initiative AI Assistance for Non-Visual Cooking by Grounding Multimodal Information Between Reality and Videos},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747650},
doi = {10.1145/3746059.3747650},
abstract = {Videos offer rich audiovisual information that can support people in performing activities of daily living (ADLs), but they remain largely inaccessible to blind or low-vision (BLV) individuals. In cooking, BLV people often rely on non-visual cues—such as touch, taste, and smell—to navigate their environment, making it difficult to follow the predominantly audiovisual instructions found in video recipes. To address this problem, we introduce Aroma, an AI system that provides timely responses to the user based on real-time, context-aware assistance by integrating non-visual cues perceived by the user, a wearable camera feed, and video recipe content. Aroma uses a mixed-initiative approach: it responds to user requests while also proactively monitoring the video stream to offer timely alerts and guidance. This collaborative design leverages the complementary strengths of the user and AI system to align the physical environment with the video recipe, helping the user interpret their current state and make sense of the steps. We evaluated Aroma through a study with eight BLV participants and offered insights for designing interactive AI systems to support BLV individuals in performing ADLs.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {144},
numpages = {15},
keywords = {video recipes, cooking, multimodal perception, accessibility},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3571560.3571574,
author = {Imam, Mohamed and Ba\"{\i}na, Karim and Tabii, Youness and Benzakour, Intissar and Adlaoui, Youssef and Ressami, El Mostafa and Abdelwahed, El Hassan},
title = {Anti-Collision System for Accident Prevention in Underground Mines using Computer Vision},
year = {2023},
isbn = {9781450396943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571560.3571574},
doi = {10.1145/3571560.3571574},
abstract = {Underground prospecting operations are often characterized by critical safety issues mainly due to poor visibility and blind spots around large vehicles and equipment. This can result in vehicle-to-vehicle collisions, as well as vehicle-to-pedestrian or structural-element collisions, resulting in accidents. In this article, we discuss an anti-collision system for pedestrian identification in deep mines under the premise that we are looking to prevent collisions with moving machinery. This study presents the findings from testing an image processing module and sensory system based on deep learnig in the context of "smart connected mine" project.},
booktitle = {Proceedings of the 6th International Conference on Advances in Artificial Intelligence},
pages = {94–101},
numpages = {8},
keywords = {Deep Learning, Pedestrian detection, Underground mining, Yolov5},
location = {Birmingham, United Kingdom},
series = {ICAAI '22}
}

@article{10.1007/s11042-022-13837-5,
author = {Agarwal, Shalini and Bhat, Aruna},
title = {A survey on recent developments in diabetic retinopathy detection through integration of deep learning},
year = {2022},
issue_date = {May 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {82},
number = {11},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-022-13837-5},
doi = {10.1007/s11042-022-13837-5},
abstract = {Diabetes, nowadays, is a very common disease throughout the world among people of all ages. The higher level of blood sugar in the blood more often leads to Damage to blood vessels in the retina leads to blindness if left untreated or undetected on time. Diabetic Retinopathy (DR) screening programs like retinal fundus image analysis help ophthalmologists deal with some visual impairment problems. Computer-Aided Diagnosis aims to detect the severity of DR as early as possible so that it can be handled before the occurrence of any irreversible vision loss. With the help of many advancements in artificial intelligence techniques, a highly efficient and accurate system can be designed to help medical professionals automatically diagnose DR at an early stage without any special clinical resources. This paper conducts a thorough investigation into several recent frameworks proposed based on machine learning and deep learning networks to classify non-proliferative diabetic retinopathy, exudates, hemorrhages, and micro aneurysms. Several promising pre-trained deep learning model to classify DR stages exploited by researchers, and also investigated Transfer learning on pre-trained GoogLeNet and AlexNet, VGG etc. models. It is observed that almost all public and private data sets widely available for research are imbalanced. To alleviate these issues, generative adversarial networks (GANs) and their variants were also used to generate label-preserving data. In this study, the authors also list the recently proposed GAN-based frameworks and their impact on model performance. The paper concludes with the current challenges and future directions in the early and accurate classification of the severity level of DR.},
journal = {Multimedia Tools Appl.},
month = sep,
pages = {17321–17351},
numpages = {31},
keywords = {Diabetic retinopathy, Deep learning, GAN}
}

@inproceedings{10.1145/3672919.3672968,
author = {Li, Bo},
title = {A Study of DistilBERT-Based Answer Extraction Machine Reading Comprehension Algorithm},
year = {2024},
isbn = {9798400718212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672919.3672968},
doi = {10.1145/3672919.3672968},
abstract = {The task of answer extraction for machine reading comprehension has been one of the most popular and emphasized directions of research in recent years, which requires a machine to be able to accurately predict the answer to a given question across the span of answer intervals. Generally speaking, increasing the model size usually improves the performance of the downstream task when pre-training the language model, however, in some cases, due to the limitations of computational resources and training time, further increasing the model's size will be not suitable for specific operations. In order to effectively solve the above problems, based on the full consideration of the target task, this paper proposes a model named Parallel-DistilBERT-QA for the answer extraction machine reading comprehension task. The model combines the methods of knowledge distillation and parameter sharing, and uses DistilBERT as well as the dual-stream network structure to construct a feature extraction module, which uses three parallel DistilBERTs to process question features, question and document joint features, and document features. Between the first one and the second one as well as the second one and the third one of DistilBERTs, the dual-stream network structure is used, which realizes the sharing of weight parameters; at the same time, in order to improve the computational accuracy, an information interaction module is used after the feature extraction module, which calculates the correlation of the local features in order to capture the blind spots that are omitted by the global features. The experiments show that the computational performance of the model in this paper outperforms BERT on SQuAD1.1 and SQuAD2.0 in all aspects, and the amount of parameters is reduced by about 31% compared with that of BERT-large, and the EM and F1 scores on the test set of SQuAD2.0 are ahead of BERT-large, with the leading scores of 1.91 points and 1.73 points.},
booktitle = {Proceedings of the 2024 3rd International Conference on Cyber Security, Artificial Intelligence and Digital Economy},
pages = {261–268},
numpages = {8},
location = {Nanjing, China},
series = {CSAIDE '24}
}

@inproceedings{10.1145/3584931.3611279,
author = {Deng, Wesley Hanwen and Lam, Michelle S. and Cabrera, \'{A}ngel Alexander and Metaxa, Dana\"{e} and Eslami, Motahhare and Holstein, Kenneth},
title = {Supporting User Engagement in Testing, Auditing, and Contesting AI},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584931.3611279},
doi = {10.1145/3584931.3611279},
abstract = {In recent years, interest in directly involving end users in testing, auditing, and contesting AI systems has grown. The involvement of end users, especially from diverse backgrounds, can be essential to overcome AI developers’ blind spots and to surface issues that would otherwise go undetected prior to causing real-world harm. Emerging bodies of work in CSCW have begun to explore ways to engage end-users in testing and auditing AI systems, and to empower users to contest problematic or erroneous AI outputs. As this is a nascent area of work, we still know little about how to support effective user engagement. In this one-day workshop, we will bring together researchers and practitioners from academia, industry, and non-profit organizations to share ongoing efforts related to this workshop’s theme. Central to our discussions will be the challenges encountered in developing tools and processes to support user involvement, strategies to incentivize involvement, the asymmetric power dynamic between AI developers and end users, and the role of regulation in enhancing the accountability of AI developers and ameliorating potential burdens towards end-users. Overall, we hope the workshop will help shape the future of user engagement in building more responsible AI.},
booktitle = {Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {556–559},
numpages = {4},
keywords = {algorithm auditing, human-centered AI, responsible AI, usability testing},
location = {Minneapolis, MN, USA},
series = {CSCW '23 Companion}
}

@article{10.1109/COMST.2023.3339371,
author = {Arya, Sudhanshu and Chung, Yeon Ho},
title = {A Comprehensive Survey on Optical Scattering Communications: Current Research, New Trends, and Future Vision},
year = {2024},
issue_date = {Secondquarter 2024},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {1553-877X},
url = {https://doi.org/10.1109/COMST.2023.3339371},
doi = {10.1109/COMST.2023.3339371},
abstract = {To meet high data rate requirements of future wireless communication systems, there is a need for advanced communication technologies that could be used in combination with existing wireless RF technologies. Recently, optical wireless communication (OWC) has been extensively investigated as an attractive alternate technology to RF. OWC uses the optical carrier to convey data, with wavelengths ranging from ultraviolet (UV) to infrared (IR) to visible light. In the past years, there is a spike in interest over optical scattering communications (OSCs) employing UV wavelengths, thanks to the recent advances and rapid developments in deep UV light-emitting diodes (LEDs), laser diodes, and solar-blind UV filters and detectors. The unique atmospheric scattering and absorption properties of the deep UV band, which is solar-blind at the ground level, are the motivation for the recent development of the OSC systems. However, there is a clear gap in the existing literature that the OSC systems are yet to be systematically surveyed for their applicability to future wireless communications. In this context, this paper bridges the gap by providing the first contemporary and comprehensive survey on recent and new advancements in the OSCs, commonly known as UV communications. In summary, this survey is expected to provide a largely missing articulation between various aspects of UV communications. To be easy to follow, we commence our discourse by surveying the propagation concepts and historic evolution of UV communication systems. Next, we provide a detailed survey on UV channel modeling because accurate channel characterization is important for efficient system design and performance optimization of UV communication systems. We discuss various UV channel characterization efforts thus far made. Then, we present a classification to analyze current OSC system designs. Importantly, we survey recent advancements in the NLOS UV communication systems that include the application of artificial intelligence, artificial neural networks, game theory, orbital angular momentum, etc. Moreover, we conduct a comprehensive survey on recently documented UV-based indoor communication systems. Finally, we point out several key issues yet to be addressed and collate potentially interesting and challenging topics for future research. This survey is aptly featured by in-depth discussion and analysis of UV communication systems in various aspects, many of which, to the best of authors’ knowledge, are the first time presented in this field.},
journal = {Commun. Surveys Tuts.},
month = apr,
pages = {1446–1477},
numpages = {32}
}

@inproceedings{10.1145/3544548.3581337,
author = {Kamikubo, Rie and Lee, Kyungjun and Kacorri, Hernisa},
title = {Contributing to Accessibility Datasets: Reflections on Sharing Study Data by Blind People},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581337},
doi = {10.1145/3544548.3581337},
abstract = {To ensure that AI-infused systems work for disabled people, we need to bring accessibility datasets sourced from this community in the development lifecycle. However, there are many ethical and privacy concerns limiting greater data inclusion, making such datasets not readily available. We present a pair of studies where 13 blind participants engage in data capturing activities and reflect with and without probing on various factors that influence their decision to share their data via an AI dataset. We see how different factors influence blind participants’ willingness to share study data as they assess risk-benefit tradeoffs. The majority support sharing of their data to improve technology but also express concerns over commercial use, associated metadata, and the lack of transparency about the impact of their data. These insights have implications for the development of responsible practices for stewarding accessibility datasets, and can contribute to broader discussions in this area.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {827},
numpages = {18},
keywords = {data ownership, datasets, disability, human-centered AI, privacy},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3577530.3577539,
author = {Yin, Yueqin and Huang, Lianghua and Liu, Yu and Huang, Kaiqi},
title = {DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models},
year = {2023},
isbn = {9781450397773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577530.3577539},
doi = {10.1145/3577530.3577539},
abstract = {Recent generative models show impressive results in photo-realistic image generation. However, artifacts often inevitably appear in the generated results, leading to downgraded user experience and reduced performance in downstream tasks. This work aims to develop a plugin post-processing module for diverse generative models, which can faithfully restore images from diverse generative artifacts. This is challenging because: (1) Unlike traditional degradation patterns, generative artifacts are non-linear and the transformation function is highly complex. (2) There are no readily available artifact-image pairs. (3) Different from model-specific anti-artifact methods, a model-agnostic framework views the generator as a black-box machine and has no access to the architecture details. In this work, we first design a group of mechanisms to simulate generative artifacts of popular generators (i.e., GANs, autoregressive models, and diffusion models), given real images. Second, we implement the model-agnostic anti-artifact framework as an image-to-image diffusion model, due to its advantage in generation quality and capacity. Finally, we design a conditioning scheme for the diffusion model to enable both blind and non-blind image restoration. A guidance parameter is also introduced to allow for a trade-off between restoration accuracy and image quality. Extensive experiments show that our method significantly outperforms previous approaches on the proposed datasets and real-world artifact images.},
booktitle = {Proceedings of the 2022 6th International Conference on Computer Science and Artificial Intelligence},
pages = {55–62},
numpages = {8},
keywords = {datasets, generative modeling, image generation, image restoration},
location = {Beijing, China},
series = {CSAI '22}
}

@inproceedings{10.1145/3640543.3645212,
author = {Singh, Nikhil and Wang, Lucy Lu and Bragg, Jonathan},
title = {FigurA11y: AI Assistance for Writing Scientific Alt Text},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645212},
doi = {10.1145/3640543.3645212},
abstract = {High-quality alt text is crucial for making scientific figures accessible to blind and low-vision readers. Crafting complete, accurate alt text is challenging even for domain experts, as published figures often depict complex visual information and readers have varied informational needs. These challenges, along with high diversity in figure types and domain-specific details, also limit the usefulness of fully automated approaches. Consequently, the prevalence of high-quality alt text is very low in scientific papers today. We investigate whether and how human-AI collaborative editing systems can help address the difficulty of writing high-quality alt text for complex scientific figures. We present FigurA11y, an interactive system that generates draft alt text and provides suggestions for author revisions using a pipeline driven by extracted figure and paper metadata. We test two versions, motivated by prior work on visual accessibility and writing support. The base Draft+Revise&nbsp;version provides authors with an automatically generated draft description to revise, along with extracted figure metadata and figure-specific alt text guidelines to support the revision process. The full Interactive Assistance&nbsp;version further adds contextualized suggestions: text snippets to iteratively produce descriptions, and hypothetical user questions with possible answers to reveal potential ambiguities and resolutions. In a study of authors (N=14), we found the system assisted them in efficiently producing descriptive alt text. Generated drafts and interface elements enabled authors to quickly initiate and edit detailed descriptions. Additionally, interactive suggestions from the full system prompted more iteration and highlighted aspects for authors to consider, resulting in greater deviation from the drafts without increased average cognitive load or manual effort.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {886–906},
numpages = {21},
keywords = {Accessibility, Alt text, Human-AI interaction, Image descriptions, Large language models, Natural language generation, Scientific figures, Writing assistance systems},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{10.1145/3613904.3642632,
author = {Ning, Zheng and Wimer, Brianna L and Jiang, Kaiwen and Chen, Keyi and Ban, Jerrick and Tian, Yapeng and Zhao, Yuhang and Li, Toby Jia-Jun},
title = {SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642632},
doi = {10.1145/3613904.3642632},
abstract = {Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to access video content. However, conventional static ADs can leave out detailed information in videos, impose a high mental load, neglect the diverse needs and preferences of BLV users, and lack immersion. To tackle these challenges, we introduce Spica, an AI-powered system that enables BLV users to interactively explore video content. Informed by prior empirical studies on BLV video consumption, Spica offers interactive mechanisms for supporting temporal navigation of frame captions and spatial exploration of objects within key frames. Leveraging an audio-visual machine learning pipeline, Spica augments existing ADs by adding interactivity, spatial sound effects, and individual object descriptions without requiring additional human annotation. Through a user study with 14 BLV participants, we evaluated the usability and usefulness of Spica and explored user behaviors, preferences, and mental models when interacting with augmented ADs.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {902},
numpages = {18},
keywords = {accessibility, audio description, video consumption},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3508259.3508288,
author = {Catimbang Magboo, Vincent Peter and Abad Magboo, Ma. Sheila},
title = {Imputation Techniques and Recursive Feature Elimination in Machine Learning Applied to Type II Diabetes Classification},
year = {2022},
isbn = {9781450384162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508259.3508288},
doi = {10.1145/3508259.3508288},
abstract = {Type II diabetes is a chronic metabolic disease secondary to elevated blood glucose levels. Complications of this disease include heart attack, stroke, blindness, renal failure, lower limb amputation and mortality. Due to its rising prevalence and consequent mortality, it is important to identify at an early stage those patients at high risk of developing diabetes. We applied 8 machine learning techniques namely: support vector machine, logistic regression, k-nearest neighbor, na\"{\i}ve Bayes, decision tree, random forest, AdaBoost and XGBoost in predicting diabetes using a publicly available diabetes dataset. In our study, Na\"{\i}ve Bayes with median imputation and recursive feature elimination obtained the highest performance with an accuracy rate of 81.0%. Although the results are very promising, one major limitation in this study is the small number of samples in the dataset. Early accurate detection can help patients to proactively monitor their lifestyle habits mitigating the risks of complications of uncontrolled diabetes.},
booktitle = {Proceedings of the 2021 4th Artificial Intelligence and Cloud Computing Conference},
pages = {201–207},
numpages = {7},
location = {Kyoto, Japan},
series = {AICCC '21}
}

@inproceedings{10.1145/3616901.3616913,
author = {Liu, Peng and Zhao, Zhu and Liang, Jun},
title = {A Polymorphic Smart Network Construction Method Based On Q-UCB},
year = {2024},
isbn = {9798400707544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616901.3616913},
doi = {10.1145/3616901.3616913},
abstract = {Full-dimensional defined polymorphic smart network (PINet) is a new type of network technology. PINet promote the functions, performance, efficiency and security of the Internet from the perspective of network structure. The construction method of reconfigurable service carrying network is one of the key topic of PINet, which provides diverse network services and supports the dynamic adaption between the service and resource. In such large state space control tasks, deep Reinforcement Learning method has achieved excellent results. Because of the Complexity and dynamic of the network, there are some problems in the existing exploration algorithms, such as blind exploration, and slow learning. To solve these problems, this paper put forward a Reinforcement Learning method based on upper confidence bound (Q-UCB). It determines the selection probability by reward, time-step and selection times for each action. The proposed approach assigns actions which haven't been chosen, actions have good results and actions which are more valuable, with higher probability of being selected. The method enable agents to learn more efficiently. Finally, simulation experiments verify the effectiveness of the method.},
booktitle = {Proceedings of the 2023 International Conference on Frontiers of Artificial Intelligence and Machine Learning},
pages = {51–55},
numpages = {5},
keywords = {Experience replay, Polymorphic network, Reinforcement learning, Upper confidence bound},
location = {Beijing, China},
series = {FAIML '23}
}

@inproceedings{10.5555/3692070.3692701,
author = {Glukhov, David and Shumailov, Ilia and Gal, Yarin and Papernot, Nicolas and Papyan, Vardan},
title = {Position: fundamental limitations of LLM censorship necessitate new approaches},
year = {2024},
publisher = {JMLR.org},
abstract = {Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship methods have proven to be fallible at ensuring that LLMs do not return semantically impermissible responses. We present fundamental limitations of verifying the semantic properties of LLM outputs and identifying compositional threats, illustrating inherent challenges of current approaches to censoring LLM outputs. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, and semantic properties of LLM outputs can become impossible to verify when the LLM is capable of providing "encrypted" outputs. We further show challenges of censorship can extend beyond just semantic censorship, as attackers can reconstruct impermissible outputs from a collection of permissible ones. Consequently, we call for a reevaluation of the problem of censorship and its goals, stressing the need for new definitions and approaches to censorship. In addition, we provide an initial attempt toward achieving this goal through syntactic censorship, drawing from a security perspective to design censorship methods that can provide guarantees.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {631},
numpages = {21},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{10.5555/3632186.3632209,
author = {Zhang, Zhuohao (Jerry) and Kaushik, Smirity and Seo, JooYoung and Yuan, Haolin and Das, Sauvik and Findlater, Leah and Gurari, Danna and Stangl, Abigale and Wang, Yang},
title = {ImageAlly: a human-AI hybrid approach to support blind people in detecting and redacting private image content},
year = {2023},
isbn = {978-1-939133-36-6},
publisher = {USENIX Association},
address = {USA},
abstract = {Many people who are blind take and post photos to share about their lives and connect with others. Yet, current technology does not provide blind people with accessible ways to handle when private information is unintentionally captured in their images. To explore the technology design in supporting them with this task, we developed a design probe for blind people-- ImageAlly--that employs a human-AI hybrid approach to detect and redact private image content. ImageAlly notifies users when potential private information is detected in their images, using computer vision, and enables them to transfer those images to trusted sighted allies to edit the private content. In an exploratory study with pairs of blind participants and their sighted allies, we found that blind people felt empowered by ImageAlly to prevent privacy leakage in sharing images on social media. They also found other benefits from using ImageAlly, such as potentially improving their relationship with allies and giving allies the awareness of the accessibility challenges they face.},
booktitle = {Proceedings of the Nineteenth USENIX Conference on Usable Privacy and Security},
articleno = {23},
numpages = {20},
location = {Anaheim, CA, USA},
series = {SOUPS '23}
}

@inproceedings{10.1145/3584376.3584564,
author = {Liu, Shugang and Nie, Weixin and Yu, Qiangguo and Nie, Guozheng},
title = {Design of portable data monitor for construction machinery based on FPGA},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584564},
doi = {10.1145/3584376.3584564},
abstract = {Portable data monitor is a special terminal that is widely used in construction machinery to assist the driver. The terminal can sense the running real-time data of the machine, such as the fuel volume and engine speed, so the machine can be driven in a standardized manner. In this paper, the portable terminal is designed based on Field Programmable Gate Array (FPGA). The designed terminal mainly collects data on the equipment operation status, engine parameters, and blind area images in front of the driver. The scheme and the implementation details of each functional module are also focused on based on FPGA. The key features include a 16-bit SDRAM controller, serial flash controller with 100MHz clock, industrial LCD controller with FIFO and data acquisition with dual-port RAM. The FPGA synthesis results show that the implementation of monitor uses 5871 Look Up Tables (LUTs) and 245888 memory bits, and the design scheme with a 100MHz system clock can work stably. At the same time, the running results in the practical engineering application indicate that the designed terminal can run reliably for a long time in a harsh working environment.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {1053–1058},
numpages = {6},
location = {Dongguan, China},
series = {RICAI '22}
}

@article{10.1016/j.cageo.2022.105217,
author = {Liang, Jiabin and Sun, Yongyang and Lebedev, Maxim and Gurevich, Boris and Nzikou, Michel and Vialle, Stephanie and Glubokovskikh, Stanislav},
title = {Multi-mineral segmentation of micro-tomographic images using a convolutional neural network},
year = {2022},
issue_date = {Nov 2022},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {168},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2022.105217},
doi = {10.1016/j.cageo.2022.105217},
journal = {Comput. Geosci.},
month = nov,
numpages = {6},
keywords = {Micro-tomographic images, Multi-mineral segmentation, Convolutional neural network}
}

@inproceedings{10.1007/978-3-031-19660-7_10,
author = {Dedhiya, Ronak and Kakileti, Siva Teja and Gopinath, Kanchana and Edem, Agbogah and Donkor, Bismark and Seidu, Abdulai Mahmood and Attah, Simon K. and King, Christopher L. and Opoku, Nicholas and Manjunath, Geetha},
title = {Non-invasive Thermal Imaging for Estimation of the Fecundity of Live Female Onchocerca Worms},
year = {2022},
isbn = {978-3-031-19659-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-19660-7_10},
doi = {10.1007/978-3-031-19660-7_10},
abstract = {The need for non-invasive estimation of the fecundity of Onchocerca nodules has increasingly become more important for testing drug effectiveness. The current diagnostic methodology includes nodulectomy, which can be performed at the end of the treatment but is insufficient to track the nodule changes during the treatment. Further, it is an expensive, time-consuming, and skill-required procedure. In recent studies, a non-invasive approach such as thermal imaging that captures the thermal signature of onchocerca nodules has shown the potential to detect the presence of female onchocerca worms when combined with machine learning. In this paper, we extended this research to propose automated algorithms to determine the fecundity of worms in onchocerca nodules. When the proposed algorithms were evaluated on a prospective blind set of 47 nodules (30 subjects), it resulted in a high AUC of 0.838 in distinguishing onchocerca worms with fecundity.},
booktitle = {Artificial Intelligence over Infrared Images for Medical Applications and Medical Image Assisted Biomarker Discovery: First MICCAI Workshop, AIIIMA 2022, and First MICCAI Workshop, MIABID 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18 and 22, 2022, Proceedings},
pages = {102–110},
numpages = {9},
keywords = {Onchocerciasis, Thermal imaging, Machine learning},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3656650.3656677,
author = {De Marsico, Maria and Giacanelli, Chiara and Manganaro, Clizia Giorgia and Palma, Alessio and Santoro, Davide},
title = {VQAsk: a multimodal Android GPT-based application to help blind users visualize pictures},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656677},
doi = {10.1145/3656650.3656677},
abstract = {VQAsk is an Android application that helps visually impaired users to get information about images framed by their smartphones. It enables to interact with one’s photographs or the surrounding visual environment through a question-and-answer interface integrating three modalities: speech interaction, haptic feedback that facilitates navigation and interaction, and sight. VQAsk is primarily designed to help visually impaired users mentally visualize what they cannot see, but it can also accommodate users with varying levels of visual ability. To this aim, it embeds advanced NLP and Computer Vision techniques to answer all user questions about the image on the cell screen. Image processing is enhanced by background removal through advanced segmentation models that identify important image elements. The outcomes of a testing phase confirmed the importance of this project as a first attempt at using AI-supported multimodality to enhance visually impaired users’ experience.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {39},
numpages = {5},
keywords = {Visual Question Answering, natural language processing and computer vision for scene interpretation, visually impaired users},
location = {Arenzano, Genoa, Italy},
series = {AVI '24}
}

@inproceedings{10.1145/3507548.3507615,
author = {Xue, Zhuxin and Bai, Yang and Wang, Haixin and He, Chenyu and Tan, Jian},
title = {Research on human-computer interaction portability evaluation model in complex environment},
year = {2022},
isbn = {9781450384155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507548.3507615},
doi = {10.1145/3507548.3507615},
abstract = {Human-computer interaction is a technology to study the relationship between users and systems. Good user experience can greatly enhance user stickiness. With the development of Internet technology, users begin to face a variety of systems, but the interaction methods involved are different. Learning new interaction means an increase in user learning costs. As a part of product design, the design of interaction also needs to cover special user groups as much as possible, such as blind people. Thus, it is necessary to study whether different ways of interaction can migrate to each other. Traditional research on user experience mainly focuses on qualitative aspects, such as questionnaire survey. In this paper, we propose a quantitative model to evaluate the portability of interaction. For software interaction design, the macro concept of user experience is quantified by different dimensions, and a unified index model and calculation method are output to guide and evaluate the portability of different software interactions.},
booktitle = {Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence},
pages = {434–437},
numpages = {4},
keywords = {Human-computer interaction, Portability, Quantitative evaluation, User experience},
location = {Beijing, China},
series = {CSAI '21}
}

@article{10.1016/j.inffus.2024.102526,
author = {Bidwai, Pooja and Gite, Shilpa and Pahuja, Natasha and Pahuja, Kishore and Kotecha, Ketan and Jain, Neha and Ramanna, Sheela},
title = {Multimodal image fusion for the detection of diabetic retinopathy using optimized explainable AI-based Light GBM classifier},
year = {2024},
issue_date = {Nov 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {111},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2024.102526},
doi = {10.1016/j.inffus.2024.102526},
journal = {Inf. Fusion},
month = nov,
numpages = {20},
keywords = {Diabetic retinopathy, Light GBM, Dunnock Scheduler optimization, Optical coherence tomography angiography, Fundus images, Multimodal image fusion}
}

@inproceedings{10.1007/978-3-031-86651-7_12,
author = {Ding, Yi},
title = {AI-Driven Analysis of Sequential OCT Images for Detecting Neovascular Activity in Age-Related Macular Degeneration to&nbsp;Optimize Anti-VEGF Therapy},
year = {2025},
isbn = {978-3-031-86650-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-86651-7_12},
doi = {10.1007/978-3-031-86651-7_12},
abstract = {Age-related Macular Degeneration (AMD) is a leading cause of severe visual impairment and blindness in developed countries, particularly affecting individuals over the age of 65. In this study, we propose a novel AI-driven approach that leverages sequential optical coherence tomography (OCT) images to detect neovascular activity and monitor disease evolution in AMD patients. By analyzing paired OCT scans, our model is designed to identify subtle changes indicative of disease worsening or improvement, enabling more precise and individualized treatment planning. This approach was rigorously tested in the Monitoring Age-related Macular Degeneration Progression In Optical Coherence Tomography for Task 1, part of the MICCAI 2024 challenge, where it achieved second place in the preliminary round, demonstrating its efficacy and potential in clinical applications. The results underscore the model’s ability to enhance the decision-making process in AMD management, paving the way for more effective use of anti-Vascular Endothelial Growth Factor (VEGF) therapy therapy.},
booktitle = {Image-Based Prediction of Retinal Disease Progression: MICCAI Challenges, DIAMOND 2024 and MARIO 2024, Held in Conjunction with MICCAI 2024, Marrakesh, Morocco, October 10, 2024, Proceedings},
pages = {126–134},
numpages = {9},
keywords = {Age-related macular degeneration, Machine learning, Optical coherence tomography},
location = {Marrakesh, Morocco}
}

@inproceedings{10.1145/3630106.3658981,
author = {Kraft, Angelie and Soulier, Elo\"{\i}se},
title = {Knowledge-Enhanced Language Models Are Not Bias-Proof: Situated Knowledge and Epistemic Injustice in AI},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658981},
doi = {10.1145/3630106.3658981},
abstract = {The factual inaccuracies ("hallucinations") of large language models have recently inspired more research on knowledge-enhanced language modeling approaches. These are often assumed to enhance the overall trustworthiness and objectivity of language models. Meanwhile, the issue of bias is usually only mentioned as a limitation of statistical representations. This dissociation of knowledge-enhancement and bias is in line with previous research on AI engineers’ assumptions about knowledge, which indicate that knowledge is commonly understood as objective and value-neutral by this community. We argue that claims and practices by actors of the field still reflect this underlying conception of knowledge. We contrast this assumption with literature from social and, in particular, feminist epistemology, which argues that the idea of a universal disembodied knower is blind to the reality of knowledge practices and seriously challenges claims of "objective" or "neutral" knowledge. Knowledge enhancement techniques commonly use Wikidata and Wikipedia as their sources for knowledge, due to their large scales, public accessibility, and assumed trustworthiness. In this work, they serve as a case study for the influence of the social setting and the identity of knowers on epistemic processes. Indeed, the communities behind Wikidata and Wikipedia are known to be male-dominated and many instances of hostile behavior have been reported in the past decade. In effect, the contents of these knowledge bases are highly biased. It is therefore doubtful that these knowledge bases would contribute to bias reduction. In fact, our empirical evaluations of RoBERTa, KEPLER, and CoLAKE, demonstrate that knowledge enhancement may not live up to the hopes of increased objectivity. In our study, the average probability for stereotypical associations was preserved on two out of three metrics and performance-related gender gaps on knowledge-driven task were also preserved. We build on these results and critical literature to argue that the label of "knowledge" and the commonly held beliefs about it can obscure the harm that is still done to marginalized groups. Knowledge enhancement is at risk of perpetuating epistemic injustice, and AI engineers’ understanding of knowledge as objective per se conceals this injustice. Finally, to get closer to trustworthy language models, we need to rethink knowledge in AI and aim for an agenda of diversification and scrutiny from outgroup members.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1433–1445},
numpages = {13},
keywords = {bias, epistemology, fairness, feminism, knowledge enhancement, knowledge graphs, language models, natural language processing, representation},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1145/3544548.3581373,
author = {Ahn, Yongsu and Lin, Yu-Ru and Xu, Panpan and Dai, Zeng},
title = {ESCAPE: Countering Systematic Errors from Machine’s Blind Spots via Interactive Visual Analysis},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581373},
doi = {10.1145/3544548.3581373},
abstract = {Classification models learn to generalize the associations between data samples and their target classes. However, researchers have increasingly observed that machine learning practice easily leads to systematic errors in AI applications, a phenomenon referred to as “AI blindspots.” Such blindspots arise when a model is trained with training samples (e.g., cat/dog classification) where important patterns (e.g., black cats) are missing or periphery/undesirable patterns (e.g., dogs with grass background) are misleading towards a certain class. Even more sophisticated techniques cannot guarantee to capture, reason about, and prevent the spurious associations. In this work, we propose ESCAPE, a visual analytic system that promotes a human-in-the-loop workflow for countering systematic errors. By allowing human users to easily inspect spurious associations, the system facilitates users to spontaneously recognize concepts associated misclassifications and evaluate mitigation strategies that can reduce biased associations. We also propose two statistical approaches, relative concept association to better quantify the associations between a concept and instances, and debias method to mitigate spurious associations. We demonstrate the utility of our proposed ESCAPE system and statistical measures through extensive evaluation including quantitative experiments, usage scenarios, expert interviews, and controlled user experiments.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {834},
numpages = {16},
keywords = {blind spot, concept interpretability, human-AI interaction, systematic error, unknown-unknowns, visual analytics, visualization},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1007/978-3-031-73119-8_15,
author = {Lei, Xiaofeng and Tham, Yih-Chung and Goh, Jocelyn Hui Lin and Feng, Yangqin and Bai, Yang and Soh, Zhi Da and Goh, Rick Siow Mong and Xu, Xinxing and Liu, Yong and Cheng, Ching-Yu},
title = {Enhancing Community Vision Screening: AI-Driven Retinal Photography for&nbsp;Early Disease Detection and&nbsp;Patient Trust},
year = {2024},
isbn = {978-3-031-73118-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-73119-8_15},
doi = {10.1007/978-3-031-73119-8_15},
abstract = {Community vision screening plays a crucial role in identifying individuals with vision loss and preventing avoidable blindness, particularly in rural communities where access to eye care services is limited. Currently, there is a pressing need for a simple and efficient process to screen and refer individuals with significant eye disease-related vision loss to tertiary eye care centers for further care. An ideal solution should seamlessly and readily integrate with existing workflows, providing comprehensive initial screening results to service providers, thereby enabling precise patient referrals for timely treatment. This paper introduces the Enhancing Community Vision Screening (ECVS) solution, which addresses the aforementioned concerns with a novel and feasible solution based on simple, non-invasive retinal photography for the detection of pathology-based visual impairment. Our study employs four distinct deep learning models: RETinal photo Quality Assessment (RETQA), Pathology Visual Impairment detection (PVI), Eye Disease Diagnosis (EDD) and Visualization of Lesion Regions of the eye (VLR). We conducted experiments on over 10 datasets, totaling more than 80,000 fundus photos collected from various sources. The models integrated into ECVS achieved impressive AUC scores of 0.98 for RETQA, 0.95 for PVI, and 0.90 for EDD, along with a DICE coefficient of 0.48 for VLR. These results underscore the promising capabilities of ECVS as a straightforward and scalable method for community-based vision screening.},
booktitle = {Ophthalmic Medical Image Analysis: 11th International Workshop, OMIA 2024, Held in Conjunction with MICCAI 2024, Marrakesh, Morocco, October 10, 2024, Proceedings},
pages = {146–156},
numpages = {11},
keywords = {vision screening, retinal photography, pathology-based visual impairment, eye disease visualization},
location = {Marrakesh, Morocco}
}

@article{10.1016/j.future.2021.11.018,
author = {Deperlioglu, Omer and Kose, Utku and Gupta, Deepak and Khanna, Ashish and Giampaolo, Fabio and Fortino, Giancarlo},
title = {Explainable framework for Glaucoma diagnosis by image processing and convolutional neural network synergy: Analysis with doctor evaluation},
year = {2022},
issue_date = {Apr 2022},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {129},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2021.11.018},
doi = {10.1016/j.future.2021.11.018},
journal = {Future Gener. Comput. Syst.},
month = apr,
pages = {152–169},
numpages = {18},
keywords = {Glaucoma, Convolutional neural network, Explainable artificial intelligence, Class activation mapping, Image processing}
}

@phdthesis{10.5555/AAI29288743,
author = {Lyu, Daoming and Levent, Yilmaz, and Shiwen, Mao, and Anh, Nguyen, and Shiqi, Zhang,},
advisor = {Bo, Liu,},
title = {Towards Trustworthy Decision-Making in Human-Machine Symbiosis},
year = {2022},
isbn = {9798845456618},
publisher = {Auburn University},
address = {USA},
abstract = {As artificial intelligence (AI) evolves, it becomes an integral part of our daily lives. To augment our effectiveness, human-machine symbiosis enables both humans and AI systems to offer different yet complementary capabilities. However, one of the significant concerns in human-machine symbiosis is the lack of human trust due to the potential ramifications, risks, or even dangers caused by AI. The critical question here is no longer whether AI will have an impact but by whom, how, where, and when this positive or negative impact will be felt. Trust is a prerequisite for humans to develop, deploy and use AI. Without AI being demonstrably worthy of trust, its uptake by humans might be hindered, hence undermining the realization of AI's vast economic and social benefits. This dissertation centers on building human trust in AI approaches to sequential decision problems, i.e., trustworthy decision-making. Specifically, there are three significant issues in current approaches.(i.)The first issue regards robustness where the brittleness in the planning indicates its inherent weaknesses. This identifies the potential risk that the AI system is unreliable and may lead to a blind trust that an AI system stays prone to errors even with high performances. To address the issue, I developed a framework to equip planning with the ability to learn so that the representation used for planning can be improved through the learned experience. Experimental results on benchmark domains demonstrate that the proposed approach can adapt to the domain uncertainties and changes and improve reliability.(ii.)The second issue regards interpretability where the learning behavior of deep reinforcement learning based on black-box neural networks is nontransparent and hard to explain and understand. This is identified as one of the main barriers to building human trust in the outcomes produced by the AI system. I developed a framework to address the issue by leveraging task decomposition and causal reasoning. Therefore, the task-level system behaviors can be interpreted in terms of causality -- causal relations among different sub-tasks. Experimental results on the challenging domain with high-dimensional sensory inputs empirically validate the interpretability of sub-tasks, along with improved data efficiency compared with state-of-the-art approaches.(iii.)The third issue regards adaptive autonomy where the concern is to what degree of autonomy should be granted to an AI system. Furthermore, keeping humans in a supervisory role is key to striking a balance between machine-led and human-led decision-making. Therefore, I developed a human-machine collaborative decision-making framework to empower the machine agent to make decisions, with humans maintaining oversights. In addition, the openness supported by this paradigm, i.e., the willingness to give and receive ideas, can also increase human trust. Experiments with human evaluative feedback in different scenarios also demonstrate the effectiveness of the proposed approach.},
note = {AAI29288743}
}

@inproceedings{10.1145/3491102.3501966,
author = {Lee, Jaewook and Herskovitz, Jaylin and Peng, Yi-Hao and Guo, Anhong},
title = {ImageExplorer: Multi-Layered Touch Exploration to Encourage Skepticism Towards Imperfect AI-Generated Image Captions},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501966},
doi = {10.1145/3491102.3501966},
abstract = {Blind users rely on alternative text (alt-text) to understand an image; however, alt-text is often missing. AI-generated captions are a more scalable alternative, but they often miss crucial details or are completely incorrect, which users may still falsely trust. In this work, we sought to determine how additional information could help users better judge the correctness of AI-generated captions. We developed&nbsp;ImageExplorer, a touch-based multi-layered image exploration system that allows users to explore the spatial layout and information hierarchies of images, and compared it with popular text-based (Facebook) and touch-based (Seeing AI) image exploration systems in a study with 12 blind participants. We found that exploration was generally successful in encouraging skepticism towards imperfect captions. Moreover, many participants preferred&nbsp;ImageExplorer for its multi-layered and spatial information presentation, and Facebook for its summary and ease of use. Finally, we identify design improvements for effective and explainable image exploration systems for blind users.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {462},
numpages = {15},
keywords = {Automatic image captioning, Blind, accessibility, alt text, alternative text, encourage skepticism, imperfect AI, screen reader, touch exploration, visual impairment},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1007/978-3-031-11647-6_82,
author = {Smalenberger, Michael and Smalenberger, Kelly},
title = {Toward Accessible Intelligent Tutoring Systems: Integrating Cognitive Tutors and Conversational Agents},
year = {2022},
isbn = {978-3-031-11646-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-11647-6_82},
doi = {10.1007/978-3-031-11647-6_82},
abstract = {The literature is rife with investigations into the use of ITS in mathematics, but scant on how these systems impact students with disabilities. Since ITS continue to permeate the educational landscape, and students with disabilities are co-located with their non-disabled peers, such investigations are overdue. To that end, we provide a theoretically grounded framework for authoring accessible ITS by drawing parallels between our work and relevant studies in the literature. Our framework enables the authoring of accessible ITS by integrating a cognitive tutor with a conversational agent. Our focus in this study is on an ITS created using Cognitive Tutor Authoring Tools (CTAT) and is augmented with adaptive capabilities which make it accessible to students who are blind or have motor-function impairments. We describe an ITS piloted with 115 students in two introductory college statistics courses, and share insights gained during the implementation of our framework. We highlight several contributions, including changes to the tutor interface to make it speech-interactive which is not currently available using CTAT, adaptations to the Bazaar infrastructure to enable solution step supports by the conversational agent, and how we used over 75,000 solutions steps and explanations by 415 students on 146 questions outside of an ITS to create supports within an accessible ITS. We conclude by proposing directions for future work on authoring accessible ITS.},
booktitle = {Artificial Intelligence  in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners’ and Doctoral Consortium: 23rd International Conference, AIED 2022, Durham, UK, July 27–31, 2022, Proceedings, Part II},
pages = {414–418},
numpages = {5},
keywords = {Accessibility, Intelligent Tutoring Systems (ITS), Cognitive Tutor Authoring Tools (CTAT), Conversational agents, Bazaar, Blind, Motor-function impairments},
location = {Durham, United Kingdom}
}

@inproceedings{10.1007/978-3-031-08645-8_9,
author = {Kobayashi, Makoto and Suzuki, Takuya},
title = {Accessibility Improvement of Leisure Sports “M\"{o}lkky” for Visually Impaired Players Using AI Vision},
year = {2022},
isbn = {978-3-031-08644-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08645-8_9},
doi = {10.1007/978-3-031-08645-8_9},
abstract = {To establish an accessible sport that blind and low vision players can easily play with sighted players, we focused on the Finnish leisure sport, M\"{o}lkky. Trying to play it with blind and low vision players revealed that primitive assisting way such as clapping hands or explaining positions by voice were useful to some extent. On the other hand, the fact is observed that the throwing form of a congenital blind player was apparently different from the forms of other players. Therefore, to show that difference to the blind player, a prototype system that consists of a computer and a handy size wireless haptic device equipped with servo motors and a model arm was developed. The system detects the positions of a shoulder, an elbow, and a wrist on the throwing arm using a Mediapipe framework provided by Google. Then, a shoulder angle and an elbow angle are calculated by these positions, and the computer send these data to the haptic device via Bluetooth. Finally, the blind player touches the device and understands the motion of the throwing arm. The system was assessed by three visually impaired people, and they reported that they could clearly recognize the difference between throwing motions.},
booktitle = {Computers Helping People with Special Needs: 18th International Conference, ICCHP-AAATE 2022, Lecco, Italy, July 11–15, 2022, Proceedings, Part II},
pages = {73–78},
numpages = {6},
keywords = {M\"{o}lkky, MediaPipe, Blind, Haptic device and throwing motion},
location = {Milan, Italy}
}

@inproceedings{10.1145/3490099.3511113,
author = {Lee, Sooyeon and Yu, Rui and Xie, Jingyi and Billah, Syed Masum and Carroll, John M.},
title = {Opportunities for Human-AI Collaboration in Remote Sighted Assistance},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511113},
doi = {10.1145/3490099.3511113},
abstract = {Remote sighted assistance (RSA) has emerged as a conversational assistive technology for people with visual impairments (VI), where remote sighted agents provide realtime navigational assistance to users with visual impairments via video-chat-like communication. In this paper, we conducted a literature review and interviewed 12 RSA users to comprehensively understand technical and navigational challenges in RSA for both the agents and users. Technical challenges are organized into four categories: agents’ difficulties in orienting and localizing the users; acquiring the users’ surroundings and detecting obstacles; delivering information and understanding user-specific situations; and coping with a poor network connection. Navigational challenges are presented in 15 real-world scenarios (8 outdoor, 7 indoor) for the users. Prior work indicates that computer vision (CV) technologies, especially interactive 3D maps and realtime localization, can address a subset of these challenges. However, we argue that addressing the full spectrum of these challenges warrants new development in Human-CV collaboration, which we formalize as five emerging problems: making object recognition and obstacle avoidance algorithms blind-aware; localizing users under poor networks; recognizing digital content on LCD screens; recognizing texts on irregular surfaces; and predicting the trajectory of out-of-frame pedestrians or objects. Addressing these problems can advance computer vision research and usher into the next generation of RSA service.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {63–78},
numpages = {16},
keywords = {3D maps, RSA, artificial intelligence, augmented reality, blind, camera, computer vision, conversational assistance, navigation, people with visual impairments, remote sighted assistance, smartphone},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.1016/j.compbiomed.2025.109656,
author = {Ikram, Amna and Imran, Azhar},
title = {ResViT FusionNet Model: An explainable AI-driven approach for automated grading of diabetic retinopathy in retinal images},
year = {2025},
issue_date = {Mar 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {186},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2025.109656},
doi = {10.1016/j.compbiomed.2025.109656},
journal = {Comput. Biol. Med.},
month = apr,
numpages = {15},
keywords = {Vision Transformer, Explainable AI, Retinal images, Convolutional Neural Network, Diabetic retinopathy, Grad-CAM}
}

@inproceedings{10.1007/978-3-032-06004-4_14,
author = {Ho, Duy and Alanazi, Ahmed and Alqarni, Saeed and Lee, Chi and Sutkin, Gary and Lee, Yugyung},
title = {Embodied Surgical Intelligence via&nbsp;Digital Twins: Autonomous Trocar Insertion},
year = {2025},
isbn = {978-3-032-06003-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-06004-4_14},
doi = {10.1007/978-3-032-06004-4_14},
abstract = {Robotic-assisted surgery (RAS) has improved the precision and consistency of minimally invasive procedures, yet most systems remain teleoperated and lack adaptability. We present a clinically grounded framework that integrates digital twins, virtual reality (VR), and embodied AI to enable autonomous trocar insertion in Mid-Urethral Sling (MUS) surgery—a representative task involving blind anatomical navigation. Our approach combines reinforcement learning (RL), behavioral cloning (BC), and generative adversarial imitation learning (GAIL) to train robotic agents from expert demonstrations and synthetic interactions. A high-fidelity 3D digital twin environment supports safe policy training and sim-to-real transfer to a physical robot (UFactory Lite6). Evaluation across simulation and deployment shows a 23% improvement in procedural fidelity, 0.78&nbsp;mm average deviation, and superior performance compared to manual and gesture-based control. Three operational prototypes, VR-controlled, hand-tracked, and fully autonomous, demonstrate the system’s flexibility and clinical relevance. This work underscores the potential of embodied AI and digital twins to enhance surgical autonomy, reduce variability, and scale safe, adaptive training systems for intelligent operating rooms.},
booktitle = {AI for Clinical Applications: First International Workshops, Agentic AI 2025, CREATE 2025, and Clinical MLLMs 2025, Held in Conjunction with MICCAI 2025, Daejeon, South Korea, September 23 and 27, 2025, Proceedings},
pages = {136–145},
numpages = {10},
keywords = {Embodied AI, Surgical Robotics, Digital Twin, Reinforcement Learning, Mid-Urethral Sling, Surgical Autonomy},
location = {Daejeon, Korea (Republic of)}
}

@inproceedings{10.1145/3594806.3594811,
author = {Moured, Omar and Alzalabny, Sara and Schwarz, Thorsten and Rapp, Bastian and Stiefelhagen, Rainer},
title = {Accessible Document Layout: An Interface for 2D Tactile Displays},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3594811},
doi = {10.1145/3594806.3594811},
abstract = {Reading and processing documents is a challenging task for people with blindness and visual impairment (BVI). Despite various methods and research being conducted on converting text and multimedia content to an accessible format, the task of layout navigation remains under-explored. Within a document, a demanding task for people with BVI is to understand the layout and navigate, so as to form a proper reading order. This is specifically challenging in documents with complex layouts, such as those with multiple columns and arbitrarily distributed elements. Traditional methods, such as screen readers, can be limited in their ability to navigate complex layouts and unreliable for various types of documents such as newspapers and slides. A new tactile layout reader has been developed in this work to enhance document navigation by providing pinpointed audio-tactile explanations. Our approach, as shown in Figure 1, uses state-of-the-art AI object detection to create a high-level abstract of the document structure, and an optimized interface suitable for both 2D refreshable displays and braille-embossed documents, with both audio and tactile representations. In order to enable the research community and contribute to the field, we will make our codes, models, and annotated data publicly available.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {265–271},
numpages = {7},
keywords = {2D refreshable pin-matrix displays, audio-tactile user interface, document layout, object detection},
location = {Corfu, Greece},
series = {PETRA '23}
}

@inproceedings{10.1007/978-3-032-05114-1_25,
author = {Wang, Ziheng and Yang, Shuran and Chen, Wen and Zhang, Zhen and Wang, Mengyu and Zhou, Feixiang and Tian, Yu and Wang, Meng and Zhao, Yitian and Zheng, Yalin and Meng, Yanda},
title = {Fairness-Aware vCDR-Controlled Generation for&nbsp;Glaucoma Diagnosis},
year = {2025},
isbn = {978-3-032-05113-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-05114-1_25},
doi = {10.1007/978-3-032-05114-1_25},
abstract = {Glaucoma is a leading cause of irreversible blindness, and early diagnosis is crucial for effective treatment. However, AI-assisted glaucoma diagnosis faces challenges in fairness and data scarcity, because AI model biases can lead to disparities across demographic groups. To address this, we propose GlaucoDiff, a diffusion-based generative model that synthesizes SLO images with precise control over the vertical cup-to-disc ratio. Unlike previous methods, GlaucoDiff enables bidirectional synthesis, generating both healthy and glaucomatous samples of varying severity, thus enhancing the dataset diversity. To ensure anatomical fidelity, GlaucoDiff leverages real fundus backgrounds while generating the optic nerve head regions. We also introduce a sample selection strategy that filters generated images based on the alignment agreement percentage, compared with target optic structures, ensuring the high-quality of the synthetic data. Experiments on two public ophthalmic datasets demonstrate that GlaucoDiff outperforms state-of-the-art approaches in both diagnosis and fairness measurement settings. Two independent ophthalmologists’ evaluations confirm the clinical relevance of the generated images, highlighting GlaucoDiff’s potential for improving AI-driven glaucoma diagnosis. Our code is available ().},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2025: 28th International Conference, Daejeon, South Korea, September 23–27, 2025, Proceedings, Part IX},
pages = {256–266},
numpages = {11},
keywords = {Glaucoma Diagnosis, Image Synthesis, Fairness Learning},
location = {Daejeon, Korea (Republic of)}
}

@inproceedings{10.1145/3604237.3626902,
author = {Teixeira, Ana Clara and Marar, Vaishali and Yazdanpanah, Hamed and Pezente, Aline and Ghassemi, Mohammad},
title = {Enhancing Credit Risk Reports Generation using LLMs: An Integration of Bayesian Networks and Labeled Guide Prompting},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626902},
doi = {10.1145/3604237.3626902},
abstract = {Credit risk analysis is a process that involves a wide range of complex cognitive abilities. Automating the credit risk analysis process using Large Language Models can bring transformative changes to the finance industry, but not without appropriate measures to ensure trustworthy responses. In this work, we propose a novel prompt-engineering method that enhances the ability of Large Language Models to generate reliable credit risk reports - Labeled Guide Prompting (LGP). LGP consists of: (1) providing annotated few-shot examples to the LLM that denote sets of tokens in an exemplary prompt that are of greater importance when generating sets of tokens in the exemplary response and (2) providing text in the prompt that describes the direction, pathways and interactions between variables from a Bayesian network used for credit risk assessment, thus promoting abductive reasoning. Using data from 100 credit applications, we demonstrate that LGP enables LLMs to generate credit risk reports that are preferred by human credit analysts (in 60-90% of cases) over alternative credit risk reports created by their peers in a blind review. Additionally, we found a statistically significant improvement (p-value &lt; 10− 10) in the insightfulness of the responses generated using LGP when compared to identical prompts without LGP components. We conclude that Labeled Guide Prompting can enhance LLM performance in complex problem-solving tasks, achieving a level of competency comparable to or exceeding human experts.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {340–348},
numpages = {9},
keywords = {Bayesian network, GPT-4, credit risk report, labeled guide prompting, prompt engineering},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1007/978-3-032-07845-2_11,
author = {Ghamizi, Salah and Kanli, Georgia and Perquin, Magali and Keunen, Olivier},
title = {The Data Behind the&nbsp;Model: Gaps and&nbsp;Opportunities for&nbsp;Foundation Models in&nbsp;Brain Imaging},
year = {2025},
isbn = {978-3-032-07844-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-07845-2_11},
doi = {10.1007/978-3-032-07845-2_11},
abstract = {Foundation Models (FMs) have revolutionized machine learning in medical imaging, yet their application to brain imaging remains limited and fragmented. Despite the availability of diverse and extensive neuroimaging datasets, most FM research has focused narrowly on a handful of tasks, mainly tumor classification and segmentation, while neglecting prevalent neurological disorders such as ADHD and early-stage Parkinson’s disease. In this work, we present the largest and most comprehensive atlas of brain imaging datasets to date, comprising 151 datasets and over 541k volumetric imaging studies across a wide range of modalities and pathologies. Our meta-analysis of 86 brain imaging FMs reveals a disproportionate reliance on structural MRI and a small set of popular datasets, along with critical blind spots in both disease coverage and imaging modalities. We identify systemic challenges, including inconsistent model evaluation protocols, heterogeneous data formats, and limited availability. All of which hinder reproducibility, scalability, and clinical translation. Our publicly available atlases pave the way for more robust, scalable, and clinically meaningful FMs in brain imaging.},
booktitle = {Foundation Models for General Medical AI: Third International Workshop, MedAGI 2025, Held in Conjunction with MICCAI 2025, Daejeon, South Korea, September 27, 2025, Proceedings},
pages = {109–119},
numpages = {11},
keywords = {Foundation Models, Medical Imaging, Brain diseases},
location = {Daejeon, Korea (Republic of)}
}

@article{10.61822/amcs-2024-0044,
author = {Sravya, Valluri Sri and Srinivasu, Parvathaneni Naga and Shafi, Jana and Ho\l{}ubowski, Waldemar and Zielonka, Adam},
title = {Advanced Diabetic Retinopathy Detection with the R–CNN: A Unified Visual Health Solution},
year = {2024},
issue_date = {Sep 2024},
publisher = {Walter de Gruyter &amp; Co.},
address = {USA},
volume = {34},
number = {4},
issn = {1641-876X},
url = {https://doi.org/10.61822/amcs-2024-0044},
doi = {10.61822/amcs-2024-0044},
abstract = {Diabetic retinopathy (DR) can cause blindness and vision impairment. This degenerative eye condition may lead to an irreversible vision loss. The prevalence of vision impairment and blindness caused by DR emphasizes the critical need for better screening and therapy measures. DR aetiology involves persistent hyperglycemia-induced microvascular abnormalities, oxidative stress, inflammatory reactions, and retinal blood flow changes. Common screening methods for retinal issues include fundus photography, OCT, and fluorescein angiography. For those with diabetic macular edema (DME), it is a common cause of vision loss. Our goal is to develop an automated, cost-effective method for identifying diabetic retinal disease specimens. This study introduces a faster R-CNN method for detecting and classifying DR lesions in retinal images. Those are classified across five different classes. An extensive analysis of 88,704 images from a Kaggle dataset indicates the efficiency of the proposed model, with a reasonable accuracy of 98.38%. The proposed method is robust in disease localization and classification tasks and it has outperformed other existing studies in DR recognition. On evaluating cross-datasets in Kaggle and APTOS, the model has yield better results during training and testing phases.},
journal = {Int. J. Appl. Math. Comput. Sci.},
month = sep,
pages = {661–678},
numpages = {18},
keywords = {disease diagnosis, diabetic retinopathy, convolutional neural networks.}
}

@article{10.1145/3728875,
author = {Xia, Yifan and Xie, Zichen and Liu, Peiyu and Lu, Kangjie and Liu, Yan and Wang, Wenhai and Ji, Shouling},
title = {Beyond Static Pattern Matching? Rethinking Automatic Cryptographic API Misuse Detection in the Era of LLMs},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728875},
doi = {10.1145/3728875},
abstract = {While the automated detection of cryptographic API misuses has progressed significantly, its precision diminishes for intricate targets due to the reliance on manually defined patterns. Large Language Models (LLMs) offer a promising context-aware understanding to address this shortcoming, yet the stochastic nature and the hallucination issue pose challenges to their applications in precise security analysis. This paper presents the first systematic study to explore LLMs’ application in cryptographic API misuse detection. Our findings are noteworthy: The instability of directly applying LLMs results in over half of the initial reports being false positives. Despite this, the reliability of LLM-based detection could be significantly enhanced by aligning detection scopes with realistic scenarios and employing a novel code &amp; analysis validation technique, achieving a nearly 90% detection recall. This improvement substantially surpasses traditional methods and leads to the discovery of previously unknown vulnerabilities in established benchmarks. Nevertheless, we identify recurring failure patterns that illustrate current LLMs’ blind spots, including cryptographic knowledge deficiencies and code semantics misinterpretations. Leveraging these findings, we deploy an LLM-based detection system and uncover 63 new vulnerabilities (47 confirmed, 7 already fixed) in open-source Java and Python repositories, including prominent projects like Apache.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA006},
numpages = {24},
keywords = {API Misuse Detection, Large Language Models for Software Security}
}

@article{10.1007/s00146-024-01879-2,
author = {Hong, Bin and Guo, Yihang and Chen, Meimei and Nie, Yahui and Feng, Changyuan and Li, Fugeng},
title = {Collaborative route map and navigation of the guide dog robot based on optimum energy consumption},
year = {2024},
issue_date = {Feb 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {40},
number = {2},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-024-01879-2},
doi = {10.1007/s00146-024-01879-2},
abstract = {The guide dog robot (GDR) is a low-speed companion robot that serves visually impaired people and is used to guide blind people to walk steadily, carrying a variety of intelligent technologies and needing to have the ability to guide with optimal energy consumption in specific scenarios. This paper proposes an innovative technique for virtual-real collaborative path planning and navigation of the GDR specific indoor scenarios, and designs an experimental method for virtual-real collaborative path planning of the GDR specific scenarios. The energy consumption integral equation is used to solve for the energy consumption of the GDR with virtual-real synergy, and the difference in energy consumption is compared for three different navigation directions: horizontal, vertical and oblique obstacle avoidance. The results show that the optimized GDR saves 6.91% in rectilinear movement and 10.60% in curved movement. The efficiency of planning and navigating the GDR in specific domestic scenarios is verified by a virtual-real cooperative. The realization of optimal path planning for energy consumption is instrumental in exploring many of the most significant thought in the path planning and navigation of mobile robots in indoor specific scenarios.},
journal = {AI Soc.},
month = mar,
pages = {733–739},
numpages = {7},
keywords = {Energy optimal, Path planning, Guide Dog Robot (GDR), Virtual-real collaboration}
}

@inproceedings{10.1007/978-3-031-97775-6_25,
author = {Hernandez Manzo, Diana S. and Munir, Verda and Isaacs, John},
title = {A Virtual Learning Experience in&nbsp;Sustainable Fashion: Escape Room},
year = {2025},
isbn = {978-3-031-97774-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-97775-6_25},
doi = {10.1007/978-3-031-97775-6_25},
abstract = {This paper presents an AI-enhanced virtual reality (VR) escape room designed to educate users about sustainable fashion practices, with a specific collaboration on the heritage of Scotland’s Harris Tweed Hebrides, a brand established in 1840. The project leveraged AI-driven technologies such as Polycam and Luma to scan 3D Harris Tweed garments, creating detailed models seamlessly integrated into a Unity-based VR escape room. Within the experience, players interact with AI-powered features, including OpenAI’s ChatGPT, which serves as a conversational agent to provide real-time information and answer user queries about sustainable fashion. The VR application, accessed via an Oculus Quest 2 headset, immerses users in a virtual house filled with interactive puzzles and challenges. To evaluate the effectiveness of the VR experience, a blind study was conducted, comparing the learning outcomes of participants who received the same educational content in a traditional printed format versus those who engaged with the immersive VR environment. The findings aim to demonstrate how interactive, game-based learning can enhance users’ understanding of sustainable fashion compared to traditional educational methods, ultimately fostering environmentally conscious consumer behavior.},
booktitle = {Extended Reality: International Conference, XR Salento 2025, Otranto, Italy, June 17–20, 2025, Proceedings, Part V},
pages = {405–416},
numpages = {12},
keywords = {Virtual Reality, Sustainable Fashion, Gamification},
location = {Otranto, Italy}
}

@inproceedings{10.1145/3663548.3675613,
author = {Chheda-Kothary, Arnavi and Wobbrock, Jacob O. and Froehlich, Jon E.},
title = {Engaging with Children's Artwork in Mixed Visual-Ability Families},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675613},
doi = {10.1145/3663548.3675613},
abstract = {We present two studies exploring how blind or low-vision (BLV) family members engage with their sighted children’s artwork, strategies to support understanding and interpretation, and the potential role of technology, such as AI, therein. Our first study involved 14 BLV individuals, and the second included five groups of BLV individuals with their children. Through semi-structured interviews with AI descriptions of children’s artwork and multi-sensory design probes, we found that BLV family members value artwork engagement as a bonding opportunity, preferring the child’s storytelling and interpretation over other nonvisual representations. Additionally, despite some inaccuracies, BLV family members felt that AI-generated descriptions could facilitate dialogue with their children and aid self-guided art discovery. We close with specific design considerations for supporting artwork engagement in mixed visual-ability families, including enabling artwork access through various methods, supporting children’s corrections of AI output, and distinctions in context vs. content and interpretation vs. description of children’s artwork.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {3},
numpages = {19},
keywords = {AI, Accessibility, blind or low-vision, children’s artwork, mixed-ability families},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3706599.3720265,
author = {Cao, Xinyun and Ju, Kexin Phyllis and Li, Chenglin and Jain, Dhruv},
title = {SceneGenA11y: How can Runtime Generative tools improve the Accessibility of a Virtual 3D Scene?},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720265},
doi = {10.1145/3706599.3720265},
abstract = {With the popularity of virtual 3D applications, from video games to educational content and virtual reality scenarios, the accessibility of 3D scene information is vital to ensure inclusive and equitable experiences for all. Previous work include information substitutions like audio description and captions, as well as personalized modifications, but they could only provide predefined accommodations. In this work, we propose SceneGenA11y, a system that responds to the user's natural language prompts to improve accessibility of a 3D virtual scene in runtime. The system primes LLM agents with accessibility-related knowledge, allowing users to explore the scene and perform verifiable modifications to improve accessibility. We conducted a preliminary evaluation of our system with three blind and low-vision people and three deaf and hard-of-hearing people. The results show that our system is intuitive to use and can successfully improve accessibility. We discussed usage patterns of the system, potential improvements, and integration into apps. We ended with highlighting plans for future work.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {472},
numpages = {10},
keywords = {BLV, DHH, accessibility, generative AI, virtual 3D scenes},
location = {
},
series = {CHI EA '25}
}

@article{10.1016/j.dsp.2024.104684,
author = {Trivedi, Sameer and Dam, Arkita and Chidirala, Bharathi and Acharya, Bibhudendra},
title = {Enhancing image watermarking efficiency through advanced deep learning: A novel approach with modified Res-Ception blocks and practical applications in modern scenarios},
year = {2024},
issue_date = {Nov 2024},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2024.104684},
doi = {10.1016/j.dsp.2024.104684},
journal = {Digit. Signal Process.},
month = nov,
numpages = {13},
keywords = {Image watermarking, Fidelity, Robustness, Inception resnet block, Imperceptible watermarking, Watermarking, Metadata encryption, Deep learning, Colour watermarking}
}

@inproceedings{10.1145/3644815.3644961,
author = {Poenaru-Olaru, Lorena and Karpova, Natalia and Cruz, Luis and Rellermeyer, Jan S. and van Deursen, Arie},
title = {Is Your Anomaly Detector Ready for Change? Adapting AIOps Solutions to the Real World},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644961},
doi = {10.1145/3644815.3644961},
abstract = {Anomaly detection techniques are essential in automating the monitoring of IT systems and operations. These techniques imply that machine learning algorithms are trained on operational data corresponding to a specific period of time and that they are continuously evaluated on newly emerging data. Operational data is constantly changing over time, which affects the performance of deployed anomaly detection models. Therefore, continuous model maintenance is required to preserve the performance of anomaly detectors over time. In this work, we analyze two different anomaly detection model maintenance techniques in terms of the model update frequency, namely blind model retraining and informed model retraining. We further investigate the effects of updating the model by retraining it on all the available data (full-history approach) and only the newest data (sliding window approach). Moreover, we investigate whether a data change monitoring tool is capable of determining when the anomaly detection model needs to be updated through retraining.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {222–233},
numpages = {12},
keywords = {anomaly detection, AIOps, model monitoring, model maintenance, concept drift detection},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@article{10.1002/pra2.1303,
author = {Alasmari, Bayan and Cox, Andrew and Rutter, Sophie and Vannini, Sara},
title = {Mothers' Use of Social Media as a Health Information Source about Child Autism in Saudi Arabia},
year = {2025},
issue_date = {October 2025},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {62},
number = {1},
url = {https://doi.org/10.1002/pra2.1303},
doi = {10.1002/pra2.1303},
abstract = {In Saudi Arabia, mothers of autistic children face difficulties accessing accurate information and reliable support due to limited services and social stigma. This paper presents early findings from an ongoing qualitative study exploring mothers' information behaviour, including how they seek, evaluate, and use autism‐related information via social media and AI tools. Based on interviews with six mothers of autistic children, the study shows how social media provides emotional validation, peer support, and practical insights. Mothers evaluate information based on how often they encounter it, personal experience, and ChatGPT, which they use to verify, simplify, or explain information. Alexa also became a communication aid for a blind autistic child. These findings reveal how mothers actively manage uncertainty and compensate for gaps in professional support. The paper contributes to understanding autism‐related information behaviour in a Saudi context. It highlights the need for more accessible, accurate digital resources for underserved families lacking trained professionals, public awareness, and inclusive education options.},
journal = {Proceedings of the Association for Information Science and Technology},
month = oct,
pages = {860–865},
numpages = {6},
keywords = {Autism Spectrum, Neurodiversity, Information Behavior, Social Media, Generative AI}
}

@inproceedings{10.1007/978-3-031-73119-8_11,
author = {Musluh, Saif Khalid and Okran, Ammar M. and Abdulwahab, Saddam and Puig, Domenec and Rashwan, Hatem A.},
title = {Advanced Diabetic Retinopathy Classification: Integrating Pathological Indicators Segmentation and&nbsp;Morphological Feature Analysis},
year = {2024},
isbn = {978-3-031-73118-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-73119-8_11},
doi = {10.1007/978-3-031-73119-8_11},
abstract = {Diabetic retinopathy (DR) is a common and serious complication of diabetes mellitus, often leading to blindness. Identifying DR stages accurately is essential for timely and effective treatment. This study introduces an innovative method to improve DR classification by detecting retinal lesions in fundus images (such as Hard Exudates, Soft Exudates, Microaneurysms, and Hemorrhages) and extracting visual and morphological features from these lesions. The proposed approach generates synthetic retinal lesions and creates artificial masks to highlight DR pathology regions. A segmentation model is trained to produce these masks, which are then refined using real fundus images and corresponding annotations. The model is then fine-tuned with real fundus images and corresponding masks. The model then derives morphological features (such as the number of each lesion, and maximum and minimum sizes, among others) from the generated masks and integrates them with latent features extracted from the segmentation model to enhance classification accuracy. The model can show a visual explanation generated by the to aid doctors in verifying and trusting the AI’s decisions, ultimately enhancing clinical decision-making and patient care. Experimental results on the DDR, E-Ophtha, and IDRiD datasets demonstrate the method’s effectiveness in improving lesion segmentation and DR classification. The DR classification achieved an accuracy of 88.04% and a Quadratic Weighted Kappa (QWK) score of 93.71%, surpassing state-of-the-art methods. The use of composite masks improves the model’s ability to identify subtle DR progression indicators, enabling more precise diagnostics and explainable AI-based interventions in clinical practice. The code is publicly available at .},
booktitle = {Ophthalmic Medical Image Analysis: 11th International Workshop, OMIA 2024, Held in Conjunction with MICCAI 2024, Marrakesh, Morocco, October 10, 2024, Proceedings},
pages = {104–114},
numpages = {11},
keywords = {Diabetic retinopathy, Retinal lesions, Fundus images, Deep Learning, Morphological biomarkers, Synthetic images},
location = {Marrakesh, Morocco}
}

@inproceedings{10.1007/978-3-031-62846-7_35,
author = {Moured, Omar and Farooqui, Shahid Ali and M\"{u}ller, Karin and Fadaeijouybari, Sharifeh and Schwarz, Thorsten and Javed, Mohammed and Stiefelhagen, Rainer},
title = {Alt4Blind: A User Interface to&nbsp;Simplify Charts Alt-Text Creation},
year = {2024},
isbn = {978-3-031-62845-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-62846-7_35},
doi = {10.1007/978-3-031-62846-7_35},
abstract = {Alternative Texts (Alt-Text) for chart images are essential for making graphics accessible to people with blindness and visual impairments. Traditionally, Alt-Text is manually written by authors but often encounters issues such as oversimplification or complication. Recent trends have seen the use of AI for Alt-Text generation. However, existing models are susceptible to producing inaccurate or misleading information. We address this challenge by retrieving high-quality alt-texts from similar chart images, serving as a reference for the user when creating alt-texts. Our three contributions are as follows: (1) we introduce a new benchmark comprising 5,000 real images with semantically labeled high-quality Alt-Texts, collected from Human Computer Interaction venues. (2) We developed a deep learning-based model to rank and retrieve similar chart images that share the same visual and textual semantics. (3) We designed a user interface (UI) to facilitate the alt-text creation process. Our preliminary interviews and investigations highlight the usability of our UI. For the dataset and further details, please refer to our project page:},
booktitle = {Computers Helping People with Special Needs: 19th International Conference, ICCHP 2024, Linz, Austria, July 8–12, 2024, Proceedings, Part I},
pages = {291–298},
numpages = {8},
keywords = {Alt-Text, Image Retrieval, CLIP Model},
location = {Linz, Austria}
}

@inproceedings{10.1145/3586182.3616669,
author = {Gorniak, Joshua and Ottiger, Jacob and Wei, Donglai and Kim, Nam Wook},
title = {VizAbility: Multimodal Accessible Data Visualization with Keyboard Navigation and Conversational Interaction},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586182.3616669},
doi = {10.1145/3586182.3616669},
abstract = {Data visualization serves as a crucial tool for communicating important information in our society. Yet, as visualizations grow more complex, they become less accessible to individuals with visual impairments. Traditional accessibility approaches like alternative text and data tables often fall short of capturing the full potential of data visualization. To bridge this gap, we introduce VizAbility, a novel multimodal accessible system that combines keyboard navigation with conventional interaction, enabling individuals with visual impairments to actively engage with and explore data visualizations. We built an LLM-based pipeline that classifies user queries and synthesizes underlying data, chart structure, user locality, and web-based information to answer the queries. Our preliminary evaluation using real-world questions from blind individuals demonstrates the significant potential of VizAbility.},
booktitle = {Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {18},
numpages = {3},
keywords = {accessibility, blind and low vision people, data visualization},
location = {San Francisco, CA, USA},
series = {UIST '23 Adjunct}
}

@article{10.1145/3517384,
author = {Stangl, Abigale and Shiroma, Kristina and Davis, Nathan and Xie, Bo and Fleischmann, Kenneth R. and Findlater, Leah and Gurari, Danna},
title = {Privacy Concerns for Visual Assistance Technologies},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3517384},
doi = {10.1145/3517384},
abstract = {People who are blind share their images and videos with companies that provide visual assistance technologies (VATs) to gain access to information about their surroundings. A challenge is that people who are blind cannot independently validate the content of the images and videos before they share them, and their visual data commonly contains private content. We examine privacy concerns for blind people who share personal visual data with VAT companies that provide descriptions authored by humans or artifcial intelligence (AI). We frst interviewed 18 people who are blind about their perceptions of privacy when using both types of VATs. Then we asked the participants to rate 21 types of image content according to their level of privacy concern if the information was shared knowingly versus unknowingly with human- or AI-powered VATs. Finally, we analyzed what information VAT companies communicate to users about their collection and processing of users’ personal visual data through their privacy policies. Our fndings have implications for the development of VATs that safeguard blind users’ visual privacy, and our methods may be useful for other camera-based technology companies and their users.},
journal = {ACM Trans. Access. Comput.},
month = may,
articleno = {15},
numpages = {43},
keywords = {Visual assistance technology, image description, visual question answering, remote sighted assistance, artificial intelligence, private visual content, visual personal data, privacy, privacy policy analysis, data regulation, camera-based devices, blind, visually impaired}
}

@inproceedings{10.1145/3663548.3675599,
author = {Chang, Ruei-Che and Liu, Yuxuan and Zhang, Lotus and Guo, Anhong},
title = {EditScribe: Non-Visual Image Editing with Natural Language Verification Loops},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675599},
doi = {10.1145/3663548.3675599},
abstract = {Image editing is an iterative process that requires precise visual evaluation and manipulation for the output to match the editing intent. However, current image editing tools do not provide accessible interaction nor sufficient feedback for blind and low vision individuals to achieve this level of control. To address this, we developed EditScribe, a prototype system that makes object-level image editing actions accessible using natural language verification loops powered by large multimodal models. Using EditScribe, the user first comprehends the image content through initial general and object descriptions, then specifies edit actions using open-ended natural language prompts. EditScribe performs the image edit, and provides four types of verification feedback for the user to verify the performed edit, including a summary of visual changes, AI judgement, and updated general and object descriptions. The user can ask follow-up questions to clarify and probe into the edits or verification feedback, before performing another edit. In a study with ten blind or low-vision users, we found that EditScribe supported participants to perform and verify image edit actions non-visually. We observed different prompting strategies from participants, and their perceptions on the various types of verification feedback. Finally, we discuss the implications of leveraging natural language verification loops to make visual authoring non-visually accessible.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {65},
numpages = {19},
keywords = {Accessibility, assistive technology, blind, creativity support tools, generative AI, image editing, low vision, visual authoring},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3746059.3747612,
author = {Huh, Mina and Xue, Zihui and Das, Ujjaini and Ashutosh, Kumar and Grauman, Kristen and Pavel, Amy},
title = {Vid2Coach: Transforming How-To Videos into Task Assistants},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747612},
doi = {10.1145/3746059.3747612},
abstract = {People use videos to learn new recipes, exercises, and crafts. Such videos remain difficult for blind and low vision (BLV) people to follow as they rely on visual comparison. Our observations of visual rehabilitation therapists (VRTs) guiding BLV people to follow how-to videos revealed that VRTs provide both proactive and responsive support including detailed descriptions, non-visual workarounds, and progress feedback. We propose Vid2Coach, a system that transforms how-to videos into wearable camera-based assistants that provide accessible instructions and mixed-initiative feedback. From the video, Vid2Coach generates accessible instructions by augmenting narrated instructions with demonstration details and completion criteria for each step. It then uses retrieval-augmented-generation to extract relevant non-visual workarounds from BLV-specific resources. Vid2Coach then monitors user progress with a camera embedded in commercial smart glasses to provide context-aware instructions, proactive feedback, and answers to user questions. BLV participants (N=8) using Vid2Coach completed cooking tasks with 58.5% fewer errors than when using their typical workflow and wanted to use Vid2Coach in their daily lives. Vid2Coach demonstrates an opportunity for AI visual assistance that strengthens rather than replaces non-visual expertise.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {46},
numpages = {24},
keywords = {How-To Videos, Task Assistant, Video Understanding, Accessibility},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3597638.3608402,
author = {Stangl, Abigale and Ihorn, Shasta and Siu, Yue-Ting and Bodi, Aditya and Castanon, Mar and Narins, Lothar D and Yoon, Ilmi},
title = {The Potential of a Visual Dialogue Agent In a Tandem Automated Audio Description System for Videos},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608402},
doi = {10.1145/3597638.3608402},
abstract = {The relentless pace of video production exacerbates the digital accessibility gap that individuals who are blind or low vision (BLV) face on a daily basis, resulting in disproportionate exclusion from community opportunities and risk management. Whereas previous automated audio description (AD) systems provide single-tool approaches for delivering minimum viable description (MVD) or delivering on-demand visual question answering (VQA), we present a tandem AI-based AD tool that combines MVD and on-demand VQA. A user study with 26 BLV individuals explored how the tandem system may be used under the conditions of delivering MVD and/or on-demand VQA with AI-only or human-in-the-loop support. When each tool was used in isolation, AI-only conditions scored significantly lower in both user enjoyment and comprehension. When used in tandem, AI-only conditions matched outcomes delivered with human-in-the-loop, which suggests that AI-only AD tools may be most effective when both types of tools are used in tandem. A multimodal analysis of interactions with the tandem system revealed areas for system improvement in terms of the timing of AD delivery and accurate content delivery. We discuss how the use of both types of tools in a tandem system can mitigate some of the digital frictions that have plagued efforts in machine learning and automated tools for accessibility.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {32},
numpages = {17},
keywords = {AI, Audio Description, Blind and Low Vision, Minimum Viable Description, Virtual Agents, Virtual Volunteer, Visual Assistance, Visual Dialogue, Visual Question Answering},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@article{10.1145/3762706,
author = {Mayer, Sven and Jones, Matt},
title = {PACMHCI V9, MHCI, September 2025 Editorial},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
url = {https://doi.org/10.1145/3762706},
doi = {10.1145/3762706},
abstract = {Welcome to the 2025 Mobile Human-Computer Interaction (MHCI) issue of the Proceedings of the ACM on Human-Computer Interaction (PACMHCI), which highlights current advances from the MHCI community. This issue features research on mobile, wearable, and personal technologies that shape how people work, communicate, move through cities, and experience the world. As mobile systems reach deeper into daily life, the community continues to balance technical innovation with social responsibility, addressing privacy, equity, safety, and long-term impact.This year, we received 147 valid submissions and accepted 40 papers, for an acceptance rate of 27.2%. Authors are invited to present their work at MobileHCI 2025, held September 22 to 25, 2025, in Sharm El-Sheikh, Egypt. The papers span privacy and security in mobile contexts, health and wellbeing applications, accessibility and assistive technologies, novel input and interaction techniques, sensing and on-device intelligence, and emerging experiences in mixed and extended reality.Our editorial process followed a rigorous two-round, double-blind review. We are grateful to our 36 Associate Editors (AEs), who coordinated expert reviews and constructive discussions, and to the 230 external reviewers who contributed their time and insight delivered in 269 reviews. Their efforts ensured a fair and thorough evaluation and helped authors improve their work. Additionally, this year, six submissions were desk-rejected because they were clearly out of scope or incomplete. Eight additional submissions were assisted desk rejected, where the primary AE discussed the paper with the track chairs and determined that external reviews were unnecessary. This practice ensured that reviewer capacity was focused on submissions with a reasonable chance of acceptance, while still upholding fairness and transparency in decision-making.The conference theme, “Diving into Mobile AI,” invites the community to explore how AI-driven mobile technologies can empower diverse communities, from dense urban centers to rural regions, to foster collective progress. While the theme did not influence paper selection, many accepted works speak directly to it, advancing methods, tools, and experiences that make mobile AI more usable, trustworthy, and impactful.We hope you enjoy reading the articles in this issue and that they inspire new ideas for research, design, and practice in Mobile HCI.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = sep,
articleno = {MHCI001},
numpages = {1},
keywords = {MobileHCI, PACMHCI, Editorial}
}

@inproceedings{10.1145/3526113.3545613,
author = {Chang, Ruei-Che and Ting, Chao-Hsien and Hung, Chia-Sheng and Lee, Wan-Chen and Chen, Liang-Jin and Chao, Yu-Tzu and Chen, Bing-Yu and Guo, Anhong},
title = {OmniScribe: Authoring Immersive Audio Descriptions for 360° Videos},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545613},
doi = {10.1145/3526113.3545613},
abstract = {Blind people typically access videos via audio descriptions (AD) crafted by sighted describers who comprehend, select, and describe crucial visual content in the videos. 360° video is an emerging storytelling medium that enables immersive experiences that people may not possibly reach in everyday life. However, the omnidirectional nature of 360° videos makes it challenging for describers to perceive the holistic visual content and interpret spatial information that is essential to create immersive ADs for blind people. Through a formative study with a professional describer, we identified key challenges in describing 360° videos and iteratively designed OmniScribe, a system that supports the authoring of immersive ADs for 360° videos. OmniScribe uses AI-generated content-awareness overlays for describers to better grasp 360° video content. Furthermore, OmniScribe enables describers to author spatial AD and immersive labels for blind users to consume the videos immersively with our mobile prototype. In a study with 11 professional and novice describers, we demonstrated the value of OmniScribe in the authoring workflow; and a study with 8 blind participants revealed the promise of immersive AD over standard AD for 360° videos. Finally, we discuss the implications of promoting 360° video accessibility.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {15},
numpages = {14},
keywords = {360° video, Blind, accessibility, audio description, computer vision, mobile, multimedia, sonification, virtual reality, visual impairment},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{10.1145/3613904.3642233,
author = {Li, Franklin Mingzhe and Liu, Michael Xieyang and Kane, Shaun K. and Carrington, Patrick},
title = {A Contextual Inquiry of People with Vision Impairments in Cooking},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642233},
doi = {10.1145/3613904.3642233},
abstract = {Individuals with vision impairments employ a variety of strategies for object identification, such as pans or soy sauce, in the culinary process. In addition, they often rely on contextual details about objects, such as location, orientation, and current status, to autonomously execute cooking activities. To understand how people with vision impairments collect and use the contextual information of objects while cooking, we conducted a contextual inquiry study with 12 participants in their own kitchens. This research aims to analyze object interaction dynamics in culinary practices to enhance assistive vision technologies for visually impaired cooks. We outline eight different types of contextual information and the strategies that blind cooks currently use to access the information while preparing meals. Further, we discuss preferences for communicating contextual information about kitchen objects as well as considerations for the deployment of AI-powered assistive technologies.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {38},
numpages = {14},
keywords = {Accessibility, Assistive technology, Blind, Contextual Inquiry, Cooking, People with Vision Impairments},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1111/mice.13279,
author = {Tang, Youzhi and Wang, Yi and Qian, Yu},
title = {Railroad missing components detection via cascade region‐based convolutional neural network with predefined proposal templates},
year = {2024},
issue_date = {15 October 2024},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {39},
number = {20},
issn = {1093-9687},
url = {https://doi.org/10.1111/mice.13279},
doi = {10.1111/mice.13279},
abstract = {In the field of railway infrastructure maintenance, timely and accurate detection of component anomalies is crucial for safety and efficiency. This paper presents the Cascade Region‐based convolutional neural network with&nbsp;Predefined Proposal Templates (CR‐PPT), an innovative method for railroad components inspection in complex railway infrastructure using edge‐computing devices. Unlike previous systems, CR‐PPT employs a series of predefined templates that enable it to detect both the presence and missing elements within various fastening systems. Our experimental analysis pinpoints the most effective network configurations for CR‐PPT. Furthermore, the paper examines CR‐PPT's proficiency in zero‐shot learning and fine‐tuning, highlighting its adaptability to new fastening systems. We have developed an optimized inference pipeline on NVIDIA Jetson AGX Orin, significantly enhancing its applicability for railway inspection practices. Field blind tests validate the model's high precision and efficiency, greatly reducing the time and labor required for inspections. The findings highlight CR‐PPT's potential as an efficient and robust tool for track health assessment, marking a notable progression in the integration of AI and computer vision in rail track inspection.},
journal = {Comput.-Aided Civ. Infrastruct. Eng.},
month = oct,
pages = {3083–3102},
numpages = {20}
}

@inproceedings{10.1007/978-3-031-40725-3_58,
author = {Lozano-Ju\'{a}rez, Samuel and Velasco-P\'{e}rez, Nuria and Roberts, Ian and Bernal, Jer\'{o}nimo and Basurto, Nu\~{n}o and Urda, Daniel and Herrero, \'{A}lvaro},
title = {Convolutional Neural Networks for&nbsp;Diabetic Retinopathy Grading from&nbsp;iPhone Fundus Images},
year = {2023},
isbn = {978-3-031-40724-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-40725-3_58},
doi = {10.1007/978-3-031-40725-3_58},
abstract = {Diabetic eye diseases is a major issue in Europe and the prevalence of visual impairment and blindness caused by Diabetic Retinopathy (DR) has significantly increased in the last decades. Efficient screening and early diagnose of DR by family physicians would help to reduce costs in health systems and shorten waiting lists, thus decreasing patients’ emotional stress. In this sense, the use of portable image devices (e.g., a mobile phone with a specific fundus image capturing device attach to it) combined with AI-based systems arise as a powerful tool to address this problem. This paper develops 2 well-known pre-trained Convolutional Neural Networks and fine-tune them on a local Spanish cohort and 3 more publicly available fundus image dataset for DR grading. The models trained were evaluated on fundus images captured using an iPhone mobile within the local Spanish cohort. The results of the analysis showed how in one of the settings tested, one of the models was able to surpass human-level performance achieving an AUC of 0.679 in comparison to an AUC of 0.667 achieved by ophthalmologists when diagnosing the grade of DR on the same iPhone fundus images, although further work and improvements need to take place in order to consider it for a realistic deployment in the daily clinical practice.},
booktitle = {Hybrid Artificial Intelligent Systems: 18th International Conference, HAIS 2023, Salamanca, Spain, September 5–7, 2023, Proceedings},
pages = {685–697},
numpages = {13},
keywords = {Diabetic retinopathy, fundus image, deep learning, Convolutional Neural Networks, transfer learning, fine-tuning, smartphone, screening},
location = {Salamanca, Spain}
}

@article{10.1016/j.comcom.2024.108042,
author = {Lodhi, Muhammad Ali and Wang, Lei and Farhad, Arshad and Qureshi, Khalid Ibrahim and Chen, Jenhu and Mahmood, Khalid and Das, Ashok Kumar},
title = {A Contextual Aware Enhanced LoRaWAN Adaptive Data Rate for mobile IoT applications},
year = {2025},
issue_date = {Feb 2025},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {232},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2024.108042},
doi = {10.1016/j.comcom.2024.108042},
journal = {Comput. Commun.},
month = feb,
numpages = {13},
keywords = {Smart cities, Mobility, Internet of Things, Intelligent algorithms, LoRaWAN}
}

@inproceedings{10.1007/978-3-032-05005-2_18,
author = {Vargas, Pablo Nunes and Trevisan, Daniela Gorski},
title = {Seeing What We Can’t See in&nbsp;the&nbsp;Non-textual Representation from&nbsp;the&nbsp;Educational Context},
year = {2025},
isbn = {978-3-032-05004-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-05005-2_18},
doi = {10.1007/978-3-032-05005-2_18},
abstract = {This study focuses on deepening the understanding of the challenges and existing solutions related to interactions with non-textual representations in the educational context for visually impaired students and their teachers. We present a user-centered design approach that contributes to the development of more effective interactions for teaching and learning non-textual representations. To achieve this, an exploratory study was conducted using quantitative and qualitative techniques, including questionnaires, observations, and interviews with VI students and their respective teachers. Statistical results indicated that, from the teachers’ perspective, one of the main barriers was the lack of public policies, while for VI students, the primary difficulty was related to the complexity of non-textual representations. Observations highlighted how initial interactions occurred with the adapted materials used in teaching a specific type of non-textual representation, revealing differences in collaboration between totally blind students and those with low vision, as well as challenges faced by teachers in adapting these materials. The thematic analysis of interviews identified four main themes: 1. the resources used: tactile and digital; 2. the teaching strategies applied: collaborative learning, constructivism, compartmentalization and summarization; 3. the main challenges faced: technological limitations, material adaptation and inaccessible environments; and 4. the main proposed ideas: AI-generated descriptions, interactive tactile materials, a highly collaborative platform and an interactive virtual assistant. This research aims to contribute to the advancement of assistive technologies related to non-textual representations by identifying key strategies to promote accessibility and overcome challenges. Furthermore, the study may serve as inspiration for developing more inclusive design solutions tailored to the needs of the target audience.},
booktitle = {Human-Computer Interaction – INTERACT 2025: 20th IFIP TC 13 International Conference, Belo Horizonte, Brazil, September 8–12, 2025, Proceedings, Part III},
pages = {333–355},
numpages = {23},
keywords = {non-textual representations, educational, visually impaired, assistive technologies},
location = {Belo Horizonte, Brazil}
}

@article{10.1109/TIP.2024.3512378,
author = {Liu, Shaohua and Lu, Junzhe and Dou, Hongkun and Li, Jiajun and Deng, Yue},
title = {SEGSID: A Semantic-Guided Framework for Sonar Image Despeckling},
year = {2025},
issue_date = {2025},
publisher = {IEEE Press},
volume = {34},
issn = {1057-7149},
url = {https://doi.org/10.1109/TIP.2024.3512378},
doi = {10.1109/TIP.2024.3512378},
abstract = {Sonar imagery is substantially degraded by speckle noise, making the task of despeckling crucial for improving image quality. Self-supervised despeckling methods, represented by blind-spot networks (BSNs), have shown promise in this regard. However, these methods consistently face significant challenges due to the spatial correlation of speckle noise and the inherent information loss within BSNs. In this paper, we introduce SEGSID, a BSN-based, semantic-guided sonar despeckling framework designed to address these challenges. Specifically, the SEGSID framework primarily comprises a Receptive Field Augmentation (RFA) module and a Global Semantic Enhancement (GSE) module. To address the noise spatial correlation, the RFA module is crafted to strategically extract valuable local information while avoiding the exploitation of noise-correlated pixels. Concurrently, the GSE module extracts the global semantic information from entire images and injects it into the extracted local features. This enhances BSNs’ ability to harness more comprehensive image information and compensates for their inherent information loss. Furthermore, to bolster efficiency, we employ knowledge distillation techniques to transfer the expertise from the trained SEGSID into a more streamlined network suitable for broader practical applications. Extensive experiments on three distinct sonar datasets demonstrate that SEGSID outperforms both traditional despeckling methods and state-of-the-art self-supervised despeckling techniques. The implementation is publicly accessible at &lt;uri&gt;https://github.com/deng-ai-lab/SEGSID&lt;/uri&gt;.},
journal = {Trans. Img. Proc.},
month = jan,
pages = {652–666},
numpages = {15}
}

@article{10.1109/TDSC.2023.3328663,
author = {Cao, Guodong and Wang, Zhibo and Feng, Yunhe and Dong, Xiaowei and Zhang, Zhifei and Qin, Zhan and Ren, Kui},
title = {Task-Free Fairness-Aware Bias Mitigation for Black-Box Deployed Models},
year = {2024},
issue_date = {July-Aug. 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {4},
issn = {1545-5971},
url = {https://doi.org/10.1109/TDSC.2023.3328663},
doi = {10.1109/TDSC.2023.3328663},
abstract = {With AI systems widely deployed in societal applications, the fairness of these models is of increasing concern, for instance, hiring systems should recommend applicants impartially from different demographic groups, and risk assessment systems must eliminate racial inequity in the criminal justice system. Therefore, ensuring fairness in these models is crucial. In this paper, we propose Task-Free Fairness-Aware Adversarial Perturbation (TF-FAAP), a flexible approach for improving the fairness of black-box deployed models by adding perturbations on input samples that blind their fairness-related attribute information without modifying the model's parameters or structures. The proposed TF-FAAP consists of a discriminator and a generator to create universal fairness-aware perturbations for a variety of tasks. The former aims to distinguish fairnessrelated attributes, and the latter generates perturbations to make the discriminator's prediction distribution of fairness-related attributes uniform. To preserve the utility of perturbed samples, we maximize the mutual information between their representations and corresponding original samples, retaining more original samples' information. In addition, the perturbation generated by TF-FAAP has a high transferability, i.e., the perturbations learned on one dataset can also alleviate the unfairness of a model trained on a different dataset. The extensive experimental evaluation demonstrated the effectiveness and superior performance of our method.},
journal = {IEEE Trans. Dependable Secur. Comput.},
month = jul,
pages = {3390–3405},
numpages = {16}
}

@proceedings{10.1145/3724363,
title = {ITiCSE 2025: Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
year = {2025},
isbn = {9798400715679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 30th annual conference on Innovation and Technology in Computer Science Education (ITiCSE 2025), hosted by Radboud University in Nijmegen, The Netherlands.ITiCSE 2025 will take place from Friday, 27 June to Wednesday, 2 July. The conference program includes plenary lectures, paper sessions, panels, tips, techniques &amp; courseware demonstrations, posters, a doctoral consortium, and working group presentations. Working groups meet from 27 to 29 June and will submit draft reports before the conference begins on 30 June.The program includes two keynote talks by scholars from neighbouring disciplines, to help the community explore new challenges and reflect on scientific progress in the field. Our opening speaker, Inge Molenaar, will present her views about 'Human-AI Collaboration in Education: The Hybrid Future'. At the end of the conference, Danny Beckers will address the conference in a plenary session with a talk entitled 'On the Connection between Blind Dates and Teaching Programming'.Reviewing of submissions for ITiCSE 2025 involved 323 researchers and practitioners from computing education and related fields, including 38 programme committee members. Thanks to their outstanding effort and commitment, every submission received a metareview and most received at least three reviews, providing authors of all submissions with constructive feedback. Although no review process is flawless, we are confident that this effort led to a vibrant conference program, capturing multiple voices and perspectives in the field.We received 354 full paper submissions for the conference. The programme committee worked very hard to select 99 papers to be presented at ITiCSE 2025 and published in the proceedings. The proceedings also include the opening keynote abstract, 23 tips, techniques, &amp; courseware submissions, and abstracts for 9 working groups, 3 panels, 45 posters, and 16 doctoral consortium participants.},
location = {Nijmegen, Netherlands}
}

@proceedings{10.1145/3724389,
title = {ITiCSE 2025: Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 2},
year = {2025},
isbn = {9798400715693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 30th annual conference on Innovation and Technology in Computer Science Education (ITiCSE 2025), hosted by Radboud University in Nijmegen, The Netherlands.ITiCSE 2025 will take place from Friday, 27 June to Wednesday, 2 July. The conference program includes plenary lectures, paper sessions, panels, tips, techniques &amp; courseware demonstrations, posters, a doctoral consortium, and working group presentations. Working groups meet from 27 to 29 June and will submit draft reports before the conference begins on 30 June.The program includes two keynote talks by scholars from neighbouring disciplines, to help the community explore new challenges and reflect on scientific progress in the field. Our opening speaker, Inge Molenaar, will present her views about 'Human-AI Collaboration in Education: The Hybrid Future'. At the end of the conference, Danny Beckers will address the conference in a plenary session with a talk entitled 'On the Connection between Blind Dates and Teaching Programming'.Reviewing of submissions for ITiCSE 2025 involved 323 researchers and practitioners from computing education and related fields, including 38 programme committee members. Thanks to their outstanding effort and commitment, every submission received a metareview and most received at least three reviews, providing authors of all submissions with constructive feedback. Although no review process is flawless, we are confident that this effort led to a vibrant conference program, capturing multiple voices and perspectives in the field.We received 354 full paper submissions for the conference. The programme committee worked very hard to select 99 papers to be presented at ITiCSE 2025 and published in the proceedings. The proceedings also include the opening keynote abstract, 23 tips, techniques, &amp; courseware submissions, and abstracts for 9 working groups, 3 panels, 45 posters, and 16 doctoral consortium participants.},
location = {Nijmegen, Netherlands}
}

@article{10.1007/s00779-024-01801-z,
author = {Hind, Daniel and Harvey, Carlo},
title = {Using a NEAT approach with curriculums for dynamic content generation in video games},
year = {2024},
issue_date = {Aug 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {3–4},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-024-01801-z},
doi = {10.1007/s00779-024-01801-z},
abstract = {This paper presents a novel exploration of the use of an evolving neural network approach to generate dynamic content for video games, specifically for a tower defence game. The objective is to employ the NeuroEvolution of Augmenting Topologies (NEAT) technique to train a NEAT neural network as a wave manager to generate enemy waves that challenge the player’s defences. The approach is extended to incorporate NEAT-generated curriculums for tower deployments to gradually increase the difficulty for the generated enemy waves, allowing the neural network to learn incrementally. The approach dynamically adapts to changes in the player’s skill level, providing a more personalised and engaging gaming experience. The quality of the machine-generated waves is evaluated through a blind A/B test with the Games Experience Questionnaire (GEQ), and results are compared with manually designed human waves. The study finds no discernible difference in the reported player experience between AI and human-designed waves. The approach can significantly reduce the time and resources required to design game content while maintaining the quality of the player experience. The approach has the potential to be applied to a range of video game genres and within the design and development process, providing a more personalised and engaging gaming experience for players.},
journal = {Personal Ubiquitous Comput.},
month = apr,
pages = {629–641},
numpages = {13},
keywords = {Dynamic content generation, Game experience, NEAT}
}

@inproceedings{10.1145/3640543.3645154,
author = {Kim, Suhyun and Lee, Semin and Kim, Kyungok and Oh, Uran},
title = {Utilizing a Dense Video Captioning Technique for Generating Image Descriptions of Comics for People with Visual Impairments},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645154},
doi = {10.1145/3640543.3645154},
abstract = {To improve the accessibility of visual figures, auto-generation of text description of individual images has been studied. However, it cannot be directly applied to comics as the descriptions can be redundant as similar scenes appear in a row. To address this issue, we propose generating the descriptions per group of related images and demonstrate how an dense captioning technique for videos can be utilized for this purpose and ways to improve its performance. To assess the effectiveness of our approach and to identify factors affecting the quality of text descriptions of comics, we conducted a preliminary study with 3 sighted evaluators and a main user study with 12 participants with visual impairments. The results show that text descriptions generated per group of images are perceived to be better than those generated per image in terms of accuracy, clarity, understandability, length, informativeness and preference for sighted groups, when annotator is human. In the same conditions, when the annotator is AI, it exhibited better performance in terms of length. Also, people with visual impairments prefer group descriptions because of conciseness, smooth connectivity of sentences, and non-repetitive features. Based on the findings, we provide design recommendations for generating accessible comic descriptions at a scale for blind users.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {750–760},
numpages = {11},
keywords = {comics, dense video captioning, image description, people with visual impairment},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{10.1145/3708359.3712085,
author = {Liu, Michael Xieyang and Petridis, Savvas and Tsai, Vivian and Fiannaca, Alexander J. and Olwal, Alex and Terry, Michael and Cai, Carrie J.},
title = {Gensors: Authoring Personalized Visual Sensors with Multimodal Foundation Models and Reasoning},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712085},
doi = {10.1145/3708359.3712085},
abstract = {Multimodal large language models (MLLMs), with their expansive world knowledge and reasoning capabilities, present a unique opportunity for end-users to create personalized AI sensors capable of reasoning about complex situations. A user could describe a desired sensing task in natural language (e.g., “let me know if my toddler is getting into mischief in the living room”), with the MLLM analyzing the camera feed and responding within just seconds. In a formative study, we found that users saw substantial value in defining their own sensors, yet struggled to articulate their unique personal requirements to the model and debug the sensors through prompting alone. To address these challenges, we developed Gensors, a system that empowers users to define customized sensors supported by the reasoning capabilities of MLLMs. Gensors 1) assists users in eliciting requirements through both automatically-generated and manually created sensor criteria, 2) facilitates debugging by allowing users to isolate and test individual criteria in parallel, 3) suggests additional criteria based on user-provided images, and 4) proposes test cases to help users “stress test” sensors on potentially unforeseen scenarios. In a 12-participant user study, users reported significantly greater sense of control, understanding, and ease of communication when defining sensors using Gensors. Beyond addressing model limitations, Gensors supported users in debugging, eliciting requirements, and expressing unique personal requirements to the sensor through criteria-based reasoning; it also helped uncover users’ own “blind spots” by exposing overlooked criteria and revealing unanticipated failure modes. Finally, we describe insights into how unique characteristics of MLLMs–such as hallucinations and inconsistent responses–can impact the sensor-creation process. Together, these findings contribute to the design of future MLLM-powered sensing systems that are intuitive and customizable by everyday users.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {755–770},
numpages = {16},
keywords = {Human-AI Interaction, Foundation Models, Intelligent Sensing},
location = {
},
series = {IUI '25}
}

@article{10.1109/TPDS.2023.3276759,
author = {Wang, Chunyang and Bai, Yuebin and Sun, Desen},
title = {CD-MSA: Cooperative and Deadline-Aware Scheduling for Efficient Multi-Tenancy on DNN Accelerators},
year = {2023},
issue_date = {July 2023},
publisher = {IEEE Press},
volume = {34},
number = {7},
issn = {1045-9219},
url = {https://doi.org/10.1109/TPDS.2023.3276759},
doi = {10.1109/TPDS.2023.3276759},
abstract = {With DNN turning into the backbone of AI cloud services and propelling the emergence of INFerence-as-a-Service (INFaaS), DNN-specific accelerators have become the indispensable components of cloud inference systems. Due to the conservative “one-task-at-a-time” working mode and deadline blindness of those accelerators, implementing multi-tenancy that aims to improve the cost-effectiveness and meet SLA requirements is intractable. Recent studies including the temporal and spatial approaches, employ manifold scheduling mechanisms and sophisticated architecture innovations to address the challenge. However, these researches either still neglect the deadline awareness or render inevitable and expensive hardware overheads such as switches and storage. In this paper, we present &lt;italic&gt;Cooperative and Deadline-aware Multi-Systolic-Array scheduling&lt;/italic&gt; (CD-MSA), a low-cost solution for the cloud inference that utilizes the real time mechanism and task-level parallelism to enable efficient multi-tenancy. Based on our preemptive multi-systolic-array accelerator architecture supporting the simultaneous task co-location, we first construct a fine-grained DNN execution model to lay the groundwork for the lightweight preemption. Second, we design a cooperative, deadline- and laxity-aware scheduler in conjunction with an efficient schedulability test method for better QoS guarantee without introducing additional hardware cost. Finally, to further promote the overall throughput, we propose &lt;italic&gt;dynamic task fusion&lt;/italic&gt;, a software approach that fuses different tasks into the logically “multi-threading” tasks at runtime. We compare CD-MSA with several state-of-the-art researches across three multi-DNN workloads. The evaluation results show CD-MSA improves the latency-bounded throughput, SLA satisfaction rate and weighted system throughput by up to 62%, 63% and 27%, respectively.},
journal = {IEEE Trans. Parallel Distrib. Syst.},
month = jul,
pages = {2091–2106},
numpages = {16}
}

@article{10.1016/j.cviu.2021.103303,
author = {Tripathi, Pavani and Akhter, Yasmeena and Khurshid, Mahapara and Lakra, Aditya and Keshari, Rohit and Vatsa, Mayank and Singh, Richa},
title = {MTCD: Cataract detection via near infrared eye images},
year = {2022},
issue_date = {Jan 2022},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {214},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2021.103303},
doi = {10.1016/j.cviu.2021.103303},
journal = {Comput. Vis. Image Underst.},
month = jan,
numpages = {9},
keywords = {Iris, Cataract, Biometrics, Classification, Deep learning, Multitask Learning}
}

@phdthesis{10.5555/AAI28971477,
author = {Shahsavari, Shadi and Lieven, Vandenberghe, and Cho-Jui, Hsieh,},
advisor = {P., Roychowdhury, Vwani and R, Tangherlini, Timothy},
title = {Automated Conspiracy Theory Detection and Narrative Consensus Tracking in Social Media},
year = {2022},
isbn = {9798209893554},
publisher = {University of California, Los Angeles},
abstract = {A longstanding grand-challenge problem in AI is to build machines that are able to think and interact like humans do. A specific embodiment of this problem is a generalization of the cocktail party problem encountered in signal processing and blind signal separation: If an AI agent were to drop in at a crowded cocktail party then can it separate out and reconstruct the different underlying stories and narratives being discussed from a mixture of fragments of all the on-going conversations. Such a problem has taken on a renewed urgency: Narratives play a defining role in influencing critical decisions and worldviews of both the society at large and individuals, but the continual emergence of a multitude of conflicting narratives –enabled by large-scale adoption of social media– has created a global emergency, where the basic tenets of civil society and governance are being increasingly compromised. These narratives, some of which can be labeled as conspiracy theories, are composed of numerous characters connected by semantically diverse relationships situated in multiple and overlapping contexts. Injecting false facts happens in the context of such discussions, and solving such a misinformation problem is beyond a supervised classification task in natural language processing (NLP). In this dissertation, we develop a pipeline of interlocking computational and statistical modules - based on NLP tools and complex network theories– to extract meaningful narrative networks by distilling millions of social media posts. We develop a framework for semantic parsing of such narrative graphs (e.g. who are outsiders, their motivations and threats, and strategies of insiders) and evaluate the quality of these automatically derived communities in different ways. This framework takes a step towards enabling human-smart AI, where it can view the world as a human would.},
note = {AAI28971477}
}

@article{10.1007/s00138-024-01573-9,
author = {Rahman, Sheikh Shah Mohammad Motiur and Salomon, Michel and Demb\'{e}l\'{e}, Sounkalo},
title = {Towards scanning electron microscopy image denoising: a state-of-the-art overview, benchmark, taxonomies, and future direction},
year = {2024},
issue_date = {Jul 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {4},
issn = {0932-8092},
url = {https://doi.org/10.1007/s00138-024-01573-9},
doi = {10.1007/s00138-024-01573-9},
abstract = {Scanning electron microscope (SEM) enables imaging of micro-nano scale objects. It is an analytical tool widely used in the material, earth and life sciences. However, SEM images often suffer from high noise levels, influenced by factors such as dwell time, the time during which the electron beam remains per pixel during acquisition. Slower dwell times reduce noise but risk damaging the sample, while faster ones introduce uncertainty. To this end, the latest state-of-the-art denoising techniques must be explored. Experimentation is crucial to identify the most effective methods that balance noise reduction and sample preservation, ensuring high-quality SEM images with enhanced clarity and accuracy. A thorough analysis tracing the evolution of image denoising techniques was conducted, ranging from classical methods to deep learning approaches. A comprehensive taxonomy of this reverse problem solutions was established, detailing the developmental flow of these methods. Subsequently, the latest state-of-the-art techniques were identified and reviewed based on their reproducibility and the public availability of their source code. The selected techniques were then tested and investigated using scanning electron microscope images. After in-depth analysis and benchmarking, it is clear that the existing deep learning-based denoising techniques fall short in maintaining a balance between noise reduction and preserving crucial information for SEM images. Issues like information removal and over-smoothing have been identified. To address these constraints, there is a critical need for the development of SEM image denoising techniques that prioritize both noise reduction and information preservation. Additionally, one can see that the combination of several networks, such as the generative adversarial network and the convolutional neural network (CNN), known as BoostNet, or the vision transformer and the CNN, known as SCUNet, improves denoising performance. It is recommended to use blind techniques to denoise real noise while taking into account detail preservation and tackling excessive smoothing, particularly in the context of SEM. In the future the use of explainable AI will facilitate the debugging and the identification of these problems.},
journal = {Mach. Vision Appl.},
month = jul,
numpages = {20},
keywords = {Scanning electron microscopy (SEM), Image denoising, State-of-the-art (SOTA), Spatial domain denoising, Frequency domain denoising, Deep learning based denoising, Benchmarking}
}

@inproceedings{10.1007/978-3-031-14923-8_2,
author = {Smyth, Barry and Keane, Mark T.},
title = {A Few Good Counterfactuals: Generating Interpretable, Plausible and Diverse Counterfactual Explanations},
year = {2022},
isbn = {978-3-031-14922-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-14923-8_2},
doi = {10.1007/978-3-031-14923-8_2},
abstract = {Counterfactual explanations are an important solution to the Explainable AI (XAI) problem, but good, “native” counterfactuals can be hard to come by. Hence, the popular methods generate synthetic counterfactuals using “blind” perturbation, by manipulating feature values to elicit a class change. However, this strategy has other problems, notably a tendency to generate invalid data points that are out-of-distribution or that involve feature-values that do not naturally occur in a given domain. Instance-guided and case-based methods address these problems by grounding counterfactual generation in the dataset or case base, producing synthetic counterfactuals from naturally-occurring features, and guaranteeing the reuse of valid feature values. Several instance-guided methods have been proposed, but they too have their shortcomings. Some only approximate grounding in the dataset, or do not readily generalise to multi-class settings, or are limited in their ability to generate alternative counterfactuals. This paper extends recent case-based approaches by presenting a novel, general-purpose, case-based solution for counterfactual generation to address these shortcomings. We report a series of experiments to systematically explore parametric variations on common datasets, to establish the conditions for optimal performance, beyond the state-of-the-art in instance-guided methods for counterfactual XAI.},
booktitle = {Case-Based Reasoning Research and Development: 30th International Conference, ICCBR 2022, Nancy, France, September 12–15, 2022, Proceedings},
pages = {18–32},
numpages = {15},
location = {Nancy, France}
}

@inproceedings{10.1145/3632314.3632346,
author = {Guo, Fengfeng and Chen, Yupeng and Zhang, Yongjie and Feng, Qi},
title = {Evaluation Model of Disaster Rescue Effectiveness based on AHP},
year = {2023},
isbn = {9798400709401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632314.3632346},
doi = {10.1145/3632314.3632346},
abstract = {This study aims to address challenges faced by visually impaired individuals in daily travel, such as inability to perceive traffic lights, navigate obstacles, and locate tactile pavings. The goal is to develop a specialized travel assistance algorithm for visually impaired individuals. After thorough research on existing technologies including GPS, IrDA, AI-assisted technology, mobile applications, and object recognition, and analyzing their limitations, we proposed a blind navigation algorithm based on the PP-YOLO model. This algorithm combines traffic sign recognition, object recognition, and tactile paving recognition to achieve multi-task image recognition. Through dataset preparation, data annotation, and parameter adjustment, the algorithm was implemented on portable devices like mobile phones and wearable devices. Laboratory verification and field testing confirmed that the algorithm provides accurate navigation guidance and ensures safety for visually impaired individuals during travel. With a frame rate of 8fps, the algorithm achieved a recognition rate of 96.3% within an 8 m range on portable devices; in low-light environments, the object recognition accuracy within the same range reached 94%. Even at complex intersections with a width of 30 m, the recognition rate was 85%. This research outcome presents an innovative technical solution for travel assistance technology for visually impaired individuals and offers new research ideas for similar studies. The PP-YOLO model was chosen for its superior speed and accuracy in object detection tasks, crucial for real-time navigation. The model was streamlined for use on portable devices through lightweight design and hardware acceleration features such as NPU and GPU. The challenge with this approach involves some risk of recognition errors, similar to those in autonomous driving technology. However, considering the slower walking speed of visually impaired individuals, the consequences of these errors are mitigated. Thus, the project provides an additional mobility option for visually impaired individuals, providing more freedom than without it.},
booktitle = {Proceedings of the 2023 International Conference on Intelligent Sensing and Industrial Automation},
articleno = {27},
numpages = {5},
location = {Virtual Event, China},
series = {ISIA '23}
}

@inproceedings{10.1145/3514094.3539534,
author = {Pr\'{e}gent, Alexandra},
title = {Emotion Recognition Technology: Re-Shaping Human Relationships},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539534},
doi = {10.1145/3514094.3539534},
abstract = {Over the recent years, emotions have taken more and more place in relatively different spheres of society, including politics, sciences, and philosophy. Contradictory to what was preached in the 20th century, we now accept emotions as important part of ourselves that contributes to our reasoning and decision-making process. Furthermore, we recognized them as necessary components in the construction of relationships. This shift in the paradigm towards emotions has increased their value in the public eye. In a paradoxical way, we now tend to communicate more than ever this subjective experience and thus, as we communicate it, we insist on its personal and private status. Our emotions are ours and ours alone. But is it true? By being both recognized as a part of the inner life that is never completely reachable and as a necessary component of relationships, emotions appear to be at the edge of every concept of privacy.This research project will work on this specific blind spot in the literature through the analysis of emotion recognition software (ERS) as a threat to both the privacy of the inner life and the privacy of relationships. The main claims are that emotions must be both recognized as (1) being more than a part of the inner self, e.g., for instance, as actively contributing to communication and the construction of relationships, and (2) considered off-limits from scrutinization and monitoring by governments and corporate in contexts in which the use of ERS disrupt and restrict the possibility to freely experience them as inner affective states and/or to freely use them as a communication tool. The hypothesis is as follows. The first premise assume that the phenomenon of emotion is commonly accepted as being a fundamental component of human experience. From that first assumption is derive the second one; that it is, therefore, a necessary element to live a meaningful life.From that standpoint, I draw upon Helen Nissenbaum's framework of contextual integrity [1] and hypothesize that the context in which the individual shares his or her emotions daily should be recognized as a non-violable whole (set of natural, social, cultural conditions) whose integrity allows for the exercise of this fundamental component of human experience, which is a necessity for living a meaningful life. While emotions are seen as "in the public space" because we constantly use them to communicate, I want to argue that emotions "in the public space" should be considered a priori off-limit from monitoring and scrutinization by State and corporate until proven otherwise by the contextual integrity framework.The general stance is that a constant monitoring and scrutinization by ERS will have a disruptive effect on the way we experience and express emotions, which will have a direct impact on our way of building and maintaining relationships, putting at higher risk the possibility of achieving a meaningful life. Therefore, I want to draw what could be seen as a draconian line between what citizens on one side and what States and corporate on the other should be allowed to do and not do. Emotions should not be considered something that individuals should refrain from experiencing, communicating, and accessing, however, they should be considered highly sensitive and intimate information and, thus, States and corporate should refrain from monitoring and scrutinizing them. Furthermore, I argue that a massive deployment of ERS by government and private companies will transform the cultural and societal practices as well as affecting our fundamental rights and values.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {909},
numpages = {1},
keywords = {emotion recognition software, emotions, privacy, public and private space, relationships},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@proceedings{10.1145/3558489,
title = {PROMISE 2022: Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 18th ACM International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2022), to be held in hybrid mode (physically and virtually) on November 18th, 2022, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). PROMISE is an annual forum for researchers and practitioners to present, discuss and exchange ideas, results, expertise and experiences in the construction and/or application of predictive models and data analytics in software engineering. Such models and analyses could be targeted at planning, design, implementation, testing, maintenance, quality assurance, evaluation, process improvement, management, decision making, and risk assessment in software and systems development. This year PROMISE received a total of 18 paper submissions. The review process was double blind and each paper was reviewed by at least three members of the program committee. An online discussion was also held for 8 days. Based on this procedure, we accepted a total of 10 full papers, which will be presented in 3 technical sessions. The acceptance criteria were entirely based on the quality of the papers, without imposing any constraint on the number of papers to be accepted.    We are delighted to announce an outstanding keynote: Release Engineering in the AI World: How can Analytics Help? By Prof. Bram Adams, Queen’s University, Canada    We would like to thank all authors for submitting high quality papers, and program committee members for their timely and accurate reviewing activity. Last, but not least, we would like to thank the FSE 2022 organizers for hosting PROMISE 2022 as a co-located event and for their logistic support in the organization of the conference.   We hope you will enjoy PROMISE 2022.   We certainly will!    Many thanks from   Shane McIntosh (General Chair),   Gema Rodriguez-Perez and Weiyi Shang (Program Chairs).},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3552484,
title = {MADiMa '22: Proceedings of the 7th International Workshop on Multimedia Assisted Dietary Management},
year = {2022},
isbn = {9781450395021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 7th International Workshop on Multimedia Assisted Dietary Management -- MADiMa 2022. After the success of the past MADiMa workshops, we would like to present to you the MADiMa 2022 to be held with the 30th ACM International Conference in Multimedia 2022 in Lisbon, Portugal. For the second time, MADiMa and the International Workshop on Multimedia for Cooking, Eating, and related APPlications (CEA) are organized at the same place. The workshop provides a platform, in which researchers, students, and industry players can meet in order to explore and discuss state of the art in research and technology, to investigate the challenges faced during the design and development of multimedia assisted dietary assessment and management systems, as well as to exchange ideas in future research trends.The call for papers attracted submissions from Asia, Europe, and the United States. The workshop had in total 11 submissions. A double-blind review process yielded to 10 papers that were accepted in this year's program. The workshop is complemented by three invited speakers: Dr. Arindam Ghosh, from Oviva AG, will present the mediPiatto project, in which an AI-based end-to-end automatic system was developed to estimate the Mediterranean Diet Adherence of users, Oliver Amft, from University of Freiburg, Germany, will discuss about the use of sensors and wearable devices in automated dietary monitoring and technology-based dietary intervention, and George Hadjigeorgiou, co-founder of ZOE, will present his mission to improve the health of millions by moving the world from calorie counting and foods that poison our health to the truth of personalized advice based on how food affects our bodies and health. We believe that using OpenReview together with Microsoft CMT (or a similar tool) will raise the scientific standards and extend the scientific impact of future ACM Multimedia editions.},
location = {Lisboa, Portugal}
}

@article{10.1007/s10579-023-09682-z,
author = {Babakov, Nikolay and Logacheva, Varvara and Panchenko, Alexander},
title = {Beyond plain toxic: building datasets for detection of flammable topics and inappropriate statements},
year = {2023},
issue_date = {Jun 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {58},
number = {2},
issn = {1574-020X},
url = {https://doi.org/10.1007/s10579-023-09682-z},
doi = {10.1007/s10579-023-09682-z},
abstract = {Toxicity on the Internet is an acknowledged problem. It includes a wide range of actions from the use of obscene words to offenses and hate speech toward particular users or groups of people. However, there also exist other types of inappropriate messages which are usually not viewed as toxic as they do not contain swear words or explicit offenses. Such messages can contain covert toxicity or generalizations, incite harmful actions (crime, suicide, drug use), and provoke “heated” discussions. These messages are often related to particular sensitive topics, e.g. politics, sexual minorities, or social injustice. Such topics tend to yield toxic emotional reactions more often than other topics, e.g. cars or computing. At the same time, not all messages within “flammable” topics are inappropriate. This work focuses on automatically detecting inappropriate language in natural texts. This is crucial for monitoring user-generated content and developing dialogue systems and AI assistants. While many works focus on toxicity detection, we highlight the fact that texts can be harmful without being toxic or containing obscene language. Blind censorship based on keywords is a common approach to address these issues, but it limits a system’s functionality. This work proposes a safe and effective solution to serve broad user needs and develop necessary resources and tools. Thus, machinery for inappropriateness detection could be useful (i) for making communication on the Internet safer, more productive, and inclusive by flagging truly inappropriate content while not banning messages blindly by topic; (ii) for detection of inappropriate messages generated by automatic systems, e.g. neural chatbots, due to biases in training data; (iii) for debiasing training data for language models (e.g. BERT and GPT-2). Towards this end, in this work, we present two text collections labeled according to a binary notion of inappropriateness (124,597 samples) and a multinomial notion of sensitive topic (33,904 samples). Assuming that the notion of inappropriateness is common among people of the same culture, we base our approach on a human intuitive understanding of what is not acceptable and harmful. To devise an objective view of inappropriateness, we define it in a data-driven way through crowdsourcing. Namely, we run a large-scale annotation study asking workers if a given chatbot-generated utterance could harm the reputation of the company that created this chatbot. High values of inter-annotator agreement suggest that the notion of inappropriateness exists and can be uniformly understood by different people. To define the notion of a sensitive topic in an objective way we use guidelines suggested by specialists in the Legal and PR departments of a large company. We use the collected datasets to train inappropriateness and sensitive topic classifiers employing both classic and Transformer-based models.},
journal = {Lang. Resour. Eval.},
month = oct,
pages = {459–504},
numpages = {46},
keywords = {Sensitive topics, Toxicity detection, Data collection, Crowdsourcing, Text categorization}
}

@phdthesis{10.5555/AAI30241565,
author = {Syed, Fahad Iqbal and Russell, Ostermann, and Chengwu, Yuan, and Prasad, Kulkarni,},
advisor = {Kalantari, Dahaghi, Amirmasoud and Shahin, Negahban,},
title = {Smart Physics-Inspired Compositional Dimensionless Type Curves for Unconventional Enhanced Oil Recovery},
year = {2022},
isbn = {9798368449227},
publisher = {University of Kansas},
address = {USA},
abstract = {Dimensionless Type curves have been developed and used in the Oil and Gas industry for primary production performance evaluation. To Date, there is no physics-based dimensionless performance type curve developed for Enhanced Oil Recovery (EOR) of even conventional hydrocarbon-producing reservoirs. Predicting the production performance of unconventional and tight hydrocarbon reservoirs is challenging. Each unconventional well drilling and completion normally cost a company $ 6-12 Million. Unconventional EOR (UEOR) is the next step in unlocking untapped unconventional and tight hydrocarbon reservoirs' full potential and helps in minimizing environmental footprints by targeting the remaining hydrocarbon left behind and consequently avoiding unnecessary drilling and minimizing carbon emission.To conduct a successful UEOR project, oil and gas companies perform comprehensive simulation studies to screen and select candidate wells (pilot) for UEOR, predict their response to the UEOR methods and agents, and forecast the performance of wells' ongoing UEOR. This requires running thousands of simulation cases that might take several months to complete comprehensive techno-economic assessment and evaluation. AI-empowered Dimensionless Type Curves that honor physical laws can offer fast-track screening and accurate solutions.In this dissertation, Smart Physics-Inspired Compositional Dimensionless Type Curves (SPiC TCD) for UEOR are presented that aim to address the above-mentioned problem and save millions of dollars by optimizing the UEOR practice and consequently reducing the carbon emission and environmental footprints and using subsurface resources in an environmentally beneficial way, which is the current portfolio of the oil and gas industry.SPiC TCD respond to operators' W3H questions (Where to inject, When to inject, What to inject, and How to inject an EOR solvent) while performing comprehensive field screening and designing unconventional EOR pilot(s). W3H methodology provides fast-track AI-aided physics-inspired solutions based on historical wells' performance with existing subsurface reservoir and fluid descriptions and hydraulic fracture geometries and flow properties. This technique enables operators to make quick decisions on unconventional EOR pilot candidates' selection and design to optimize design criteria such as the choice of injection solvent type and volume estimation, the optimum start of injection and soaking time as well as the frequency of this cyclic process and estimation and the soaking duration for the optimum oil recovery.To generate Smart Physics-Inspired Compositional Dimensionless Type Curves, a manageable number of numerical simulation cases are defined through the Physics-Guided Design of Experiment workflow. The workflow covers a wide range of operational design parameters pertaining to W3H criteria, reservoir rock and fluid properties, hydraulic fracture design, and their corresponding flow-related parameters such as fracture conductivity, fracture half-length, fracture height, fracture spacing, and the number of hydraulic fracture clusters per stage.Conventional design of experiment workflows fails in case of dealing with a system that operates based on known governing physical laws. This affects the accuracy of the proxy models and probabilistic modeling. Therefore, a detailed workflow is developed which is a physics quality control module to evaluate the response of the generated cases using the design of experiment techniques. It ensures the generated multidimensional distribution of the input parameters creates physically meaningful responses when solving the fluid flow equations. The next step is to train a family of machine-learning algorithms. Deep neural network algorithms are employed to build the proxy models for Smart UEOR dimensionless type curve generation. Upon completion of the training, the physics-based blind hindcasting and model response evaluation according to the physical laws are conducted. The generated physics-based AI proxy models are capable of generating thousands of cases based on the different reservoir and fluid descriptions as well as hydraulic fracture properties and W3H operational design criteria within an hour instead of months. It enables fast and accurate decision-making for optimal UEOR practice in unconventional and tight oil reservoirs.},
note = {AAI30241565}
}

@proceedings{10.1145/3698364,
title = {ISPD '25: Proceedings of the 2025 International Symposium on Physical Design},
year = {2025},
isbn = {9798400712937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the organizing committee, we are delighted to welcome you to the 34th ACM International Symposium on Physical Design (ISPD), held in Austin, Texas, providing a premier forum to exchange ideas, highlight key technology challenges, present leading-edge theoretical and experimental contributions, and identify future research directions in this field. We extend the good practice of having a YouTube channel to view the talks during the symposium and afterward, improving access to the presentations.Across three days, ISPD 2025 has 3 keynotes, 18 accepted papers, 13 invited talks; one panel on Monday with 4 panelists; and 4 speakers with longer talks in Professor Jason Cong''s commemorative session, and finally the ISPD 2025 contest results.This year, we received a total of 64 abstracts and we received 48 full manuscripts, from which 18 were selected - a 37.5% acceptance rate. The regular papers in the ISPD 2025 program were selected, after a rigorous month-long double-blind review process and virtual meetings, by the Technical Program Committee with 20 outstanding international professionals from both academia and industry. These papers exhibit the latest advancements in a variety of topics in physical design, including global placement; mixed cell-height legalization; PCB placement; inverse lithography; quantum layout; photonic integrated circuits and side channel analysis; timing propagation; logic optimization; crosstalk mitigation; DRC checking; gate sizing; standard cell layout; CFET cell design; timing rule generation; analog cell design; 3D net-to-pad assignment, and 3D power delivery. A number of these papers utilize advanced mathematical programming, satisfiability modulo theories, GPU acceleration, and machine learning techniques.We are delighted to host three distinguished keynote speakers. The Monday keynote "Towards Designing and Deploying Ising Machines" will be given by Professor Sachin Sapatnekar of the University of Minnesota. He will explore opportunities for solving complex combinatorial optimization problems using Ising machines. The discussion will emphasize the layout and timing challenges specific to Ising machines that are based on coupled CMOS ring oscillators. On Tuesday morning, Dr. Charles J Alpert of Cadence Design Systems will present "How Automotive Functional Safety is Disrupting Digital Implementation". He will provide an overview of the challenges in achieving functional-safe ICs for automotive applications and discuss their impact on the digital implementation flow, including synthesis, placement, and routing. Wednesday's keynote is given by Dr. Henry Sheng from Synopsys on "Automation and Optimization of Heterogeneous Systems". He will explore the challenges and strategies for automating heterogeneous integration, an area that has traditionally relied on disconnected workflows and significant manual effort.The ISPD 2025 program is complemented by invited talks on topics ranging across design technology co-optimization; systolic array-based ICs; ECO automation, ML &amp; AI methods in design automation; 3D IC design and integration; power design and analysis; and an analysis of the historical impact of ISPD. Additionally, we have a panel discussing EDA challenges in heterogeneous integration.In the ISPD Lifetime Achievement Award session on Tuesday, we will pay tribute to Professor Jason Cong. This session highlights Professor Cong's pioneering contributions in the algorithmic aspects of the physical design of silicon chips. This session has talks from his mentor, labmate and former students. Dr Bryan Preas will present the Innovation in Times of Technology Disruption. Followed by the talk from Professor Martin Wong from Hong Kong Baptist University. Professor David Pan, from University of Texas at Austin, will discuss the future of interconnected Physical Design. Finally, Professor Jason Cong will also grace the symposium with a delightful talk on coping with interconnects and a recount of his numerous contributions in EDA and Physical Design..Since 2005, ISPD has organized highly competitive contests to promote and advance research in placement, global routing, clock network synthesis, gate sizing, detailed routing-driven placement, and hardware security. The contest this year features the second contest on GPU-accelerated and machine learning-enhanced large scale global routing. This time the contest integrates timing and power metric. The contest is again organized by NVIDIA. The contest aims to not only deliver substantial reductions in global routing runtime but take performance and power into account as required in real-world applications. This year's contest attracted 46 teams from all over the world, reaching a record high.This year, we continue with a short format for the invited talks and regular papers. Presentations are 12 minutes in length, plus about 5 minutes for questions and answers after each talk, and some time at the end of each session for open discussion. This format is intended to encourage discussion among our research community. The talks in the lifetime achievement session are 20 minutes in length, with 5 minutes for questions and answers. Keynotes remain about 40 minutes in length, with 10 minutes for discussion.},
location = {Austin, TX, USA}
}

@proceedings{10.1145/3505170,
title = {ISPD '22: Proceedings of the 2022 International Symposium on Physical Design},
year = {2022},
isbn = {9781450392105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 31st edition of the ACM International Symposium on Physical Design (ISPD). We continue the great tradition established by its thirty predecessors despite the COVID-19 pandemic. ISPD is a premier venue for the dissemination of manuscripts of the highest quality related to the physical design of integrated circuits. ISPD 2022 provides the forum to present leading-edge research results, exchange ideas, and promote research on critical areas related to the physical design of VLSI and other systems. Across the three days of ISPD 2022, we have 6 keynotes; 12 accepted papers; 12 invited talks; two panels on Monday and Wednesday - each with 6 panellists; 3 speakers including Ricardo Reis with longer talks for his commemorative session, and finally the ISPD 2022 contest results.The regular papers in the ISPD 2022 program were selected after a rigorous, month-long, double-blind review process and virtual meetings, by the Technical Program Committee (TPC) members. These papers exhibit the latest advancements in a variety of topics in physical design, including design flow parameter optimization; floorplanning and macro placement; global placement for both conventional 2D as well as 3D ICs; mixed-cell-height placement legalization; well tap placement for analog and mixedsignal designs; a clock tree design methodology for a Bitcoin mining ASIC; routing for technology nodes below 7nm and for 3D ICs; and kernel mapping for deep learning on the Cerebras wafer-scale engine, the largest chip ever built. A number of these papers utilize advanced machine learning and reinforcement learning techniques.The ISPD 2022 program is complemented by invited talks on topics ranging across analog design automation; 3D IC design; packaging; and advanced algorithmic techniques for performance optimization and power minimization such as machine learning and Lagrangian relaxation. Additionally, we have a panel discussing challenges in VLSI routing, and another panel discussing traditional EDA algorithmic approaches and heuristics compared to more recent machine learning techniques.The conference will feature six keynote addresses. The first keynote on Monday is presented by Dean Drako, CEO of Eagle Eye Networks, IC Manage, and Drako Motors. Mr. Drako will deliver the Monday morning keynote address entitled "The Need for Speed: From Electric Supercars to Cloud Bursting for Design," which will compare our industry's drive for speed to electric supercars, then delve into key elements that design and verification teams use to speed the delivery of their products to market. Jean- Philippe Fricker, Chief System Architect at Cerebras Systems, will deliver the Monday afternoon keynote address entitled "The Cerebras CS-2: Designing an AI Accelerator Around the World's Largest 2.6 Trillion Transistor Chip." This keynote will explore how neural networks' rapidly increasing demands for performance and memory benefit from a new architecture with increased performance, discussing the design principles and trade-offs considered in the Cerebras chip architecture.},
location = {Virtual Event, Canada}
}

