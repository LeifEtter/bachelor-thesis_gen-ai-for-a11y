@INPROCEEDINGS{11134965,
  author={Ramos, David Livingstone T. and Ventayen, Randy Joy M.},
  booktitle={2025 6th International Conference on Data Intelligence and Cognitive Informatics (ICDICI)}, 
  title={Monitoring the Perceived Quality of Artificial Intelligence Composed Music Through Blind Listener Evaluation}, 
  year={2025},
  volume={},
  number={},
  pages={1489-1493},
  abstract={Artificial Intelligence (AI) has emerged as a transformative tool in music composition, facilitating the efficient creation of diverse and sophisticated musical pieces. This study investigates audience perceptions of AI-generated music through a blind listening test involving 50 participants who evaluated a musical composition without knowing its origin. Participants assessed the piece based on three criteria: quality, creativity, and emotional resonance. The quantitative results reflected generally favorable responses, with mean scores of 4.8, 4.5, and 4.3, respectively, on a 5-point Likert scale. Qualitative feedback further emphasized the music's technical precision and structural coherence. Remarkably, upon being informed of the composition's AI origin, none of the participants had previously suspected it was machine-generated. While 68% reported that their perception remained unchanged, a minority still expressed a preference for human-composed music. Despite this, 98% of participants expressed heightened appreciation for AI's capabilities in music creation, with only 2% reporting a decrease in their impression. Overall, the findings underscore the potential of AI to produce high-quality, emotionally engaging musical works and its growing acceptance among listeners.},
  keywords={Music;Production;Coherence;Resonance;Complexity theory;Cognitive informatics;Artificial intelligence;Creativity;Monitoring;music production;artificial intelligence;music},
  doi={10.1109/ICDICI66477.2025.11134965},
  ISSN={},
  month={July},}@INPROCEEDINGS{10293380,
  author={Oureshi, Muhammad Shuaib and Khan, Inam Ullah and Qureshi, SMuhammad Bilal and Khan, Fida Muhammad and Aleshaiker, Sama},
  booktitle={2023 IEEE International Smart Cities Conference (ISC2)}, 
  title={Empowering the Blind: AI-Assisted Solutions for Visually Impaired People}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Physical disability has an influence on individuals everywhere. One of these disabilities that significantly affect a large number of individuals is visual loss. Basic mobility problems, including as crossing the street, reading, driving, and socializing, are commonly a problem for blind people. To go around or complete other daily duties, they typically need on particular assistance devices, including walking sticks. There is still more work to be done in the field of blindness treatment, despite ongoing scientific research. In addition, research has suggested solutions to the problems faced by blind individuals, but these solutions need the necessary technology support. The goal of this research project is to employ smart devices to make daily routines simpler for blind persons of all categories. This smart gadget can recognize faces, different objects, and colors by employing artificial intelligence and picture processing. The notification of the visually impaired individual results from the detecting procedure and might take the form of a vibration or sound alarm. This study also includes a tangible survey that includes local residents who are blind or visually handicapped. In order to program and implement the project, Open CV and Python are both used. This project prototype's effort looks into the methods that are employed for object detection. It also shows how this smart gadget can recognize certain physical objects and how it may alert the user when faced with hurdles. By helping blind individuals with the use of smart technology, this research will, overall, be a helpful contribution to the field of healthcare.},
  keywords={Vibrations;Surveys;Visualization;Smart cities;Face recognition;Blindness;Object detection;Artificial Intelligence;Smart Education;Face Recognition;Smart Health;Open CV},
  doi={10.1109/ISC257844.2023.10293380},
  ISSN={2687-8860},
  month={Sep.},}@INPROCEEDINGS{9893718,
  author={Van Nguyen, Thai and Dai, Xincheng and Khan, Ibrahim and Thawonmas, Ruck and Pham, Hai V.},
  booktitle={2022 IEEE Conference on Games (CoG)}, 
  title={A Deep Reinforcement Learning Blind AI in DareFightingICE}, 
  year={2022},
  volume={},
  number={},
  pages={632-637},
  abstract={This paper presents a deep reinforcement learning agent (AI) that uses sound as the input on the DareFightingICE platform at the DareFightingICE Competition in IEEE CoG 2022. In this work, an AI that only uses sound as the input is called blind AI. While state-of-the-art AIs rely mostly on visual or structured observations provided by their environments, learning to play games from only sound is still new and thus challenging. We propose different approaches to process audio data and use the Proximal Policy optimization algorithm for our blind AI. We also propose to use our blind AI in evaluation of sound designs submitted to the competition and define two metrics for this task. The experimental results show the effectiveness of not only our blind AI but also the proposed two metrics.},
  keywords={Measurement;Visualization;Games;Reinforcement learning;Artificial intelligence;Task analysis;Optimization;Sound;Blind AI;Deep Reinforcement Learning;Proximal Policy optimization;Fighting Game;FightingICE;DareFightingICE},
  doi={10.1109/CoG51982.2022.9893718},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{10930208,
  author={Chi, Chun-Nan and Hsu, Jia-Lien and Hung, Chi-Huang and Hsu, Chia-Hsun and Tan Phuong, To Nguyen and Juan, Wei-Chih},
  booktitle={2025 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={InstaSight: Leveraging AI to See Beyond Sight for Blind Users on Social Media}, 
  year={2025},
  volume={},
  number={},
  pages={1-2},
  abstract={This study introduces “InstaSight,” a system enabling visually impaired individuals to engage with social media focused on visual content. InstaSight integrates a Single Board Computer, head-mounted webcam, and Bluetooth headset for seamless voice interactions. By utilizing the APIs of social media, speech recognition, and Large Language Models (LLMs), users can post photos and videos with voice-to-text captions and listen to comments converted from text or images. This system fosters social engagement, helping blind users connect and share experiences, improving their psychological well-being.},
  keywords={Headphones;Visualization;Electric potential;Social networking (online);Webcams;Large language models;Psychology;Speech recognition;Consumer electronics;Videos;accessible technology;visually impaired users;social media participation;speech recognition;large language models},
  doi={10.1109/ICCE63647.2025.10930208},
  ISSN={2158-4001},
  month={Jan},}@INPROCEEDINGS{10866520,
  author={Patil, AaditYa and Bendale, Yash and Bhangare, Prathamesh and Patil, Siddhasen},
  booktitle={2024 4th International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS)}, 
  title={OdinEye: An AI Based Visual Assistive Device for the Blind and Partially Sighted}, 
  year={2024},
  volume={},
  number={},
  pages={158-163},
  abstract={Vision impairment is a significant global challenge, with over 2.2 billion people affected by near or distance vision impairment, and 36 million experiencing complete blindness. Many of these cases remain unaddressed, despite technological advancements. Existing solutions often fall short of providing comprehensive assistance due to limitations in adaptability and precision. To address this gap, this work proposes OdinEye, an AI-based visual assistive system designed to support individuals with blindness or severe visual impairment. By leveraging cutting-edge generative AI, OdinEye aims to offer a more versatile and personalized solution. The system integrates advanced components, including the Gemini API to enhance real-time object detection, navigation assistance, and environment interpretation, empowering users with greater independence and safety.},
  keywords={Performance evaluation;Visualization;Text recognition;Visual impairment;Blindness;Cameras;Ubiquitous computing;Real-time systems;Text to speech;Sensors;Visual Impairment;Assistive Technology;Artificial Intelligence (AI);Generative AI;OdinEye;Object Detection;Navigation Assistance;Real-Time Assistance;Blindness Solutions;Computer Vision;Accessibility;Visual Assistive Devices;AI for Accessibility},
  doi={10.1109/ICUIS64676.2024.10866520},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10701255,
  author={Christian, Michael and Pardede, Ratlan and Gularso, Kurnadi and Wibowo, Suryo and Muzammil, Oktafalia Marisa and Sunarno},
  booktitle={2024 3rd International Conference on Creative Communication and Innovative Technology (ICCIT)}, 
  title={Evaluating AI-Induced Anxiety and Technical Blindness in Healthcare Professionals}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The widespread implementation of artificial intelligence (AI) in various industries, including healthcare, provides significant benefits but raises concerns. Notably, apprehensions about AI generating issues, experiencing malfunctions, and hindering work extend to fears of replacing medical professional positions. This research aims to quantitatively assess the impact of anxiety forms, specifically social, technical blindness and AI configuration, on job replacement anxiety among medical doctors. A pre-test approach was employed to gather data from 38 doctors in Jakarta who regularly interact with AI in their daily work. The study used PLS-SEM structural modelling with SmartPLS 3.0 as the analysis tool. Findings indicate that both social and technical blindness and AI configuration significantly influence job replacement anxiety. Additionally, the study reveals that elements within the AI configuration interact with social-technical blindness, and the AI setup effectively mitigates its impact on job replacement anxiety. These anxieties notably affect medical professionals’ fears of being replaced in their current roles. Previous research shows that anxiety about AI among healthcare personnel is persistent. However, this worry can be reduced or eliminated through continuous awareness, education, and understanding of AI’s purpose and limitations. This study aims to provide medical practitioners with a comprehensive understanding of AI constraints and the aspects requiring direct doctor involvement.},
  keywords={Training;Industries;Ethics;Anxiety disorders;Psychology;Medical services;Blindness;Personnel;Artificial intelligence;Guidelines;Artificial Intelligence (AI);Social Technical Blindness;AI Configuration;Job Replacement Anxiety;Medical Doctors},
  doi={10.1109/ICCIT62134.2024.10701255},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10698153,
  author={Jawad, Rawaa and Jawad, Raheel},
  booktitle={2024 International Conference on Electrical, Computer and Energy Technologies (ICECET}, 
  title={Design and Implementation of a Stick for Blind Humans Using Artificial Intelligence System and FPGA}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Blind people need a means of assistance to help them carry out their daily activities without relying on the people close to them, so the researchers thought of using intelligence methods to make a smart stick that helps them bypass the obstacles they face while walking, the research examined the use of an image as a blind environment in a fuzzy logic system to find a path and avoid obstacles for blind stick. This demonstrates the effectiveness of the fuzzy logic algorithm for the smart stick and ensures the path by performing path behavior and obstacle avoidance to identify the best path for smart stick can access the target and avoidance obstacles. In this paper, one of the methods of artificial intelligence is presented, which is the fuzzy logic of making a smart stick, where an algorithm has been built in the Matlab program to avoid and bypass obstacles and cross safely and then upload a code in FPGA then connect with power supply, flat Vibrator motor, ultrasonic sensor, buzzer that alert when detect an obstacles. The result use of fuzzy logic gives the best results for overcoming obstacles because fuzzy logic depends on the principle of probabilities and the use of the FPGA chip is the best chip from the Arduino and the Raspberry because the FPGA works in parallel with several inputs and sensors. Thus, a smart stick has been designed with artificial intelligence techniques and a FPGA chip with great success.},
  keywords={Fuzzy logic;Legged locomotion;Codes;Power supplies;Motors;Artificial intelligence;Collision avoidance;Field programmable gate arrays;Faces;Matlab;Stick Blind;Artificial Intelligence;Fuzzy Logic;FPGA},
  doi={10.1109/ICECET61485.2024.10698153},
  ISSN={},
  month={July},}@ARTICLE{11036720,
  author={Acosta-Vargas, Patricia and Salvador-Acosta, Belén and Loachamín-Valencia, Mauricio and Chacón-Castro, Marcos and Jadán-Guerrero, Janio and Salvador-Ullauri, Luis},
  journal={IEEE Access}, 
  title={Evaluating Web Accessibility in Countries Included in the Latin American Artificial Intelligence Index}, 
  year={2025},
  volume={13},
  number={},
  pages={104872-104887},
  abstract={This study examines the relationship between the Latin American Artificial Intelligence Index (ILIA) and the Web Accessibility Index (WAIN) in 19 Latin American countries to evaluate how technological innovation aligns with digital inclusivity. Results reveal notable disparities: Uruguay, Chile, and Brazil excel in both indices, whereas Jamaica and Venezuela face critical challenges. Significantly, high scores in artificial intelligence adoption do not always correlate with improved web accessibility, underscoring the need for integrated strategies that prioritize compliance with the Web Content Accessibility Guidelines (WCAG) 2.2 and inclusive design. The research highlights accessibility as a fundamental pillar for sustainable digital ecosystems and equitable access to technology. Limitations include the regional focus on Latin America, the reliance on automated tools, and the dynamic nature of technology. Future research should incorporate longitudinal analyses, user-centered evaluations, and comparative studies across diverse regions to develop comprehensive frameworks that are grounded in empirical evidence. By aligning technological progress with inclusivity, this study emphasizes the importance of fostering sustainable and accessible digital environments. Recommendations include adopting accessibility standards in early development stages, enhancing public policy, and prioritizing universal design. These steps are essential to achieving the Sustainable Development Goals (SDGs) related to reducing inequalities and promoting inclusive institutions. This work is a foundation for advancing equitable digital ecosystems in Latin America.},
  keywords={Artificial intelligence;Indexes;Ecosystems;Digital divide;Standards;Government;Technological innovation;Public policy;Portals;Open data;Artificial intelligence;digital inclusion;Latin America;sustainable development goals;web accessibility;WCAG 2.2},
  doi={10.1109/ACCESS.2025.3579751},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10368901,
  author={Eatesh, Nalluru Mourya Sai and Vemuru, Venu Dattathreya and Muga, Rithvik Raj and Dhanush, Nellore and C, Indumathi and K, Nandeeswari},
  booktitle={2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={AI-Powered Visual Aid System for the Blind}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={One of the most prevalent and disabling of the numerous disabilities is blindness. Based on the World Health Organization (WHO), there are 285 million blind people worldwide. Among them, 39% of the population is totally blind. Individuals encounter a myriad of challenges regularly, especially when they undertake independent journeys from one place to another. Oftentimes, people heavily depend on the support and aid of others to fulfill their daily requirements. Therefore, implementing a technological solution to assist them poses a considerable challenge. So, an effective method like a machine learning algorithm is suggested as a remedy for such individuals. The essential data input is gathered using a method called image classification in order to access machine learning algorithms. The items around where blind persons are located include-with a camera, the surroundings of blind persons are photographed as images. It is capable of accurately detecting every object within a certain range. The visuals are then transformed into aural signals to give the blind persons practical means of assistance. This also generates alerts if they are far or very close to the object.},
  keywords={Visualization;Machine learning algorithms;Sociology;Blindness;Organizations;Machine learning;Knowledge management;Intelligent machine learning system;User-friendly gadget;Input of data;Image classification;Blind individuals;Collected photos},
  doi={10.1109/RMKMATE59243.2023.10368901},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9994958,
  author={Yang, Lei and Xie, Yao and Islam, Md Rafiqul and Xu, Guandong},
  booktitle={2022 9th International Conference on Behavioural and Social Computing (BESC)}, 
  title={Big Data and Artificial Intelligence (AI) to Detect Glaucoma}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Glaucoma is an eye condition that is one of the most prevalent causes of blindness due to damage to the optic nerve. According to existing studies, it is the second most common reason for vision loss worldwide. With the advanced development of artificial intelligence (AI), it is necessary to develop a glaucoma detection system for diagnosis. Therefore, the primary objective of this paper is to create a system for detecting glaucoma using retinal fundus images, which can help determine if the patient was affected by glaucoma. Although several methods have been applied to detect glaucoma in the past decades, it is essential to use an advanced AI technique with a glaucoma detection system. Thus, in this paper, we divided our task into threefold: 1) segmentation, 2) classification, and 3) deployment. The U-Net architecture is implemented for segmentation. The pretrained GC-Net model is proposed for classification. Finally, based on the segmentation and classification, we developed a glaucoma detection system for diagnosis. This study uses ACRIMA datasets for training and testing. The result of this model is evaluated using various parameters such as accuracy, sensitivity, specificity, f1-score, and auc score. The output is compared to deep learning models such as ResNet, CNN, Inception V3, and TB-Net. The proposed model achieved 96% accuracy in training and 93% accuracy in testing. Overall, the performance of the proposed model is better in all the analyses.},
  keywords={Training;Deep learning;Social computing;Sensitivity;Computational modeling;Retina;Optical sensors;Glaucoma Detection;Deep Learning;Segmentation;Classification;Deployment},
  doi={10.1109/BESC57393.2022.9994958},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10418344,
  author={Kang, Honggu and Ko, Seonghyeon and Kim, Juchan and Le, Duc-Tai and Bum, Junghyun and Han, Jongchul and Choo, Hyunseung},
  booktitle={2024 18th International Conference on Ubiquitous Information Management and Communication (IMCOM)}, 
  title={Visual Field Prediction for Fundus Image with Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-3},
  abstract={Accurate diagnosis of glaucoma is crucial due to its high risk of blindness, but the long examination time often undermines the reliability of results by examinee's subjective factors. We propose a method to reduce examination time by generating static perimetry results with Conventional Fundus Images (CFIs) utilizing the CFI2GM technique, which leverages multimodal data. Based on data from 3,306 glaucoma patients at Samsung Medical Center in Seoul, we conducted ophthalmic image translation utilizing the Pix2Pix model. Our method, combining cGAN, L1, and SSIM loss, achieved MSE 57.9886 and PSNR 30.6057 dB. Furthermore, we received positive feedback from ophthalmologist regarding the high practical applicability of the images generated by our method. This indicates that CFI2GM can enhance the reliability of glaucoma examination results as well as reduce testing time.},
  keywords={Glaucoma;Performance evaluation;Visualization;Information management;Reliability;Medical diagnostic imaging;Testing;fundus image;glaucoma;visual field;generative AI},
  doi={10.1109/IMCOM60618.2024.10418344},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11081555,
  author={Acosta-Vargas, Patricia and Acosta-Vargas, Gloria and Salvador-Acosta, Belén and Jadán-Guerrero, Janio},
  booktitle={2025 Eleventh International Conference on eDemocracy & eGovernment (ICEDEG)}, 
  title={Web Accessibility in the Portals of the Countries in the Latin American Index of Artificial Intelligence}, 
  year={2025},
  volume={},
  number={},
  pages={73-80},
  abstract={This study assesses the web accessibility of portals from the Latin American Artificial Intelligence Index (ILIA) countries, emphasizing the digital inclusion of users with disabilities. Using the Web Content Accessibility Guidelines (WCAG 2.2) as a reference point, the study focuses on the four principles of accessibility: perceptibility, operability, comprehensibility, and robustness. The results show that 43.20 % of the sites met the (minimum) contrast requirements, and 26.48 % met the navigability W3C recommendations. Chile (ranked first, score 73.07) presented 15 contrast issues and six errors overall, demonstrating a firm adherence to accessibility. Brazil (ranked second, with a score of 69.30) showed six contrast issues and eight errors, indicating a solid performance. However, Cuba and Venezuela had significant problems, with 25 and 34 errors, respectively. In contrast, Uruguay ranked high with no errors in contrast and perceptibility, highlighting its leadership in web accessibility in the region. These findings provide valuable insights for policymakers and developers looking to improve accessibility and digital inclusion across Latin America's digital infrastructure.},
  keywords={Leadership;W3C;Solids;Robustness;Digital divide;Indexes;Artificial intelligence;Portals;Guidelines;digital inclusion;Latin American artificial intelligence index;ILIA;web accessibility;WCAG 2.2},
  doi={10.1109/ICEDEG65568.2025.11081555},
  ISSN={2573-1998},
  month={June},}@ARTICLE{10471537,
  author={Munyer, Travis and Tanvir, Abdullah All and Das, Arjon and Zhong, Xin},
  journal={IEEE Access}, 
  title={DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text}, 
  year={2024},
  volume={12},
  number={},
  pages={40508-40520},
  abstract={The rapid advancement of Large Language Models (LLMs) has significantly enhanced the capabilities of text generators. With the potential for misuse escalating, the importance of discerning whether texts are human-authored or generated by LLMs has become paramount. Several preceding studies have ventured to address this challenge by employing binary classifiers to differentiate between human-written and LLM-generated text. Nevertheless, the reliability of these classifiers has been subject to question. Given that consequential decisions may hinge on the outcome of such classification, it is imperative that text source detection is of high caliber. In light of this, the present paper introduces DeepTextMark, a deep learning-driven text watermarking methodology devised for text source identification. By leveraging Word2Vec and Sentence Encoding for watermark insertion, alongside a transformer-based classifier for watermark detection, DeepTextMark epitomizes a blend of blindness, robustness, imperceptibility, and reliability. As elaborated within the paper, these attributes are crucial for universal text source detection, with a particular emphasis in this paper on text produced by LLMs. DeepTextMark offers a viable “add-on” solution to prevailing text generation frameworks, requiring no direct access or alterations to the underlying text generation mechanism. Experimental evaluations underscore the high imperceptibility, elevated detection accuracy, augmented robustness, reliability, and swift execution of DeepTextMark.},
  keywords={Watermarking;Blindness;Transformers;Semantics;Robustness;Proposals;Large language models;Deep learning;Text processing;Detection algorithms;Text source detection;large language model text detection;text watermarking;deep learning},
  doi={10.1109/ACCESS.2024.3376693},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10511173,
  author={Chaple, Shreya and Raut, Vedanti and Patni, Jagdish Chandra and Banode, Ayush and Ninawe, Samiksha and Shelke, Nilesh},
  booktitle={2024 5th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)}, 
  title={Artificial Intelligence on Visually Impaired People: A Comprehensive Review}, 
  year={2024},
  volume={},
  number={},
  pages={304-308},
  abstract={Artificial Intelligence has become a significant tool in modern technology, enabling people to interact with machines through various methods. Individuals with visual impairments have trouble doing tasks because they are either blind or have poor vision. BVI stands for Blind and Visually Impaired. Solutions must also advance with technology to ensure that individuals can effectively navigate their surroundings and assist them in real-time navigation. The study conducts surveys on visually challenged individuals in the community and aims to assist them by providing smart gadgets to identify faces, colours, and objects. Moreover, this study emphasizes more on different technologies and methods that are used earlier to help visually impaired people in their day-to-day life.},
  keywords={Surveys;Navigation;Reviews;Visual impairment;Real-time systems;Problem-solving;Object recognition;Artificial intelligence;Task analysis;Sustainable development;Artificial Intelligence;Object Recognition;smart stick;Convolutional Neural Networks (CNN);SKYE;Red Green Blue-Depth (RGB-D) camera;navigation;Voice Assistant},
  doi={10.1109/ICICV62344.2024.00052},
  ISSN={},
  month={March},}@INPROCEEDINGS{10085638,
  author={Rajput, Gaurav Kumar and Sharma, Sachin and Dash, Bibhu Prasad and Ansari, Meraj Farheen and Sharma, Pawankumar and Shukla, Surendra Kumar},
  booktitle={2023 International Conference on Artificial Intelligence and Smart Communication (AISC)}, 
  title={The Way to Make Blind People Use the Email System: Voice Based Email Generating System Using Artificial Intelligence}, 
  year={2023},
  volume={},
  number={},
  pages={1120-1123},
  abstract={People are becoming used to fake world and vocal communication as technology advances. There are a few ways to communicate with people online in this modern invention. A large number of individuals often choose and use the simplest form of reporting, namely email. The era of encrypted email allows users to communicate with other people by posting signals and also facilitates cross-border corporate communication. Some individuals are unable to take advantages of this technology due to their ignorant or lack the necessary visible screen ability. Therefore, a Speech completely messaging device is suggested using Py but instead Ai to save time for externally examined people. Its device gives those who have been evaluated on the outside some power of contact and considerably increases their sense of stability as objectivity. With the use of that invention, blind persons will indeed be able to send out emails just like other regular citizens. Voice-based messaging systems use cutting-edge technology to ensure their legitimacy to people who have been vetted on the outside.},
  keywords={Technological innovation;Blindness;Electronic mail;Cryptography;Artificial intelligence;Communication;Artificial Intelligence;blind people;Email;customer},
  doi={10.1109/AISC56616.2023.10085638},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10200143,
  author={Mahalingesh, T C and Karthik, A and K L, Banashankar and M C, Basavaradya and R, Abhishek},
  booktitle={2023 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES)}, 
  title={Blind-Aid: Empowering the Visually Impaired using Artificial Intelligence and Image Processing}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={In the modern world, very few tools have been created to assist visually impaired persons (VIPs), despite the fact that many innovations have been made to improve the lives of regular people. WHO estimates that there are 39 million blind persons in the globe who struggle to communicate with others, comprehend their thoughts and feelings, and recognize some common objects. VIPs are intelligent enough to hear people's emotions in their voices. This approach, meanwhile, is ineffective in noisy settings. By touching or sensing the object, they can identify it, but if the object is out of reach or cannot be touched, they cannot identify it. Through the use of artificial intelligence and machine learning techniques, this project seeks to address these issues by identifying a few typical household objects and identifying common human emotions or facial expressions and convert text to audio speaker output.},
  keywords={Technological innovation;Text recognition;Wearable computers;Visual impairment;Speech recognition;Object detection;Machine learning},
  doi={10.1109/ICSSES58299.2023.10200143},
  ISSN={},
  month={July},}@INPROCEEDINGS{10039557,
  author={R, Shree Lakshmi and M, Sneha and K, Swetha and Thilagavathy, A.},
  booktitle={2022 5th International Conference on Advances in Science and Technology (ICAST)}, 
  title={AI-Powered Smart Glasses for Blind, Deaf, and Dumb}, 
  year={2022},
  volume={},
  number={},
  pages={280-285},
  abstract={Back then, technology was far more advanced (developed) than time. We can now determine what a deaf or mute person is saying only by capturing it and comparing it to predetermined datasets, showing what the individual is striving to achieve. In this study, we propose a way of supporting impaired people, such as those who are Deaf, Dumb, or Blind, by giving them a new tech that acts as an eye, ear, and brain to them. Machine learning methods are employed for object recognition with the aid of image processing to give the blind, a pair of eyes. The deaf and the dumb can both benefit from text-to-speech communication using Bluetooth or radio technology and speech-to-text translation. The convergence of all these technologies, together with AI, AR, VR, and IoT technologies, will help in finding solutions to the problems that these people who face disabilities.},
  keywords={Legged locomotion;Machine learning algorithms;Image resolution;Image communication;Face recognition;Machine learning;Ear;Internet of things;Artificial Intelligence;processing of images;wireless communication;Bluetooth communication;Augmented Reality;Virtual Reality},
  doi={10.1109/ICAST55766.2022.10039557},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10652516,
  author={Hamed, Hamed and Khan, Md. Raisuddin},
  booktitle={2024 9th International Conference on Mechatronics Engineering (ICOM)}, 
  title={AI-based Detection of Potholes Ahead of a Visually Impaired Person Using Ultrasonic Sensors Array and Camera for Blind Navigation}, 
  year={2024},
  volume={},
  number={},
  pages={448-453},
  abstract={Visually impaired individuals usually depend on assistive devices like white canes, frequently equipped with ultrasonic sensors, for navigation. However, these devices face significant limitations, particularly in detecting specific hazards such as potholes and deep trenches on walkways. This gap in functionality increases the risk of accidents and impedes safe, independent navigation for the visually impaired. The research developed a prototype of a blind assistive system equipped with an array of ultrasonic sensors and a Raspberry Pi integrated with Firebase for loT capabilities. AI models, trained on the collected datasets of road images and ultrasonic sensor readings, were deployed on the Raspberry Pi. Testing in real-world scenarios was conducted to validate the prototype's effectiveness. The results showed that the AI model successfully detected potholes with an accuracy of 93%. The prototype could detect both large and small potholes using ultrasonic sensors and a camera but faced challenges in cases where potholes were filled with water or in complex environments.},
  keywords={Accuracy;Navigation;Prototypes;Cameras;Acoustics;Sensor systems;Artificial intelligence;Blind Navigation;Pothole Detection;Artificial Intelligence;Assistive Devices;Wearable Technology},
  doi={10.1109/ICOM61675.2024.10652516},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10371553,
  author={De Vito, Gabriele and Palomba, Fabio and Gravino, Carmine and Di Martino, Sergio and Ferrucci, Filomena},
  booktitle={2023 49th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={ECHO: An Approach to Enhance Use Case Quality Exploiting Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={53-60},
  abstract={UML use cases are commonly used in software engineering to specify the functional requirements of a system since they are an effective tool for interacting with stakeholders thanks to the use of natural languages. However, producing high-quality use cases can be challenging due to the lack of precise guidelines and suitable tools. This can lead to problems, e.g. inaccuracy and incompleteness, in the derived software artifacts and the final product. Recent advancements in Natural Language Processing and Large Language Models (LLMs) can provide the premises for developing tools supporting activities based on natural languages. In this paper, we propose ECHO, a novel approach for supporting software engineers in enhancing the quality of UML use cases using LLMs. Our approach consists of a co-prompt engineering approach and an iterative and interactive process with the LLM to improve the quality of use cases, based on practitioners’ feedback. To prove the feasibility of the proposal, we instantiated the approach using ChatGPT and performed a controlled experiment to assess its effectiveness by involving seven software engineering professionals. Three were part of the experimental group and used ECHO to improve the quality of the use cases. Three others were the control group and enhanced the quality of use cases manually. Finally, the last participant acted as an oracle, blind w.r.t. the groups, and evaluated the quality of the enhanced use cases, both qualitatively by means of a questionnaire, and quantitatively, by means of the Use Case Points metric. Results show that ECHO can effectively support software engineers to improve use cases’ quality thanks to the prompts suitably designed to interact with ChatGPT.},
  keywords={Measurement;Unified modeling language;Estimation;Chatbots;Software;Stakeholders;Proposals;UML Use Cases;Large Language Models;Prompt Engineering;Size/Effort estimation},
  doi={10.1109/SEAA60479.2023.00017},
  ISSN={2376-9521},
  month={Sep.},}@INPROCEEDINGS{10928085,
  author={Katara, Sanjeev Kumar and Pathak, Rajesh Kumar and Sharma, Ashwini Kumar and Sharma, Sadhna},
  booktitle={2024 International Conference on Computer and Applications (ICCA)}, 
  title={Web Accessibility Framework and Analysis for Persons with Disabilities: Case Studies of Indian Web Portals}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={People with disabilities have increased their use of the web technologies to share their common obstacles. For people with disabilities, their main struggle is integration within today's world. This paper will explore what assistive technologies are available to people with disabilities, and how these assistive technology improves their living conditions. It also presents accessibility approaches, accessibility framework, assessment analysis and compliance of Web Content Accessibility Guidelines (WCAG) of selected Indian web portals like Most visited, for Disabled Persons and for Educational Institutions. The paper will also explore emerging technologies like Artificial Intelligence (AI) based accessibility tools & effective digital technologies and initiatives in Indian context including the role of various stake holders who need to come forward towards reductions of the disability digital divide. It will also explore a few challenges and solutions. It also campaigns for use of assistive technologies to bring e-Governance services closer to people with disabilities.},
  keywords={Social networking (online);W3C;Assistive technologies;Media;People with disabilities;Information and communication technology;Stakeholders;Artificial intelligence;Portals;Guidelines;Accessibility;Artificial Intelligence;Assistive Technologies;Framework;Human Computer Interaction;India;Persons with Disabilities;WCAG;Web Portals},
  doi={10.1109/ICCA62237.2024.10928085},
  ISSN={},
  month={Dec},}@ARTICLE{10574854,
  author={Fu, Jun and Zhou, Wei and Jiang, Qiuping and Liu, Hantao and Zhai, Guangtao},
  journal={IEEE Signal Processing Letters}, 
  title={Vision-Language Consistency Guided Multi-Modal Prompt Learning for Blind AI Generated Image Quality Assessment}, 
  year={2024},
  volume={31},
  number={},
  pages={1820-1824},
  abstract={Recently, textual prompt tuning has shown inspirational performance in adapting Contrastive Language-Image Pre-training (CLIP) models to natural image quality assessment. However, such uni-modal prompt learning method only tunes the language branch of CLIP models. This is not enough for adapting CLIP models to AI generated image quality assessment (AGIQA) since AGIs visually differ from natural images. In addition, the consistency between AGIs and user input text prompts, which correlates with the perceptual quality of AGIs, is not investigated to guide AGIQA. In this letter, we propose vision-language consistency guided multi-modal prompt learning for blind AGIQA, dubbed CLIP-AGIQA. Specifically, we introduce learnable textual and visual prompts in language and vision branches of CLIP models, respectively. Moreover, we design a text-to-image alignment quality prediction task, whose learned vision-language consistency knowledge is used to guide the optimization of the above multi-modal prompts. Experimental results on two public AGIQA datasets demonstrate that the proposed method outperforms state-of-the-art quality assessment models.},
  keywords={Task analysis;Measurement;Visualization;Transformers;Image quality;Electronic mail;Correlation;Multi-modal prompt learning;vision-language consistency;AGIQA},
  doi={10.1109/LSP.2024.3420083},
  ISSN={1558-2361},
  month={},}@INPROCEEDINGS{11156811,
  author={Mandal, Bratati and Singh, Pritha and Ratna, Sanatan},
  booktitle={2025 12th International Conference on Emerging Trends in Engineering & Technology - Signal and Information Processing (ICETET - SIP)}, 
  title={Real-Time Object Detection and Environmental Feedback: A Smart Navigation System for the Blind Using AI}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents a novel AI-powered Smart Navigation System designed to enhance the mobility and independence of visually impaired individuals. Traditional navigation aids such as guide dogs, white canes, and GPS systems provide limited situational awareness and lack real-time environmental feedback crucial for safe navigation. Our system integrates advanced computer vision technology using the YOLOv8 model to enable real-time detection of obstacles, such as pedestrians and vehicles, while offering additional functionalities like traffic light recognition and weather alerts. Speech-to-speech communication transforms visual data into auditory cues, empowering users with immediate and actionable information. Field tests demonstrate that the system significantly improves navigation safety and user confidence, offering a transformative solution in assistive technology. This research highlights the potential of artificial intelligence to revolutionize support for the visually impaired, providing a comprehensive platform for safe and informed navigation in dynamic environments.},
  keywords={Visualization;Pedestrians;Navigation;Transforms;Object detection;Real-time systems;Safety;Artificial intelligence;Vehicle dynamics;Meteorology;Navigation System;YOLOv8;Deep Learning;AI},
  doi={10.1109/ICETETSIP64213.2025.11156811},
  ISSN={2157-0485},
  month={Aug},}@ARTICLE{10504109,
  author={Shang, Yuting},
  journal={Journal of Web Engineering}, 
  title={Music Curriculum Research Using a Large Language Model, Cloud Computing and Data Mining Technologies}, 
  year={2024},
  volume={23},
  number={2},
  pages={251-273},
  abstract={This paper presents a method to enhance the scientific nature of the music curriculum model by integrating a large language model, cloud computing and data mining technology for the analysis of the music teaching curriculum model. To maintain the integrity of the mixing matrix while employing the frequency hopping frequency, the paper suggests dividing the mixing matrix into a series of sub-matrices along the vertical time axis. This approach transforms wideband music signal processing into a narrowband processing problem. Additionally, two hybrid matrix estimation algorithms are proposed in this paper using underdetermined conditions. Furthermore, utilizing the estimated mixing matrix and the detected time-frequency support domain, the paper employs the subspace projection algorithm for underdetermined blind separation of music signals in the time-frequency domain. This procedure, along with the integration of the estimated direction of arrival (DoA), enables the completion of frequency-hopping network station music signal sorting. Extensive simulation teaching demonstrates that the music curriculum model proposed in this paper, based on a large language model, cloud computing and data mining technologies, significantly enhances the quality of modern music teaching.},
  keywords={Cloud computing;Time-frequency analysis;Direction-of-arrival estimation;Computational modeling;Education;Music;Frequency conversion;Large language model;cloud computing;data mining;music;curriculum model},
  doi={10.13052/jwe1540-9589.2323},
  ISSN={1544-5976},
  month={March},}@INPROCEEDINGS{11161313,
  author={Golam, Mohtasin and Alam, Md Mahinur and Subhan, Md Raihan and Kim, Dong-Seong and Lee, Jae-Min},
  booktitle={ICC 2025 - IEEE International Conference on Communications}, 
  title={BLIND-TWIN: Blockchain-Assisted LLM-Based Cds for Digital Twin-Enhanced Industrial AIoT}, 
  year={2025},
  volume={},
  number={},
  pages={4981-4986},
  abstract={The interconnected and diverse nature of Digital Twin (DT)-based industrial Artificial Internet of Things (AIoT) systems exposes them to potential cyber threats and malicious activities. This paper introduces a novel framework called BLIND-Twin, which leverages blockchain, DT, and Large Language Model (LLM) technologies to address critical security and scalability challenges in industrial AIoT networks. By integrating DT technology, BLIND-Twin continuously mirrors physical environments, enabling real-time monitoring and synthetic data generation to simulate diverse threat scenarios. Data from the DT undergoes feature extraction via a Long Short-Term Memory (LSTM) Autoencoder (LSTM-AE) to extract essential temporal patterns, while the LLM enables adaptive, contextaware intrusion detection without retraining. A permissioned blockchain layer ensures data integrity, privacy, and secure logging through smart contracts, supporting automated threat response with verifiable audit trails. The framework's decentralized architecture mitigates Single Points of Failure (SPoF), addressing scalability and privacy concerns. Performance evaluations utilizing datasets like 5G-NIDD and CICIoT2023 demonstrate BLINDTwin's capability in accurately detecting various cyber threats by achieving 99.63 % accuracy with minimal latency, showcasing its effectiveness for complex industrial AIoT environments.},
  keywords={Accuracy;Scalability;Data integrity;Intrusion detection;Feature extraction;Real-time systems;Blockchains;Digital twins;Internet of Things;Long short term memory;Artificial Internet of Things (AIoT);Blockchain;Digital Twin (DT);Federated Learning (FL);Unmanned Aerial Vehicle (UAV)},
  doi={10.1109/ICC52391.2025.11161313},
  ISSN={1938-1883},
  month={June},}@INPROCEEDINGS{10561228,
  author={M S, Sri Ram and Joy, Emmanuel and J, Leka Shree},
  booktitle={2024 International Conference on Science Technology Engineering and Management (ICSTEM)}, 
  title={WebSight: An AI-based Approach to Enhance Web Accessibility for the Visually Impaired}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Web accessibility plays a significant role in shaping our online experiences in today's ever-changing digital world. People with vision impairmentsstill have trouble using most websites. This paper introduces WebSight an innovative AI-based Image Description Generator web extension to address these challenges. By seamlessly interfacing with a Flask server, Optical Character Recognition, and a dedicated Image Description Generator model, this browser extension systematically enhances web accessibility by methodically replacing ‘alt’ attributes of images with meticulously generated descriptions. This approach significantly contributes to web inclusivity, providing visually impaired users with accurate and contextually rich information. The extension's commitment to inclusivity is further demonstrated through the intelligent integration of Tesseract-OCR and Text-to-Speech ensuring a novel and far-reaching impact, offering an enhanced and tailored browsing experience for users with visual impairments.},
  keywords={Training;Technological innovation;Accuracy;Visual impairment;Optical character recognition;Web pages;Transformers;Image Captioning;Browser Extension;OCR;Web Accessibility;Speech Synthesis;Assistive Technology},
  doi={10.1109/ICSTEM61137.2024.10561228},
  ISSN={},
  month={April},}
