@INPROCEEDINGS{10628549,
  author={Restrepo, David and Nakayama, Luis Filipe and Dychiao, Robyn Gayle and Wu, Chenwei and McCoy, Liam G. and Artiaga, Jose Carlo and Cobanaj, Marisa and Matos, João and Gallifant, Jack and Bitterman, Danielle S. and Ferrer, Vincenz and Aphinyanaphongs, Yindalon and Anthony Celi, Leo},
  booktitle={2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)}, 
  title={Seeing Beyond Borders: Evaluating LLMs in Multilingual Ophthalmological Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={565-566},
  abstract={Large Language Models (LLMs), such as GPT-3.5 [1] and GPT-4 [2], have significant potential for transforming several aspects of patient care from clinical note summarization to performing board-level clinical question-answering tasks [3], [4]. Ophthalmology, is a field with high patient volume and therefore holds high documentation burden for physicians but great opportunities for leveraging LLMs. Furthermore, given the critical and permanent nature of negative disease outcomes like blindness and their ensuing social and financial damage to patients, the need for reliable, accessible, and robust tools is urgent. Several studies have already showcased the practicality of GPT applications in ophthalmology [5], [6], and in specific ophthalmology subspecialties, such as glaucoma and retina [7], [8]},
  keywords={Glaucoma;Large language models;Medical services;Documentation;Retina;Question answering (information retrieval);Ophthalmology;Large Language Models;Language Bias;Health Inequalities;LLMs Evaluation;Medical Board Exam},
  doi={10.1109/ICHI61247.2024.00089},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{10351460,
  author={Kumari, Alpana and Garg, Vivek Kumar},
  booktitle={2023 3rd International Conference on Innovative Sustainable Computational Technologies (CISCT)}, 
  title={Role of Artificial Intelligence in mode of Diabetic Retinopathy Imaging Techniques: A Review Artificial Intelligence in Diabetic Retinopathy}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Artificial intelligence (AI), usually referred to as machine intelligence, is a scientific field that gives robots the ability to think like people. AI is a term used to describe a system that simulates human intellect using computer programming. AI is advancing quickly in many multidisciplinary fields, including ophthalmology, from healthcare to the accurate preventive measures, investigation, and treatment plans of illnesses. Because of the emphasis on imaging in the diagnosis of eye illnesses, optometry and ophthalmology is at the forefront of AI in medical field and medicine. Lately, the most prevalent illnesses that cause blindness and visual impairment, such as diabetic retinopathy (DR), have been subjected to deep learning-based AI screening and predictive algorithm models. Deep learning machine with algorithms are the computer models made up of several layers of simulated neurons, are largely responsible for the success of AI in medical field and medicine. These models are capable of learning how data is represented at various levels of abstraction. The trained AI system classified the various forms of DR on optical coherence tomography pictures with accuracy equivalent to that of human specialists. In this study, we focus on the core ideas of AI and how it is applied to the various forms of DR. We also go into further detail on how AI and DL are used in the future of ophthalmology.},
  keywords={Diabetic retinopathy;Computational modeling;Visual impairment;Programming;Predictive models;Prediction algorithms;Ophthalmology;Artificial Intelligence;Diabetic retinopathy;Machine learning;hemorrhages;neovascularization},
  doi={10.1109/CISCT57197.2023.10351460},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10303628,
  author={Foysal, Nasir Uddin and Thamid, Thamid Junaeid and Uddin, Md Shihab and Islam, Md Fakrul and Islam, Muhammad Nazrul and Faisal, Faiz Al},
  booktitle={2023 International Conference on Information and Communication Technology for Sustainable Development (ICICT4SD)}, 
  title={Advancing AI-based Assistive Systems for Visually Impaired People: Multi-Class Object Detection and Currency Classification}, 
  year={2023},
  volume={},
  number={},
  pages={438-442},
  abstract={Blind and Visually Impaired (BVI) persons depend on the assistive systems. An intelligent assistive system may provide situational awareness and support in the age of machine learning and computer vision with the help of deep learning and computer vision. Among many, multi-classed object detection and currency classification are the two key means for providing situational awareness and environmental information to the BVI people. Thus, the objective of this research is to propose a modified YOLOv7 and a YOLOv8 algorithm for multi-classed object detection and currency classification, respectively. To attain this objective, a modified YOLOv7 and a YOLOv8 was proposed and evaluated considering the datasets for object identification and currency classification, respectively. The dataset for object detection was created from scratch that includes 3046 photos and spans 20 distinct classes, while the dataset used for currency classification used 295 photos divided into eight distinct groups. The evaluation results showed that in comparison to the basic YOLOv7 (89.3% mAP) the modified YOLOv7 has obtained 91.4% mAP for multi-classed object detection, while the YOLOv8 has reached 94.6% mAP for currency classification comparing to the Radial Basis Function Network (91.51% mAP).},
  keywords={Deep learning;Computer vision;Object detection;Computer architecture;Radial basis function networks;Classification algorithms;Information and communication technology;Computer Vision;Visual Assistance;Object Detection;Object Classification;YOLO;Deep Learning Model},
  doi={10.1109/ICICT4SD59951.2023.10303628},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10701130,
  author={Christian, Michael and Nan, Guan and Gularso, Kurnadi and Dewi, Yuli Kartika and Sunarno and Wibowo, Suryo},
  booktitle={2024 3rd International Conference on Creative Communication and Innovative Technology (ICCIT)}, 
  title={Impact of AI Anxiety on Educators Attitudes Towards AI Integration}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={The present progress in implementing artificial intelligence (AI) has encountered attitudes in human existence, encompassing both professional and everyday aspects, particularly within higher education. AI does not exclude those with anxiety, especially social anxiety, over the existence of technology and all types of complexity. Several types of research have established a correlation between social and technical blindness and views toward broad AI usage. However, no specific gender or age-generation characteristics have been identified. This study employs quantitative research methodology, specifically Smart-PLS software for PLS-SEM modelling. The research included a sample of 93 lecturers, both male and female, from generations X and Y in Indonesia. The findings of this study demonstrate that social technical blindness detrimentally impacts individuals’ views toward the utilization of AI. From a gender standpoint, the path coefficients have no notable disparity. Therefore, the path coefficients for both women and men are not significantly distinct. Consequently, the need for greater awareness and understanding of AI’s social and technological aspects impacts both women and men. This research provides practical implications by enhancing instructors’ comprehension and perception of AI’s efficacy, aiming to mitigate prevailing concerns. In theory, this research enhances the concept of self-determination theory. Subsequent investigations can include the constraints of this study, such as the specific nature and duration of artificial intelligence utilization.},
  keywords={Training;Seminars;Ethics;Data privacy;Anxiety disorders;Psychology;Focusing;Blindness;Software;Artificial intelligence;Artificial Intelligence (AI);AI Usage Attitudes;AI Anxiety;Social Technical Blindness;Higher education},
  doi={10.1109/ICCIT62134.2024.10701130},
  ISSN={},
  month={Aug},}@ARTICLE{10490142,
  author={Van Huynh, Nguyen and Wang, Jiacheng and Du, Hongyang and Hoang, Dinh Thai and Niyato, Dusit and Nguyen, Diep N. and Kim, Dong In and Letaief, Khaled B.},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={Generative AI for Physical Layer Communications: A Survey}, 
  year={2024},
  volume={10},
  number={3},
  pages={706-728},
  abstract={The recent evolution of generative artificial intelligence (GAI) leads to the emergence of groundbreaking applications such as ChatGPT, which not only enhances the efficiency of digital content production, such as text, audio, video, or even network traffic data, but also enriches its diversity. Beyond digital content creation, GAI’s capability in analyzing complex data distributions offers great potential for wireless communications, particularly amidst a rapid expansion of new physical layer communication technologies. For example, the diffusion model can learn input signal distributions and use them to improve the channel estimation accuracy, while the variational autoencoder can model channel distribution and infer latent variables for blind channel equalization. Therefore, this paper presents a comprehensive investigation of GAI’s applications for communications at the physical layer, ranging from traditional issues, including signal classification, channel estimation, and equalization, to emerging topics, such as intelligent reflecting surfaces and joint source channel coding. We also compare GAI-enabled physical layer communications with those supported by traditional AI, highlighting GAI’s inherent capabilities and unique contributions in these areas. Finally, the paper discusses open issues and proposes several future research directions, laying a foundation for further exploration and advancement of GAI in physical layer communications.},
  keywords={Physical layer;Artificial intelligence;Channel estimation;Surveys;Generative adversarial networks;Wireless communication;Generative AI;Generative AI;physical layer communications;channel estimation and equalization;physical layer security;IRS;beamforming;joint source channel coding},
  doi={10.1109/TCCN.2024.3384500},
  ISSN={2332-7731},
  month={June},}@INPROCEEDINGS{10652439,
  author={Thakur, Rijul and Thakur, Akarshit and Viral, RajKumar and Asija, Divya},
  booktitle={2024 IEEE Students Conference on Engineering and Systems (SCES)}, 
  title={AI Based Visual Impairment System for Blind and People with Least Reading Capabilities}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The topic of this research study is the development of a Text-to-Speech converter utilizing the Raspberry Pi 5 single-board computer and the Pi Camera Module. The idea was inspired by the rising need for creative, hardware-and-software-integrated solutions to assist people with diverse communication and accessibility requirements. The main platform for this project is the Raspberry Pi 5, which has improved processing power and capabilities. The background and context of this technology, as well as the Raspberry Pi platform, are covered in the study's introduction, which also emphasizes the need for such a system. The methodology section explains the hardware and software setup in-depth, including how the Raspberry Pi 5 was set up and how text-to-speech technology was incorporated. The method of capturing and processing text for conversion, as well as the difficulties faced and the solutions developed, are described in data collection and picture processing components. The text-to-speech conversion section demonstrates the Raspberry Pi 5's text-to-speech functionality. A crucial stage of the research article is system testing, during which the system's performance is assessed, results are presented, and the practical implications and constraints are discussed. The results are interpreted, the developed system is compared to other alternatives, and its importance is highlighted in the discussion section. In conclusion, this study shows that by utilizing the Raspberry Pi 5 and Pi Camera Module, it is possible to build an affordable text-to-speech converter. The study has possible uses for assisted communication devices, education, and accessible technologies. Suggestions for future work focus on potential areas for development and improvement.},
  keywords={Privacy;System testing;Accuracy;Text recognition;Face recognition;Visual impairment;Blindness;text-to-speech converter;raspberry pi 5;pi camera module},
  doi={10.1109/SCES61914.2024.10652439},
  ISSN={},
  month={June},}@INPROCEEDINGS{11158229,
  author={Babu, B.S.S.V.Ramesh and Jayakrishna, Gedela and Mahendra, Balasani and Gowtham, Medisetti and Manasa, Appana Lohitha Sri Sai and Chandrakanth, Munubarthi},
  booktitle={2025 IEEE Wireless Antenna and Microwave Symposium (WAMS)}, 
  title={AI-Powered Smart Glasses for the Blind using OpenCV & Python}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={This research provides a novel approach in the field of object detection through the use of OpenCV and Python programming and solves the difficulties with the conventional approach, as the system includes an ESP32 module for audio-specific object recognizing signals and a GPS module for avoiding location loss, which is an improvement for participants who are visually impaired. It performs functions of detecting faces, more specifically eyes and nose, without relying on external sources, thereby increasing the accuracy and efficiency of the process, while the use of strong hardware combined with computer vision ensures that the project addresses the problems of navigation and object recognition in a more practical and wider sense.},
  keywords={Microwave antennas;Wireless communication;Wireless sensor networks;Object detection;Programming;Internet of Things;Object recognition;Python;Global Positioning System;Smart glasses;Internet of Things (IOT);Object Detection;OpenCV;ESP32 Module;Facial Feature Recognition;GPS Tracking;Arduino IDE},
  doi={10.1109/WAMS64402.2025.11158229},
  ISSN={},
  month={June},}@ARTICLE{11132297,
  author={Woesle, Christian and Fischer-Brandies, Leopold and Buettner, Ricardo},
  journal={IEEE Access}, 
  title={A Systematic Literature Review of Hallucinations in Large Language Models}, 
  year={2025},
  volume={13},
  number={},
  pages={148231-148253},
  abstract={This review systematically maps research on hallucinations in large language models using a descriptive scheme that links model outputs to four system architectures: unaugmented generation, post-hoc reactive validation, proactive detection-and-mitigation, and fully integrated detection-and-mitigation designs. Our methodology for this systematic review follows the PRISMA guidelines to ensure transparency and reproducibility. We searched IEEE Xplore, ACM Digital Library, and ScienceDirect for studies published between 2015 and January 2025 and extracted 125 peer-reviewed papers across nine application domains. Quantitative analysis shows that question answering and multimodal tasks account for 48% of all papers, whereas software engineering, educational technology, and autonomous systems are underexplored. Although 87.5% of the studies rely on additional reactive or proactive defenses, only 8.8% implement integrated architecture-level safeguards, revealing a critical gap in unified and dynamic architectures. The resulting classification matrix and domain map provide a diagnostic tool for locating blind spots and comparing architectural maturity. Three actionable priorities emerge: develop integrated reasoning-and-verification loops that pre-empt hallucinations; transfer proven causal-intervention and multi-agent validation pipelines to high-stakes, under-represented domains and benchmark them under real conditions; and build modular, cross-domain evaluation frameworks that isolate the contribution of individual mitigation components and support ablation studies. By consolidating fragmented evidence and quantifying architecture-domain imbalances, this review establishes a traceable foundation for engineering reliable, explainable, and domain-adaptable countermeasures to hallucinations in generative language technology.},
  keywords={Prevention and mitigation;Systematic literature review;Large language models;Systems architecture;Semantics;Question answering (information retrieval);Computer architecture;Taxonomy;Technological innovation;Retrieval augmented generation;Large language models;hallucinations;architecture;detection techniques;mitigation strategies;systematic literature review},
  doi={10.1109/ACCESS.2025.3601206},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10986494,
  author={Vishwakarma, Aryan and Panchal, Nehal and Dave, Vaibhav and Jethani, Hetal and Parashar, Deepak and Bahadure, Nilesh Bhaskarrao},
  booktitle={2025 3rd International Conference on Disruptive Technologies (ICDT)}, 
  title={Multi-Modal Glaucoma Detection Framework using Explainable AI and LLMs}, 
  year={2025},
  volume={},
  number={},
  pages={793-798},
  abstract={Glaucoma is a group of diseases that can lead to partial or complete blindness due to excessive pressure on the eyes, often caused by anxiety or depression. This pressure damages the optic nerve. The condition requires close, regular cooperation between doctors and patients over time. The increasing number of screenings and time constraints on medical practitioners can lead to errors in disease identification and treatment. Many attempts have been made to automate glaucoma screening and provide interpretability to ensure reliable and accurate solutions. A proposed multi-modal approach combines LIME and Generative AI to help medical practitioners detect glaucoma and provide explanations},
  keywords={Glaucoma;Accuracy;Biomedical optical imaging;Generative AI;Transfer learning;Predictive models;Reliability;Usability;Medical diagnostic imaging;Diseases;Glaucoma detection;Generative AI;LIME;multimodal framework;optic nerve damage},
  doi={10.1109/ICDT63985.2025.10986494},
  ISSN={},
  month={March},}@INPROCEEDINGS{11101957,
  author={Faisal, Maha and AlQouz, Hadeel and Ashkanani, Hussain and Alkandari, Sulaiman and Boland, Abdullah and Mohammed, Salem},
  booktitle={2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS)}, 
  title={SightNavigator: AI-Powered Smart Glasses to Enable Independent Shopping for Visually Impaired Individuals}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Blind and visually impaired individuals often rely on assistance when navigating crowded public spaces and locating products in markets. We introduce SightNavigator, a pair of AI-powered smart glasses that combines real-time obstacle detection, section recognition, and product identification to enable fully independent shopping. An integrated OAKD Lite depth camera and YOLOv8 model detect and guide users around obstacles, while voice commands allow section selection and ondemand retrieval of product information (e.g., price, expiration date, nutritional content). RFIDbased wayfinding ensures accurate market navigation without preplanning. In prototype trials, SightNavigator achieved over 95% object detection accuracy and reduced average aislefinding time by 40%. By uniting advanced computer vision, spatial sensing, and speech interaction in a lightweight wearable, SightNavigator delivers an inclusive shopping experience that restores autonomy, confidence, and safety for people with visual impairments.},
  keywords={Accuracy;Navigation;Visual impairment;Speech recognition;User experience;Real-time systems;Sensors;Safety;Visual databases;Smart glasses;AI;Blind;Machine Learning;Obstacle Detection;Voice Recognition;Accessibility;Inclusive user experience;shopping. I},
  doi={10.1109/ISAS66241.2025.11101957},
  ISSN={},
  month={June},}@ARTICLE{10486966,
  author={Rahman, Moshfeka and Yan, Jun and Thepie Fapi, Emmanuel},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Adversarial Artificial Intelligence in Blind False Data Injection in Smart Grid AC State Estimation}, 
  year={2024},
  volume={20},
  number={6},
  pages={8873-8883},
  abstract={Artificial intelligence (AI) plays an imperative role in next-generation critical infrastructures like the smart grid, whose power can be harnessed by not only operators, but also cyber adversaries. This article investigates a potential threat from adversarial AI in blind false data injection attacks (FDIA) targeting the ac state estimators in the smart grid. Assuming no access to the grid topology required in most FDIA, we propose an adversarial model based on artificial neural networks (ANNs) to infer grid topology from historical measurements. Following the topology inference, a substitute bad data detector (BDD) model is further proposed in the attack model to filter the false data before injection, reducing the risk of detection given potential bad data in normal operations. We also refine the common evaluation of FDI stealthiness by including the presence of bad data among normal and false data when assessing the detection performance. Simulations on the IEEE 30-bus system reveal that significant deviations can be inflicted stealthily by the proposed blind FDI attack. Detailed analyses of the stealthiness, impacts, and parameters are also presented to shed more light on the threats for further studies and effective countermeasures.},
  keywords={Topology;Data models;Smart grids;Power measurement;Artificial intelligence;Noise measurement;Adversarial machine learning;Artificial intelligence;Data integrity;Fake news;State estimation;Adversarial artificial intelligence (AI);data integrity attacks;false data injection (FDI);smart grid;state estimation (SE)},
  doi={10.1109/TII.2024.3374374},
  ISSN={1941-0050},
  month={June},}@ARTICLE{10050828,
  author={Farhad, Arshad and Pyun, Jae-Young},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={AI-ERA: Artificial Intelligence-Empowered Resource Allocation for LoRa-Enabled IoT Applications}, 
  year={2023},
  volume={19},
  number={12},
  pages={11640-11652},
  abstract={Adaptive data rate (ADR) is a widely adopted resource assignment approach in long-range wide-area networks (LoRaWANs) for static Internet of Things (IoT) applications such as smart grids and metering. Blind ADR (BADR) has been recommended for mobile IoT applications such as pet and industrial asset tracking. However, ADR and BADR cannot provide appropriate measures to alleviate the massive packet loss problem caused by the unsuitable spreading factors (SFs) assigned to end devices when they are mobile. This article proposes a novel proactive approach—“artificial intelligence-empowered resource allocation” (AI-ERA)—to address the resource assignment issue in static and mobile IoT applications. The AI-ERA approach consists of two modes, namely offline and online modes. First, a deep neural network (DNN) model is trained with a dataset generated at ns-3 in the offline mode. Second, the proposed AI-ERA approach utilizes the pretrained DNN model in the online mode to proactively assign an efficient SF for the end device before each uplink packet transmission. The proactive behavior of the AI-ERA improved the packet success ratio by an average of 32% and 28% in static and mobility scenarios compared with the typical LoRaWAN ADR, respectively.},
  keywords={Internet of Things;Resource management;Artificial intelligence;Signal to noise ratio;Predictive models;Deep learning;Interference;Artificial intelligence;deep neural network;Internet of Things;LoRaWAN;resource allocation;the adaptive data rate},
  doi={10.1109/TII.2023.3248074},
  ISSN={1941-0050},
  month={Dec},}@INPROCEEDINGS{11189900,
  author={Chmaisseni, Mariam and Tabikh, Fatima and Ibrahim, Ali and Hassan, Houssein Hajj and Cherry, Ali. and Hajj-Hassan, Mohamad},
  booktitle={2025 Sixth International Conference on Advances in Computational Tools for Engineering Applications (ACTEA)}, 
  title={AI-Powered Navigation Assistance for the Visually Impaired}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper presents a wearable assistive device designed to support individuals who are blind or visually impaired by providing real-time information about surrounding objects and their distances. The system integrates camera-based object detection with depth estimation and delivers spoken feedback through headphones, enabling users to better perceive and navigate their environment. Initial evaluations demonstrate that the device can accurately recognize common objects and estimate distances, offering a practical advancement toward safer and more independent mobility.},
  keywords={Performance evaluation;Navigation;Depth measurement;Visual impairment;Refining;Object detection;Real-time systems;Stereo vision;Reliability;Wearable devices;Vision Impairment;Computer Vision;Stereo Vision;Object Detection;Depth Estimation},
  doi={10.1109/ACTEA66485.2025.11189900},
  ISSN={2993-3765},
  month={Sep.},}@INPROCEEDINGS{10888524,
  author={Wen, Wen and Wang, Yilin and Birkbeck, Neil and Adsumilli, Balu},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={An Ensemble Approach to Short-form Video Quality Assessment Using Multimodal LLM}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={The rise of short-form videos, characterized by diverse content, editing styles, and artifacts, poses substantial challenges for learning-based blind video quality assessment (BVQA) models. Multimodal large language models (MLLMs), renowned for their superior generalization capabilities, present a promising solution. This paper focuses on effectively leveraging a pretrained MLLM for short-form video quality assessment, regarding the impacts of pre-processing and response variability, and insights on combining the MLLM with BVQA models. We first investigated how frame pre-processing and sampling techniques influence the MLLM’s performance. Then, we introduced a lightweight learning-based ensemble method that adaptively integrates predictions from the MLLM and state-of-the-art BVQA models. Our results demonstrated superior generalization performance with the proposed ensemble approach. Furthermore, the analysis of content-aware ensemble weights highlighted that some video characteristics are not fully represented by existing BVQA models, revealing potential directions to improve BVQA models further.},
  keywords={Adaptation models;Analytical models;Refining;Predictive models;Signal processing;Robustness;Quality assessment;Ensemble learning;Speech processing;Videos;Video quality assessment;short-form video;multimodal large language model;content-aware ensemble},
  doi={10.1109/ICASSP49660.2025.10888524},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10101653,
  author={Paul, Laboni and Talukder, Kamrul Hasan},
  booktitle={2023 International Conference on Electrical, Computer and Communication Engineering (ECCE)}, 
  title={Blindness Risk Prediction caused by Diabetic Retinopathy from Retinal Image}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Type-II diabetes is growing at an alarming rate worldwide. Eventually, it leads to visual impairment by damaging the retinal blood vessels, which is known as Diabetic Retinopathy (DR). A plethora of research is ongoing to automate the early detection of DR to help the doctor with periodic eye examinations due to advancements in AI based computer vision algorithms and camera technology. In this paper, we have proposed a comparison between two widely used very deep convolutional neural network architectures (ResNet-101 v2, InceptionResNet V2) with the comparatively new optimized and composite scalable architecture (EfficientNet B5) with while training them from scratch and using them as a pre-trained to transfer learning. We have found out the pre-trained EfficientNet B5 has outperformed our other candidates as well as available methods in the current literature by achieving accuracy of 97.78%. We also provide detailed enough information to make the result reproducible.},
  keywords={Training;Retinopathy;Transfer learning;Visual impairment;Computer architecture;Blindness;Medical services;diabetic retinopathy detection;medical image analysis;transfer learning;APTOS 2019 Blindness Detection},
  doi={10.1109/ECCE57851.2023.10101653},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10704129,
  author={Madake, Jyoti and Sondur, Mrunal and Morey, Sumit and Naik, Atharva and Bhatlawande, Shripad},
  booktitle={2024 International Conference on Emerging Techniques in Computational Intelligence (ICETCI)}, 
  title={Comparative study of different LLM's for Captioning Images to Help Blind People}, 
  year={2024},
  volume={},
  number={},
  pages={9-15},
  abstract={The proposed system leverages LLMs, including GPT2, DistillGPT2, BERT, and RoBERTa, to provide detailed scene descriptions for the visually impaired. It employs an Encoder-Decoder architecture, with the Vision Transformer as the encoder and a distilled GPT-2 model as the decoder, facilitating the generation of comprehensive image captions. Training used a diverse dataset of around a hundred thousand samples on hardware equipped with an Nvidia Tesla V100 PCIE card and two Intel Xeon Silver CPUs. The model achieved a ROUGE score of 21.69%, signifying its potential to generate captions closely resembling human descriptions after training on suitable LLMs in the near future. This research has profound implications for enhancing the independence, education, and employment prospects of the visually impaired.},
  keywords={Training;Silver;Computer vision;Employment;Bidirectional control;Predictive models;Transformers;Encoding;Hardware;Decoding;GPT2;LLM;Self-attention;Tokens;ViT},
  doi={10.1109/ICETCI62771.2024.10704129},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10765167,
  author={Omary, Danah},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Multi-Modal, Accessible Mixed Reality Framework for Object Exploration and Accompanying Hardware Exploration}, 
  year={2024},
  volume={},
  number={},
  pages={660-661},
  abstract={With our research, propose a multi-modal Mixed Reality (MR) system framework with glove-based finger and hand movement tracking and tactile feedback, allowing blind and visually impaired individuals to have robust interactions with virtual objects that are not physically present. Our proposed framework will support interactions not only through haptics but also through other modalities including a personalized voice assistant audio interface, and visual interfaces. The several interaction modalities in our proposed MR framework and glove-based system would allow BVI users to experience virtual objects of any 3D model in greater depth without being constrained by their limited vision. However, contrary to common belief, most BVI are not fully blind and even prefer vision over other senses when possible. This means that visuals are still important and need to be used, though the system will be specifically designed to not completely rely on them. Our system framework’s features and modalities will both be configurable and thus enable a more customized experience that can be adapted to the various dynamic needs of both regular users and BVI users. In addition to successful implementation of the system framework, we propose the following research directions: explorations of the application of Machine Learning (ML) and Artificial Intelligence (AI) on sensor data for hand movement tracking, 3D model data, and voice data within the context of our framework, exploration of different haptic device implementations, and the exploration of hardware design for smartphone-based Augmented Reality (AR) Head-Mounted Displays (HMDs).},
  keywords={Solid modeling;Visualization;Three-dimensional displays;Target tracking;Personal voice assistants;Mixed reality;Tactile sensors;Hardware;Haptic interfaces;Augmented reality;Extended Reality;Virtual Reality;Augmented Reality;Mixed Reality Assistive Technology;Blind and Visually Impaired Accessibility;User Interface;Haptic Feedback},
  doi={10.1109/ISMAR-Adjunct64951.2024.00199},
  ISSN={2771-1110},
  month={Oct},}@INPROCEEDINGS{11118939,
  author={Isaac, Joshua and Santhiya, P and Archpaul, Jenefa and Mathu and A, Lincy and V, Ebenezer},
  booktitle={2025 International Conference on Computing Technologies (ICOCT)}, 
  title={HearTheSigns: AI-Based Road Sign Detection with Real-Time Voice Feedback for Blind}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={In this paper we propose HearTheSigns, an artificial intelligence (AI) powered road sign detection system designed to help visually challenged people in their movement by providing live feedback in voice. Blind People Cannot Recognize Vehicles on the Road to Cross it Safely. Navigation aids for the blind are not adapted to give non-visual feedback of any kind, according to specifications like dynamic sign detection. These systems don’t usually provide useful, real-time, contextual guidance required for safe independent travel. We present a novel use of a convolutional neural network (CNNs) coupled with real-time text-to-speech to detect a road sign and subsequently inform the user instantly. This approach uses a lightweight but powerful CNN architecture to allow for real-time processing on mobile devices. We have used the dataset that consists of diverse traffic sign images which is of the YOLOv8 style of annotated data. We have captured the images of the traffic signs under different weather conditions. Our model shows high accuracy and speed on this dataset, establishing its applicability for real-time use and improving upon existing methods. Our system achieved a 98.60% validation accuracy and works in real-time which makes it practical for mobile applications as per the experimental results. To conclude, HearTheSigns is a significant technological advance that offers a scalable and effective solution for enhancing the independence and safety of the visually impaired in urban settings.},
  keywords={Hands;Accuracy;Roads;Computational modeling;Assistive technologies;Real-time systems;Text to speech;Convolutional neural networks;Artificial intelligence;Vehicle dynamics;Road sign detection;deep learning;CNN;YOLOv8;assistive technology},
  doi={10.1109/ICOCT64433.2025.11118939},
  ISSN={},
  month={June},}@INPROCEEDINGS{9765294,
  author={Jain, Mayank and Kulkarni, Prasanna},
  booktitle={2022 International Conference on Decision Aid Sciences and Applications (DASA)}, 
  title={Application of AI, IOT and ML for Business Transformation of The Automotive Sector}, 
  year={2022},
  volume={},
  number={},
  pages={1270-1275},
  abstract={Automotive industry is essential in human lives. It is not possible to imagine a day without driving or some public transport. Today, digital technologies are making motor vehicles and the industry more intelligent. The entire value chain of automotive business is transforming. A better connect with customers is needed. All this is possible through advanced digital technologies. Automotive companies are overhauling business processes and relationships. Legacy IT systems for manufacturing, engineering, supply chain etc. are being reinvented. This transformation encompasses software, robotics, connected devices, and artificial intelligence. Artificial intelligence (AI) made the dream of self-driving cars possible. AI will soon transform every device. Tesla, Google Waymo, and Nvidia are examples of machine learning algorithms used to detect how far different objects are, from the car. Augmented reality (AR) and virtual reality (VR) analysis enables users to watch blind spots. AI enhances security by simultaneous coordination with many sensors. With AR, VR and mixed reality (MR), automotive companies have a personalized retail platform and a competitive edge. This paper studies AI applications in the automotive sector. It studies the recent developments, and applications of AI. It discusses how companies use AI for cost reduction, market strategies, sales promotion, and even funding.},
  keywords={Machine learning algorithms;Software algorithms;Companies;Software;Sensors;Safety;Manufacturing;Artificial Intelligence;Business Transformation;Automotive Sector;Digital Transformation},
  doi={10.1109/DASA54658.2022.9765294},
  ISSN={},
  month={March},}@INPROCEEDINGS{10464805,
  author={Khan, Muhammad Farhan and Iqbal, Adeel and Shakeel, Atif and Rashid, Adnan and Pesch, Dirk},
  booktitle={2023 IEEE Globecom Workshops (GC Wkshps)}, 
  title={Enhancing Industrial 4.0 Connectivity: A D2D-Based Algorithm for Blind Spot Mitigation in 5G Future Networks Enabled Smart Industry}, 
  year={2023},
  volume={},
  number={},
  pages={2012-2017},
  abstract={The Industry 4.0 is characterized by the integration of advanced and emerging technologies such as Fifth Generation (5G) and Next-Generation Networks & beyond, Artificial Intelligence (AI), Internet-of- Things (IoT), Machine Learning (ML), Robotics, Automation, Blockchain, Cloud Computing, Edge Computing, Augmented Reality (AR), Virtual Reality (VR) and many others. Despite the widespread adoption of 5G, Wireless Fidelity (Wi-Fi) routers, and other wireless technologies in the industry, there are still areas with limited or no coverage, commonly known as blind spots or coverage holes. The Device-to-Device (D2D) communication enables direct communication without a central entity, known as a coordinator or base station. By leveraging D2D communication, IoT nodes can serve as relays to deal with blind spots. In this research, we propose a novel D2D communication-based algorithm to cope with the challenge of blind spots in IoT -based smart industries. The algorithm focuses on nodes situated in blind spots by establishing connections to the network through nearby IoT nodes that function as relays. We demonstrate through simulations that our approach can effectively remove blind spots, improve coverage, increase throughput, and enhance overall system efficiency.},
  keywords={Industries;Wireless communication;Solid modeling;5G mobile communication;Service robots;Throughput;Mathematical models;Industry 4.0;IoT;D2D;5G;Blind Spots;Relay Devices;Smart Industry},
  doi={10.1109/GCWkshps58843.2023.10464805},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10823653,
  author={Bharti, Tushar and Singh, Shashwat and Chaubey, Ved Prakash and Gupta, Shivangini and Sharma, Shamneesh and Gochhait, Saikat},
  booktitle={2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)}, 
  title={Next-Gen Assistive Technology: AI Applications for Deaf-Mute and Blind Communities}, 
  year={2024},
  volume={},
  number={},
  pages={359-365},
  abstract={In today's interconnected world, communication gaps still exist between individuals with sensory impairments, specifically the blind and deaf-mute communities. This paper introduces BridgeAI, a novel assistive technology platform designed to overcome these communication barriers by leveraging cutting-edge AI technologies. BridgeAI employs Convolutional Neural Networks (CNN) for sign language recognition, speech-to-text, and text-to-speech models tailored for communication between the blind and deaf-mute individuals. The key innovation lies in the use of a custom-built dataset, enhanced through MediaPipe for real-time hand tracking, alongside an integrated Region of Interest (ROI)-based approach to improve gesture recognition accuracy. This study presents the architecture, implementation, and results of BridgeAI, which achieved a sign language recognition accuracy of 96.8%, demonstrating significant potential in improving accessibility for the blind and deaf-mute communities.},
  keywords={Technological innovation;Sign language;Accuracy;Assistive technologies;Real-time systems;Text to speech;Convolutional neural networks;Artificial intelligence;Informatics;Speech to text;convolutional neural networks (CNN);speech recognition;mediaPipe;openCV;artificial intelligence},
  doi={10.1109/3ict64318.2024.10823653},
  ISSN={2770-7466},
  month={Nov},}@INPROCEEDINGS{10404679,
  author={Navaneethan, S and Deepak Raj, N S and Gokula Raman, R S},
  booktitle={2023 2nd International Conference on Automation, Computing and Renewable Systems (ICACRS)}, 
  title={A Novel AI based Early Diabetic Retinopathy Detection using Retinal Images}, 
  year={2023},
  volume={},
  number={},
  pages={543-547},
  abstract={Diabetic Retinopathy (DR), a common complication of diabetes, is a leading cause of blindness. Early detection is important to prevent vision loss. This study explores the application of artificial intelligence (AI) for early diagnosis of DR from retinal images. We highlighted the importance of early detection, the limitations of traditional methods, and present a comparative analysis of different AI techniques including Convolutional Neural Networks (CNN), transfer learning, ensemble methods, explainable AI, and an integrated, AI-interpretable approach that integrates with clinical workflows. The results indicate that these AI methods have different levels of sensitivity, specificity, and usefulness. The challenges and future directions of AI in the field of DR are also briefly explained. The application of AI in DR detection has the potential to revolutionize screening processes, improve access and accuracy, thereby improving patient outcomes and minimizing the global impact of DR on with visual health.},
  keywords={Diabetic retinopathy;Visualization;Transfer learning;Retina;Convolutional neural networks;Ensemble learning;Artificial intelligence;Diabetic Retinopathy (DR);Transfer Learning;Ensemble Methods;Artificial Intelligence;Screening;Convolutional Neural Networks},
  doi={10.1109/ICACRS58579.2023.10404679},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10655378,
  author={Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs}, 
  year={2024},
  volume={},
  number={},
  pages={9568-9578},
  abstract={Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent MultiModal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify “CLIP-blind pairs”- images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.},
  keywords={Representation learning;Visualization;Computer vision;Systematics;Correlation;Grounding;Large language models;Multimodal LLMs;Vision Language Model},
  doi={10.1109/CVPR52733.2024.00914},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10828978,
  author={Geetha Rani, E and Guru Sneha, B and Harsha Vardhan, N and Sai Kumar, B and Anusha, D and Vadlamudi, Sandyarani},
  booktitle={2024 International Conference on Signal Processing and Advance Research in Computing (SPARC)}, 
  title={Generative AI-Based Currency Detector for Visually Impaired}, 
  year={2024},
  volume={1},
  number={},
  pages={1-6},
  abstract={A visually challenged person may find it challenging to distinguish between various denominations of currency. For those who are blind, this process is still challenging even though most of the Indian currency is written in unique characters. Portable solutions for isolation have emerged because of a shortage of available tools. This study details the creation and evaluation of a new assistive technology that uses an easy-to-use currency detection system to improve the financial independence of people with visual impairments. The apparatus employs sophisticated image processing techniques and deep learning algorithms to precisely recognize different banknote denominations. A compact, user-friendly interface that provides audio feedback to convey the denomination to the user ensures ease of use and privacy. A portable computer device connected to a high-resolution camera forms the basis of the system architecture. On this device, a convolutional neural network (CNN) model is trained using many datasets of cash photos in various orientations and light conditions. A high recognition accuracy rate is attained by the model, which is essential for real-world uses. Voice commands and tactile buttons work together to make it easy for users to engage with the gadget. Many daily living tasks are made more difficult by the prevalence of visual impairment, particularly those that require the recognition of currency, leaving the affected person dependent on others to complete financial transactions. Its ability to quickly and accurately identify different currencies and output format in audio feedback has proven to be highly beneficial for those with visual impairments who travel or live in multicultural environments. Moreover, positive feedback from user experience evaluations indicates that the system's practical impact extends beyond its technical achievements, with considerable improvements in confidence during financial transactions and ease of use. To ease that dependency, a unique Currency Detector System (CDS) made especially for visually impaired individuals has been created.},
  keywords={Privacy;Computational modeling;Visual impairment;Systems architecture;Signal processing algorithms;Detectors;Signal processing;User experience;Convolutional neural networks;Currencies;CNN;Gen ai;streamlit;LLM;Gemini AI},
  doi={10.1109/SPARC61891.2024.10828978},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11120880,
  author={Dhar, Shivam and Razdan, Niveadita and Razdan, Sidhi},
  booktitle={2025 IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)}, 
  title={SiftIQ: Unraveling the Ethical Dilemma of AI in Healthcare}, 
  year={2025},
  volume={},
  number={},
  pages={494-495},
  abstract={Artificial Intelligence (AI) is revolutionizing medicine, yet whether it is trustworthy remains an open question. Hidden beneath its complex algorithms are biases, unexplained decisions, and unpredictable risks that can mean the difference between life-saving treatments and harmful misdiagnoses. This paper outlines a systematic evaluation framework that puts healthcare AI in the dock—trialing bias, explainability, and reliability through targeted prompts and a risk matrix. By uncovering vulnerabilities and offering a clear path forward for improvement, our approach exceeds blind trust in AI, rendering these systems not only powerful but also fair, transparent, and truly safe for the patients who rely on them.CCS Concepts· Computing methodologies → Artificial intelligence.},
  keywords={Measurement;Ethics;Systematics;Explainable AI;Computational modeling;Medical services;Rendering (computer graphics);Reliability;Artificial intelligence;Connected health;artificial intelligence;ethical AI;explainable AI (XAI);biases in AI;AI evaluation},
  doi={},
  ISSN={2832-2975},
  month={June},}
