@INPROCEEDINGS{9788361,
  author={Shruti, S and M, Shravya and Mohan, Spurthi and Y, Suhail and R C, Radha},
  booktitle={2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
  title={AI-based Solutions for ADAS}, 
  year={2022},
  volume={},
  number={},
  pages={1009-1012},
  abstract={In today's time, the Advanced Driver Assistance System (ADAS) has become indispensable to top car manufacturers. With the constant evolution of vehicle safety systems, automobile manufacturers and consumers can envision a world where vehicle collisions are a thing of the past. Object detection is a crucial part of ADAS for pedestrian and vehicle detection, and for enabling various functions like automatic emergency braking, blind spot monitoring and so on. This paper aims to develop a You Only Look Once version 4 (YOLOv4)-based object detection model. This model is trained on our custom dataset which has ten object classes that one would find on Indian roads.},
  keywords={Roads;Computational modeling;Vehicle detection;Object detection;Predictive models;Real-time systems;Automobiles;Advanced Driver Assistance System (ADAS);Object detection;You Only Look Once (YOLO);Artificial Intelligence;Deep Learning},
  doi={10.1109/ICICCS53718.2022.9788361},
  ISSN={2768-5330},
  month={May},}@INPROCEEDINGS{9761308,
  author={Srivastava, Punit and Shrivastav, Priyanshi and Chaturvedi, Nikhil and Yadava, R.L.},
  booktitle={2022 International Conference on Smart Technologies and Systems for Next Generation Computing (ICSTSN)}, 
  title={Retracted: Performance Analysis of AI-based Optical guiding Device for visually compromised person}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={To defeat the voyaging trouble for an outwardly impeded individual in chiefly outside climate, this paper presents a thought of savvy directing gadget dependent on CNN and fuzzy to distinguish snags like bicycle, vehicle, individual and give an appropriate sound criticism to the client in the wake of working out speed, distance and in the wake of applying fuzzy to exact the result. Work is to diminish the different sensors of distance and speed estimating and perform calculation on pictures to set exact outcomes up to make it financially savvy. It is a difficult situation for blind individual to stroll on themselves in free space or in street of snags. Different techniques have appeared for directing them to move like headlock strategies, multi-sensors combination Algorithm, yet among every one of them because of utilization of sensors its size turns out to be enormous and furthermore get high in cost. To lessen this, we have presented different calculations for exact speed and distance estimations in order to give appropriate sound criticism.},
  keywords={},
  doi={10.1109/ICSTSN53084.2022.9761308},
  ISSN={},
  month={March},}@INPROCEEDINGS{11076231,
  author={Natarajan, Isaac and S, Hemanath and Micheal, A. Annie},
  booktitle={2025 7th International Conference on Intelligent Sustainable Systems (ICISS)}, 
  title={Enhanced Spatial Awareness Enhanced Through Smart AI Glasses}, 
  year={2025},
  volume={},
  number={},
  pages={700-704},
  abstract={The assistive technology, namely smart glasses, for visually impaired users. It effectively uses object detection and audio feedback in a novel manner. This thesis focuses on the enhancement of brand awareness through real-time audio cues concerning the surroundings for visually impaired users. There is a fairly straightforward design comprising of the Mounting of Pre-Trained YOLO11n for Object Detection and Camera on the Glasses. It is trained on the COCO dataset which contains various classes of objects making it suitable for real-world scenarios. Detected objects are further analyzed to develop audio feedback via speech synthesis. YOLO11n displayed effective performance in object detection and tagging in a controlled environment. The objects were detected and announced some tagged labels like "person," "chair," and "car." Confidence for objects like "bottle," "person," "cell phone," and "keyboard" was found to be 0.53, 0.66, 0.70, and 0.55, respectively. Preliminary tests have shown promising results in regard to the accuracy and speed. Sensor performance is hindered by ambient light changing and individuals intruding in the open in front. The project has brought forth the right kind of solution from the domain of assistive technologies for the blind. This project emphasizes the integration of computer vision and audio feedback that provides the disabled mobility and situational awareness. Future work shall include optimizing models for real-time practical applications, introduced diversity in recognized object types, and improvements to model’s stability in highly varying environmental conditions.},
  keywords={Computer vision;Computational modeling;Object detection;Glass;Assistive technologies;Tagging;Real-time systems;Stability analysis;Speech synthesis;Smart glasses;Visually impaired;computer vision;YOLO11n;COCO dataset;Object detection},
  doi={10.1109/ICISS63372.2025.11076231},
  ISSN={},
  month={March},}@INPROCEEDINGS{10893922,
  author={Shopika, S. and Senthil Velan, A S and SR, Dharun Prakash and S, Pavithra and Lakshmi, R Poornima and Ponnusamy, R.},
  booktitle={2024 International Conference on System, Computation, Automation and Networking (ICSCAN)}, 
  title={AI-Based Indian Currency Detection for Visually Challenged Users}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={For denomination recognition, this Indian Currency Detection System will use a CNN trained on an extremely large and diverse set of Indian banknotes from $$ 10 to $$ 2000. It possesses an amicable interface through which images can be captured using the camera of the user's smartphone, providing real-time audio feedback to denominate the banknote without requiring any visual assistance. Made with the features of capturing an image, real-time recognition, and user-friendly design, these are made to add a great experience for smooth navigation by a blind person. The novel and innovative features make this Indian Currency Detection System unique. Using CNN, which has been trained on the comprehensive set of Indian banknotes from $$10 to $$2000, the software lets a user capture banknote image directly by the smartphone camera. It allows knowing the denomination through real-time audio feedback without any visual assistance. Such characteristics include image capture, real-time identification, and designing user-friendly user interfaces that make easy navigation of the surroundings possible for the visually impaired. It does more than detect currency. It significantly contributes to the discussion regarding the development and availability of more assistive technologies. It is usable and aims to bring about greater accessibility and inclusivity in promoting financial independence among visually impaired people. Contributing to the ability of cash transactions among the visually impaired to gain confidence in freedom and security, this project aligns with a broader discussion related to assistive technologies.},
  keywords={Visualization;Image recognition;Navigation;Blindness;Assistive technologies;User interfaces;Cameras;Real-time systems;Security;Currencies;Indian Currency Detection;Visually Impaired;Image Recognition;Machine Learning;Convolutional Neural Network (CNN);Real-time Recognition;Text-to-Speech;Financial Autonomy},
  doi={10.1109/ICSCAN62807.2024.10893922},
  ISSN={},
  month={Dec},}@ARTICLE{11119643,
  author={Madadi, Yeganeh and Raja, Hina and Vermeer, Koenraad A. and Lemij, Hans G. and Huang, Xiaoqin and Kim, Eunjin and Lee, Seunghoon and Kwon, Gitaek and Kim, Hyunwoo and Kim, Jaeyoung and Galdran, Adrian and González Ballester, Miguel A. and Presil, Dan and Aguilar, Kristhian and Cavalcante, Victor and Carvalho, Celso and Sabino, Waldir and Oliveira, Mateus and Lin, Hui and Apostolidis, Charilaos and Katsaggelos, Aggelos K. and Kubrak, Tomasz and Casado-Garćıa, Á. and Heras, J. and Ortega, M. and Ramos, L. and Zhang, Philippe and Li, Yihao and Zhang, Jing and Jiang, Weili and Conze, Pierre-Henri and Lamard, Mathieu and Quellec, Gwenolé and Daho, Mostafa El Habib and Shaurya, Madukuri and Varma, Anumeha and Agrawal, Monika and Yousefi, Siamak},
  journal={IEEE Transactions on Medical Imaging}, 
  title={JustRAIGS: Justified Referral in AI Glaucoma Screening Challenge}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={A major contributor to permanent vision loss is glaucoma. Early diagnosis is crucial for preventing vision loss due to glaucoma, making glaucoma screening essential. A more affordable method of glaucoma screening can be achieved by applying artificial intelligence to evaluate color fundus photographs (CFPs). We present the Justified Referral in AI Glaucoma Screening (JustRAIGS) challenge to further develop these AI algorithms for glaucoma screening and to assess their efficacy. To support this challenge, we have generated a distinctive big dataset containing more than 110,000 meticulously labeled CFPs obtained from approximately 60,000 patients and 500 distinct screening centers in the USA. Our objective is to assess the practicality of creating advanced and dependable AI systems that can take a CFP as input and produce the probability of referable glaucoma, as well as outputs for glaucoma justification by integrating both binary and multi-label classification tasks. This paper presents the evaluation of solutions provided by nine teams, recognizing the team with the highest level of performance. The highest achieved score of sensitivity at a specificity level of 95% was 85%, and the highest achieved score of Hamming losses average was 0.13. Additionally, we test the top three participants’ algorithms on an external dataset to validate the performance and generalization of these models. The outcomes of this research can offer valuable insights into the development of intelligent systems for detecting glaucoma. Ultimately, findings can aid in the early detection and treatment of glaucoma patients, hence decreasing preventable vision impairment and blindness caused by glaucoma.},
  keywords={Glaucoma;Artificial intelligence;Visualization;Optical imaging;Biomedical optical imaging;Image color analysis;Blindness;Visual impairment;Multi label classification;Computer science;Artificial intelligence;justified referral glaucoma screening;classification task},
  doi={10.1109/TMI.2025.3596874},
  ISSN={1558-254X},
  month={},}@INPROCEEDINGS{11166011,
  author={Sow, Papa Ibrahima and Idy Diop, Pr and Sow, Saidou},
  booktitle={2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)}, 
  title={AI-Powered DR Screening in Africa: Bridging Deep Learning, DICOM Tools, and Healthcare Gaps}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Artificial intelligence (AI) is increasingly recognized as a promising approach for the early diagnosis of diabetic retinopathy (DR), a major cause of preventable blindness world-wide. In Senegal, and particularly in rural areas, DR remains a significant public health issue due to limited access to specialized care and diagnostic infrastructure. This review aims to provide a comprehensive and critical analysis of current deep learning-based approaches for DR screening, with a focus on their applicability in low-resource contexts. We examine the socio-demographic factors influencing disease prevalence, the potential of open-source platforms such as Orthanc for managing medical imaging data, and the performance of state-of-the-art AI models, including Convolutional Neural Networks (CNNs) such EfficientNet, GoogleNet, VGG16, etc. By highlighting existing gaps and challenges, this review outlines key considerations for the development of accessible, portable, and context-adapted AI tools for DR diagnosis in sub-Saharan Africa particularly in Senegal.},
  keywords={Diabetic retinopathy;Sensitivity;Reviews;Africa;Robustness;Ophthalmology;Artificial intelligence;Public healthcare;Secure storage;Interoperability;AI;Diabetic Retinopathy;Orthanc;Medical Imaging},
  doi={10.1109/ACDSA65407.2025.11166011},
  ISSN={},
  month={Aug},}@INBOOK{10953709,
  author={Raut, Roshani and Jadhav, Anuja and Jaiswal, Swati and Pathak, Pranav},
  booktitle={Intelligent Systems for Rehabilitation Engineering}, 
  title={IoT&#x2010;Assisted Smart Device for Blind People}, 
  year={2022},
  volume={},
  number={},
  pages={129-150},
  abstract={Summary <p>Those who are blind have a lot of trouble going through their daily lives. A lot of effort has gone into making it easier for blind people to complete tasks on their own rather than relying on others. With this inspiration in mind, we proposed and created an intelligent blind stick. The smart walking stick assists visually impaired people in identifying obstacles and getting to their destination. There are a variety of walking sticks and devices that assist users in moving around both indoor and outdoor environments, but none of them include run&#x2010;time autonomous navigation, object detection and identification warnings, or face and voice recognition. The stick uses IoT, echolocation, image processing, artificial intelligence, and navigation system technology to identify close and far obstacles for the user. If the blind person falls or has some other problem, then the system will send a warning to the designated person. The system uses voice recognition to recognise loved ones.</p>},
  keywords={Convolutional neural networks;Neurons;Convolution;Biological neural networks;Blindness;Feature extraction;Visualization;Speech recognition;Smart devices;Intelligent systems},
  doi={10.1002/9781119785651.ch6},
  ISSN={},
  publisher={Wiley},
  isbn={9781119785644},
  url={https://ieeexplore.ieee.org/document/10953709},}@INPROCEEDINGS{10411546,
  author={Malic, Vincent Quirante and Kumari, Anamika and Liu, Xiaozhong},
  booktitle={2023 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={Racial Skew in Fine-Tuned Legal AI Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={245-252},
  abstract={Rapid growth in the application of large language models to an immense variety of use-case scenarios has occurred alongside increasing concern that such models learn, encode, and perpetuate harmful social biases. This concern merits particular scrutiny in legal contexts, which involve search, research, and reasoning tasks that stand to benefit greatly from LLMs, but also demand adherence to rigorous standards of fairness and equality before the law. In this paper, we operationalize legal equality in the U.S. legal context as a "blind" language model that, when prompted to select a race to fill in a blank, assigns equal probability to all choices. We then fine-tune a pretrained GPT-2 model on multiple subsets of American case law texts, accounting for time and the political lean of each court’s host state. We identify and measure the degree to which these models are unfair, deviating from legal equality by learning to associate different races with different legal contexts.},
  keywords={Law;Conferences;Data models;Data mining;Task analysis;Standards;Context modeling;legal AI;large language models;case law;bias;fairness},
  doi={10.1109/ICDMW60847.2023.00037},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{10927778,
  author={Wroblewski, Claudia and Mirza, Arbaaz B. and Lin, Wenjun},
  booktitle={2024 International Conference on Computer and Applications (ICCA)}, 
  title={Bridging the Accessibility Gap in Online Shopping with AI - Driven Solutions}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Despite advancements in assistive technologies, blind and low vision (BLV) individuals continue to face significant challenges in online shopping, particularly with interpreting visual content and comparing products. Existing tools often lack seamless integration and intuitive interaction, hindering an equitable shopping experience. This paper introduces Shop Sight, a Chrome browser extension designed to enhance online shopping accessibility for BLV users. Shop Sight leverages artificial intelligence (AI) and voice-activated capabilities to generate context-rich product image descriptions and to facilitate simplified product comparisons through voice commands. The development and evaluation of Shop Sight demonstrate its potential to bridge the gap between current AI capabilities and the real-world needs of BLV users in e-commerce. By providing AI-generated image descriptions and voice-activated product comparisons, Shop Sight empowers BLV individuals with greater independence and a more personalized, efficient online shopping experience. Future work will focus on refining the tool's features, incorporating user feedback, and addressing identified limitations to further enhance its usability and effectiveness.},
  keywords={Visualization;Visual impairment;Refining;Personal voice assistants;Assistive technologies;Browsers;Electronic commerce;Speech synthesis;Artificial intelligence;Usability;Artificial intelligence;assistive technologies;blindness;browsers;electronic commerce;human computer interaction;personal voice assistants;speech synthesis;visual impairment},
  doi={10.1109/ICCA62237.2024.10927778},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10412506,
  author={Keshavarz, Mostafa and Ahmadi, Mohammad Javad and Naseri, Shima Sadat and Ghorbani, Parisa and Zadeh, Maryam Mohammad and Pour, Hossein Farokh and Mohammadi, Seyed Farzad and Taghirad, Hamid D.},
  booktitle={2023 11th RSI International Conference on Robotics and Mechatronics (ICRoM)}, 
  title={AI-Driven Keratoconus Detection: Integrating Medical Insights for Enhanced Diagnosis}, 
  year={2023},
  volume={},
  number={},
  pages={654-660},
  abstract={The proper diagnosis of keratoconus as a main ocular disorder is imperative for reducing the risk of vision blurring and potential blindness. Additionally, keratoconus (KC) significantly increases the risk of refractive eye surgeries. By using Artificial Intelligence (AI) and machine vision for early and automatic diagnosis, the risks and costs in medical systems could be significantly reduced. This study introduces an image dataset acquired by the Pentacam device, which is commonly used in keratoconus diagnosis. These images include four eye topography maps and have been labeled by three experts in two categories Suitable for refractive surgery and Non-suitable for refractive surgery. Distinguishing between these two categories in certain images poses a significant challenge. In the diagnosis of keratoconus, ophthalmologists perform a comprehensive evaluation of corneal topography maps and assign different levels of significance to each map. This research paper presents a novel algorithm that utilizes AI to imitate this medical insight. For this purpose, a regression network has been integrated into the classification algorithm to obtain the importance degree for each map. The degree of importance, which was created for each of the four maps, quantifies the level of attention given to each map in order to facilitate ideal classification. The suggested algorithm in this article utilizes a range of backbones as well as transfer learning techniques. The most favorable outcome was observed while utilizing the VGG backbone in the algorithm, yielding an F1-score of 95%, which is very promising.},
  keywords={Heuristic algorithms;Surgery;Surfaces;Feature extraction;Classification algorithms;Artificial intelligence;Medical diagnostic imaging;Keratoconus Diagnosis;Medical Image;Artificial Intelligence;CNN;Data Fusion},
  doi={10.1109/ICRoM60803.2023.10412506},
  ISSN={2572-6889},
  month={Dec},}@INPROCEEDINGS{10111863,
  author={S, Rajesh Kannan and P, Ezhilarasi and VG, Rajagopalan and Krishnamithran, Sushanth and H, Ramakrishnan and Balaji, Harish Kumar},
  booktitle={2023 International Conference on Recent Trends in Electronics and Communication (ICRTEC)}, 
  title={Integrated AI Based Smart Wearable Assistive Device for Visually and Hearing-Impaired People}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={An Intelligent Robotics Device (IRD) is developed in order help the disabled people and elderly people with day-to-day activities. The system offers five main functions: obstacle detection and avoidance through bone conduction, live tracking, GPS navigation, GSM SOS alert system and AI based image and face recognition System. It works with a combination of ultrasonic detection and bone conduction, detecting obstacles and letting the user know about them. To assure safe mobility, the product offers some core features namely, GPS live tracking and GPS navigation. GPS live tracking feature is useful in case if any of the user's relative wants to continuously monitor the movements of the blind user. The GSM - SOS alert system is included in the product, which comes into handy when the user finds any exigencies while travelling with just a push of button, the SOS alert system will send customized help request SMS along with location link to their career. GPS navigation system sends alert vibrations regarding the directions to be taken for navigating from one place to another. Those alerts are conveyed to the user through bone conduction phones. Finally, AI-facial and image recognition feature is also included which helps the blind to distinguish between known person and unknown person. All the electronic components which support these features are assembled and embedded on a wearable vest for ease of access.},
  keywords={GSM;Vibrations;Navigation;Tracking;Bones;Biomedical monitoring;Artificial intelligence;Artificial Intelligence;Machine Learning;GPS;GSM;Cloud;IoT;False Acceptance Rate (FAR);False Rejection Rate (FRR)},
  doi={10.1109/ICRTEC56977.2023.10111863},
  ISSN={},
  month={Feb},}@ARTICLE{9961911,
  author={Pereira, Filipe Dwan and Rodrigues, Luiz and Henklain, Marcelo Henrique Oliveira and Freitas, Hermino and Oliveira, David Fernandes and Cristea, Alexandra I. and Carvalho, Leandro and Isotani, Seiji and Benedict, Aileen and Dorodchi, Mohsen and de Oliveira, Elaine Harada Teixeira},
  journal={IEEE Transactions on Learning Technologies}, 
  title={Toward Human–AI Collaboration: A Recommender System to Support CS1 Instructors to Select Problems for Assignments and Exams}, 
  year={2023},
  volume={16},
  number={3},
  pages={457-472},
  abstract={Programming online judges (POJs) have been increasingly used in CS1 classes, as they allow students to practice and get quick feedback. For instructors, it is a useful tool for creating assignments and exams. However, selecting problems in POJs is time consuming. First, problems are generally not organized based on topics covered in the CS1 syllabus. Second, assessing whether problems require similar effort to be completed and map onto the same topic is a subjective and expert-dependent task. The difficulty increases if the instructor must create variations of these assessments, e.g., to avoid plagiarism. Thus, here, we research how to support CS1 instructors in the task of selecting problems, to compose one-size-fits-all or personalized assignments/exams. Our solution is to propose a novel intelligent recommender system, based on a fine-grained data-driven analysis of the students' effort on solving problems in the integrated development environment of a POJ system, and automatic detection of topics for CS1 problems, based on problem descriptions. Data collected from 2714 students are processed to support, via our artificial intelligence (AI) method recommendations, the instructors' decision-making process. We evaluated our method against the state of the art in a simple blind experiment with CS1 instructors ($N =$ 35). Results show that our recommendations are 88% accurate, surpassing our baseline ($p<$ 0.05). Finally, our work paves the way for novel POJ smart learning environments, wherein instructors define learning tasks (assignments/exams) supported by AI.},
  keywords={Task analysis;Programming;Codes;Geometry;Data structures;Recommender systems;Plagiarism;CS1 assessment;data driven;programming online judges (POJs);recommender systems;student effort;topic detection},
  doi={10.1109/TLT.2022.3224121},
  ISSN={1939-1382},
  month={June},}@INPROCEEDINGS{10560740,
  author={Lokesh Naikl, S. K. and Kiran, Ajmeera and Kumar, Vadapally Praveen and Mannam, Shanmukh and Kalyani, Yesarapu and Silparaj, Manda},
  booktitle={2024 International Conference on Science Technology Engineering and Management (ICSTEM)}, 
  title={Fraud Fighters - How AI and ML are Revolutionizing UPI Security}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Unified Payments Interface (UPI) is a highly efficient and securely encrypted monetary mechanism extensively deployed across India for the immediate transfer of funds between various bank accounts of the involved parties. However, with the rise of digital payments, UPI fraud has become a serious problem. Since more and more transactions are taking place online, scammers have discovered new ways to take advantage of holes in the system and steal money from blind people. The consequences of UPI fraud can be devastating for individuals and businesses alike. Not only does it result in financial losses, but it can also damage reputations and erode trust in the system. That's where AI and ML come in. By leveraging the power of artificial intelligence and machine learning, we can effectively detect potentially fraudulent Unified Payments Interface transactions. By using advanced algorithms and machine learning strategies, we can detect as well as prevent fraud more effectively than ever before. The goal of this project is to identify frauds by analyzing public data, managing imbalanced data, adapting to changes in fraud approaches, and minimizing false alarms. The primary goal is to utilize Machine Learning algorithms and Artificial Intelligence that have been recently developed for this objective.},
  keywords={Machine learning algorithms;Accuracy;Machine learning;Blindness;Fraud;Cryptography;Business;Unified Payment Interface (UPI);Encrypted Monetary Mechanism;Artificial Intelligence;Machine learning},
  doi={10.1109/ICSTEM61137.2024.10560740},
  ISSN={},
  month={April},}@INPROCEEDINGS{10932984,
  author={Shashanka, R and Devi, Anisha and Lakith Gowda, K and Kushal, S and Ravi Kumar, V},
  booktitle={2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)}, 
  title={VisioGuide: An AI for Visually Impaired}, 
  year={2025},
  volume={},
  number={},
  pages={1084-1090},
  abstract={In today's rapidly changing technological environment, there is a rising acknowledgment of the need for independent living for visually impaired people who confront major social inclusion issues. They may need human support to navigate these situations. Visually impaired people frequently lack access to critical information about their surroundings because visual information is crucial to most daily actions. Support for visually impaired people is on the rise; however, some factors bring about disparity in this support. Most of the recent developments in assistive technology create a platform to not only address this particular gap but also enhance, especially the support for the visually impaired. This research intends to develop an application for the Android platform to encourage self-reliance and social inclusion of the visually impaired. It uses current communication technologies, artificial intelligence, visual recognition, and machine learning. The application includes features like voice command, OCR, navigation, emergency call, screen reader, and connectivity to other service apps such as uber, among others. Moreover, it works in Kannada also to encourage its usage. The primary objective is to promote more freedom and inclusion among the sight impaired. This smart solution seeks to enhance the standards of living for blind or vision impaired people and motivate their active participation in modern technological advances by encouraging them to interact with the surroundings through the different features of the app.},
  keywords={Visualization;Sentiment analysis;Navigation;Optical character recognition;Urban areas;Speech recognition;Assistive technologies;Real-time systems;Text to speech;Standards;Assistive Technology;Artificial Intelligence(AI);Optical Character Recognition (OCR);Speech-to-text(STT);Text-to-Speech(TTS)},
  doi={10.1109/ICSADL65848.2025.10932984},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10780106,
  author={R, Sivaprasad and J, Jayashree and C, Yazhini and G S, Harini and K, Prathibanandhi and Yaashuwanth, C.},
  booktitle={2024 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)}, 
  title={Smart Goggles for the Visually Impaired}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={The goal of this project is to develop AI-powered smart glasses that will help people who are blind or visually impaired navigate and interact with their surroundings more successfully. The smart goggles combine cutting-edge natural language processing and computer vision technologies to offer text-to-speech conversion, face recognition, and real-time object detection. The glasses' built-in ultrasonic sensor determines how far away the subject must be from the object that will be the subject of the captured image in order to take a sharp picture. We have used Tinkercad, Proteus, and Python to simulate this.},
  keywords={Navigation;Face recognition;Object detection;Glass;Control systems;Real-time systems;Natural language processing;Text to speech;Eye protection;Smart glasses;Smart Goggles;Text-to-speech;Facial recognition;Object detection;Artificial Intelligence;Visually Impaired People (VIP)},
  doi={10.1109/ICPECTS62210.2024.10780106},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11089871,
  author={Vijay, Sreelakshmi and N, Hari Krishnan and T, Anjali},
  booktitle={2025 6th International Conference on Inventive Research in Computing Applications (ICIRCA)}, 
  title={Explainable AI for Diabetic Retinopathy Classification and Prediction}, 
  year={2025},
  volume={},
  number={},
  pages={1907-1913},
  abstract={Diabetic retinopathy (DR), a leading cause of preventable blindness, demands accurate and interpretable AI screening. A deep learning framework integrating Swin Transformers and Grad-CAM was developed to classify diabetic retinopathy (DR) severity and generate interpretable visual explanations. A hierarchical attention mechanism is used by the model to process retinal fundus images in order to capture pathological features at various scales. When tested on the Messidor dataset, it achieved a precision of $\mathbf{8 8. 4 6 \%}$ for severe cases and an overall accuracy of $\mathbf{9 5. 8 3 \%}$ across four severity levels. Grad- CAM's inclusion makes clinically meaningful heatmaps possible, emphasising areas linked to DR, Clinical decision-making is supported by this interpretability, which increases openness and confidence in AI-driven diagnostic tools. The framework solves a major issue in medical AI applications by striking a balance between explainability and performance. Although the findings are encouraging, more extensive validation in a range of clinical settings and populations is necessary to verify generalisability and clinical utility. This approach has the potential to enhance routine ophthalmic practice's early detection and DR grading.},
  keywords={Deep learning;Heating systems;Diabetic retinopathy;Visualization;Pathology;Accuracy;Explainable AI;Transformers;Retina;Medical diagnostic imaging;Diabetic retinopathy;deep learning;Explainable AI;Swin Transformer;Grad-CAM},
  doi={10.1109/ICIRCA65293.2025.11089871},
  ISSN={},
  month={June},}@INBOOK{10951710,
  author={Iyer, Rajiv and Bakshi, Aarti},
  booktitle={Deep Learning Tools for Predicting Stock Market Movements}, 
  title={Artificial Intelligence and Quantum Computing Techniques for Stock Market Predictions}, 
  year={2024},
  volume={},
  number={},
  pages={123-146},
  abstract={Summary <p>The financial crisis of 2008 had far&#x2010;reaching effects on the world economy. Repercussions of this event are seen today in the Indian economy. Fast forward to 2022, we are looking at another impending crisis in 2023 on similar lines. The thing that is different this time around is that progress in the fields of artificial intelligence and quantum computing has reached a level where we can make predictions in the stock market. This will, in turn, help us make informed decisions thus preventing losses at every level. In the fields of statistics and finance, the stock market is considered a complex nonlinear dynamic system with multiple variables. Various techniques have been developed to analyze and predict stock market behavior. Two such techniques are blind quantum computing (BQC) and quantum neural networks (QNNs).</p> <p>These techniques have been explored and studied in the context of stock price prediction and financial engineering applications. Researchers have developed models and algorithms, such as a quantum artificial neural network for stock closing price prediction and a hybrid deep QNN for financial predictions. Overall, these techniques leverage the power of quantum computing and neural networks to analyze and predict stock market behavior, offering potential benefits in the field of finance. The aim of this book chapter is to analyze the artificial intelligence&#x2013; and quantum computing&#x2013;based algorithms available for stock market predictions and propose the most accurate ones.</p>},
  keywords={Stock markets;Predictive models;Quantum computing;Deep learning;Artificial intelligence;Computational modeling;Quantum mechanics;Accuracy;Stability analysis;Protocols},
  doi={10.1002/9781394214334.ch5},
  ISSN={},
  publisher={Wiley},
  isbn={9781394214327},
  url={https://ieeexplore.ieee.org/document/10951710},}@INPROCEEDINGS{10704868,
  author={Lowe, Jordon and Kuru, Kaya},
  booktitle={2024 20th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications (MESA)}, 
  title={Design & Development of a Smart Blind System Using Fuzzy Logic}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Thanks to cyber-physical systems (CPSs) and enhanced AI techniques, the "everyday things" in our environment have become increasingly intelligent in recent years in the aspects of Automation of Everything (AoE). An IoT application for a smart blind system (SBS) using fuzzy logic to intelligently adjust to an optimal position based on data from local sensors, Wisdom-as-a-Service (WaaS) and Insight-as-a-Service (InaaS) from smart domains and customer preferences has been developed. Testing of this system as a proof-of-concept yields promising results, as the fuzzy logic can effectively control the blind to a position that is desirable under the given environmental conditions. Further, all the input data is utilised productively, ensuring that decisions made by the system are well-informed. Compared to competitors’ offerings, the proposed system offers superior performance due to its extensive input consideration and the precise design of the fuzzy logic for automated decision-making.},
  keywords={Fuzzy logic;Fuzzy sets;Automation;Decision making;Cyber-physical systems;User interfaces;Sensor systems and applications;Cognition;Intelligent sensors;Testing;Fuzzy Logic;Smart Technology;Automation of Everything (AoE);Internet of Things (IoT);Smart City;Smart Building;Smart Blind;Automated Decision-Making;Approximate Reasoning},
  doi={10.1109/MESA61532.2024.10704868},
  ISSN={2835-902X},
  month={Sep.},}@INPROCEEDINGS{10465276,
  author={Ziryawulawo, Ali and Mduma, Neema and Lyimo, Martine and Mbarebaki, Adonia and Madanda, Richard and Sam, Anael},
  booktitle={2023 First International Conference on the Advancements of Artificial Intelligence in African Context (AAIAC)}, 
  title={An Integrated Deep Learning-based Lane Departure Warning and Blind Spot Detection System: A Case Study for the Kayoola Buses}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Deep learning-based driver assistance systems (ADAS) have attracted interest from researchers due to their impact on improving vehicle safety and reducing road traffic accidents. In Uganda, road accidents have continued to soar with an increase of up to 42% in 2021 due to the growing road traffic density. To curb the high rates of road accidents, especially for heavy-duty vehicles, Kiira Motors Corporation a state-owned mobility solutions enterprise needs advanced driver assistance systems for improved safety of their market entry products- the Kayoola buses. This research presents an approach to vehicular safety enhancement through the integration of Lane Departure Warning (LDW) and Blind Spot Detection systems (BSD) using advanced deep learning algorithms. The resultant LDW and BSD system is realized on the Raspberry Pi platform, incorporating diverse sensors. By combining these advanced features, the study not only bridges an essential research void but also offers a practical resolution to pressing road safety concerns in the East African context. The integration of LDW and BSD systems through deep learning techniques marks a pivotal advancement in vehicular safety. The lane detection model was tested on DET and TuSimple datasets. Our model attained a mean F1 Score of 77.59% and a mean IoU of 65.26% on the Dataset for Lane Extraction (DET) and an overall accuracy of 97.96% on the TuSimple dataset. Our work presents an integrated lane departure warning and blind spot detection system that will be able to alert the driver using the graphical user interface, and auditory feedback. The anticipated real-world implementation is poised to substantiate the system’s effectiveness, thereby contributing to safer roads regionally and inspiring innovation in automotive engineering by leveraging artificial intelligence.},
  keywords={Deep learning;Technological innovation;Road accidents;Sensor systems;Road traffic;Road safety;Real-time systems;Blind Spot Detection;Deep Learning;Autonomous Vehicle;Lane Departure Detection},
  doi={10.1109/AAIAC60008.2023.10465276},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11172166,
  author={Sayduzzaman, Mohammad and Sadik, Shibly and Hasan, Samiul and Rahman, Tawhidur and Rahman, Anichur and Kundu, Dipanjali and Tasnim Tamanna, Jarin and Rahman, Muaz},
  booktitle={2025 International Conference on Quantum Photonics, Artificial Intelligence, and Networking (QPAIN)}, 
  title={Tokenizing Trust: Leveraging Blockchain and AI to Develop Next-Generation Two-Factor Authentication}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Strong, scalable, and transparent authentication systems are desperately needed, as seen by the rapid development of digital ecosystems. In order to improve security and user trust, The paper introduces an adaptive risk-based framework for next-generation two-factor authentication (2FA), combining blockchain-based blind tokenization with AI-driven risk assess-ment. Further, this article integrates AI algorithms to evaluate contextual and behavioral patterns in real-time, while securing the second authentication factor through decentralized token generation and storage on a blockchain network. While tokenization substitutes dynamic cryptographically protected tokens for traditional static credentials, the blockchain component offers an immutable ledger that guarantees data integrity and resistance to tampering. The AI module concurrently keeps an eye on usage trends and irregularities to reduce threats like phishing and man-in-the-middle assaults. Initial simulations show that the combined method improves user authentication effectiveness without sacrificing usability and lessens typical weaknesses seen in conventional 2FA systems. By providing a decentralized, flexible, and affordable solution that balances the requirements of privacy, real-time intelligence, and secure distributed ledger technology, this study adds to the developing field of security architectures.},
  keywords={Privacy;Smart contracts;Authentication;Tokenization;Real-time systems;Blockchains;Artificial intelligence;Usability;Next generation networking;Engines;Blockchain;Tokenization;Two-Factor Authentication;AI;Smart Contracts;Cybersecurity;Digital Trust;Anomaly Detection;Behavioral Biometrics},
  doi={10.1109/QPAIN66474.2025.11172166},
  ISSN={},
  month={July},}@INPROCEEDINGS{10206191,
  author={Mendis, G. L. M. M. and Deshan, W. M. Y and Bandara, H. M. G. M. and Gunethilake, K. C. and Wijendra, Dinuka and Krishara, Jenny},
  booktitle={2023 6th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={Look AI – An Intelligent System for Socialization of Visually Impaired}, 
  year={2023},
  volume={},
  number={},
  pages={351-356},
  abstract={It has found that 30% of the people among the visually impaired people are having a significant number of depressive symptoms whereas, the prevalence of depression in blind people was reported to be 33%, or nearly double the rate of the general population. Therefore, social activities among visually impaired people should increase. This research paper provides a unique system for assisting visually impaired individuals in navigating indoor environments heavily relies on the use of artificial intelligence, particularly deep learning and computer vision techniques. By leveraging these advanced technologies, the system can analyze and process large amounts of visual data in real-time, accurately identifying and locating various objects in the environment, including faces, doors, stairs, cross walks, traffic lights, potholes and other obstacles. The system then provides real-time feedback to the user via an audio interface, enabling them to navigate indoor and outdoor environments safely and independently. The use of artificial intelligence in this system is particularly significant, as it enables the system to adapt and improve over time based on user feedback and additional data. That means, the system can continuously improve its performance and accuracy, ultimately resulting in a more effective and reliable tool for visually impaired individuals. The authors have also evaluated the system in a real-world setting, demonstrating its effectiveness in assisting visually impaired individuals with indoor navigation. Overall, the proposed system has the potential to significantly enhance the quality of life for visually impaired individuals, showcasing the transformative power of artificial intelligence in addressing real-world challenges.},
  keywords={Visualization;Machine learning algorithms;Local government;Smart homes;Organizations;Stairs;User experience;artificial intelligence;machine learning;convolutional neural networks;recurrent neural network},
  doi={10.1109/ICAIBD57115.2023.10206191},
  ISSN={2769-3554},
  month={May},}@INPROCEEDINGS{10872363,
  author={Hariprasad, S. and Bharathiraja, N. and T, Deepa. and Nehaa, R. K. and Samuel, Daniel Joseph and Dhivya Sri, S. N.},
  booktitle={2024 4th International Conference on Mobile Networks and Wireless Communications (ICMNWC)}, 
  title={AIoT Blind Stick Based Independent Navigation with Enhanced YOLO Object Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Nowadays, Artificial Intelligence (AI) and the Internet of Things (IoT) herald a transformative leap in assistive technologies, epitomized by the AIoT Blind Stick. This smart mobility aid the computational prowess of the Raspberry Pi to process a multitude of sensory inputs, delivering realtime environmental awareness to visually impaired users. The system provides auditory and haptic feedback to navigate complex urban landscapes safely and is equipped with GPS for location tracking, ultrasonic sensors for obstacle detection, and a camera interfaced with advanced object detection algorithm using YOLO8. Voice Based assistance given based on the object detected at each frame. The proposed YOLOv8m model demonstrates superior detection performance, achieving the highest Average Precision (AP) of 50.4 among the evaluated models. Although the frame rate (FPS) of 137 is slightly lower than that of the smaller models (YOLOv5s and YOLOv8s), it provides a significant balance between accuracy and processing speed, making it highly suitable for real-time object detection applications where precision is critical, such as assistive technologies for the visually impaired.},
  keywords={YOLO;Navigation;Heuristic algorithms;Assistive technologies;Real-time systems;Internet of Things;Reliability;Artificial intelligence;Intelligent sensors;Sensor arrays;assistive technology;aiot;smart navigation devices;realtime object detection;yolo algorithm},
  doi={10.1109/ICMNWC63764.2024.10872363},
  ISSN={},
  month={Dec},}@ARTICLE{10916582,
  author={Bahrami Moqadam, Saeed and Saleh Asheghabadi, Ahmad and Xu, Jing},
  journal={IEEE Sensors Journal}, 
  title={A Novel Approach to Material Detection for Blind Amputee Users With Hand Prosthetic}, 
  year={2025},
  volume={25},
  number={8},
  pages={14415-14426},
  abstract={The lack of sensory feedback in upper limb prostheses restrains multiple disability individuals (MDIs) from interacting effectively with their environment. This study proposes a human-computer interaction (HCI)-based framework that uses auditory feedback to overcome visual limitations in recognizing material of objects with a customized hand prosthesis, using a material detection sensor (MDS) to record acoustic and reaction force signals simultaneously. We use clustering techniques to group acoustic data based on reaction force signals clustered (C). The constant amplitude is converted into instantaneous amplitude by calculating each cluster’s mean peak envelope (MPE). We then apply an absolute fast Fourier transform (AFFT) to transfer the signal into the frequency domain. Finally, acoustic features are extracted, specifically, Mel frequency cepstral coefficients (MFCCs), fed into the artificial intelligence (AI) system. The study evaluates ten participants: three upper limb amputees, two blind amputees MDIs, and five nondisabled individuals as the control group. The results demonstrated that the C-MPE–AFFT-MFCC bidirectional long short-term memory (BiLSTM) algorithm outperforms more accuracy than artificial neural network (ANN), linear discrimination analysis (LDA), support vector machine (SVM), and convolutional neural network (CNN) with no significant difference between groups in classification accuracy (CA) (P-value =0.0105). MDIs and the control group classified 30 objects with 92.46% and 97.57% accuracy, respectively, corresponding to a 94.97% reduction in execution time compared with conventional methods. The proposed method could greatly enhance MDIs’ ability to use sophisticated hand prostheses.},
  keywords={Robot sensing systems;Sensors;Hands;Acoustics;Visualization;Force;Mel frequency cepstral coefficient;Fast Fourier transforms;Artificial intelligence;Thumb;Auditory feedback;clustered (C)-mean peak envelope (MPE)-absolute fast Fourier transform (AFFT)-Mel frequency cepstral coefficient (MFCC)-bidirectional long short-term memory (BiLSTM);human-computer interaction (HCI);material detection sensor (MDS);multiple disabilities’ individuals;sensory substitution},
  doi={10.1109/JSEN.2025.3546613},
  ISSN={1558-1748},
  month={April},}@ARTICLE{9745592,
  author={Kunumpol, Patthapol and Lerthirunvibul, Nichapa and Phienphanich, Phongphan and Munthuli, Adirek and Temahivong, Kanjapat and Tantisevi, Visanee and Manassakorn, Anita and Chansangpetch, Sunee and Itthipanichpong, Rath and Ratanawongphaibul, Kitiya and Rojanapongpun, Prin and Tantibundhit, Charturong},
  journal={IEEE Access}, 
  title={GlauCUTU: Time Until Perceived Virtual Reality Perimetry With Humphrey Field Analyzer Prediction-Based Artificial Intelligence}, 
  year={2022},
  volume={10},
  number={},
  pages={36949-36962},
  abstract={The increasing prevalence of glaucomatous optic neuropathy, which can result in permanent blindness (visual impairment), accentuates the importance of screening and early diagnosis for prevention of blindness. GlauCUTU, a novel time until perceived (TUP) visual field (VF) testing, utilizes a portable virtual reality (VR) headset with visual stimulus that progressively increases in intensity to detect VF defects. GlauCUTU was evaluated on participants with normal visual fields and those with early, moderate, and severe glaucoma. Responses were collected in terms of time until response (TUR). TUR was used to calculate TUP and reported in terms of GlauCUTU sensitivity. False positives were detected with pretest and latency analysis using reaction time (RT). In addition, a novel automated transformation was developed to convert GlauCUTU sensitivity into HFA sensitivity using machine learning (ML) and deep learning (DL) algorithms. Visual field index (VFI) was generated from HFA sensitivity to determine severity of glaucoma. The VFI results were evaluated using post-hoc analysis from two-way analysis of variance (ANOVA). Results demonstrate no significant difference (p=0.073) between Humphrey visual field analyzer (HFA) and GlauCUTU with machine learning transformation (GlauCUTU-ML) in all glaucoma stages. However, there was a significant difference between HFA and GlauCUTU with deep learning transformation (GlauCUTU-DL) in severe glaucoma (p<0.050). GlauCUTU-ML generates the lowest root mean square error (RMSE) of 4.92. Meanwhile, GlauCUTU-DL yields the highest Pearson’s  $r$  correlation coefficient with HFA of 0.74, but produces the highest RMSE of 6.31. Comparison between three expert ophthalmologists’ grading of glaucomatous eyes on GlauCUTU-ML and HFA aligns with the majority voting with an average agreement of 0.83, which is highly reliable. All in all, the portable and inexpensive GlauCUTU perimetry system introduces the use of TUP for VF evaluation with results comparable to HFA. GlauCUTU proves to be a promising method to increase accessibility to glaucoma screening, particularly in low-resource setting countries.},
  keywords={Sensitivity;Visualization;Lenses;Blindness;Monitoring;Headphones;Virtual reality;Virtual reality;glaucoma;visual defect;visual field test;portable perimetry},
  doi={10.1109/ACCESS.2022.3163845},
  ISSN={2169-3536},
  month={},}@ARTICLE{10045656,
  author={Lima, Rui and Barreto, Luís and Amaral, António and Paiva, Sara},
  journal={IEEE Sensors Journal}, 
  title={Visually Impaired People Positioning Assistance System Using Artificial Intelligence}, 
  year={2023},
  volume={23},
  number={7},
  pages={7758-7765},
  abstract={Blindness and visual impairment are commonly associated with social and functional limitations, almost 45 million people in the world have blindness, and 135 million have any visual impairment. This condition has a significant impact on the quality of life and brings many challenges to the individual, one of which is the navigation and positioning tasks. Although there are already apps capable of helping visually impaired people (VIP) for mobility purposes, most of them focus on detecting obstacles and, therefore, on avoiding dangerous situations. However, mobility of VIP involves many more tasks, such as knowing their exact position and staying informed along an entire route. For this purpose, a standalone and customizable solution is proposed that uses traditional visual recognition of landmarks to process the surroundings of the current location of the visually impaired person using a smartphone and informing about the nearby places assuring the user a sense of the site. For feature detection, it used the oriented features from accelerated segment test (FAST) and rotated binary robust-independent elementary feature (BRIEF) (ORB) algorithm, and for feature matching, it used the brute-force method with the k-nearest neighbor (KNN) algorithm. Results show that the proposed solution can analyze pictures in fractions of a second with satisfactory accuracy.},
  keywords={Visualization;Feature extraction;Currencies;Classification algorithms;Navigation;Software algorithms;Blindness;Blindness;feature detection;feature matching;landmark detection;oriented features from accelerated segment test (FAST) and rotated binary robust-independent elementary feature (BRIEF) (ORB) algorithm;visual impairment},
  doi={10.1109/JSEN.2023.3244128},
  ISSN={1558-1748},
  month={April},}
