@INPROCEEDINGS{11063736,
  author={Sharma, Sumit and Sharma, Kanta Prasad and Saini, Kavita},
  booktitle={2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)}, 
  title={Integrating Explainability in AI for Retinal Imaging: Enhancing Diabetic Retinopathy Diagnosis Accuracy}, 
  year={2025},
  volume={3},
  number={},
  pages={2083-2086},
  abstract={Diabetic retinopathy (DR: preventable cause of blindness till date as it is irreversible till now.) Widely available adoption of clinical features is hard because traditional models have not been interpretable; AI (Artificial Intelligence) has so far made an exception in automating diagnosis for diabetic retinopathy. Explainable AI enables the generation of raw, simple results by machines which one as a human might agree on and hence gets re-empowers physician. In this paper, Explainable AI framework is discussed in detail. This is a novel approach to deal with the diabetic retinopathy which deals with various issues such as low-labelled-data requirement, lack of validations among various populations and sub-optimal performance in real and unseen data. The paper presents the framework that includes scalable models for resource-constrained environments, multistage detection network and differential-patient diagnosis. The framework is also generalize with the latest interpretability frameworks (i.e. Grad-CAM and LIME) where explanation can be given to clinically grey matter while keeping accuracy. Clinical multi-centre research done shown that near 94.5% accuracy versus current 88% solutions currently using the framework can lead to substantial improvements in the diagnosis from the framework. Furthermore proof-of-concept studies in a smaller setting have shown that it is scalable and physician surveys found 40% increase in trust so too. The vast and critical knowledge gaps are filled by the presented work, allowing for solution to enhance the diagnostic scalability accuracy and transparency. These results explain the support of explainable AI towards the retinal imaging forward, especially at diabetic retinopathy diagnosis and will towards new breakthroughs in clinical predictive AI.},
  keywords={Surveys;Diabetic retinopathy;Accuracy;Explainable AI;Scalability;Imaging;Grey matter;Medical services;Retina;Security;XAI;Explainable Artificial Intelligence (XAI);Diabetic Retinopathy (DR);Retinal Imaging;Grad-CAM;LIME;Multi-Stage Detection;Personalized Diagnosis;AI Scalability;Clinical Trust;Healthcare Diagnostics},
  doi={10.1109/ICCSAI64074.2025.11063736},
  ISSN={},
  month={April},}@ARTICLE{10520989,
  author={Zhou, Tianwei and Tan, Songbai and Zhou, Wei and Luo, Yu and Wang, Yuan-Gen and Yue, Guanghui},
  journal={IEEE Transactions on Broadcasting}, 
  title={Adaptive Mixed-Scale Feature Fusion Network for Blind AI-Generated Image Quality Assessment}, 
  year={2024},
  volume={70},
  number={3},
  pages={833-843},
  abstract={With the increasing maturity of the text-to-image and image-to-image generative models, AI-generated images (AGIs) have shown great application potential in advertisement, entertainment, education, social media, etc. Although remarkable advancements have been achieved in generative models, very few efforts have been paid to design relevant quality assessment models. In this paper, we propose a novel blind image quality assessment (IQA) network, named AMFF-Net, for AGIs. AMFF-Net evaluates AGI quality from three dimensions, i.e., “visual quality”, “authenticity”, and “consistency”. Specifically, inspired by the characteristics of the human visual system and motivated by the observation that “visual quality” and “authenticity” are characterized by both local and global aspects, AMFF-Net scales the image up and down and takes the scaled images and original-sized image as the inputs to obtain multi-scale features. After that, an Adaptive Feature Fusion (AFF) block is used to adaptively fuse the multi-scale features with learnable weights. In addition, considering the correlation between the image and prompt, AMFF-Net compares the semantic features from text encoder and image encoder to evaluate the text-to-image alignment. We carry out extensive experiments on three AGI quality assessment databases, and the experimental results show that our AMFF-Net obtains better performance than nine state-of-the-art blind IQA methods. The results of ablation experiments further demonstrate the effectiveness of the proposed multi-scale input strategy and AFF block.},
  keywords={Visualization;Distortion;Task analysis;Quality assessment;Feature extraction;Adaptation models;Image quality;AI-generated images;blind image quality assessment;adaptive feature fusion;multi-scale feature},
  doi={10.1109/TBC.2024.3391060},
  ISSN={1557-9611},
  month={Sep.},}@INPROCEEDINGS{10930049,
  author={Hasan, Md. Manzurul and Hossain, Shahadat and Mohammad, Rafeed and Hossain, Gahangir},
  booktitle={2025 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={JIBON++: AI Enabled Intelligent Voice Assistant for Blind People Understanding Negative Sentiments}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Computer-assisted language interpretation is an increasingly promising field, leveraging Natural Language Processing (NLP) to handle spoken and written data. This research introduces a method tailored for discovering valuable skills within Bangla intelligent voice assistants. The primary objective of language processing, whether in voice or text form, remains understanding underlying information. Our method entails creating an artificial intelligence system that deduces meanings from input formats. Although there are already Bangla voice assistants available, our approach improves accuracy substantially. Interestingly, we concentrate on identifying negative comments and train our model to react correctly in those kinds of situations-a domain that hasn't gotten much attention in voice command interpretation. Our model demonstrates its ability to accurately estimate output, achieving a precision rate of 96% with a loss of about 5%. This study aims to bridge the existing gap in understanding and enhance knowledge within the specific domain of contextual information accessibility for individuals who are blind. Our proposed solution not only facilitates comprehension of audible negative contexts as exceptions but also effectively addresses complex navigation challenges faced by blind users. Furthermore, the findings of this research highlight promising directions for future exploration in this field.},
  keywords={Sentiment analysis;Accuracy;Systematics;Social networking (online);Navigation;Personal voice assistants;Artificial neural networks;Internet;Artificial intelligence;Time complexity;Voice Assistant;Natural Language Processing (NLP);Deep Neural Network (DNN);Assistive Technology;Context Understanding},
  doi={10.1109/ICCE63647.2025.10930049},
  ISSN={2158-4001},
  month={Jan},}@ARTICLE{9697978,
  author={Pan, Jianxiong and Ye, Neng and Yu, Hanxiao and Hong, Tao and Al-Rubaye, Saba and Mumtaz, Shahid and Al-Dulaimi, Anwer and Chih-Lin, I.},
  journal={IEEE Transactions on Wireless Communications}, 
  title={AI-Driven Blind Signature Classification for IoT Connectivity: A Deep Learning Approach}, 
  year={2022},
  volume={21},
  number={8},
  pages={6033-6047},
  abstract={Non-orthogonal multiple access (NOMA) promises to fulfill the fast-growing connectivities in future Internet of Things (IoT) using abundant multiple-access signatures. While explicitly notifying the utilized NOMA signatures causes large signaling cost, blind signature classification naturally becomes a low-cost option. To accomplish signature classification for NOMA, we study both likelihood- and feature-based methods. A likelihood-based method is firstly proposed and showed to be optimal in the asymptotic limit of the observations, despite high computational complexity. While feature-based classification methods promise low complexity, efficient features are non-trivial to be manually designed. To this end, we resort to artificial intelligence (AI) for deep learning-based automatic feature extraction. Specifically, our proposed deep neural network for signature classification, namely DeepClassifier, establishes on the insights gained from the likelihood-based method, which contains two stages to respectively deal with a single observation and aggregate the classification results of an observation sequence. The first stage utilizes an iterative structure where each layer employs a memory-extended network to explicitly exploit the knowledge of signature pool. The second stage incorporates the straight-through channels within a deep recurrent structure to avoid information loss of previous observations. Experiments show that DeepClassifier approaches the optimal likelihood-based method with a reduction of 90% complexity.},
  keywords={NOMA;Feature extraction;Transmitters;Modulation;Deep learning;Uplink;Wireless communication;Non-orthogonal multiple access;signature classification;deep learning;recurrent neural network;automatic feature extraction},
  doi={10.1109/TWC.2022.3145399},
  ISSN={1558-2248},
  month={Aug},}@INPROCEEDINGS{10801606,
  author={Hao, Yu and Magay, Alexey and Huang, Hao and Yuan, Shuaihang and Wen, Congcong and Fang, Yi},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={ChatMap: A Wearable Platform Based on the Multi-modal Foundation Model to Augment Spatial Cognition for People with Blindness and Low Vision}, 
  year={2024},
  volume={},
  number={},
  pages={129-134},
  abstract={Spatial cognition refers to the ability to gain knowledge about their surroundings and utilize this information to identify their location, acquire resources, and navigate their way back to familiar places. People with blindness and low vision (pBLV) face significant challenges with spatial cognition due to the reliance on visual input. Without the full range of visual cues, pBLV individuals often find it difficult to grasp a comprehensive understanding of their environment, leading to obstacles in scene recognition and precise object localization, especially in unfamiliar environments. This limitation extends to their ability to independently detect and avoid potential tripping hazards, making navigation and interaction with their environment more challenging. In this paper, we present a pioneering wearable platform tailored to enhance the spatial cognition of pBLV through the integration of multi-modal foundation model. The proposed platform integrates a wearable camera with audio module and leverages the advanced capabilities of vision language foundation model (i.e., GPT-4 and GPT-4V), for the nuanced processing of visual and textual data. Specifically, we employ vision language models to bridge the gap between visual information and the proprioception of visually impaired users, offering more intelligible guidance by aligning visual data with the natural perception of space and movement. Then we apply prompt engineering to guide the large language model to act as an assistant tailored specifically for pBLV users to produce accurate answers. Another innovation in our model is the incorporation of a chain of thought reasoning process, which enhances the accuracy and interpretability of the model, facilitating the generation of more precise responses to complex user inquiries across diverse environmental contexts. To assess the practical impact of our proposed wearable platform, we carried out a series of real-world experiments across three tasks that are commonly challenging for people with blindness and low vision: risk assessment, object localization, and scene recognition. Additionally, through an ablation study conducted on the VizWiz dataset, we rigorously assess the contribution of each individual module, substantiating the integral role in the model’s overall performance.},
  keywords={Location awareness;Visualization;Foundation models;Navigation;Large language models;Blindness;Cognition;Risk management;Prompt engineering;Context modeling},
  doi={10.1109/IROS58592.2024.10801606},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{9776881,
  author={Alagarsamy, Saravanan and Kusuma, Bendela and Mohan, Cheedella Venkata Naga and Reddy, Gudala venkata Parameswara and Sukumar, Malleboina Venkata and Babu, Dora Veera Venkata Sai Sri Sujan and Devendrareddy, Musalappagari},
  booktitle={2022 6th International Conference on Trends in Electronics and Informatics (ICOEI)}, 
  title={Smart System for Reading the Bar Code using Bayesian Deformable Algorithm for Blind People}, 
  year={2022},
  volume={},
  number={},
  pages={424-429},
  abstract={The primary goal of this research is to offer an AI-based system to aid the blind people for identify the text labels and detail of product on hand-held entities in their regular lives. This initiative was created to make the lives of blind people easier. This is a camera-based technology that scans the bar-code behind the image and skims the contour of the product with the help of the Id keep within the bar-code. This is particularly beneficial when searching for the outline of supermarket items for blind people and assisting them in deciding whether or not to purchase a product, particularly those that are wrapped. This is frequently as a result of it becoming extremely difficult for blind people to distinguish between different things. To employ this method, all the consumer has to do is capture an image of the product on their mobile device, which then resolves the bar-code, implying it scans the image to find the Id keep. As a result, this programme greatly benefits blind and visually impaired people, making their job of recognizing products much easier. Because it requires a scanner to scan the bar-code and a camera phone to take the image of the image including the bar-code, this is frequently quite simple to use and inexpensive. This is generally straightforward to implement since that most mobile phone contain the desired resolution as well as product description. This project is frequently implemented in any shopping precinct, grocery, book stores, medical stores, and so on, to scan the barcode to spot the Id keep in it and skim out the exhaustive search.},
  keywords={Location awareness;Image resolution;Text recognition;Image edge detection;Blindness;Cameras;Market research;Barcode;Bayesian deformable algorithm;recognition devices;text reading;hand-held items;Text to speech;outlet store;hearing output},
  doi={10.1109/ICOEI53556.2022.9776881},
  ISSN={},
  month={April},}@INPROCEEDINGS{10837943,
  author={Sanjorjo, Jaybie and Apiag, Janmarjun and Lahinao, Sepriel and Villacencio, Rima B. and Perez, Novie Dave},
  booktitle={2024 4th Asian Conference on Innovation in Technology (ASIANCON)}, 
  title={Design and Development of a Smart Blind Stick for Enhanced Mobility and Independence of Visually Impaired Individuals}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={This study delves into the innovative design and development of a smart blind stick aimed at revolutionizing the mobility and independence of visually impaired individuals. Traditional white canes have long served as a primary aid for navigation, yet their limitations in detecting obstacles and providing real-time assistance underscore the necessity for a more advanced solution. The proposed smart blind stick integrates cutting-edge technologies such as ultrasonic sensors, artificial intelligence, fall detection and haptic feedback mechanisms to enhance the user’s spatial awareness and navigation experience. By leveraging ultrasonic sensors, the device accurately detects obstacles in the path and provide context-aware guidance. Furthermore, haptic feedback mechanisms deliver intuitive cues to the user, enabling them to distance their self from obstacles. When there is a sudden drop or falls, the accelerometer transmits data to GSM Module and send message notification to the user’s caretakers including the location. There is also an AI camera that allowed the caretaker’s to captured and view the surroundings of visually impaired through a mobile app. The design prioritizes user-centered principles, ensuring comfort, portability, and ease of use. Through rigorous development and iterative testing, the smart blind stick emerges as a promising tool to empower visually impaired individuals, fostering greater mobility, confidence, and independence in navigating their daily lives. The smart cane with overall user satisfaction of 4.11, proves the efficiency and effectiveness of it.},
  keywords={Technological innovation;Navigation;Acoustics;Real-time systems;Haptic interfaces;Mobile applications;Iterative methods;Artificial intelligence;Intelligent sensors;Testing;Raspberry Pi 3;ESP32;Visually Impaired},
  doi={10.1109/ASIANCON62057.2024.10837943},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9936585,
  author={Singh, Aashdeep and Murugeswari, P. and Ragavendiran, S D Prabu and Kaur, Amanpreet and Singh, Gurpreet and Margabandu, Sathiyamoorthy},
  booktitle={2022 International Conference on Edge Computing and Applications (ICECAA)}, 
  title={AI-based Chatbot for Physically Challenged People}, 
  year={2022},
  volume={},
  number={},
  pages={1039-1044},
  abstract={Blindness is a state where a person loses his/her ability to see completely or partially. It is also said to be one of the most widely found disabilities across the world. Most of the time, it is impossible to find a complete cure for this condition. Thus, people with blindness adapt themselves to living with this blindness as a part of them. To help visually impaired people, this study aims in developing a chatbot which uses artificial intelligence to predict the most relevant answers to the users’ questions. This bot is designed in such a way that it allows the user to send voice notes instead of texts like the conventional chatbots. The chatbot is developed based on a particular architecture. The user-sent text is first converted to text by this architecture. Deep neural networks and APIs that are used for speech-to-text and text-to-speech conversion make up the chatbot's main architecture. The optimal response is then determined for the user from this text, and it is sent to the user. The inner parts of the architecture consist of an AI algorithm named the deep neural networks, speech-to-text, and text-to-speech APIs, libraries like GTTS, etc. This chatbot can be trained to add more functions, such as the ability to automatically order essentials from the websites indicated, transmit emergency signals, and remind users to take their medications, eat their meals, and drink their water in the future.},
  keywords={Deep learning;Computer architecture;Blindness;Artificial neural networks;Chatbots;Software;Libraries;Artificial Intelligence;Chatbot;Preprocessing;Deep Neural Network;Speech Recognition},
  doi={10.1109/ICECAA55415.2022.9936585},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10973830,
  author={Abbo, Giulio Antonio and Belpaeme, Tony},
  booktitle={2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots}, 
  year={2025},
  volume={},
  number={},
  pages={1176-1180},
  abstract={In the rapidly evolving landscape of human-robot interaction, the integration of vision capabilities into conversational agents stands as a crucial advancement. This paper presents a ready-to-use implementation of a dialogue manager that leverages the latest progress in Large Language Models (e.g., GPT-4o mini) to enhance the traditional text-based prompts with real-time visual input. LLMs are used to interpret both textual prompts and visual stimuli, creating a more contextually aware conversational agent. The system's prompt engineering, incorporating dialogue with summarisation of the images, en-sures a balance between context preservation and computational efficiency. Six interactions with a Furhat robot powered by this system are reported, illustrating and discussing the results obtained. The system can be customised and is available as a stand-alone application, a Furhat robot implementation, and a ROS2 package.},
  keywords={Visualization;Image resolution;Limiting;Large language models;Machine vision;Social robots;Human-robot interaction;Oral communication;Real-time systems;Prompt engineering;Large Language Model;Vision Language Model;Dialogue;HRI;Conversation;Prompt Engineering;ROS},
  doi={10.1109/HRI61500.2025.10973830},
  ISSN={},
  month={March},}@INPROCEEDINGS{10340454,
  author={Sankarnarayanan, Tharangini and Paciorkowski, Lev and Parikh, Khevna and Hamilton-Fletcher, Giles and Feng, Chen and Sheng, Diwei and Hudson, Todd E. and Rizzo, John-Ross and Chan, Kevin C.},
  booktitle={2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={Training AI to Recognize Objects of Interest to the Blind and Low Vision Community}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Recent object detection models show promising advances in their architecture and performance, expanding potential applications for the benefit of persons with blindness or low vision (pBLV). However, object detection models are usually trained on generic data rather than datasets that focus on the needs of pBLV. Hence, for applications that locate objects of interest to pBLV, object detection models need to be trained specifically for this purpose. Informed by prior interviews, questionnaires, and Microsoft’s ORBIT research, we identified thirty-five objects pertinent to pBLV. We employed this user-centric feedback to gather images of these objects from the Google Open Images V6 dataset. We subsequently trained a YOLOv5x model with this dataset to recognize these objects of interest. We demonstrate that the model can identify objects that previous generic models could not, such as those related to tasks of daily functioning – e.g., coffee mug, knife, fork, and glass. Crucially, we show that careful pruning of a dataset with severe class imbalances leads to a rapid, noticeable improvement in the overall performance of the model by two-fold, as measured using the mean average precision at the intersection over union thresholds from 0.5 to 0.95 (mAP50-95). Specifically, mAP50-95 improved from 0.14 to 0.36 on the seven least prevalent classes in the training dataset. Overall, we show that careful curation of training data can improve training speed and object detection outcomes. We show clear directions on effectively customizing training data to create models that focus on the desires and needs of pBLV.Clinical Relevance— This work demonstrated the benefits of developing assistive AI technology customized to individual users or the wider BLV community.},
  keywords={Training;YOLO;Training data;Data models;Orbits;Internet;Object recognition},
  doi={10.1109/EMBC40787.2023.10340454},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10986575,
  author={Munjal, Geetika and Dious, Austin and Agarwal, Devansh and Sharma, Rahul and Jarawat, Gunjan},
  booktitle={2025 3rd International Conference on Disruptive Technologies (ICDT)}, 
  title={Blindviz — Intelligent Navigation and Environmental Awareness for the Visually Impaired}, 
  year={2025},
  volume={},
  number={},
  pages={492-496},
  abstract={The blind and visually impaired people must face significant challenges while navigating their environment independently. This paper presents “BlindViz”, a mobile application specially designed to aid the visually impaired by providing real-time object detection and scene description using the latest advancements in the AI technology. This app integrates Large Language Models (LLMs) and leverages You Only Look Once (YOLOv8) for object detection, Easy Optical Character Recognition (EasyOCR) for Text Recognition and leveraging Image Segmentation for natural language scene descriptions. As the word “Activate” is spoken into the app, it takes an image, processes it, and delivers a description through an audible output. This paper outlines the development, implementation, and evaluation of BlindViz, along with the suggestions for future enhancements to improve its usability and effectiveness.},
  keywords={YOLO;Image segmentation;Visualization;Accuracy;Navigation;Large language models;Real-time systems;User experience;Usability;Speech to text;BlindViz;Object detection;LLMs;YOLOv8;OpenAI API;Natural language processing;Text Recognition;Image Segmentation;Image processing;Visually Impaired;Blind;Visual Assistance},
  doi={10.1109/ICDT63985.2025.10986575},
  ISSN={},
  month={March},}@INPROCEEDINGS{10333256,
  author={Van Nguyen, Thai and Khan, Ibrahim and Nimpattanavong, Chollakorn and Thawonmas, Ruck and Pham, Hai V.},
  booktitle={2023 IEEE Conference on Games (CoG)}, 
  title={A Cross-Modality Transfer Reinforcement Learning Blind Agent on DareFightingICE}, 
  year={2023},
  volume={},
  number={},
  pages={1-2},
  abstract={This demo paper presents a deep reinforcement learning blind agent (blind agent) that uses only sound as the input on the DareFightingICE platform at the DareFightingICE Competition in IEEE CoG 2022 and 2023. A past study used reinforcement learning to train a blind agent and used it as the sample for the AI track of DareFightingICE Competition. However, this approach suffered from significant time expenses due to training the blind agent to recognize sounds and act in an end-to-end manner. To address this challenge, we propose a novel approach for blind agents where sound recognition and decision-making tasks are trained separately.},
  keywords={Training;Deep learning;Decision making;Reinforcement learning;Games;Task analysis;Artificial intelligence;Fighting Game;FightingICE;DareFightingICE;Deep Reinforcement Learning;Cross-Modality;Transfer Learning;Sound},
  doi={10.1109/CoG57401.2023.10333256},
  ISSN={2325-4289},
  month={Aug},}@ARTICLE{10948175,
  author={},
  journal={Journal of Cyber Security and Mobility}, 
  title={Editorial Preface: Futuristic AI Embedded Solutions for Cyber Security}, 
  year={2024},
  volume={13},
  number={1},
  pages={i-iii},
  abstract={The integration of AI into cybersecurity represents a pivotal moment in the ongoing battle against cyber threats. With its ability to analyze vast datasets, identify patterns, and adapt in real-time, AI holds the potential to transform how we defend against cyberattacks. Current trends indicate that AI will continue to play a central role in cybersecurity, from zero trust architectures to quantum computing defense. However, challenges such as data privacy, adversarial attacks, bias, and skills gaps must be addressed to fully realize the benefits of AI in cybersecurity. As organizations strive to secure their digital assets, collaboration between humans and AI will become increasingly important. The future of cybersecurity is intertwined with the evolution of AI. By embracing this powerful technology and addressing the challenges it presents, we can bolster our defenses and better protect the digital world from ever-evolving cyber threats. The articles featured in this special issue will explore a wide array of topics from the development of AI-powered threat detection algorithms to the role of machine learning in anomaly detection and the potential of neural networks in enhancing intrusion detection systems. These innovations promise not only to strengthen our ability to detect threats but also to automate responses to reduce the critical human factor in response times. In this special issue, we had a positive response from the international research community, and after a careful double-round blind review process 6 papers were accepted. We summarise their contributions to this special issue subsequently.},
  keywords={Technological innovation;Quantum computing;Reviews;Transforms;Threat assessment;Real-time systems;Zero Trust;Time factors;Artificial intelligence;Computer crime},
  doi={},
  ISSN={2245-4578},
  month={January},}@INPROCEEDINGS{10346765,
  author={Dhamane, Shreyash and Ainapure, Atman and Dhage, Sudhir},
  booktitle={2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)}, 
  title={Breaking Silence, Embracing Inclusivity: The Power of AI in Communication}, 
  year={2023},
  volume={},
  number={},
  pages={231-236},
  abstract={This study presents a cutting-edge web platform powered by artificial intelligence (AI) that enables seamless communication for people of all abilities. By utilising the power of artificial intelligence (AI) and machine learning (ML), the platform meets the communication demands of the blind, deaf, and mute. With the use of text-to-speech, text-to-text, text-to-speech, and hand gesture detection driven by AI, users may effectively communicate. Additionally, the site provides video calls and browsing speech recognition technologies. It also has two emergency indicators so that mute people can readily access emergency services like ambulances or police. The platform's success depends on advancing kind technology, closing gaps in traditional communication channels, and fostering an inclusive community for users of varied abilities. The research explores the potential impact of this innovative communication platform, incorporating user feedback to highlight its positive effects on the lives of people with disabilities.},
  keywords={Law enforcement;Machine learning;Speech recognition;Communication channels;Emergency services;Cognition;Cybernetics;speech to text;text to speech;sign language;hand signs;communication},
  doi={10.1109/ICCCMLA58983.2023.10346765},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11126726,
  author={Moterani, Gabriel and Lin, Wenjun Randy},
  booktitle={2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Breaking the Linear Barrier: A Multi-Modal LLM-Based System for Navigating Complex Web Content}, 
  year={2025},
  volume={},
  number={},
  pages={2066-2075},
  abstract={Visually impaired users still face fundamental obstacles when interacting with complex, dynamic websites. Conventional screen readers expose pages in a strict linear order, offer little semantic context for visual media, and provide limited context regarding the page content. This paper introduces a multi-modal accessibility framework combining Large Language Models (LLMs), Computer Vision, and dynamic DOM manipulation to significantly enhance semantic clarity, non-linear navigation, and interaction richness. By interpreting visual and textual web content contextually and adapting it into an intuitive, conversationally navigable interface, our method provides a foundation for visually impaired users to interact effectively with previously inaccessible or challenging digital experiences.The deployment of a functional prototype on a modern web browser illustrates the capability of the proposed system to interact with diverse websites and tasks. The research team selected Canada’s most frequented websites to assess the system’s efficacy in enhancing contextual understanding of the page content and enabling navigation through pages and actions via a chat-driven interface. A comprehensive demonstration was executed using a prominent ticketing site, which facilitated users in obtaining a deeper understanding of the page while guiding them towards the successful purchase of concert tickets. By illustrating how vision language and LLM reasoning can be coupled with low-level browser control, this work lays the groundwork for future efforts in performance optimization, large-scale evaluation, and personalization across diverse web contexts.},
  keywords={Visualization;Computer vision;Navigation;Computational modeling;Semantics;Prototypes;Browsers;Iterative methods;Usability;Manipulator dynamics;Web Accessibility;LLM;Accessibility DOM;Human-Computer Interaction;Context-Aware Systems},
  doi={10.1109/COMPSAC65507.2025.00289},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{11101812,
  author={Faisal, Maha and AlQouz, Hadeel and Aldousari, Lujane Mabkhout and Alkhaldi, Estabraq Adnan and Almutairi, Retaj Sultan and Alotaibi, Rehab Mshaan},
  booktitle={2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS)}, 
  title={Autonomous guiding robot (AGuro): An AI Mueseum Robot for Visually Impaired Individuals}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={In light of the transformative influence of technology on contemporary society, the integration of intelligent robots equipped with sophisticated sensing capabilities and advanced artificial intelligence emerges as a promising avenue to enhance the mobility of individuals with blindness or low vision, especially in scenarios where access to a human guide is unfeasible. Our overarching objective is to facilitate the navigation of visually impaired and blind individuals within museum environments. At the core of our initiative is the Autonomous Guiding Robot (AGuro), a groundbreaking solution meticulously designed to cater primarily to the needs of the visually impaired and blind community, offering autonomy without reliance on human intervention.},
  keywords={Technological innovation;Navigation;Tactile sensors;Museums;User experience;Cultural differences;Time factors;Artificial intelligence;Robots;Standards;AI;Blind;Robotics;Obstacle Detection;Voice Recognition;Accessibility;Inclusive user experience},
  doi={10.1109/ISAS66241.2025.11101812},
  ISSN={},
  month={June},}@ARTICLE{9873918,
  author={Lopez-Ballester, Jesus and Felici-Castell, Santiago and Segura-Garcia, Jaume and Cobos, Maximo},
  journal={IEEE Internet of Things Journal}, 
  title={AI-IoT Platform for Blind Estimation of Room Acoustic Parameters Based on Deep Neural Networks}, 
  year={2023},
  volume={10},
  number={1},
  pages={855-866},
  abstract={Room acoustical parameters have been widely used to describe sound perception in indoor environments, such as concert halls, conference rooms, etc. Many of them have been standardized and often have a high computational demand. With the increasing presence of deep learning approaches in automatic monitoring systems, wireless acoustic sensor networks (WASNs) offer great potential to facilitate the estimation of such parameters. In this scenario, convolutional neural networks (CNNs) offer significant reductions in the computational requirements for in-node parameter predictions, enabling the so-called Artificial Intelligence-Internet of Things (AI-IoT). In this article, we describe the design and analysis of a CNN trained to predict simultaneously a set of common room acoustical parameters directly from speech signals, without the need for specific impulse response measurements. The results show that the proposed CNN-based prediction of room acoustical parameters and speech intelligibility achieves a relative error rate of less than a 5.5%, accompanied by a computational speedup factor close to 250 with respect to the conventional signal processing approach.},
  keywords={Acoustics;Acoustic measurements;Reverberation;Estimation;Indexes;Deep learning;Convolutional neural networks;Convolutional neural networks (CNNs);Internet of Things;room acoustic parameters;speech intelligibility;wireless acoustic sensor networks (WASNs)},
  doi={10.1109/JIOT.2022.3203570},
  ISSN={2327-4662},
  month={Jan},}@ARTICLE{10947292,
  author={Liu, Xuelin and Yan, Jiebin and Lai, Chenyi and Li, Yang and Fang, Yuming},
  journal={IEEE Signal Processing Letters}, 
  title={Viewport-Independent Blind Quality Assessment of AI-Generated Omnidirectional Images via Vision-Language Correspondence}, 
  year={2025},
  volume={32},
  number={},
  pages={1630-1634},
  abstract={The advancement of deep generation technology has significantly enhanced the growth of artificial intelligence-generated content (AIGC). Among these, AI-generated omnidirectional images (AGOIs), hold considerable promise for applications in virtual reality (VR). However, the quality of AGOIs varies widely, and there has been limited research focused on their quality assessment. In this letter, inspired by the characteristics of the human visual system, we propose a novel viewport-independent blindquality assessment method for AGOIs, termed VI-AGOIQA, which leverages vision-language correspondence. Specifically, to minimize the computational burden associated with viewport-based prediction methods for omnidirectional image quality assessment, a set of image patches are first extracted from AGOIs in Equirectangular Projection (ERP) format. Then, the correspondence between visual and textual inputs is effectively learned by utilizing the pre-trained image and text encoders of the Contrastive Language-Image Pre-training (CLIP) model. Finally, a multimodal feature fusion module is applied to predict human visual preferences based on the learned knowledge of visual-language consistency. Extensive experiments conducted on publicly available database demonstrate the promising performance of the proposed method.},
  keywords={Visualization;Feature extraction;Databases;Quality assessment;Predictive models;Training;Computational modeling;Vectors;Solid modeling;Image quality;AI-generated omnidirectional image;vision-language correspondence;blind image quality assessment},
  doi={10.1109/LSP.2025.3556791},
  ISSN={1558-2361},
  month={},}@INPROCEEDINGS{11099991,
  author={A, Sathya Sofia and M, Soundarya Devi and A, Viniksha Shree and E, Yashika and K, Yuvashree},
  booktitle={2025 International Conference on Emerging Technologies in Engineering Applications (ICETEA)}, 
  title={DRDD: AI-Powered Multi-Modal System for Diabetic Retinopathy Disease Detection}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The disease named diabetic retinopathy destroys the eye's light sensitive tissue and cause blindness. It often occurs due to diabetics. This disease can be detected by using automated system that trained using machine learning algorithms with the needed dataset. In many approached system, majorly retinal images were only used for the detection of this disease, meanwhile our work focus on integrating the result obtained from both retinal images and clinical data of the patients. This enhances the accuracy rate and efficiency of diagnosis. This study introduces the fusion model that integrates deep learning and data analysis to enhance the accuracy level in detection. This model uses EfficientNet-B3 and Vision Transformer to find abnormalities in eye from eye image and TabNet for analyzing clinical data and improve the accuracy of the result. Generative AI model powered by GPT-4 to provide a personalized suggestion to patients based on their health condition. This integrated approach achieves an accuracy of 94.7% in disease detection.},
  keywords={Diabetic retinopathy;Analytical models;Accuracy;Generative AI;Computational modeling;Medical services;Retina;Transformers;Data models;Diseases;Diabetic Retinopathy;Fusion model;Retinal Image;ViT;EfficientNet-B3;Clinical data;GPT 4;Tabular Neural Network;Gen AI},
  doi={10.1109/ICETEA64585.2025.11099991},
  ISSN={},
  month={June},}@INPROCEEDINGS{9853318,
  author={Wang, Han and Lung Chong, Kelvin Kam and Li, Zefeng},
  booktitle={2022 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)}, 
  title={Applications of AI to Age-Related Macular Degeneration: a case study and a brief review}, 
  year={2022},
  volume={},
  number={},
  pages={586-589},
  abstract={Age-Related Macular Degeneration (AMD) is one of the major causes of blindness. The artificial intelligence (AI) based method is a novel solution computer-aided diagnosis approach. This study implemented a brief review by utilizing a case study on AI-based AMD diagnosis studies. Challenges and solutions are discussed., which makes a reference value for future research.},
  keywords={Blindness;Computer aided diagnosis;Artificial intelligence;Task analysis;Diseases;Age-Related Macular Degeneration;Artificial intelligence;computer-aided diagnosis;case study},
  doi={10.1109/ICCEAI55464.2022.00125},
  ISSN={},
  month={July},}@INPROCEEDINGS{11135953,
  author={Sathishkumar, B. R. and Sudevan, S. and Vignesh, M. and Kumar, R. Vishnu},
  booktitle={2025 International Conference on Sensors and Related Networks (SENNET) Special Focus on Digital Healthcare(64220)}, 
  title={AI-Driven Glaucoma Detection Using Deep Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Glaucoma stands as a major reason for permanent blindness worldwide yet patients experience no detectable symptoms before their vision worsens significantly. The discovery of glaucoma at its early stage remains essential to achieve proper care and treatment methods. Researchers have researched glaucoma detection in retinal fundus pictures through DL and AI technology systems to operate autonomously. The evaluation includes examining glaucoma indicator features including cup-to-disc ratio and optic nerve head structure through the use of convolutional neural networks (CNNs). The model obtained its parameters from public datasets before receiving assessment of its accuracy and performance metrics. The diagnostic capabilities of artificial intelligence outperform the reliability standards achieved with traditional diagnostic methods. The study reveals deep learning models have boosted early glaucoma identification efficiency to the point where they decrease medical professional workloads while creating better patient care results. Future research aims to make the models more understandable while also working to incorporate AI assisted screening systems into patient care environments.},
  keywords={Glaucoma;Deep learning;Head;Optical computing;Optical fiber networks;Retina;Optical imaging;Optical sensors;Convolutional neural networks;Artificial intelligence;Glaucoma;permanent blindness;early detection;deep learning;artificial intelligence;retinal fundus images;convolutional neural networks;cup-to-disc ratio;optic nerve head;accuracy;sensitivity;specificity;public datasets;AI-based detection;diagnostic techniques;early glaucoma detection;ophthalmologists;patient outcomes;model interpretability;AI-assisted screening},
  doi={10.1109/SENNET64220.2025.11135953},
  ISSN={},
  month={July},}@INPROCEEDINGS{11157138,
  author={Zhang, Keke and Hu, Qianqian and Shi, Shuhan and Zhao, Weikang},
  booktitle={2025 26th International Conference on Electronic Packaging Technology (ICEPT)}, 
  title={Research on the interconnection reliability of substrate-like printed circuit board for AI servers}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={To explore the feasibility of CPU and multi-GPU expansion interconnection for AI servers, a substrate-like printed circuit board was designed, featuring 32 layers with the thickness of 4.4mm, 7th-order HDI (High-Density Interconnect), and 8 layers of 2oz power planes. It includes four interconnection schemes: high aspect ratio Through-Holes, Direct Stacked Vias, Offset Stacked Vias, and Buried-Blind Vias. An interconnection stress test platform was built to apply periodic current to coupons, measure the resistance change rate of the interconnection link in each cycle, and monitor the failure cycle counts of the four interconnection schemes. After testing, failed samples were sectioned for observation to analyze failure modes. Experimental results show that failures in high aspect ratio Through-Holes occurred in the pad area of the internal wiring layer. For the other three interconnection schemes, failures all occurred at the bottom of blind vias. Among them, Through-Holes had the worst reliability, Offset Stacked Vias had the best reliability, and Buried-Blind Vias and Direct Stacked Vias had basically the same reliability. Therefore, for the via interconnection design of AI server substrate-like boards, high aspect ratio through-holes should be avoided as much as possible, and Direct Stacked Vias or Offset Stacked Vias should be selected according to signal requirements and routing density.},
  keywords={Wiring;Printed circuits;Integrated circuit interconnections;Routing;Servers;Artificial intelligence;Integrated circuit reliability;Substrates;Stress;Testing;Substrate-like Printed Circuit Board;AI Servers;Interconnected Stress Testing},
  doi={10.1109/ICEPT67137.2025.11157138},
  ISSN={2836-9734},
  month={Aug},}@ARTICLE{10949140,
  author={Tsihrintzis, George A. and Sarmas, Elissaios and Marinakis, Vangelis and Panagoulias, Dimitrios P. and Tsichrintzi, Evangelia-Aikaterini and Virvou, Maria},
  journal={IEEE Access}, 
  title={Energy Urban Domain: Personalized Evaluation of Expert and Non-Expert Stakeholder Interaction With Artificial Intelligence Through ChatGPT Using the VIRTSI Model}, 
  year={2025},
  volume={13},
  number={},
  pages={62506-62526},
  abstract={This paper explores the application of Generative AI, specifically ChatGPT, in urban energy management, assessing human users’ trust in AI systems that, while often accurate, can still make mistakes. It examines how different stakeholder groups—AI experts, energy domain experts, and non-experts—develop trust, distrust, or overtrust in AI-generated outputs and highlights the risks associated with these trust states. While overtrust can lead to blind reliance on incorrect AI outputs, distrust can result in the unnecessary rejection of accurate AI recommendations, ultimately reducing the effectiveness of AI-assisted decision-making. Using the VIRTSI (Variability and Impact of Reciprocal Trust States towards Intelligent Systems) methodology, this research monitors human-AI trust evolution through Deterministic Finite Automata (DFA) and quantifies trust behaviors using user-adapted Confusion Matrices, addressing a critical gap in AI trust dynamics that traditional acceptance models overlook. The findings validate VIRTSI’s ability to track trust transitions. The study reveals that AI experts exhibit skepticism due to their awareness of AI’s limitations, energy experts tend to overtrust AI, likely influenced by its confident and seemingly reliable responses, and non-experts display inconsistent trust, highlighting decision-making challenges. These findings confirm VIRTSI’s premise that trust in AI is dynamic, varies by user expertise, and must be continuously monitored and assessed. Ultimately, this study strengthens VIRTSI as a necessary framework for assessing and optimizing trust in AI-driven sustainability solutions, ensuring that AI systems are not only trusted but also used effectively and responsibly in energy applications. Unlike other models of technology acceptance that focus solely on adoption, VIRTSI provides a continuous and quantifiable approach to trust calibration, identifying harmful trust patterns and guiding improvements in AI-human interaction over time.},
  keywords={Artificial intelligence;Chatbots;Stakeholders;Decision making;Accuracy;Reliability;Focusing;Calibration;Business;Analytical models;Artificial intelligence;artificial intelligence in energy;artificial intelligence trust;human-artificial intelligence interaction;large language models;user modeling},
  doi={10.1109/ACCESS.2025.3557907},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10201487,
  author={Thomas, Likewin and L, Thara K and R, Sunil Kumar H},
  booktitle={2023 5th International Conference on Energy, Power and Environment: Towards Flexible Green Energy Technologies (ICEPE)}, 
  title={Artificial Intelligence for Face recognition and Assistance for Visually Impaired}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In this world, many people are affected by blindness and they face so many difficulties in their daily life. According to World Health Organization (WHO), there are approximately 2.2 billion people who are completely blind. They need to depend on primitive solutions like white canes, trained dogs, or other people. But these helping hands cannot always assist them. The affected people need a smart assisting device that avoids bumping into an obstacle and helps in navigating from one place to another independently. This proposed work describes the smart walking stick which makes blind people walk safely. By using the latest technologies and IoT devices, this smart walking stick can be developed where it provides safe navigation to the user. The proposed system employs a novel solution for navigation in indoor with the help of deep learning algorithms. In case of panic situations or emergency conditions, the predefined message with the user's location will be sent to the caretaker using an API. This smart walking stick is affordable, durable and provides more convenience to the user to walk safely, and gives more confidence without depending on any other externals.},
  keywords={Legged locomotion;Deep learning;Navigation;Face recognition;Green products;Blindness;Organizations;Visually impaired;Word Health Organization;IoT;API},
  doi={10.1109/ICEPE57949.2023.10201487},
  ISSN={2832-8973},
  month={June},}@INPROCEEDINGS{10369794,
  author={C, Pandi and Lingham N, Siva Rama and S, Nithya and R, Aruna},
  booktitle={2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={AI Based Human Computer Interactions for Specially-Abled}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Artificial intelligence (AI) plays a crucial role in supporting people with special needs. People who are blind, deaf, or possibly disabled require ongoing extra care and attention when it comes to education because they have difficulties with everything from keeping track of attendance to completing tests. They cannot use computers for their needs; hence, they are dependent on someone. The use of a computer requires physical contact or interaction, such as using a mouse, keyboard, or touch screen. We have created a touch less sketching interface utilizing the pattern recognition method to overcome these problems. In pattern recognition, machine learning algorithms are used to automatically identify patterns and regularities in data that is given as words, pictures, sounds, or other recognizable components. Others include a virtual mouse that may be operated with the eyeball for those who are unable to move their hands. To quantify eye movement, common eye tracking algorithms analyze the separation between infrared LEDs projected onto the eye and the pupil of the eye. The mediapipe framework, the tensorflow model, and the openCV library are required for these two algorithms, which enable blind individuals to hear news, music, and other information without having to browse the internet.},
  keywords={Computers;Machine learning algorithms;Speech recognition;Touch sensitive screens;Light emitting diodes;Mice;Libraries;Artificial Intelligence(AI);Touch less user interface;Eyeball-controlled virtual mouse;virtual desktop voice assistant;Pattern Recognition;Eye Tracking},
  doi={10.1109/RMKMATE59243.2023.10369794},
  ISSN={},
  month={Nov},}
