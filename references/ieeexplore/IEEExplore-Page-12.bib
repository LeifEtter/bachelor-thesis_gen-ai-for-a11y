@INPROCEEDINGS{10134901,
  author={Rajaraman, Sanjay and Shivapriya, Ps},
  booktitle={2023 3rd International conference on Artificial Intelligence and Signal Processing (AISP)}, 
  title={A-Eye: Computer vision and Deep learning based Smart-Glasses with Edge computing for Visually Impaired}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={India, where a third of the world’s blind people reside, has about 12 million blind people, especially in comparison to a total of 39 million worldwide, according to the National Programme for Control of Blindness (NPCB). Visually challenged people, including children and elderly people with low vision, still have trouble performing everyday tasks, even in this world of rapidly developing technologies. The existing solutions provide a straight forward solution using Artificial Intelligence technologies. To take this technological solution to the next level we propose a solution with all the AI features seamlessly integrated together and a dedicated mobile application gives full control and independency to their life. The A-Eye (smart glasses) system that is being proposed consists of a wearable smart glass solution, a mobile application, and a cloud-based data management platform. This project shows how Mobile Edge Computing (MEC) can be used to process data on the mobile phone other than to process the camera’s video stream in the cloud or locally at the system level. The project’s use of Mobile Edge Computing (MEC) expands the scope of its ability to add and update an endless number of features without being constrained by hardware. Therefore, the suggested smart glass system can minimize latency and be highly scalable while using heavy and effective models to effectively mitigate the problem and assist visually challenged individuals.},
  keywords={Multi-access edge computing;Blindness;Streaming media;Signal processing;Mobile handsets;Mobile applications;Artificial intelligence;Artificial intelligence;Internet of Things (IoT);computer vision;Deep learning;Mobile Edge Computing},
  doi={10.1109/AISP57993.2023.10134901},
  ISSN={2640-5768},
  month={March},}@INPROCEEDINGS{9719298,
  author={Zhai, Zhenzhen and Gao, Qi and Jiang, Yuan and Chai, Xinyu and Han, Wenjie},
  booktitle={2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications (ICPECA)}, 
  title={A Software for Rapid Annotation of Scene Objects Based on Saliency Object Ranking}, 
  year={2022},
  volume={},
  number={},
  pages={378-382},
  abstract={With the development of artificial intelligence (AI), a mainstream research line in building a non-invasive intelligent system for assisting people with visual disabilities is to capture images with a camera, and then using AI to identify the content of images and convert visual information into auditory information. The key point is to establish high-quality personalized datasets based on real open scenes for AI model training. However, most of existing annotation software is designed for face recognition, automatic driving and other tasks, whose annotation information type is monotonous. Also, when converting an image with multiple objects into audio information, the order of objects needs to be determined. Few studies have focused on this issue currently. Therefore, we develop an image annotation software for rapid construction of small sample databases of AI models that can be used in intelligent blindness assistance systems. The software can additionally mark information such as size and affiliation. Moreover, it has a database module to facilitate the management of multiple annotation tasks. Then, a saliency object ranking network based on multi-task cascade is designed. The object ranking results can provide a reference for the sequence of audio information. The Hit Rate, Relative Ranking accuracy and Saliency Object Ranking score of the ranking network are better than that existing saliency object network on the validation set. Finally, the network is deployed into the software to realize the AI-assisted annotation, and to quickly establish the personalized dataset.},
  keywords={Training;Visualization;Annotations;Databases;Blindness;Multitasking;Software;saliency object ranking;image annotation software;intelligent blindness assistance systems;personalized dataset},
  doi={10.1109/ICPECA53709.2022.9719298},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11189150,
  author={Gu, Chuqiao and Zhang, Wuyang and Huang, Zhenqian and Kou, Jieren and Liu, Zhenyao and Zhao, Chenjun and Liu, Chang and Zhang, Lifeng and Lin, Wenjie and Wang, Zhongda and Deng, Jianwei and Xie, Yuhuan and Huang, Guoxin and Zhang, Charles and Lu, Xiuyuan and Wang, Chengming and Zhang, Zejun and Yuan, Hao and Duan, Xiaoman and Fang, Yajun},
  booktitle={2024 7th International Conference on Universal Village (UV)}, 
  title={LENS: Layers of Evaluation of Hallucination in GenAI Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-85},
  abstract={Large Language Models (LLMs) and Vision-Language Models (VLMs) demonstrate remarkable capabilities but remain vulnerable to hallucinations—producing plausible yet factually incorrect content—with error rates ranging from as low as 1.47% in clinical applications [1] to as high as 75% in domain-specific queries [2]. Despite growing attention, existing hallucination evaluation frameworks remain insufficient to meet critical needs. Through a comprehensive survey of over 100 evaluation methods spanning six methodological paradigms (probe-based, adversarial testing, causal intervention, uncertainty-guided, internal state analysis, and online evaluation), we identify fundamental limitations: current approaches fall short of enabling objective model comparison, providing diagnostic insights into failure modes, supporting domain-evolving benchmark construction, and guiding targeted mitigation strategies. Our analysis reveals that evaluations remain fragmented, operating either horizontally—comparing models across tasks and domains—or vertically—probing reasoning chains within single outputs. This fragmentation limits holistic assessment: horizontal evaluations provide breadth but risk superficiality, while vertical assessments deliver depth but lack generalizability. Moreover, we identify five critical gaps: (1) dimensional poverty reducing hallucinations to binary metrics, (2) failure to integrate horizontal breadth with vertical depth, (3) metacognitive blind spots overlooking when models should seek external verification, (4) adaptability crisis from static benchmarks, and (5) transparency deficits providing scores without actionable insights. Beyond surveying the landscape, this paper articulates eight fundamental challenges confronting comprehensive evaluation—from epistemological difficulties in defining ground truth and computational complexity of scaling assessment, to attribution opacity obscuring causal mechanisms and dynamic knowledge evolution rendering benchmarks obsolete. These challenges span multimodal complexity, scale and diversity requirements, adversarial robustness, and human alignment considerations. We present LENS (Layers of Evaluation of Hallucination in GenAI Systems), a unified framework addressing these gaps through hierarchical, tree-based query decomposition. LENS transforms complex evaluation tasks into multi-layered assessment structures via a six-stage pipeline (task formulation, decomposition, tool-augmented execution, structured generation, multi-dimensional scoring, and trace analysis), enabling MRI-like scanning of inference processes to reveal where and why hallucinations originate. The framework introduces four key innovations: (1) Tool Necessity Detection and Selection (TND/TSA) – explicitly evaluating when models should consult external sources versus relying on parametric memory, addressing a fundamental hallucination source. (2) Multi-Dimensional Metrics – assessing degree (accuracy, faithfulness, tool appropriateness), quantity (coverage, completeness), stability (consistency, robustness), and risk (uncertainty quantification) beyond binary detection. (3) User-Centric Benchmark Construction – empowering organizations to design custom evaluations from their evolving knowledge bases while maintaining methodological rigor. (4) Actionable Error Attribution – providing hierarchical decomposition traces with causal attribution, evidence chains, and OpenTelemetry-based reproducibility for transparent auditing. Our systematic taxonomy unifies previously fragmented approaches across evaluation targets (task-specific, modality-based, hallucination-type, domain-specific), dimensions (factuality, faithfulness, consistency, robustness, causal reasoning, interpretability), and methodologies. We introduce unified metrics transcending individual dimensions and present mitigation-aware evaluation strategies integrating RAG, parameter-efficient fine-tuning, knowledge distillation, preference optimization, and temporal intervention approaches. By combining horizontal breadth (across domains and architectures) with vertical depth (into reasoning processes), LENS advances hallucination evaluation from post-hoc error detection to proactive risk assessment. Case studies in medical diagnosis, legal analysis, and financial reasoning demonstrate the framework’s transformative impact, enabling objective model comparison, informed selection, diagnostic insights, domain-evolving benchmarks, and targeted mitigation development—fostering calibrated trust in AI systems deployed in safety-critical applications where accuracy, interpretability, and accountability are indispensable.},
  keywords={Analytical models;Accuracy;Uncertainty;Prevention and mitigation;Large language models;Transforms;Benchmark testing;Cognition;Robustness;Lenses;Large language models;vision-language models;hallucination evaluation;hallucination mitigation;retrieval-augmented generation;comprehensive hallucination evaluation;horizontal vs vertical evaluation;generative AI evaluation;LENS framework;multi-dimensional metrics;multi-layered assessment structures;cross-domain evaluation;visual-textual misalignment;cross-modal inconsistencies;dynamic knowledge;evolving benchmarks;tool necessity detection;tool selection accuracy;hierarchical query decomposition;faithfulness evaluation;domain-specific benchmarks;user-centric benchmark construction;transparency and interpretability;surface-level accuracy;compositional complexity;ground truth ambiguity;reasoning failures;adversarial robustness;factuality assessment;uncertainty quantification;evidence grounding;tree-of-thoughts;reliability and robustness;causal reasoning capability;proactive risk assessment;computational efficiency;actionable error attribution;traceable evidence chains;post-hoc error detection;trustworthy AI;domain-specific queries;temporal drift;Universal Village;system theory;metacognitive evaluation;mechanistic interpretability;multimodal hallucination;causal attribution},
  doi={10.1109/UV63228.2024.11189150},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11194853,
  author={Mrayhi, Salwa and Khribi, Mohamed Koutheair and Jemni, Mohamed},
  booktitle={2025 IEEE International Conference on Advanced Learning Technologies (ICALT)}, 
  title={RECMOOC4ALL: Bridging the Accessibility Gap in MOOCs}, 
  year={2025},
  volume={},
  number={},
  pages={196-198},
  abstract={The exponential growth of Massive Open Online Courses (MOOCs) has expanded educational access but has also overwhelmed learners with an excessive number of choices, making it difficult to identify courses that align with their skills, needs, and goals. To address this challenge, we introduce RecMOOC4All, an AI-powered, accessible MOOC aggregator that centralizes course discovery and intelligently recommends personalized learning pathways. At its core, RecMOOC4All integrates a hybrid recommender system, combining content-based and collaborative filtering to suggest courses based on learner profiles, preferences, accessibility requirements, and past interactions. An automated accessibility checker and remediation module ensures WCAG compliance, incorporating real-time captions, alternative text, and enhanced navigation to foster inclusivity. Furthermore, a generative AI-powered conversational chatbot provides real-time, context-aware assistance, while engagement detection modules analyze learner behavior to trigger timely interventions, improving retention and learner success. By seamlessly integrating personalization, accessibility, and engagement tracking, RecMOOC4All creates a cohesive, learner-centric MOOC environment, towards inclusive, adaptive, and effective education is for all learners.},
  keywords={Computer aided instruction;Ethics;Electronic learning;Navigation;Collaborative filtering;Media;Chatbots;Real-time systems;Recommender systems;Standards;MOOCs;Recommender system;adaptive Learning;Accessibility;Inclusivity;AI in education},
  doi={10.1109/ICALT64023.2025.00061},
  ISSN={2161-377X},
  month={July},}@INPROCEEDINGS{9938266,
  author={Pilania, Urmila and Diwakar, Chetna and Arora, Khushi and Chaudhary, Shruti},
  booktitle={2022 IEEE Global Conference on Computing, Power and Communication Technologies (GlobConPT)}, 
  title={An Optimized Hybrid approach to Detect Cataract}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Cataract, a disease in which the lens of the eye slowly turns opaque resulting in blurry vision, is one of the most occurring diseases among older people. According to the sources, more than 50% of people aged 80 or more have had eye cataract. While cataracts only used to affect elderly people, nowadays it is prevalent to see them in ages below 50 due to modern lifestyles, lack of exercise, continuous usage of mobile phones and computers. Over 12 million people are blind in India and 62.6% of blindness is a result of cataract. If the detection is done before the final stages of cataract, this disease can be cured in less time. Artificial Intelligence (AI) is a significant tool for the development of computer-aided technology. The retinal image is an important medical reference that helps in diagnosing cataract. This paper proposes a hybrid method using the Fuzzy Technique, Retinal Image Quality Assessment (RIQA), and Convolutional Neural Networks (CNN). The fuzzy technique is used for image quality improvement, RIQA is used for image quality assessment and CNN is used for processing of the retina to automatic cataract detection.},
  keywords={Cataracts;Image quality;Retina;Mobile handsets;Convolutional neural networks;Artificial intelligence;Older adults;Convolutional Neural Network;Retinal Image Quality Assessment;Fuzzy Technique;Cataract;Artificial Intelligence},
  doi={10.1109/GlobConPT57482.2022.9938266},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10493879,
  author={Kaushal, Rajanish Kumar and Pavan Kumar, T V V and N, Sharath and Parikh, Swapnil and L, Natrayan and Patil, Harshal},
  booktitle={2024 5th International Conference on Mobile Computing and Sustainable Informatics (ICMCSI)}, 
  title={Navigating Independence: The Smart Walking Stick for the Visually Impaired}, 
  year={2024},
  volume={},
  number={},
  pages={103-108},
  abstract={Blindness is a common disability that affects millions of people globally. According to the World Health Organization (WHO), there are approximately 285 million individuals in the globe who are visually challenged, with 39 million classified as blind and 246 million having low vision. The vast majority, about 90%, of these individuals live in developing countries. Conventional walking sticks are ineffective in identifying obstacles, offering a substantial challenge to the visually impaired. In response to this problem, the study proposes a revolutionary solution: the "smart blind stick." The recent technologies integrates sensors with Artificial Intelligence (AI) and wireless transmission to help visually impaired individuals. The identification of obstacles is vital to its functioning, which is especially important for people with visual impairments. The model consists of an ultrasonic sensor, a camera, and a speaker into the smart blind stick to tackle this issue. These components operate in tandem to detect items and notify the user as soon as they are detected. The smart stick also includes an emergency switch, GSM, and GPS capabilities. In an emergency, the user can activate this option, causing the system to send an SMS to a specified contact, replete with the user's actual location. The incorporation of these characteristics is an important step toward increasing the self-reliance of visually impaired people while navigating their surroundings. The smart blind stick is a companion as well as a walking assistance, allowing users to face daily problems with greater confidence and independence.},
  keywords={Legged locomotion;Wireless sensor networks;Technological innovation;Navigation;Virtual assistants;Visual impairment;Time factors;Brain Tumor;Deep Learning;Medical Image;Metrics;Segmentation;Performance Evaluation},
  doi={10.1109/ICMCSI61536.2024.00022},
  ISSN={},
  month={Jan},}@INBOOK{10952277,
  author={Ammanath, Beena},
  booktitle={Trustworthy AI: A Business Guide for Navigating Trust and Ethics in AI}, 
  title={Explainable}, 
  year={2022},
  volume={},
  number={},
  pages={75-92},
  abstract={Summary <p>Explainability in AI means it is possible to understand how an AI output was calculated, and the more explainable the system, the greater the human understanding of the AI's internal mechanics. Fostering trust in AI means in part demystifying how it works, and the variety of stakeholders surrounding AI deployment have different needs for explainability. The challenge for every enterprise leader is to determine who requires explainability in AI function, which type, and how that impacts business. Explainable AI allows data scientists to look deeply into parameters, weights, and correlations and seek blind spots and new opportunities that together deliver improved model performance. While driving toward explainability, there are considerations for intellectual property, privacy, and security. Explainability methods and deployment of AI generally can trend toward technocentric application and interrogation. The AI decision&#x2010;making and development process should make room for varying perspectives and needs that fall outside of the technical elements in AI.</p>},
  keywords={Artificial intelligence;Business;Data models;Lead;Automobiles;Stakeholders;Public transportation;Predictive models;Industries;Explainable AI},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781119867968},
  url={https://ieeexplore.ieee.org/document/10952277},}@INBOOK{10933823,
  author={Uyar, M. Umit},
  booktitle={Machine Learning and AI with Simple Python and Matlab Scripts: Courseware for Non-computing Majors}, 
  title={CNNs for Optical Character Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={115-131},
  abstract={Summary <p>This chapter introduces a convolutional neural network (CNN) for implementing a simple Optical Character Recognition (OCR) task, namely recognizing several hand&#x2010;written characters. The CNN in this example reads a hand&#x2010;drawn greyscale image of size and identifies the input character as one of the reference images that it has stored. The chapter introduces a CNN for the OCR class of problems, where a set of input image samples are used to train a CNN. OCR is widely used in applications such as postal automation, capturing licence plate numbers, conversion of physical documents to editable text files and assistance for the blind and visually impaired. We cannot employ artificial neural networks for OCR applications since they would be overwhelmed if multi&#x2010;dimensional images with high pixel counts were directly fed into them, which makes CNNs the most suitable AI tools for OCR applications.</p>},
  keywords={Optical character recognition;Training;Load modeling;Filters;Image recognition;Character recognition;Organizations;Urban areas;Text recognition;Presses},
  doi={10.1002/9781394294985.ch8},
  ISSN={},
  publisher={IEEE},
  isbn={9781394294978},
  url={https://ieeexplore.ieee.org/document/10933823},}@INPROCEEDINGS{11011637,
  author={Kumar, Neeraj and Kadam, Manjushri Abhishek and Santosh Gulavani, Sampada and Sankaradass, Veeramalai and Kadam, Kirti Rahul and Tiwari, Mohit},
  booktitle={2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)}, 
  title={IOT and NLP based Voice Assistant for Visual Impaired People}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={The visual impaired face a lot of limitations in today's culture where independence is crucial. The blind suffer from a disadvantage since they have to receive manual assistance to explore their surroundings. In addition to walking, eating, conversing with a group of people, etc., it can be challenging for someone with impaired vision to accomplish many necessary daily tasks. There has been a significant increase in the potential of artificial intelligence (AI) over the past few years because many professions require visual information. A voice assistant can converse with humans in natural language through cloud computing, and they unify AI through natural language processing.A voice assistant is most commonly used in smart speakers. In order to determine whether there is a connection between voice assistants and smart speakers and mental health, the study examined how voice assistants and smart speakers are used in daily life. In order to assist the blind, this test could serve as a study for a variety of computational, speech-recognition, and image-recognition approaches. Using Android smartphones, people who are blind can interact with their surroundings using voice commands, environment recognition, and feedback. The proposal is designed to create a single device solution that is simple, fast, accurate, and cost-effective. A number of remedies have been put forward after thorough research into each issue. However, all problems are not resolved simultaneously. A visionblind individual can see, hear, and speak with the technology, increasing their independence and confidence. People with visual impairments frequently seek outside assistance while making judgments. This proposal aims to aid blind people in overcoming their lack of visual perception by utilizing other senses, such as sound and touch.},
  keywords={Visualization;Visual impairment;Personal voice assistants;Mental health;Media;Natural language processing;Proposals;Artificial intelligence;Smart phones;Visual perception;Image recognition;Natural language processing;Artificial Intelligence;Voice Assistant},
  doi={10.1109/ICDSAAI65575.2025.11011637},
  ISSN={},
  month={March},}@INBOOK{10950375,
  author={Lavista Ferres, Juan M. and Weeks, William B. and Smith, Brad},
  booktitle={AI for Good: Applications in Sustainability, Humanitarian Action, and Health}, 
  title={Screening Premature Infants for Retinopathy of Prematurity in Low&#x2010;Resource Settings}, 
  year={2024},
  volume={},
  number={},
  pages={252-263},
  abstract={Summary <p>Retinopathy of prematurity (ROP) is a vision&#x2010;threatening disorder affecting premature infants. ROP is treatable: pediatric ophthalmologists can use lasers to stop abnormal vascular development that can otherwise lead to retinal detachment, visual impairment, and blindness. This chapter demonstrates how artificial intelligence&#x2010;informed models can be trained to assist the diagnosis and treatment process, expand screening in low&#x2010;resource settings, and better use scarce resources like pediatric ophthalmologists. ROP has become the leading cause of preventable childhood blindness throughout the world. ROP results from abnormal retinal vascular development. In later stages, fibrovascular proliferation leads to retinal detachment, visual impairment, and blindness. The chapter seeks to develop an artificial intelligence&#x2010;assisted solution to help screen prematurely born infants for ROP.</p>},
  keywords={Retina;Pediatrics;Retinopathy;Blindness;Medical services;Visual impairment;Personnel;Medical diagnostic imaging;Machine learning;Image color analysis},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781394235896},
  url={https://ieeexplore.ieee.org/document/10950375},}@INPROCEEDINGS{11063948,
  author={Bhosale, Ashish and Ashtekar, Abhinandan and Bang, Sushil and Raut, Roshani},
  booktitle={2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)}, 
  title={Optimizing Ethical Investment Decisions with Hybrid Heuristic}, 
  year={2025},
  volume={3},
  number={},
  pages={1924-1929},
  abstract={Utilizing Artificial Intelligence (AI) in ethical investing has seen rapid growth felicitating transparent corporate evaluations. Current assessments are done manually which have certain limitations and may tend to inaccuracy. This paper conducts an analysis of blind search and A* algorithm to evaluate their effectiveness in assessing reports for ethical investment opportunities. To identify and extract ethical terms fuzzy matching is used assisted by summarization tools providing a complete overview of reports to investors. The AI platform embedded with a customized heuristic search algorithm helps simplifying the decision-making process and generating insightful details for the right choices. After experimentation, results show that the time taken by heuristic search to display the relevant details is reduced significantly compared to the blind search with similar memory usage. Therefore, the proposed solution will transform the investing dynamics using AI, paving the way for more sustainable and socially responsible financial practices.},
  keywords={Ethics;Heuristic algorithms;Semantics;Decision making;Companies;Transforms;Transformers;Stakeholders;Artificial intelligence;Investment;Ethical Investing;Heuristic Search;Fuzzy Matching;Text Summarization},
  doi={10.1109/ICCSAI64074.2025.11063948},
  ISSN={},
  month={April},}@INPROCEEDINGS{11097658,
  author={Kong, Xiangrui and Liang, Li and Li, Jichuiyang and Quirke-Brown, Kieran and Lai, Zhihui and Olaru, Doina and Bräunl, Thomas},
  booktitle={2025 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={A Generative Self-Diagnosis Disengagement Reporting System for Autonomous Shuttles}, 
  year={2025},
  volume={},
  number={},
  pages={716-721},
  abstract={The increasing presence of autonomous vehicles on public roads has highlighted the limitations of traditional incident reporting systems, which rely on human-generated tables and descriptions. To address this, we propose a generative reporting framework that integrates a large language model (LLM) with semantic scene generation models. This framework utilizes perception snapshots and self-diagnostic data to generate detailed incident reports, addressing environmental blind spots. Our 3D scene completion network, combining diffusion and state-space models, reconstructs blind zones undetected by exterior sensors, achieving IoU scores of 41.92 on SSCBench-KITTI360 and 44.13 on SemanticKITTI. Public road experiments validate the system's ability to improve incident report quality while maintaining performance.},
  keywords={Visualization;Three-dimensional displays;Roads;Semantics;State-space methods;Sensors;Safety;Reliability;Autonomous vehicles;Testing},
  doi={10.1109/IV64158.2025.11097658},
  ISSN={2642-7214},
  month={June},}@INPROCEEDINGS{10859362,
  author={K, Vinoth Kumar and R, Thangamani and Ghalagi, Nikhita G and Rame, Neiphretuonuo and Purohith, T A Aruni and Badigar, Vinay V},
  booktitle={2024 9th International Conference on Communication and Electronics Systems (ICCES)}, 
  title={A Review Investigation on Current Trends in Smart Unsighted Cane Technology}, 
  year={2024},
  volume={},
  number={},
  pages={1417-1420},
  abstract={Blindness and other forms of vision impairment affect millions of people globally. By 2015,940 million people will have some degree of vision loss, predicts the World Health Organization. Most people with poor vision are above an age of 50 years. The advancement of technology has significantly improved the quality of life for individuals with visual impairments. Smart blind sticks, also known as electronic white canes or smart canes, are innovative devices that integrate cutting-edge technologies to enhance mobility, safety, and independence for visually impaired individuals. The review paper explores potential future approaches, including the integration of artificial intelligence and energy-efficient technology, and identifies present problems and constraints, including cost, power consumption, and social acceptance. This study seeks to contribute to the improvement of smart blind stick technologies, ultimately enhancing the quality of life for those who are visually impaired by offering a thorough analysis of recent developments.},
  keywords={Wireless sensor networks;Technological innovation;Navigation;Reviews;Visual impairment;Machine learning;Haptic interfaces;Stakeholders;Security;Smart devices;Haptic feedback;Secondary vision using Artificial Intelligence;Gesture Recognition using machine learning},
  doi={10.1109/ICCES63552.2024.10859362},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10333245,
  author={Khan, Ibrahim and Van Nguyen, Thai and Nimpattanavong, Chollakorn and Thawonmas, Ruck},
  booktitle={2023 IEEE Conference on Games (CoG)}, 
  title={Fighting Game Adaptive Background Music for Improved Gameplay}, 
  year={2023},
  volume={},
  number={},
  pages={1-2},
  abstract={This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding adaptive features. The adaptive BGM consists of three different categories of instruments playing the BGM of the winner sound design from the 2022 DareFightingICE Competition. The BGM adapts by changing the volume of each category of instruments. Each category is connected to a different element of the game. We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI agent that only uses audio as input (Blind DL AI). The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM.},
  keywords={Deep learning;Instruments;Games;Reinforcement learning;Artificial intelligence;adaptive BGM;Rule-based adaptive background music;background music},
  doi={10.1109/CoG57401.2023.10333245},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{11021071,
  author={Lecardonnel, Théo and Hurter, Christophe and Hurtut, Thomas},
  booktitle={2025 IEEE 18th Pacific Visualization Conference (PacificVis)}, 
  title={GenQA: A Method for Generating and Validating Question/Answer Pairs from Journalistic Data Material}, 
  year={2025},
  volume={},
  number={},
  pages={296-306},
  abstract={Data-visualizations are now commonly used in online press articles which often supports engaging data-driven stories. However, due to its visual nature, this type of content inherently lacks accessibility (e.g. when one wants to consume those visualizations using conversational agents, hearing them in audible formats, or using screen reader). Writing alternative texts is the recommended standard in order to provide text descriptions associated with an image. However, newsrooms rarely produce them for data-visualizations, or when they do, these are overly simplistic. Several intertwined limitations explain that situation like the limited amount of time journalists have to produce these expected detailed descriptions or the lack of precise and standardized writing guidelines for describing visualizations. To address this issue, we propose a new approach to help journalists generate descriptions of data-visualizations, based on a set of generated question and answer pairs (hereafter referred to as Q/A). Due to the previously enumerated limitations, our method first generates those Q/As using a generative AI model of Natural Language Processing (NLP). This approach alleviates and homogenizes the writing task workload and allows for a systematic and more exhaustive exploration of the possible Q/As for a given visualization. However, among the critical challenges of using AI-based generative tools in a journalism context is the risk of publishing unreliable or biased information. Therefore, the methodology proposed in this paper gives the journalist user a high level of control over the AI-generated Q/As. To enable and optimize this mandatory validation task, we design an interface where Q/As are grouped in terms of semantic and textual content, and accessibility interest. Visual cues are also displayed to improve the journalist’s decision-making. To evaluate this proposed methodology, that we call GenQA, we conducted a comparative design study that gathered journalists from two different Canadian newsrooms and teachers. We observed that GenQA was efficiently used by those users and helped them to produce detailed visualization descriptions that met their expectations in terms of quality and workload. This study also showed that GenQA triggered significant serendipity potential, allowing users to explore and produce Q/As that cover aspects they might not have considered.},
  keywords={Visualization;Presses;Systematics;Generative AI;Semantics;Writing;Journalism;Natural language processing;Standards;Guidelines;Accessibility;Data-visualization;Online press article;Artificial Intelligence;Natural Language Processing},
  doi={10.1109/PacificVis64226.2025.00036},
  ISSN={2165-8773},
  month={April},}@INPROCEEDINGS{10112926,
  author={Jagadesh, P. and Priya, Palagiri Vishnu and Deepak, T. and Anto, A. Gilbert and Jagannath, D. J.},
  booktitle={2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)}, 
  title={Design and Development of an Intelligent and Smart Helmet for Visually Impaired}, 
  year={2023},
  volume={1},
  number={},
  pages={249-253},
  abstract={In our society, it can be difficult for people who are blind or physically disabled to move around daily. They frequently fail to recognize nearby obstacles due to poor visibility and suffer injuries. Designing an assistive device that can take corrective action based on location, proximity, and fall event information could be one way to lessen the suffering of the blind as well as sight-impaired people. For this reason, the Smart Helmet for Sight Impaired is functional. This laser-equipped helmet can identify text, recognize obstacles, and describe people you encounter. By using picture recognition, its internal Artificial Intelligence (AI) can give rudimentary profiles of people. This helmet seems to take it a step beyond with its word and image detection. The helmet will make it easier for blind people to carry out daily activities. The wearers of the helmet will be allowed to go out, socialize, and engage in everyday activities, thanks to it.},
  keywords={Head;Text recognition;Communication systems;Blindness;Safety;Assistive devices;Artificial intelligence;Blind People;Physically disabled;Assistive Device;HC-SR04;Pi camera;obstructions;Gyro-sensor;Piezoelectric crystals;Ultrasound;Voice Feedback},
  doi={10.1109/ICACCS57279.2023.10112926},
  ISSN={2575-7288},
  month={March},}@INPROCEEDINGS{11114386,
  author={Khan, Ibrahim and Gursesli, Mustafa Can and Van Nguyen, Thai and Thawonmas, Ruck},
  booktitle={2025 IEEE Conference on Games (CoG)}, 
  title={Sound Judgment: A Multi-Year Evaluation of Audio Aesthetics and Gameplay Impact in DareFightingICE Sound Design Competition}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper presents a multi-year evaluation (2022-2024) of winning entries from the DareFightingICE Sound Design Competition, analyzing the interplay between audio aesthetics and gameplay functionality for visually impaired players and the Blind AI agent. Through user studies ($\mathrm{n} {=} {2 6}$ and $\mathrm{n} {=} {2 1}$) and a comprehensive ablation study of the 2024 winning sound design, we reveal three key findings: (1) While winning entries consistently achieved high aesthetic ratings, scores plateaued despite year-on-year improvements in Blind AI performance, indicating a decoupling of these dimensions; (2) Systematic muting identified “critical” sounds whose absence severely impaired AI decision-making, whereas static background music introduced detrimental noise; (3) Drastically reducing sound design to ${1 0 \%}$ of sound effects degraded both AI win ratios and human aesthetic perception. These results demonstrate that practical accessible audio requires prioritizing unambiguous cues for frequent actions, minimizing sonic redundancy like non-adaptive BGM, and validating sound designs through dual AI/human evaluation. We provide actionable strategies for designers targeting future competitions and inclusive gaming.},
  keywords={Systematics;Noise;Decision making;Redundancy;Music;Games;Minimization;Artificial intelligence;Sound Design;DareFightingICE;Sound Design Competition;Game Audio;Fighting Games},
  doi={10.1109/CoG64752.2025.11114386},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{10242558,
  author={Denandra, Rafi and Fariza, Arna and Prayogi, Yanuar Risah},
  booktitle={2023 International Electronics Symposium (IES)}, 
  title={Eye Disease Classification Based on Fundus Images Using Convolutional Neural Network}, 
  year={2023},
  volume={},
  number={},
  pages={563-568},
  abstract={The eye is a vital organ in the human body. Through the eyes, humans can absorb over 80% of visual information used to carry out various daily activities. However, in Indonesia, there are many cases of vision impairments that, if not properly addressed, can lead to blindness. Examples of such impairments include cataracts, glaucoma, and diabetic retinopathy. Currently, there are many research studies on eye diseases that are aided by technologies like Artificial Intelligence (AI). One of the AI technologies that is commonly developed is Convolutional Neural Network (CNN). CNN is a deep learning framework that excels in solving image or picture classification problems because its architecture applies convolution layers, which can break down images to extract features and easily reduce the high dimensionality without losing the image’s information itself. In this study, the author used various CNN models to find the most suitable model for classifying eye diseases, including EfficientNet, ResNet, Inception V3, and many others. Through various model training experiments, the accuracy for the ResNet50 model was found to be 95% with a loss of 17%, followed by Xception with an accuracy of 95% and a loss of 21%, and EfficientNetV2B2 with an accuracy of 92% and a loss of 25%.},
  keywords={Training;Glaucoma;Visualization;Electric potential;Visual impairment;Feature extraction;Convolutional neural networks;Eye;Visual Impairments;Cataract;Glaucoma;Diabetic Retinopathy;Fundus;AI;DL;ML;CNN},
  doi={10.1109/IES59143.2023.10242558},
  ISSN={2687-8909},
  month={Aug},}@INPROCEEDINGS{10392325,
  author={Sahu, Santosh Kumar and Sawarkar, Ankush D. and Sahu, Ajay Kumar and Anjikar, Akhil and Bhopale, Amol P and Chakole, Jagdish},
  booktitle={2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)}, 
  title={Analysis of Transfers Learning Techniques for Early Detection and Grading of Diabetic Retinopathy on Retinal Images}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Eye complications of diabetes include diabetic retinopathy (DR). It's caused by retinal blood vessel alterations. Working-age individuals' major cause of blindness is diabetic retinopathy. Early identification of diabetic retinopathy may prevent or postpone visual loss. Artificial intelligence (AI) can use computational image analysis methods to automatically identify and categorize diabetic retinopathy in retinal pictures. In this paper we study and analyze the different transfer learning models for early detection of diabetic retinopathy for retinal images. According to our findings, the accuracy of predictions ranges from 74% to 81%. It is worth noting that AI-based systems for diabetic retinopathy detection are still in the research phase, and more research is needed to evaluate their accuracy and effectiveness in real-world settings.},
  keywords={Computers;Diabetic retinopathy;Visualization;Analytical models;Image analysis;Computational modeling;Transfer learning;Artificial intelligence;Deep Learning;Transfer Learning;Diabetic Retinopathy;Multiclass Classification},
  doi={10.1109/ICACTA58201.2023.10392325},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11042812,
  author={K, Prathiksha and S G, Tamizh Malar and P, Nivedha and L, Pooja Sri and Johnson, Sandra},
  booktitle={2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={Ocularis-Ensuring Healthy Vision Through Intelligent Detection}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Ocular diseases can cause vision impairment or blindness if not detected early. “Ocular Disease Intelligent Recognition (Ocularis)” is a multimodal system designed for automated ocular disease detection and classification. It utilizes a dataset of 5,000 patient records, incorporating age, diagnostic keywords, and high-resolution fundus images captured from diverse imaging devices. The model focuses on classifying eight primary disease categories. To enhance diagnostic accuracy, ensemble learning techniques are employed by integrating ResNet, EfficientNet, and VGG19, leveraging their strengths for robust classification. Additionally, SHAP-based Explainable AI (XAI) provides interpretable visualizations of model predictions, aiding clinical validation. A Google Gemini LLM-powered recommendation system further assists in generating personalized insights based on the detected condition. Ocularis is designed to be a scalable and reliable screening tool for real-world healthcare applications. Future enhancements include predictive analytics for disease progression and extending the system to broader ophthalmic applications, improving early detection and clinical decision-making.},
  keywords={Deep learning;Visualization;Accuracy;Explainable AI;Decision making;Visual impairment;Medical services;Predictive models;Retina;Diseases;Ocular Disease;Fundus Imaging;Multimodal Approach;Machine Learning;Deep Learning;Ensembling Techniques;Explainable AI (XAI);Diagnostic Keywords;Medical Imaging},
  doi={10.1109/RMKMATE64874.2025.11042812},
  ISSN={},
  month={May},}@INPROCEEDINGS{10200300,
  author={Wang, Ru-Jun and Lai, Han-Rong and Wang, Shih-Jui and Kuo, Yu-Hsun and Wang, Chih-Hang and Chen, Wen-Tsuen and Yang, De-Nian},
  booktitle={2023 IEEE 97th Vehicular Technology Conference (VTC2023-Spring)}, 
  title={Adverse Event Prevention on The Road System with Collaborative MEC}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={The localization of Road Users (RUs) is an important issue in adverse event prevention due to the unreliable nature of GPS and the high cost of high-precision location acquisition sensors. In addition, previous research on adverse event prevention on roads has not taken into account RUs in blind spots at the same time. To address these issues, we investigate a collaboration system for heterogeneous RUs and Mobile Edge Computing (MEC) servers, called Collaborative Adverse Event Prevention system (CAEP) to efficiently alert RUs to potential adverse events and perceive the blind spot of the RUs. CAEP includes two AI-based functional modules, a localization module and a blind spot detection module, and an adverse event prevention algorithm. The localization module localizes each RU and the blind spot detection module detects the other RUs in the blind spot. The adverse event prevention algorithm jointly considers general road collision events and the event of a difference in radius between the inner wheels of a vehicle to completely include adverse events on the road. We implement CAEP in a real-world traffic environment with Jetson AGX Xavier devices and cameras to evaluate the performance. Our evaluation shows that CAEP provides RUs with sufficient preparation time to prevent adverse events and correctly detects the RUs in blind spots.},
  keywords={Location awareness;Vehicular and wireless technologies;Roads;Collaboration;Wheels;Feature extraction;Cameras;Collaborative MEC;adverse event prevention;localization;feature extraction;intelligent traffic system},
  doi={10.1109/VTC2023-Spring57618.2023.10200300},
  ISSN={2577-2465},
  month={June},}@INPROCEEDINGS{10962734,
  author={Sahu, Tirupati and Padhy, Neelamadhab and Panigrahi, Rasmita and Rajguru, Bijay Kumar},
  booktitle={2025 International Conference on Emerging Systems and Intelligent Computing (ESIC)}, 
  title={Object Detection and Tracking for the Visually Impaired with YOLO 11}, 
  year={2025},
  volume={},
  number={},
  pages={648-652},
  abstract={The latest advancements in AI and computer vision technology can grant artificial eyes to those who have not seen the world. With computer vision model YOLO 11, this is becoming a reality. This is a solution, which can empower blind people to "see" their surroundings through their ears. The people who are blind have had to depend heavily on others to carry out daily activities or simple tasks like traveling, reading, or counting currency are significant challenges. While it may not be possible to restore solution vision to everyone, we can use other senses to mitigate these difficulties. By using AI and computer vision, we are not just giving vision means sight to machines, but also providing enhanced sensory experiences like ears and a mouth. The same technology can provide eyes, ears, and even a mouth to humans who lack them. In this experiment we have taken small steps to aid those with visual impairments, enabling them to navigate their world with ease and confidence.Through this small experiment, blind people can detect approaching objects and respond accordingly. By utilizing a custom-trained model, the model can recognize familiar persons, surroundings, text (OCR), currency, and even obstacles like potholes on roads. The model can detect and track these nearby elements, generating text that is then converted into sound. Blind individuals can listen to this audio through earphones in real time and navigate their environment with greater independence and confidence.},
  keywords={YOLO;Computer vision;Navigation;Text recognition;Visual impairment;Mouth;Ear;Blindness;Artificial intelligence;Currencies;component;formatting;style;styling;insert (keywords)},
  doi={10.1109/ESIC64052.2025.10962734},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10620516,
  author={Ali, Waleed Abd-Elshafi and Saleh, Hatem Abdelhamid and Eid, Haneen Salah and Fathi, Ahmed Mohamed and Mohamed, Abd-El Rahman Amr},
  booktitle={2024 International Telecommunications Conference (ITC-Egypt)}, 
  title={Design and Implementation of Advanced Driver Assistance Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Advanced Driver Assistance Systems (ADAS) are a pivotal advancement in automotive technology, enhancing vehicle safety and driving experience. In our project, we have successfully designed and built a robotic prototype car. We have implemented a comprehensive control system for the car's movement, and the following features of ADAS systems: Bump Detection, Precise Lane Tracking, Adaptive Cruise Control, Traffic Sign Recognition, and Blind Spot Detection. Bump detection systems are equipped to detect and alert drivers of speed bumps and other road irregularities, in which we use NVIDIA Jetson Nano, Astra Pro Plus Depth camera as RGB and Depth sensor and use Custom AI Model based on YOLOv5 to detect different Bumps on the road and we use Python, TensorRT, and OpenCV. Lane Departure Warning (LDW) alerts drivers when their vehicle unintentionally drifts out of their lane, we use NVIDIA Jetson Nano as our main microcontroller unit (MCU) responsible for our Machine learning and Astra Pro Plus Depth camera as our RGB sensor, we use Python, Python's libraries such as OpenCV, and NumPy to implement LDW. Adaptive Cruise Control (ACC) adjusts the vehicle's speed automatically to maintain a safe distance from the vehicle ahead, we use NVIDIA Jetson Nano, Astra Pro Plus Depth camera as RGB and Depth sensor and use Custom AI Model based on YOLOv5 to detect different vehicles on the road, we use Python, TensorRT, and OpenCV. Traffic Sign Recognition can interpret and display traffic signs to keep drivers informed about speed limits, no-entry signs, and more, we use NVIDIA Jetson Nano, and Astra Pro Plus Depth camera as our RGB sensor and use Custom AI Model based on YOLOv5 to detect different Traffic Signs on the road, we use Python, TensorRT, and OpenCV. Blind Spot Detection (BSD) technology helps drivers identify vehicles in their blind spots and alerts them to avoid dangerous maneuvers, we use ESP32 as an MCU and Ultrasonic sensor, and use C++ to program the BSD feature.},
  keywords={YOLO;Microcontrollers;Robot sensing systems;Cameras;Feature extraction;Automobiles;Advanced driver assistance systems;ADAS;Robotic car;Bump Detection;Traffic Sign Recognition;Lane Tracing;Adaptive Cruise control;Blind Spot Detection;Object recognition using AI},
  doi={10.1109/ITC-Egypt61547.2024.10620516},
  ISSN={},
  month={July},}@INPROCEEDINGS{9824598,
  author={Duvvuri, Kavya and Chethana, Savarala and Charan, Sreevathsa Sree and Srihitha, Vemula and Ramesh, T. K. and K S, Srikanth},
  booktitle={2022 3rd International Conference for Emerging Technology (INCET)}, 
  title={Grad-CAM for Visualizing Diabetic Retinopathy}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={Diabetic Retinopathy (DR) is damage to the retina that occurs due to high blood glucose. This disease can lead to early blindness and hence it is important to diagnose it at an earlier stage. The use of artificial intelligence to aid a doctor in identifying such disorders is beneficial as it reduces the time spent by them. State of art model using the convolutional neural network (CNN) has an accuracy of 75%, however, CNNs are black-box models, and it is difficult to interpret their functioning. Hence, the outcome provided by the model cannot be trusted which warrants the use of explainable AI techniques to provide explanations that justify the model’s output. In this paper, we have enhanced the state of art model using a CNN to identify DR which gave an accuracy of 92%. Gradient-Weighted Class Activation Map (Grad-CAM) explainability is used to verify the regions considered significant by the model. The application presents the classification outcome along with explainability thereby aiding the medical practitioners to arrive at a decision.},
  keywords={Training;Visualization;Art;Retinopathy;Computational modeling;Retina;Real-time systems;CNN;Explainable Artificial Intelligence;Grad-CAM;Diabetic Retinopathy},
  doi={10.1109/INCET54531.2022.9824598},
  ISSN={},
  month={May},}@INPROCEEDINGS{10093089,
  author={Aiche, Ishaq and Brik, Youcef and Attallah, Bilal and Lahmar, Hanine and Zohra, Ziani},
  booktitle={2022 International Conference of Advanced Technology in Electronic and Electrical Engineering (ICATEEE)}, 
  title={Transfer Learning for Diabetic Retinopathy Detection}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={According to the International Diabetes Federation (IDF), there will be 552 million diabetics by 2034. The most common form of diabetic disease can affect the eyes, which called Diabetic Retinopathy (DR). It is a major factor in the development of blindness. Recently, Artificial Intelligence (AI) and deep learning (DL), two emerging computer science approaches, have boosted the possibility of detecting DR in its early phases, which means patient’s chances of losing their vision in the future will decrease. This paper presents a multilevel detection system for DR with different severity using transfer learning techniques. We used the APTOS2019 dataset from Kaggle, which contains retinal pictures, to conduct our experiments. Then, we use deep transfer learning technique with five models to generate the DR images features. The obtained results are very satisfactory in terms of accuracy.},
  keywords={Protocols;Retinopathy;Transfer learning;Medical services;Feature extraction;Retina;Diabetes;Diabetic Retinopathy (DR);disease detection;deep transfer learning;medical imaging;multilevel classification},
  doi={10.1109/ICATEEE57445.2022.10093089},
  ISSN={},
  month={Nov},}
