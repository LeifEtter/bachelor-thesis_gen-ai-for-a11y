@INPROCEEDINGS{10431857,
  author={Da Silva, Márcio Antônio Pontes and Mafalda, Me. Salomão Machado and Alvarez, Ana Beatriz and Chavez, Roger Fredy Larico},
  booktitle={2023 IEEE 23rd International Conference on Bioinformatics and Bioengineering (BIBE)}, 
  title={Optic Disc Localization from Retinal Fundus Image Using Discrete Cosine and Hough Transforms}, 
  year={2023},
  volume={},
  number={},
  pages={134-139},
  abstract={Diabetic retinopathy, resulting from diabetes mellitus, is one of the main causes of global blindness and visual impairment. The integration of artificial intelligence in the analysis of retinal images helps healthcare professionals, with the precise location of the optic disc being essential for detecting vital components of the retina. In this paper, we present a method that uses the cosine transform in conjunction with the circular Hough function. This method also involves converting retinal images from the RGB color space to an adapted grayscale, focusing only on the Red and Green channels. The dataset used for the experiment was RIGA, consisting of the subsets Messidor, Magrabia, and BinRushed. The results showed 100% accuracy for Messidor, 97.87% for Magrabia, and 98.46% for BinRushed, achieving an overall accuracy of 99.33%.},
  keywords={Location awareness;Biomedical optical imaging;Image color analysis;Transforms;Gray-scale;Retina;Optical imaging;optic disc localization;cosine transform;hough transform;eye disease},
  doi={10.1109/BIBE60311.2023.00029},
  ISSN={2471-7819},
  month={Dec},}@INPROCEEDINGS{11156650,
  author={Kataria, Preeti and Khatkar, Monika},
  booktitle={2025 2nd International Conference On Multidisciplinary Research and Innovations in Engineering (MRIE)}, 
  title={Diabetic Retinopathy Detection: A Survey of Cutting-Edge Methods and Comparative Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={144-148},
  abstract={One of the leading causes of preventable blindness in the world, diabetic retinopathy must be managed effectively, and it must be managed at an early stage. In this review article, we have presented extensive coverage of the current advances in computer-aided diabetic retinopathy detection and diagnosis using artificial intelligence-based approaches. This review considers the application of deep learning and machine learning algorithms to retinal imaging for an overview of their capabilities, performance, and potential in practice. The review encompasses diabetic retinopathy at all stages, from diagnosis to severity level determination. It offers insight into how these novel methods could streamline screening processes while improving patient outcomes. This paper comprehensively compares both pre-trained models, using deep learning and machine learning methods.},
  keywords={Deep learning;Training;Diabetic retinopathy;Technological innovation;Reviews;Computational modeling;Transfer learning;Blindness;Data models;Convolutional neural networks;Deep Learning;Convolutional Neural Networks;Diabetic Retinopathy Detection},
  doi={10.1109/MRIE66930.2025.11156650},
  ISSN={},
  month={July},}@INPROCEEDINGS{10809027,
  author={Liu, Xiaohan and Hu, Menghan and Wu, Yue},
  booktitle={2024 7th International Conference on Information Communication and Signal Processing (ICICSP)}, 
  title={Prediction of Cup-to-Disc Ratio Progression Based on Longitudinal Datasets of Glaucoma Patients}, 
  year={2024},
  volume={},
  number={},
  pages={1099-1103},
  abstract={Glaucoma is the first irreversible cause of blindness. People have been working hard on the prediction of glaucoma and the progression of visual field, the segmentation of optic cup and disc, and the influencing factors of the change of cup-disc ratio, but there are few studies on the prediction of cup-disc ratio. In this paper, we propose an artificial intelligence model to predict future progress in the cup-to-plate ratio of glaucoma patients based on individual basic data and longitudinal data related to their disease. The main challenge is that the existing data sets have problems with time alignment. To solve this challenge, we introduce a method called feature transformation. In addition, we also use data smoothing methods to deal with data noise and improve model generalization. The experimental results show that our model has ideal effect and excellent generalization. After data smoothing, in test sets, R2 of cross-validation (k=10) is from 0.97 to 0.98, MSE of cross-validation (k=10) is from 0.0003 to 0.0005. In the external validation sets, R2 is 0.85 and the MSE is 0.0034.},
  keywords={Glaucoma;Integrated optics;Visualization;Smoothing methods;Noise;Blindness;Predictive models;Data models;Optical signal processing;Diseases;glaucoma;cup-disc ratio;artificial intelligence;model;progression},
  doi={10.1109/ICICSP62589.2024.10809027},
  ISSN={2770-792X},
  month={Sep.},}@INPROCEEDINGS{11093191,
  author={Zhang, Xuanyu and Tang, Zecheng and Xu, Zhipei and Li, Runyi and Xu, Youmin and Chen, Bin and Gao, Feng and Zhang, Jian},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={OmniGuard: Hybrid Manipulation Localization via Augmented Versatile Deep Image Watermarking}, 
  year={2025},
  volume={},
  number={},
  pages={3008-3018},
  abstract={With the rapid growth of generative AI and its widespread application in image editing, new risks have emerged regarding the authenticity and integrity of digital content. Existing versatile watermarking approaches suffer from tradeoffs between tamper localization precision and visual quality. Constrained by the limited flexibility of previous framework, their localized watermark must remain fixed across all images. Under AIGC-editing, their copyright extraction accuracy is also unsatisfactory. To address these challenges, we propose OmniGuard, a novel augmented versatile watermarking approach that integrates proactive embedding with passive, blind extraction for robust copyright protection and tamper localization. OmniGuard employs a hybrid forensic framework that enables flexible localization watermark selection and introduces a degradation-aware tamper extraction network for precise localization under challenging conditions. Additionally, a lightweight AIGC-editing simulation layer is designed to enhance robustness across global and local editing. Extensive experiments show that OmniGuard achieves superior fidelity, robustness, and flexibility. Compared to the recent state-of-the-art approach EditGuard, our method outperforms it by 4.25dB in PSNR of the container image, 20.7% in F1-Score under noisy conditions, and 14.8% in average bit accuracy.},
  keywords={Location awareness;Visualization;Accuracy;Generative AI;Forensics;Watermarking;Transforms;Copyright protection;Robustness;Pattern recognition;generative ai securityproactive manipulation localization;versatile deep image watermarking;aigc editing},
  doi={10.1109/CVPR52734.2025.00286},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10465934,
  author={Abbas, Ahsan and Alzahrani, Abdulkareem and Imran, Azhar and Almuhaimeed, Abdullah and Khan, Ali Haider},
  booktitle={2023 25th International Multitopic Conference (INMIC)}, 
  title={A Transfer Learning Based Detection and Grading of Cataract using Fundus Images}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Among the most prominent causes for visual impairment, especially among older adults, is cataract. As per the World Health Organisation (WHO), there are 2.2 billion individuals globally are estimated to be blind or possess an eyesight problem. One of the most identified and important causes of this is cataracts. Cataracts should be identified and treated as soon as possible to avoid blindness. Ophthalmologists use an expensive slit lamp to diagnose cataracts in regions with few medical facilities. Consequently, the issue is that a lack of skilled ophthalmologists may delay the identification of cataracts, for which medical treatment is unavoidable. Medical image analysis based on artificial intelligence provides a rapid and precise diagnosis in modern healthcare. We utilized deep learning models based on transfer learning, namely VGG19, and ResNet-50, to diagnose cataracts using fundus images and enhance classification accuracy. The performance metric for the model is accuracy; the highest achieved accuracy was 98%.},
  keywords={Cataracts;Deep learning;Measurement;Glaucoma;Image analysis;Transfer learning;Visual impairment;Medical treatment;Older adults;Medical diagnostic imaging;Deep learning Models;Medical Data;Fundus images;Cataract},
  doi={10.1109/INMIC60434.2023.10465934},
  ISSN={2835-8864},
  month={Nov},}@INPROCEEDINGS{10086210,
  author={Liu, Jinhao and Lv, Zhiyong and Xu, Jiahui},
  booktitle={2022 3rd International Conference on Electronics, Communications and Information Technology (CECIT)}, 
  title={A review of research on traceability and positioning of marine rescue}, 
  year={2022},
  volume={},
  number={},
  pages={124-128},
  abstract={In ship capsize emergency rescue, time is life, and traceability and identification are the keys to winning rescue time. Through the research on the detection and identification method of traceability information, the physical connotation of the signal can be identified at the first time, the air pressure, draft and remaining buoyancy of the submerged cabin can be detected, and the call for help and traceability can be heard. Combined with relevant research at home and abroad, it focuses on four aspects: traceable signal propagation mechanism, cavity hydroelastic modal analysis, multi-parameter signal identification, and black-box traceable passive positioning, using the integrated empirical modal analysis of traceable mixed signals and blind source separation. technique, Analytical Modal Characteristics. Use the cepstrum to identify the modal signal, establish the mapping relationship between the water pressure, draught, residual buoyancy and modal characteristics of the cabin, analyze the excitation, transmission and response mechanism of the propagation characteristics of the traceable signal of the submerged cabin, and use artificial nerves. The network method avoids the difficulty of detection and identification of the traceability black box, and realizes the detection of rescue information of overturned ships. Reconstructing the rescue call sound through artificial intelligence and using the reconstructed time domain signal for passive passive positioning of the call for help, filtering the noise of the monitoring sound, avoiding the blindness, physical exertion and risk of diving and exploration, and improving the rescue efficiency.},
  keywords={Water;Location awareness;Filtering;Modal analysis;Buoyancy;Information and communication technology;Object recognition;ship capsize rescue;blind source separation;modal feature analysis;black box traceability and localization},
  doi={10.1109/CECIT58139.2022.00030},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11141198,
  author={Masri, Sari and Ashqar, Huthaifa I. and Elhenawy, Mohammed},
  booktitle={2025 IEEE 4th International Conference on Computing and Machine Intelligence (ICMI)}, 
  title={Visual Reasoning at Urban Intersections: Fine-Tuning GPT-4O for Traffic Conflict Detection}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Traffic control in unsignalized urban intersections presents significant challenges due to the complexity, frequent conflicts, and blind spots. This study explores the capability of leveraging Multimodal Large Language Models (MLLMs), such as GPT-4o, to provide logical and visual reasoning by directly using birds-eye-view videos of four-legged intersections. In this proposed method, GPT-4o acts as intelligent system to detect conflicts and provide explanations and recommendations for the drivers. The fine-tuned model achieved an accuracy of 77.14 %, while the manual evaluation of the true predicted values of the fine-tuned GPT-4o showed significant achievements of 89.9 % accuracy for model-generated explanations and 92.3 % for the recommended next actions. These results highlight the feasibility of using MLLMs for real-time traffic management using videos as inputs, offering scalable and actionable insights into intersections traffic management and operation. Code used in this study is available at https://github.com/sarimasri3/Traffic-Intersection-Conflict-Detection-using-images.git.},
  keywords={Visualization;Accuracy;Large language models;Manuals;Predictive models;Traffic control;Cognition;Real-time systems;Machine intelligence;Videos;Intersection Management;Intersections;Multimodal Large Language Models (MLLMs);Conflict Detection},
  doi={10.1109/ICMI65310.2025.11141198},
  ISSN={},
  month={April},}@INPROCEEDINGS{10672972,
  author={Wase, Ayush Mahadeo and Siddharth Wankhede, Chaitanya and Malode, Atharva Ankush and Madankar, Mangala},
  booktitle={2024 7th International Conference on Circuit Power and Computing Technologies (ICCPCT)}, 
  title={Mitigating Class Imbalance through Multimodal Fusion in VGG19 for Cataract Detection}, 
  year={2024},
  volume={1},
  number={},
  pages={6-10},
  abstract={Global analysis indicates a notable rise in non- infectious vision impairments, with India seeing a pronounced spike in cases. Numerous international investigations have shown that Indian populations face an elevated likelihood of developing cataracts, glaucoma, and even permanent blindness. This article puts forth a feasible solution for early cataract screening, tapping into state-of-the-art Artificial Intelligence technologies to pinpoint potential cataract affliction. Remarkably, this was accomplished with tremendous computational efficiency—the VGG19 deep neural network model managed to train on a modest dataset of only 870 photographic images, yet still achieved excellent diagnostic performance with 99% accuracy. Though rudimentary, this preliminary AI framework demonstrates considerable promise to assist in cataract risk stratification and prevention efforts on a global scale, with particular utility in high-risk Indian communities. Additional research with more expansive datasets will further refine the precision and generalizability of the model across human populations. But these initial findings provide a crucial first step toward ameliorating worldwide visual morbidity using sophisticated decision-support tools.},
  keywords={Cataracts;Deep learning;Visualization;Accuracy;Computational modeling;Prevention and mitigation;Visual impairment;Cataract Detection;VGG19;Deep Learning;Ocular Disease Screening},
  doi={10.1109/ICCPCT61902.2024.10672972},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10390523,
  author={Kumar, Deepak and Bakariya, Brijesh and Verma, Chaman and Illes, Zoltan},
  booktitle={2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS)}, 
  title={Cataract Disease Identification Using Transformer and Convolution Neural Network: A Novel Framework}, 
  year={2023},
  volume={},
  number={},
  pages={1230-1235},
  abstract={The significance of disease detection approaches based on deep learning (DL) in medical research, driven by artificial intelligence (AI), is gaining considerable attention. However, research in this domain encounters challenges in achieving the desired level of progress. These challenges stem from the diverse range of health diseases and the unique regional characteristics associated with many of these disease types. Among the diseases affecting the eyes, cataracts, a frequently encountered eye condition, can lead to visual impairments. Detecting cataracts accurately and in a timely manner is crucial for effective risk management and preventing the potential progression toward blindness. This paper introduces a deep neural network that utilizes convolutional neural network (CNN) models, namely VGG16 and ResNet50, and a Vision Transformer (ViT) based approach. These models are specifically designed for automatic cataract detection in eye images. Additionally, media noise filtering, implemented as median filtering, is employed as a preprocessing technique to reduce noise and enhance overall image quality. In addition, methods of data augmentation are utilised to combat the problem of overfitting. These methods involve expanding the size of the dataset prior to the training of the model. Based on the results of the experimental study, it is evident that the ViT method outperforms existing cataract detection approaches, demonstrating an impressive accuracy of 70%.},
  keywords={Cataracts;Adaptation models;Filtering;Transformers;Convolutional neural networks;Diseases;Residual neural networks;Cataract Identification;Deep Neural Network;Classification;Transformer;Data Augmentation},
  doi={10.1109/ICTACS59847.2023.10390523},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10270111,
  author={Al-Haj, Yousif A. and Al-Badawi, Badr A. and Al-Nashad, Khadeja M. and Al-Falah, Marwan M. and Nasr, Akram Ali Othman and Al-Dubaibi, Haitham A.},
  booktitle={2023 International Conference on Artificial Intelligence Science and Applications in Industry and Society (CAISAIS)}, 
  title={Smart Diagnosis System For Diabetic Retinopathy}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Diabetic retinopathy (DR) presents a substantial vision health challenge, demanding early detection. However, limited ophthalmologist resources and meticulous manual diagnosis hinder swift and precise identification. Leveraging cutting-edge advances in smart healthcare and AI, we introduce an innovative solution-a groundbreaking DR diagnostic system powered by EfficientNet CNN and transfer learning. Utilizing the APTOS 2019 Blindness Detection dataset ensures both efficiency and accuracy. Notably, our system stands out by offering a simultaneous diagnosis of various retinal diseases, setting it apart from previous efforts. Impressively, our model achieves a remarkable 93.80% accuracy, surpassing benchmarks across dimensions. Guided by user-friendly interfaces, doctors and assistants seamlessly input data, generate medical cards, and access patient reports. Balancing design, ImageNet pre-training, and deep learning, our solution bridges ophthalmologist gaps, providing precise DR detection. Its potential extends further, promising improved patient care and broader medical applications. Amid AI-driven healthcare transformation, our pioneering intelligent diagnostic system shines as a beacon of positive impact.},
  keywords={Industries;Deep learning;Diabetic retinopathy;Transfer learning;Medical services;Manuals;Retina;Diabetic retinopathy;EfficientNet;Transfer learning;Ophthalmology;Computer vision;fluorescein},
  doi={10.1109/CAISAIS59399.2023.10270111},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11156527,
  author={Rout, Himansu Sekhar and S, Saravana Kumar},
  booktitle={2025 International Conference on Innovations in Intelligent Systems: Advancements in Computing, Communication, and Cybersecurity (ISAC3)}, 
  title={Diabetic Retinopathy Detection and Grading: A Comparative Survey of Existing Techniques}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Diabetic retinopathy (DR), a primary contributor to vision impairment in individuals with diabetes, requires automated systems for prompt detection and classification. This paper offers a thorough comparative analysis of traditional machine learning and deep learning methodologies for DR assessment, emphasising architectures, datasets, evaluation metrics, and clinical relevance. Despite the superior performance of convolutional neural networks (CNNs) and attention-based models, issues such as class imbalance, dataset bias, and model interpretability remain unresolved. We assess the shortcomings of accuracy-focused metrics in imbalanced datasets and recommend supplementary measures such as F1-score and AUC-ROC. The survey emphasises the inadequate use of cross-dataset validation and external testing, which are essential for real-world generalisation. Strategies to alleviate these challenges—such as lightweight CNNs, federated learning, and explainable AI (XAI)—are presented as viable solutions. Additionally, we examine hybrid frameworks (e.g., ensemble models, domain adaptation) and their function in addressing computational efficiency with diagnostic precision. The paper concludes with practical recommendations for future endeavours, highlighting the necessity for ethical AI implementation, integration of multimodal imaging, and collaborative frameworks to mitigate global diabetic blindness. This review connects technological innovations with clinical applicability, providing researchers a systematic framework for creating resilient, interpretable, and implementable diabetic retinopathy screening systems.},
  keywords={Deep learning;Surveys;Diabetic retinopathy;Technological innovation;Adaptation models;Reviews;Federated learning;Explainable AI;Visual impairment;Blindness;Diabetic retinopathy;detection;grading;deep learning;CNN;machine learning},
  doi={10.1109/ISAC364032.2025.11156527},
  ISSN={},
  month={July},}@INPROCEEDINGS{10401089,
  author={Nalini, M. and Bommi, R. M. and Uganya, G. and Mary Joy Kinol, A.},
  booktitle={International Conference on Computer Vision and Internet of Things 2023 (ICCVIoT'23)}, 
  title={Intelligent diagnostic system for early stage prediction of diabetes}, 
  year={2023},
  volume={2023},
  number={},
  pages={218-223},
  abstract={The common fatal disease in today's world is diabetes. Additionally, it is the common creator of a number of many types of illnesses, including cardiac failure, blindness, diseases of the urinary system, etc. Diabetes is a long-term illness that arises either whenever the pancreas is unable to create the required or when the human body is unable to utilize the insulin. The hormone insulin is in charge of controlling how much sugar is present in the bloodstream. A prevalent complication of uncontrolled diabetes is hyperglycemia, also referred to as hyperglycemia. Numerous organs, most notably the nerves and blood arteries, sustain considerable damage from hyperglycemia over time. As per the WHO- World Health Organization, more than 1.5 million population die from diabetes every year. In these circumstances, the patient must visit a hospital or diagnostic facility for a check-up and to receive reports. By automating tasks that would otherwise be done by people, the application of artificial intelligence improves the lives of patients, doctors, and hospital administrators (Choudhury and Gupta, 2019). AI's current objectives include enhancing medical treatment procedures and reducing the frequency of diabetes diagnosis errors. The medical infrastructure of today produces a considerable amount of data. As a result, more advanced techniques rely on this information to create more accurate models.In this study, an effort will be made to produce a model that utilizes machine learning principles to analyze both new and old cases of diabetes in order to accurately identify new problems. The hospital's administrative personnel and patients will profit from this. In this work, a machine learning-based model for predicting diabetes is presented.},
  keywords={},
  doi={10.1049/icp.2023.2879},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11089411,
  author={Gupta, Indresh Kumar and Mishra, Awanish Kumar and Srivastava, Swati and Rodrigues, Joel J. P. C.},
  booktitle={2025 6th International Conference on Recent Advances in Information Technology (RAIT)}, 
  title={Enhanced Diabetic Retinopathy Detection and Classification Model Using SqueezeNet with Whale Optimization Algorithm}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Diabetic Retinopathy (DR) is a severe eye condition predominantly impacting people suffering from diabetes, and it stands as a major contributor to adult blindness. Traditional detection methods involving optometric physicians are laborious and time-consuming. Recent advancements in artificial intelligence (AI) enable more efficient automatic DR detection. This paper presents an Enhanced Diabetic Retinopathy Detection and Classification Model using the Whale Optimization Algorithm (EDRDCM-WOA). The EDRDCM-WOA automates DR classification by employing Gabor filtering (GF) for noise removal during preprocessing. It uses the SqueezeNet method to generate feature vectors and applies the Whale Optimization Algorithm (WOA) for optimal hyperparameter tuning, enhancing classification performance. The Kernel Extreme Learning Machine (KELM) is then used for final classification. Simulations on benchmark dataset demonstrate that the EDRDCM-WOA outperforms recent models, highlighting its effectiveness in DR detection and classification.},
  keywords={Diabetic retinopathy;Filtering;Extreme learning machines;Noise;Benchmark testing;Whale optimization algorithms;Vectors;Classification algorithms;Kernel;Tuning;Diabetic Retinopathy Detection;Gabor Filtering;Deep Learning;Whale Optimization Algorithm;Kernel Extreme Learning Machine},
  doi={10.1109/RAIT65068.2025.11089411},
  ISSN={2994-287X},
  month={March},}@INPROCEEDINGS{11072956,
  author={Sah, Swati and Sulaiman, Rejwan Bin and Aljaidi, Mohammad and Nur, Abdullah Hafez and Zaman, Snigdha and Alsarhan, Ayoub and Alshammari, Sami Aziz},
  booktitle={2025 16th International Conference on Information and Communication Systems (ICICS)}, 
  title={Accurate Prediction of the Diabetic Retinopathy Using Fundonet Memory Classifier}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Diabetic retinopathy (DR) is a serious complication of diabetes that can lead to blindness if not detected early. Key indicators include microaneurysms (red spots) and exudates (yellow lesions), and early detection can help preserve vision. This study proposes an effective method for DR detection using a sequence of image processing and AI techniques. First, histogram equalization and median Gabor filtering are applied to retinal fundus images to reduce noise. Segmentation is performed using the Wrapper Minute Seed Approach (WMSA), followed by feature extraction via the Dimension-Radii Co-occurrence Matrix (DRCOM). An AI-based Fundonet Memory Classifier (FMC) then categorises images as normal, abnormal, or severe. The proposed method outperforms traditional techniques in terms of accuracy, precision, reliability, and predictive value.},
  keywords={Road transportation;Optical filters;Diabetic retinopathy;Accuracy;Noise;Retina;Optical imaging;Wool;Reliability;Medical diagnostic imaging;Highway Fundonet memory classifier;Retinal image;Diabetic retinopathy},
  doi={10.1109/ICICS65354.2025.11072956},
  ISSN={2573-3346},
  month={July},}@INPROCEEDINGS{10941258,
  author={Islam, Md. Sariful and Bappy, Md Tusher Ahmad and Shawon, Jubayer Ahmed and Hasan, Mehedi and Rahman, Wahidur and Akter, Lija and Azad, Mir Mohammad},
  booktitle={2024 IEEE International Conference on Signal Processing, Information, Communication and Systems (SPICSCON)}, 
  title={Cat-CNN: Human Eye Cataract Detection from Color Fundus Photograph with Deep CNN with Optimized Cascaded Network}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Cataracts are clouding of the lens in the eye, leading to loss of vision that can progress to blindness if not treated. This paper proposed a new method for automatic cataract detection using color fundus images and deep learning methods. A dataset of 1,105 color fundus images labeled by expert ophthalmologists was used in this process. We used seven pretrained CNNs (DenseNet121, EfficientNetB0, MobileNetV2, InceptionV3, Xception, ResNet50, VGG16, and VGG19) for feature extraction before reducing the extracted features using PCA. We used the following combination of machine learning classifiers: SVC, RF, Decision Tree, Gaussian Naive Bayes, XGBoost, K-Nearest Neighbors, and Logistic Regression. For evaluating the models' performance, we used accuracy, precision, recall, F1-score, and computational efficiency. For all metrics, MobileNetV2with Random Forest achieved perfect scores: 100% accuracy, precision, recall, and F1-score, with an average processing time of 669 ms ± 28.8 ms. Thus, it can be applied in real-time applications. EfficientNetB0 with SVC gave an average accuracy of 87.33%, with the rest of the precision and recall metrics above 86%. Then, ResNet50, VGG16, and VGG19 followed with high accuracies between the range of 89.64% to 90.50%. It systemizes the proper choice of architectures of CNNs and classifiers, making the system both accurate and computationally efficient. Future work will include augmentation of the dataset, real-time support in the clinical setting, and advanced techniques for image preprocessing, generative adversarial networks. In addition, the development of an automated annotation tool, improvement of explainable AI, will further improve the deployment of robust AI systems in early diagnosis of cataracts, enhancing the outcome for patients.},
  keywords={Cataracts;Measurement;Deep learning;Accuracy;Image color analysis;Static VAr compensators;Feature extraction;Real-time systems;Residual neural networks;Principal component analysis;Cataract detection;color fundus photography;deep learning;CNN;feature extraction;machine learning;PCA;image classification;ophthalmology},
  doi={10.1109/SPICSCON64195.2024.10941258},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10917507,
  author={Hossain, Shahed and Hasan, Md. Zahid and Jim, Risul Islam and Shuva, Taslima Ferdaus and Rahman, Md. Tanvir and Bulbul, Abdullah Al-Mamun and Khan, Risala Tasin and Kaiser, M. Shamim and Moni, Mohammad Ali},
  booktitle={2024 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={GDRNet: A Novel Graph Neural Network Architecture for Diabetic Retinopathy Detection}, 
  year={2024},
  volume={},
  number={},
  pages={378-387},
  abstract={Diabetic retinopathy is a significant cause of global blindness, requiring practical early detection approaches that could save vision loss in millions of people. However, manual DR analysis is time-consuming and requires skilled clinicians. The advancement of artificial intelligence can facilitate early DR predictions. This study proposed GDRNet, a novel AI-empowered diagnosis system that utilizes graph theory for effective feature selection in DR grading classification. The EyePACS, Messidor, APTOS, IDRid, and DDR datasets are initially balanced using the nearest neighbor oversampling approach. A deep graph correlation network (DGCN) extracts unique features from color eye fundus images by identifying intra-class connections. Then, an iterative random forest algorithm is employed for feature curation, ranking the most significant features from the DGCN. Subsequently, the iterative random forest enhances classification robustness by refining feature representations and aggregating multi-scale contextual information. Finally, a classifier using extreme gradient boosting based on a decision tree algorithm is trained with the optimized features to predict the outcomes. Experimental results reveal that GDRNet outperforms state-of-the-art DR grading classification methods with outstanding performance across various datasets: 100% specificity, 99.67% sensitivity, and 99.80% accuracy on Messidor; 100% specificity, 99.61% sensitivity, and 99.41% accuracy on APTOS; and comparable results on IDRid and DDR datasets. On the EyePACS dataset, it achieves 100% specificity, 99.20% sensitivity, and 99.50% accuracy. Based on these numerical findings, we expect that GDRNet could be utilized in healthcare for early and automated DR detection.},
  keywords={Diabetic retinopathy;Accuracy;Sensitivity;Correlation;Feature extraction;Boosting;Graph neural networks;Classification algorithms;Random forests;Medical diagnostic imaging;Diabetic Retinopathy;Graph Correlation Network;Nearest Neighbour Oversampling;Extreme Gradient Boosting},
  doi={10.1109/ICDMW65004.2024.00055},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{11088736,
  author={Devendiran, RamKumar and Vyshnavi, Kolluri and Gajula, Sruthi Krishna and Vidavalapati, Harshitha},
  booktitle={2025 11th International Conference on Communication and Signal Processing (ICCSP)}, 
  title={DiabEye: an Automated Deep Learning Approach for Early Detection of Diabetic Retinopathy}, 
  year={2025},
  volume={},
  number={},
  pages={426-430},
  abstract={Diabetic Retinopathy (DR) is among the most common retinopathy diseases caused by diabetes and is a leading factor of vision loss and blindness, especially among diabetics. The prevention of severe complications and better outcomes of patients is highly dependent on timely diagnosis and treatment. This study discusses the creation of DiabEye, a deep learning- based automated diagnostic system for Diabetic Retinopathy aimed at developing AI powered solutions. The research utilizes the DenseNet121 model, where the network was trained to maximize validation accuracy of 73.01%, which turned out to be the highest accuracy that was achieved among all tested configurations. The system is able to accurately stratify and classify DR severity into five categories: none, mild, moderate, severe and proliferative DR by analyzing fundus visual. This allows more accurate decision making by the professionals involved in the healthcare service delivery. DiabEye provides effective and automated solution for early detection of DR while enabling real time predictions and minimizing the need for head-on diagnosis, thus improving clinical efficiency. DiabEye can greatly improve DR screening due to its high accuracy combined with real-time capabilities making it a revolutionary software in the field of ophthalmology and public health. This research contributes to one of the steps forward in enhancing the application of AI in medical diagnostics.},
  keywords={Deep learning;Diabetic retinopathy;Visualization;Accuracy;Decision making;Visual impairment;Blindness;Real-time systems;Medical diagnosis;Tuning;Diabetic Retinopathy;DenseNet121;Deep Learning;Hyperparameter;Tuning;Fundus Imaging;Automated Detection;AI in Healthcare;Real-Time Prediction;Medical Diagnostics;Image Classification Introduction},
  doi={10.1109/ICCSP64183.2025.11088736},
  ISSN={2836-1873},
  month={June},}@INPROCEEDINGS{11188200,
  author={J, Suriya Prakash and Y, Varshitha and Jain, Yash and Satpute, Babasaheb and Arasappa, Kirankumar},
  booktitle={2025 Third International Conference on Networks, Multimedia and Information Technology (NMITCON)}, 
  title={Deep Learning in Ophthalmology: A Novel Approach for Retinal Condition Prediction}, 
  year={2025},
  volume={},
  number={},
  pages={01-07},
  abstract={Macular degeneration and diabetic eye disease are two common vision disorders that impact people worldwide. If left untreated, these conditions can lead to severe vision loss. The aim is to develop an intelligent computer system capable of identifying such disorders from eye images, enabling immediate treatment. A broad collection of retinal images was used, including suboptimal-quality ones. The dataset was enhanced using imageimprovement algorithms, and the AI was trained to distinguish between healthy and diseased eyes. Advanced imaging techniques were applied to analyze the features the AI focused on to understand its reasoning. The early detection of retinal disorders holds the potential to transform eye care, significantly improving patient outcomes and helping prevent avoidable blindness. With the support of this system, problems can be detected in time, allowing individuals to preserve their vision much longer.},
  keywords={Deep learning;Macular degeneration;Imaging;Transforms;Retina;Ophthalmology;Diabetes;Artificial intelligence;Information technology;Image classification;Deep Learning;Retinal Diseases;Image Classification},
  doi={10.1109/NMITCON65824.2025.11188200},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11141307,
  author={Wu, Emily},
  booktitle={2025 IEEE 4th International Conference on Computing and Machine Intelligence (ICMI)}, 
  title={Advancing Early Detection of Diabetic Retinopathy: A Novel Combo 4+1 Deep Learning Architecture with Classification by Regression}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Diabetic retinopathy (DR) is a severe complication of diabetes, affecting over half of patients over time. This retinal disease often progresses without any symptoms in its early stages, then leading to irreversible blindness if left untreated. Accurate diagnosis and severity determination of DR are critical for successful treatment, but these steps are often time-consuming, costly, and requires expertise of trained ophthalmologists. The rise of artificial intelligence (AI) technology provides a paradigmshifting alternative to traditional diagnostic practices; still, previous studies have mainly focused on conventional classification schemes, often disregarding the ordinal nature that lies behind severity stages in diabetic retinopathy and the possibility of multi-scale feature extraction. The current study proposes a deep-learning (DL) architecture using ResNet50 transfer learning, demonstrating regression-based classification outperforming classical classification methods. This paper proposes a novel Combo $4+1$ architecture, which divides each fundus image into its four quadrants and inputs them into the DL system, together with one downsized whole image. This design gives an effectively higher resolution for extracting features, allowing the detection of subtler lesions of neovascularization, while there is also an input of the whole image that gives a very different scale for extracting features. Using the APTOS 2019 dataset, the proposed model exhibits excellent results, particularly a binary classification accuracy of $\mathbf{9 9. 0 5 \%}$, as well as a quadratic weighted kappa (QWK) of $\mathbf{9 3. 6 4 \%}$ in five-class classification, which is a great improvement over existing results on the APTOS dataset. The proposed system has the potential to be a reliable screening mechanism and progression monitor of a patient's DR.},
  keywords={Diabetic retinopathy;Accuracy;Image resolution;Feature extraction;Retina;Reliability;Lesions;Residual neural networks;Diseases;Biomedical imaging;diabetic retinopathy;early detection;artificial intelligence;deep learning;classification by regression;image processing;Combo 4+1;ResNet50},
  doi={10.1109/ICMI65310.2025.11141307},
  ISSN={},
  month={April},}@ARTICLE{10559800,
  author={Abushawish, Israa Y. and Modak, Sudipta and Abdel-Raheem, Esam and Mahmoud, Soliman A. and Jaafar Hussain, Abir},
  journal={IEEE Access}, 
  title={Deep Learning in Automatic Diabetic Retinopathy Detection and Grading Systems: A Comprehensive Survey and Comparison of Methods}, 
  year={2024},
  volume={12},
  number={},
  pages={84785-84802},
  abstract={Diabetic Retinopathy is one of the leading global causes of vision impairment and blindness in humans. It has seen a rise in prevalence, necessitating the development of advanced automatic detection methods. This paper presents a survey of the evolution in deep learning techniques for diabetic retinopathy detection, emphasizing the transition from traditional machine learning to sophisticated deep learning architectures such as convolutional neural networks. It discusses the role of transfer learning, end-to-end learning, and hybrid models in overcoming medical detection challenges while highlighting the need for artificial intelligence interpretability and real-time screening integration in clinical workflows. Building on this survey, the paper introduces a focused study on cross-dataset deployment of transfer learning for diabetic retinopathy detection and grading. Consequently, this paper evaluates 26 pre-trained models from various convolutional neural network families to provide a comprehensive comparison between the state-of-the-art CNN architectures in the field. Additionally, this study also employs Grad-CAM visualization to interpret the model’s decision-making, bridging advanced artificial intelligence techniques with practical healthcare applications for diabetic retinopathy.},
  keywords={Diabetic retinopathy;Retina;Deep learning;Blood vessels;Surveys;Training;Blindness;Detection algorithms;Convolutional neural networks;Transfer learning;Diabetic retinopathy detection;deep learning;convolutional neural networks;transfer learning},
  doi={10.1109/ACCESS.2024.3415617},
  ISSN={2169-3536},
  month={},}@ARTICLE{10343102,
  author={Arya, Sudhanshu and Chung, Yeon Ho},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={A Comprehensive Survey on Optical Scattering Communications: Current Research, New Trends, and Future Vision}, 
  year={2024},
  volume={26},
  number={2},
  pages={1446-1477},
  abstract={To meet high data rate requirements of future wireless communication systems, there is a need for advanced communication technologies that could be used in combination with existing wireless RF technologies. Recently, optical wireless communication (OWC) has been extensively investigated as an attractive alternate technology to RF. OWC uses the optical carrier to convey data, with wavelengths ranging from ultraviolet (UV) to infrared (IR) to visible light. In the past years, there is a spike in interest over optical scattering communications (OSCs) employing UV wavelengths, thanks to the recent advances and rapid developments in deep UV light-emitting diodes (LEDs), laser diodes, and solar-blind UV filters and detectors. The unique atmospheric scattering and absorption properties of the deep UV band, which is solar-blind at the ground level, are the motivation for the recent development of the OSC systems. However, there is a clear gap in the existing literature that the OSC systems are yet to be systematically surveyed for their applicability to future wireless communications. In this context, this paper bridges the gap by providing the first contemporary and comprehensive survey on recent and new advancements in the OSCs, commonly known as UV communications. In summary, this survey is expected to provide a largely missing articulation between various aspects of UV communications. To be easy to follow, we commence our discourse by surveying the propagation concepts and historic evolution of UV communication systems. Next, we provide a detailed survey on UV channel modeling because accurate channel characterization is important for efficient system design and performance optimization of UV communication systems. We discuss various UV channel characterization efforts thus far made. Then, we present a classification to analyze current OSC system designs. Importantly, we survey recent advancements in the NLOS UV communication systems that include the application of artificial intelligence, artificial neural networks, game theory, orbital angular momentum, etc. Moreover, we conduct a comprehensive survey on recently documented UV-based indoor communication systems. Finally, we point out several key issues yet to be addressed and collate potentially interesting and challenging topics for future research. This survey is aptly featured by in-depth discussion and analysis of UV communication systems in various aspects, many of which, to the best of authors’ knowledge, are the first time presented in this field.},
  keywords={Optical scattering;Surveys;Nonlinear optics;Scattering;Wireless communication;Optical transmitters;Communication systems;Atmospheric turbulence;Poisson channel;optical scattering communications;ultraviolet},
  doi={10.1109/COMST.2023.3339371},
  ISSN={1553-877X},
  month={Secondquarter},}@INPROCEEDINGS{11152860,
  author={Nayak, Nancy and Leung, Kin K. and Hanzo, Lajos},
  booktitle={IEEE INFOCOM 2025 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
  title={DRL-based Dolph-Tschebyscheff Beamforming in Downlink Transmission for Mobile Users}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={With the emergence of AI technologies in next-generation communication systems, machine learning plays a pivotal role due to its ability to address high-dimensional, non-stationary optimization problems within dynamic environments while maintaining computational efficiency. One such application is directional beamforming, achieved through learning-based blind beamforming techniques that utilize already existing radio frequency (RF) fingerprints of the user equipment obtained from the base stations and eliminate the need for additional hardware or channel and angle estimations. However, as the number of users and antenna dimensions increase, thereby expanding the problem’s complexity, the learning process becomes increasingly challenging, and the performance of the learning-based method cannot match that of the optimal solution. In such a scenario, we propose a learnable Dolph-Tschebyscheff antenna array based deep reinforcement learning framework for blind beamforming enabling dynamic beam pattern adaptation to accommodate mobile users. Our simulation results show that the proposed method can support data rates very close to the best possible values.},
  keywords={Radio frequency;Array signal processing;Simulation;Adaptive arrays;Voltage;Deep reinforcement learning;Sensors;Arrays;Optimization;Next generation networking;MIMO;Blind Beam Alignment;Deep Reinforcement Learning;Dolph-Tschebyscheff Antenna array},
  doi={10.1109/INFOCOMWKSHPS65812.2025.11152860},
  ISSN={2833-0587},
  month={May},}@INPROCEEDINGS{9989099,
  author={Uke, Shailaja and Lokhande, Hrishikesh and Lohar, Durva and Lathiya, Devansh and Langhe, Aditya and Lautawar, Tanmay and Likhitkar, Pranali},
  booktitle={2022 IEEE 4th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)}, 
  title={Virtual Voice Assistant In Python (Friday)}, 
  year={2022},
  volume={},
  number={},
  pages={164-167},
  abstract={We are living the era of computers where nowadays everything can be done using computers i.e., Artificial Intelligence and Machine Learning, have made technology very easy that now humans can interact with machines in such a way that their task can be done only by interacting with Machines i.e., Computers. Yes, we are talking about Virtual Personal Assistants (VPAs). As of now there are many such VPAs like Alexa, Bixby, Echo, Siri, Google Assistant available on desktop, mobiles and in device also but as the technology becomes easier it becomes difficult to use for particular people such as senior citizens, blind people, also children below certain age. Also, this VPAs which are present nowadays do not provide as much facilities. To overcome this, we developed voice assistant in python on windows system which provide user to perform any task without using keyboard. Also, user can send email just by giving receiver’s email and the message user wants to send. Also, now user can book a cab just by saying where he wants to go and also book bus and train tickets. Our main aim towards developing this is to make certain things more efficient.},
  keywords={Computers;Industries;Virtual assistants;Keyboards;Machine learning;Electronic mail;Internet;Artificial Intelligence;Machine Learning;VPA’s;Blind People;Send Email;Booking.},
  doi={10.1109/ICCCMLA56841.2022.9989099},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10307136,
  author={Sreerenganathan, Abirami and K P, Vyshali Rao and Dhanalakshmi, M},
  booktitle={2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={Assistive Application for the Visually Impaired using Machine Learning and Image Processing}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The task of interpreting visual information poses a difficulty for artificial intelligence due to the intricate and diverse characteristics of visual data. Visual data can be disrupted and deficient, which complicates the process of machines attempting to precisely comprehend and decipher the meaning of an image. In this paper, a new method for image captioning for people who are blind is suggested. This method involves using a CNN-LSTM architecture, where a CNN is utilized to extract visual features from the image, and an LSTM generates a text-based description based on these features. A vast dataset of images and their corresponding captions are used to train the suggested model, and its effectiveness is assessed using the BLEU metric. Our model is validated using the benchmark dataset Flickr8K. The outcomes of the experiment demonstrate that the suggested technique has the capability to produce relevant and precise descriptions, which can help visually impaired people to access visual content. This method has the potential to fill the gap and provide a solution to the challenge of accessing visual media by the visually impaired.},
  keywords={Measurement;Visualization;Computational modeling;Machine learning;Computer architecture;Media;Benchmark testing;image captioning;CNN-LSTM model;deep learning},
  doi={10.1109/ICCCNT56998.2023.10307136},
  ISSN={2473-7674},
  month={July},}@INPROCEEDINGS{10008797,
  author={Qi, Zhanyuan and Jung, Cheolkon and Liu, Yang and Li, Ming},
  booktitle={2022 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, 
  title={CNN-Based Post-Processing Filter for Video Compression with Multi-Scale Feature Representation}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={In this paper, we propose a convolutional neural network (CNN)-based post-processing filter for video compression with multi-scale feature representation. The discrete wavelet transform (DWT) decomposes an image into multi-frequency and multi-directional sub-bands, and can figure out artifacts caused by video compression with multi-scale feature representation. Thus, we combine DWT with CNN and construct two sub-networks: Step-like sub-band network (SLSB) and mixed enhancement network (ME). SLSB takes the wavelet subbands as input, and feeds them into the Res2Net group (R2NG) from high frequency to low frequency. R2NG consists of Res2Net modules and adopts spatial and channel attentions to adaptively enhance features. We combine the high frequency sub-band output with the low frequency sub-band in R2NG to capture multi-scale features. ME uses mixed convolution composed of dilated convolution and standard convolution as the basic block to expand the receptive field without blind spots in dilated convolution and further improve the reconstruction quality. Experimental results demonstrate that the proposed CNN filter achieves average 2.13%, 2.63%, 2.99%, 4.8%, 3.72% and 4.5% BD-rate reductions over VTM 11.0-NNVC anchor for Y channel on A1, A2, B, C, D and E classes of the common test conditions (CTC) in AI, RA and LDP configurations, respectively.},
  keywords={Convolution;Visual communication;Video compression;Discrete wavelet transforms;Image restoration;Convolutional neural networks;High frequency;Convolutional neural network;attention mechanism;compressed video restoration;dilated convolution;wavelet},
  doi={10.1109/VCIP56404.2022.10008797},
  ISSN={2642-9357},
  month={Dec},}
