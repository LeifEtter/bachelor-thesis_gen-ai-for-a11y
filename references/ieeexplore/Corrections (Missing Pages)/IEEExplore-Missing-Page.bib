@ARTICLE{11206342,
  author={Liang, Hao and Dong, Zhipeng and Chen, Kaixin and Li, Hao and Guo, Jiyuan and Yue, Yufeng and Fu, Mengyin and Yang, Yi},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Surround-view perception has garnered significant attention for its ability to enhance the perception capabilities of autonomous driving vehicles through the exchange of information with surrounding cameras. However, existing surround-view perception systems are limited by inefficiencies in unidirectional interaction pattern with human and distortions in overlapping regions exponentially propagating into non-overlapping areas. To address these challenges, this paper introduces ChatStitch, a surround-view human-machine co-perception system capable of unveiling obscured blind spot information through natural language commands integrated with external digital assets. To dismantle the unidirectional interaction bottleneck, ChatStitch implements a cognitively grounded closed-loop interaction multi-agent framework based on Large Language Models. To suppress distortion propagation across overlapping boundaries, ChatStitch proposes SV-UDIS, a surround-view unsupervised deep image stitching method under the non-global-overlapping condition. We conducted extensive experiments on the UDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our SV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for 3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%, and SSIM improvements of 8%, 18%, and 26%, respectively. The code is available at https://github.com/lhlawrence/ChatStitch.},
  keywords={surround view;image stitching;human-machine interaction},
  doi={10.1109/TCSVT.2025.3622736},
  ISSN={1558-2205},
  month={},}@INPROCEEDINGS{10963550,
  author={Nguyen-Thanh, Tuan and Vo-Tuan, Kiet},
  booktitle={2024 7th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)}, 
  title={Improving the Efficiency of Watermarking in Medical Image Processing Applications Based on Artificial Intelligence}, 
  year={2024},
  volume={},
  number={},
  pages={818-823},
  abstract={Artificial intelligence (AI), especially neural network models, has increasingly demonstrated superiority over traditional methods in medical image processing applications, from detection to recognition. However, until now, medical records are still separate from images, making it easy to confuse or costly to store and transmit. Therefore, the solution of integrating this data information into medical images brings effective practical applications. This paper provides solutions that combine medical image processing applications based on artificial intelligence to improve the effectiveness of the Least Significant Bit (LSB)-based blind image watermarking technique suitable for each specific application requirement. By embedding 1-bit, 2-bit, and 3-bit with random text files in both disease and normal images, we collected the confidence scores before and after embedding to investigate how embedding more information affects the predictive capabilities of the AI model for diagnosing lung diseases. Experimental results show that the process of embedding and extracting information is very simple and fast, so both direct and Region Of Non-Interest (RONI) solutions can be widely used in DICOM (Digital Imaging and Communications in Medicine) image processing systems, especially applications with limited processing capabilities. This helps to support new services such as telemedicine or electronic medical records.},
  keywords={Seminars;Pulmonary diseases;Telemedicine;Watermarking;Predictive models;DICOM;Data mining;Biomedical image processing;Artificial intelligence;Medical diagnostic imaging;medical image watermarking;LSB;artificial intelligence;neural network model;lung diseases},
  doi={10.1109/ISRITI64779.2024.10963550},
  ISSN={2832-1456},
  month={Dec},}@INPROCEEDINGS{10756431,
  author={Miftah, Rim and Ibork, Yassine and Mourhir, Asmaa},
  booktitle={2024 Sixth International Conference on Intelligent Computing in Data Sciences (ICDS)}, 
  title={Enhancing Accessibility of Feedback Collection in ML Models Through Keyword Spotting: A Moroccan Darija Model}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents an innovative approach to enhancing feedback collection accessibility in machine learning models through the development of a keyword spotting system (KWS) designed for Moroccan Darija, a low-resource dialect. The solution allows blind and visually impaired users to give feedback on model outputs using voice commands, providing a hands-free method to indicate misclassification by saying “Mas7i7ch” (incorrect) or confirmation by saying “S7i7” (correct). Data collection involved gathering over 500 distinct audio samples in different environments, which formed the basis of the model training. The KWS model achieves 97.3% accuracy and 97% F1 score. We leveraged the KWS as part of the Human-AI Interaction of a machine learning application to collect user feedback about model predictions and misclassifications through voice commands. To evaluate our system's real-world applicability, a comprehensive user evaluation was conducted with over 112 participants of diverse genders, ages, vision conditions, and intellectual backgrounds. The system achieved a high accessibility rating of 88.4%, highlighting its effectiveness in improving inclusion and providing an intuitive voice-based interface for blind and visually impaired individuals.},
  keywords={Training;Accuracy;Computational modeling;Machine learning;Predictive models;Data collection;Data models;keyword spotting;machine learning;feedback collection;Moroccan Darija;human-AI interaction;TinyML},
  doi={10.1109/ICDS62089.2024.10756431},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9823962,
  author={Bhatlawande, Shripad and Shilaskar, Swati and Kumari, Aditi and Ambekar, Mahi and Agrawal, Mohit and Raj, Amit and Amilkanthwar, Siddhi},
  booktitle={2022 IEEE 7th International conference for Convergence in Technology (I2CT)}, 
  title={AI Based Handheld Electronic Travel Aid for Visually Impaired People}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Visual Impairment is a major challenge. Blind people face significant problems in interacting with their surroundings. This paper presents an electronic travel aid for safe mobility of visually impaired people. This aid is implemented in the form of a hand-held torch. It consists of a monocular camera, an ultrasonic sensor, a raspberry pi board-based control system, and an earphone. It detects the obstacles using dual confirmation through a camera and ultrasonic sensor. The camera is used to detect and recognize the position of the object and an ultrasonic sensor is used for the depth perception. It can detect any obstacle in the path of visually impaired people. It can recognize a total of 80 indoor and outdoor objects/obstacles. It uses a simplified audio feedback to convey the details of detected obstacle/recognized object. The weight of the system is 410 grams. Total four experiments were conducted with blind-folded subjects to assess utility of this aid. The system consistently detected obstacles in the surrounding environment and helped the user to negotiate with the detected obstacle.},
  keywords={Headphones;Face recognition;Conferences;Visual impairment;Blindness;Cameras;Control systems;Electronic Travel Aid;Obstacle Detection;Object Recognition;Computer Vision;Machine Learning},
  doi={10.1109/I2CT54291.2022.9823962},
  ISSN={},
  month={April},}@INPROCEEDINGS{10292482,
  author={Nieto-Chaupis, Huber},
  booktitle={2023 IEEE International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings)}, 
  title={Machine Learning As a Blind Creator of Infinite Algebras in the Context of Strings Theory}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper, presents the scenario that basic algorithms based at the principles of Machine Learning can massively produce a huge amount of algebras. The case of Witt algebra has been focused. Once this algebra is reformulated with the inclusion of real numbers, operations of commutation are derived. Thus, once the modified Witt algebra exhibits the dependence of exponential shapes, some approximations are applied. The projections of this onto an algorithm based on the criteria of Tom Mitchell have produced a huge amount of valid algebras. In this manner is seen that although Machine Learning can be efficient to produce theoretical tools, most of them might be unnecessary to consolidate a theory. Therefore, in scenarios by which Artificial Intelligence is adopted as a tool to study theoretical physics, might be seen as factory of everything but far away of realistic physics models.},
  keywords={String theory;Machine learning algorithms;Algebra;Shape;Commutation;Machine learning;Production facilities;Machine Learning;Virasoro algebra;De Witt algebra;Tom Mitchell},
  doi={10.1109/AIBThings58340.2023.10292482},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10986482,
  author={Kela, Gunjan Purushottam and Daga, Mokshada Girdhar and Khandelwal, Richa},
  booktitle={2025 3rd International Conference on Disruptive Technologies (ICDT)}, 
  title={Vision to Voice: An Advanced Blind Assistance System Integrating YOLOv3 and OCR Technologies for Enhanced Mobility}, 
  year={2025},
  volume={},
  number={},
  pages={1473-1476},
  abstract={The rise in need for assistive technologies has led to the development of AI-empowered solutions for better accessibility for visually impaired persons. The proposed system is the use of a real-time object and text detection application using a combination of deep learning and speech synthesis techniques to enhance mobility and independence. The system utilizes Tesseract OCR for text recognition and pyttsx3 for TTS for better interfacing with the environment. YOLOv3-based object detection towards improving environmental awareness involves real-time identification of objects, with confidence thresholding and NMS serving to enhance robustness of the overall detection by curbing false positives. Further, the distance estimation module helps in mobility as it provides information on how far one is situated from an object. This architecture is a hardware-independent, lowlatency, and highly scalable solution compared to conventional assistive systems; thus, such systems are more harmonized for practical applications. Experimental findings exhibited its very high accuracy, confident standing across situations, and enhanced user experience rendering it with huge potential for granting enhanced independence and safety for the visually challenged user base.},
  keywords={YOLO;Deep learning;Text recognition;Optical character recognition;Text detection;Assistive technologies;Real-time systems;User experience;Text to speech;Safety;Assistive technology;Computer Vision;Optical Character Recognition (OCR);object detection;speech synthesis;Text-to-speech engine;deep learning;visually impaired;Tesseract;YOLOv3;real-time processing},
  doi={10.1109/ICDT63985.2025.10986482},
  ISSN={},
  month={March},}@INPROCEEDINGS{11096891,
  author={Nikulin, Andrey and Smirnov, Viktor and Kazmina, Anna},
  booktitle={2025 IEEE 26th International Conference of Young Professionals in Electron Devices and Materials (EDM)}, 
  title={Method for Recognizing Small Obstacles in Assistant Devices for the Blind}, 
  year={2025},
  volume={},
  number={},
  pages={400-403},
  abstract={The article proposes an innovative method for recognizing obstacles for people with visual impairments, based on the use of two laser range-finder sensors located in parallel at an angle of 45 degrees to the plane of motion. This approach solves the key problem of detecting small objects (curbs, pits, steps) that traditional means such as a white cane or ultrasonic devices often fail to detect due to error caused by the user's hand movement. The presented method eliminates the need for expensive AI solutions, based on the analysis of the distance difference between the sensor readings, taking into account the dynamic correction of the angle of inclination of the device when moving through the gyroscope and accelerometer. A key element of the algorithm is the recognition of six types of patterns characteristic to dangerous obstacles such as pits, curbs, steps and stairs. Two laser sensors are used for detection, the measurement difference of which is recorded in a pattern based on threshold values and timers. At the output of the algorithm, a numerical code is formed, which is compared with the database of obstacle codes. The effectiveness of the method was confirmed by mathematical modeling and full-scale tests, which revealed delays in real sensor data, but retained high detection accuracy. The simplicity of the design and logic of the algorithm makes the solution available for mass use, reducing the risks of social isolation of the visually impaired. The results demonstrate the promise of the method for integration into portable assistive devices that combine reliability and low cost.},
  keywords={Codes;Visual impairment;Measurement by laser beam;Sensor phenomena and characterization;Stairs;Mathematical models;Numerical models;Reliability;Logic;Laser applications;isually impaired;obstacle recognition;laser range-finder;assistive technologies;mathematical modeling},
  doi={10.1109/EDM65517.2025.11096891},
  ISSN={2325-419X},
  month={June},}@INPROCEEDINGS{10132201,
  author={Offutt, Jeff},
  booktitle={2023 IEEE Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Test Automation: From Slow & Weak to Fast, Flaky, & Blind to Smart & Effective}, 
  year={2023},
  volume={},
  number={},
  pages={11-11},
  abstract={All technical fields add more automation over time as we replace human labor with innovative technologies. Automation comes with many advantages: It creates research opportunities, offers savings in practice, and reduces errors. Automation also comes with disruptive costs. Processes must change to accommodate the automation, and human laborers must adapt by learning new knowledge and skills. Automation also evolves over time as advances inspire more new ideas for automation. This presentation will reflect on automation through history and on years of experience inventing ways to automate software testing. The talk will review achievements in test automation, discuss challenges in cutting edge domains such as games and AI, and present open problems for future research and for practical applications.},
  keywords={Software testing;Automation;Costs;Games;History;Artificial intelligence},
  doi={10.1109/ICST57152.2023.00009},
  ISSN={2159-4848},
  month={April},}@INPROCEEDINGS{10866033,
  author={Kaur, Jasneet and Kuamr, S. Barath and Mehta, Pradnya and Ninoria, Shalini and Sharma, Vishal and Karthikeyan, C.},
  booktitle={2024 1st International Conference on Sustainable Computing and Integrated Communication in Changing Landscape of AI (ICSCAI)}, 
  title={Bibliometric Research: Tracking How the Artificial Intelligence Inntegration Changed the Medical Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={In latest generations, there has been a significant increase in study interest in the growing applications of artificial intelligence in health and medicine. The purpose of this study is to present a worldwide and chronological overview of AI study in the areas of medical care and health. The online science tool was used to obtain a total number of articles that were released between. The detailed study looked at the number of publications, as well as the cooperation between writers and nations. Generally, indicate that a factor, along with robotics, algorithms, neural networks, artificial cognitive computing, and natural language processing, were identified through a vast network of researchers' key words and phrases and message review of pertinent academic papers. These methods are regularly used in clinical forecasting and rehabilitation. The most articles were on cancer, followed by those on cardiovascular disease, stroke, blindness, Early onset dementia, and sadness. Additionally, the lack of study on applying AI to some illnesses with a high disease prevalence indicates potential paths for Ai development. The study proposes the creation of international and national guidelines and laws on the rationale and application of pertaining to medical products and provides a first and complete image of the global efforts made in this significant and lucrative study area.},
  keywords={Training;Industries;Machine learning algorithms;Databases;Reviews;Medical services;Natural language processing;Artificial intelligence;Robots;Biomedical imaging;Artificial intelligence;Medical;Disease;Bibliometric research;Development;Applications;Robotics;Algorithms;Neural networks;Artificial cognitive computing;Natural language processing;Clinical forecasting;Rehabilitation;Cancer;Cardiovascular disease},
  doi={10.1109/ICSCAI61790.2024.10866033},
  ISSN={},
  month={July},}@INPROCEEDINGS{10635680,
  author={Casado-García, Á. and Heras, J. and Ortega, M. and Ramos, L.},
  booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
  title={Deep Learning Models for Justified Referral in AI Glaucoma Screening}, 
  year={2024},
  volume={},
  number={},
  pages={1-3},
  abstract={Glaucoma is an optic disease that leads to blindness, but this might be avoided with an early diagnosis thanks to a screening test. The JustRAIGS Challenge was organised to develop solutions for glaucoma screening from retinal fundus images that not only classify fundus images as "referrable" or "no referable" but also identify specific characteristics or abnormalities that may be present in the fundus images of glaucoma patients. In this work, we present our solution to this challenge based on the study of several combinations of fundus images and their associated optic disc and cup. For the binary task, our solution consists of the ensemble of a ConvNext model and a Swin model that achieves a sensitivity at 95% specificity of 0.8570; whereas, for the multi-label task, our ConvNext model achieved a Hamming Loss of 0.1930.},
  keywords={Glaucoma;Optical losses;Sensitivity;Biomedical optical imaging;Computational modeling;Retina;Optical imaging;JustRAIGS;Glaucoma;Image Classification;Multi-label classification},
  doi={10.1109/ISBI56570.2024.10635680},
  ISSN={1945-8452},
  month={May},}@INPROCEEDINGS{11070715,
  author={Naim, Kawtar and Darouichi, Aziz},
  booktitle={2024 International Conference on Connected Innovation and Technology (ICCITX)}, 
  title={Retinal Disease Classification Using AI: A novel CapsNet-Vgg16 architecture}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Macular Holes (MH), Central Serous Retinopathy (CSR), and Diabetic Retinopathy (DR) represent prevalent ocular conditions associated with varying degrees of vision impairment and potential blindness. However, despite their widespread impact, global eye care systems encounter formidable challenges in delivering timely and effective interventions. This study addresses this pressing issue by proposing the adoption of an innovative CapsNet-Vgg16 architecture for the classification of Optical Coherence Tomography (OCT) scans, aiming to revolutionize the early detection of these retinal abnormalities through automation. Through rigorous evaluation, our implementation of the CapsNet-Vgg16 model has demonstrated exceptional performance, achieving an accuracy of 99.7% on the OCTID dataset and 98.93% on the OCT-C8 dataset for identifying these pathological conditions. These findings underscore the transfor-mative potential of leveraging advanced deep learning techniques in ophthalmic diagnostics, paving the way for enhanced early disease detection and improved patient outcomes on a global scale.},
  keywords={Deep learning;Diabetic retinopathy;Technological innovation;Pathology;Pressing;Predictive models;Retina;Artificial intelligence;Diseases;Testing;Ophthalmic Disease;Retinal;OCT;Classification;Deep Learning (DL);Artificial Intelligence (AI);CapsNet;VGG16;CapsNet-Vgg16},
  doi={10.1109/ICCITX61791.2024.11070715},
  ISSN={},
  month={April},}@INPROCEEDINGS{10779706,
  author={S, Aparna and V J, Madhurema and Nair, Nandana and Mohan, Parvathi and Rajan, Rajeev},
  booktitle={2024 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES)}, 
  title={AI-based Diabetic Retinopathy Grading Using Raspberry Pi}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Diabetic retinopathy is a progressive eye condition caused by prolonged high blood sugar levels in individuals with diabetes, leading to damage in the blood vessels of the retina. It can result in vision impairment or blindness if left untreated, making early detection and management crucial for preserving eyesight. Employing neural networks for image classification can help overcome these challenges. Convolutional neural networks (CNNs) can automatically extract relevant features and classify images into different categories, aiding in early diagnosis and intervention. It also offers a promising avenue for improving DR screening, particularly in areas with limited access to ophthalmologists. This paper aims to enhance DR grading performance by implementing a hardware solution using Raspberry Pi. One significant advantage of this approach is the potential for costeffective and portable screening in resource-limited settings. By leveraging the Raspberry Pi’s affordability and compact form factor, healthcare providers can deploy screening systems in remote areas where access to traditional medical infrastructure is limited},
  keywords={Diabetic retinopathy;Visual impairment;Neural networks;Medical services;Signal processing;Feature extraction;SPICE;Retina;Convolutional neural networks;Medical diagnostic imaging;Diabetic Retinopathy;Convolutional Neural Network;Raspberry Pi},
  doi={10.1109/SPICES62143.2024.10779706},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10602883,
  author={Devi, B. Rama and Venkata, Rohit Sanam and Kurada, Sai Sharan and Reddy, Nukala Anjani and Muzamil, Mohd Anas and Harish, Gajji},
  booktitle={2024 International Conference on Integrated Circuits, Communication, and Computing Systems (ICIC3S)}, 
  title={Decoding the Eye: AI-Driven Diabetic Retinopathy Classification for Precision Healthcare}, 
  year={2024},
  volume={1},
  number={},
  pages={1-5},
  abstract={Diabetic Retinopathy (DR) stands as a predominant cause of blindness in individuals with diabetes, demanding laborious manual screening processes for accurate diagnosis. DR develops as a consequence of an extended duration of diabetes. The consequential delay in this procedure poses a significant obstacle to timely intervention, emphasizing the imperative for an automated and precise approach to expedite the identification and classification of DR from retinal images. DR has been diagnosed using a variety of methods in the past, such as fuzzy approaches, image processing techniques, and machine learning algorithms like Support Vector Machine (SVM). This research harnesses the capabilities of Convolutional Neural Networks (CNNs) to craft an advanced model specially designed for the nuanced categorization of DR severity. This work examined different pre-trained models for their effectiveness in DR classification. Each pre-trained model underwent assessment using accuracy, confusion matrix, and Cohen's kappa score. Notably, the DenseNet121 model emerged as the frontrunner, showcasing exceptional performance.},
  keywords={Support vector machines;Diabetic retinopathy;Accuracy;Machine learning algorithms;Medical services;Manuals;Retina;Diabetic Retinopathy;Diabetes;Support Vector Machine;Convolutional Neural Networks;DenseNet121},
  doi={10.1109/ICIC3S61846.2024.10602883},
  ISSN={},
  month={June},}@INPROCEEDINGS{10928364,
  author={Hossain, Fahad Mohammad and Alavi, Rafit and Safat, Yeamin and Sarker, Gobinda Chandra and Noor, Nafisa and Hossain, Ekram},
  booktitle={2024 IEEE 3rd International Conference on Robotics, Automation, Artificial-Intelligence and Internet-of-Things (RAAICON)}, 
  title={i-Glove: An Intelligent Glove System Based on Deep Learning to Support Deaf-blind Individuals in Recognizing Banknotes}, 
  year={2024},
  volume={},
  number={},
  pages={24-29},
  abstract={The deaf-blind community faces significant challenges in identifying banknotes due to simultaneous hearing and vision impairments, which complicate their ability to conduct daily financial transactions. Recent advances in Artificial Intelligence (AI)-based assistive technologies offer promising potential to address these challenges. This study presents the "i-Glove," a wearable device specifically developed to assist the deaf-blind community in Bangladesh with accurate banknote identification. The i-Glove is designed to recognize nine common Bangladeshi banknotes (e.g., BDT 20, 50, 100) through a camera module integrated on the palmar side of the glove, which captures real-time images of the banknotes. These images are then processed by a convolutional neural network (CNN) model to accurately classify each banknote. Identified notes are translated into unique vibration patterns that are delivered to the user through a shaftless vibration motor embedded in the glove, providing intuitive, non-visual communication. Compared to previous approaches, our model demonstrates superior performance even with a compact dataset, achieving an overall F1-score accuracy of 96.55%, with class-specific precision, recall, and F1-score metrics ranging from 90% to 100%. Additionally, a dedicated application is developed which supports real-time banknote detection and interfaces seamlessly with the glove. The proposed system represents a significant step forward in assistive technology for the deaf-blind community, empowering the users with greater autonomy in financial transactions.},
  keywords={Vibrations;Visualization;Accuracy;Image recognition;Tactile sensors;Assistive technologies;Motors;Real-time systems;Convolutional neural networks;Wearable devices;dually sensory impaired people (DIP);Center for Disability Development (CDD);Convolutional Neural Network (CNN)},
  doi={10.1109/RAAICON64172.2024.10928364},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9972653,
  author={C, Chaitra and Chennamma and R, Vethanayagi and V, Manoj Kumar M and S, Prashanth B and R, Snehah H and Thomas, Likewin and L, Shiva Darshan S},
  booktitle={2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon)}, 
  title={Image/Video Summarization in Text/Speech for Visually Impaired People}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={In the year 2022, an estimated 2.2 billion people around the globe will have a visual impairment. The problem may be hereditary or due to accidents. Nonetheless, technological advancements in helping visually impaired people have been going on for a long time. The admittance of technical concepts such as robotics, Machine Learning, and Artificial Intelligence for societal needs has proven worthwhile. The blind or visually impaired people learn about their surroundings through other senses, such as touch, hearing, and smell. Our proposed work aims to build an end-to-end solution for visually impaired people to help them grasp the environment by summarizing the images or video streams with the help of Machine Learning paradigms. The proposed work uses a pre-trained Caffe Object Detection model and requires less data for training and detection. We have developed a Client-Server model for our proposed idea wherein the significant computations happen on the server side, which is the Object detection model, and the client App is developed using Android. The app also has a text-to-signal processing feature that helps summarize the objects detected in the form of an audio catalog.},
  keywords={Training;Computational modeling;Visual impairment;Object detection;Machine learning;Streaming media;Robot sensing systems;Visually Impaired;Caffe;Object Detection;Android},
  doi={10.1109/MysuruCon55714.2022.9972653},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11013768,
  author={Al–Zaatreh, Lina and Abdelhadi, Tala},
  booktitle={2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)}, 
  title={AI-Driven Diagnostic Tools for Diabetic Retinopathy: Enhancing Early Detection and Healthcare Efficiency}, 
  year={2025},
  volume={},
  number={},
  pages={01-06},
  abstract={Diabetic Retinopathy (DR) is a leading cause of blindness in diabetic patients, with traditional screening methods that mostly rely on clinical experience. Moreover, these methods are time-consuming, expensive, and less accessible in resource-limited areas. This study explores AI-driven automated screening for improved accuracy and efficiency. Using the Diabetic Retinopathy Detection dataset from Kaggle, we developed a convolutional neural network (CNN) model. The model was preprocessed using Gaussian filtering and resized to 224×224 pixels. The model architecture consists of three convolutional layers with max pooling, batch normalization, and dense layers, ending with a softmax activation function for classification. Our CNN achieved 96.5% accuracy in binary classification (DR vs. no DR) and showed promising performance for multi-class classification of the different DR stages. These findings indicate that AI-driven methods can significantly enhance DR detection, providing a reliable and scalable solution for early diagnosis and treatment, and potentially reducing the risk of blindness in diabetic patients. By using AI-driven automated screening, healthcare providers can save time on pointless examinations of patients without diabetic retinopathy and concentrate their efforts on those who actually require care.},
  keywords={Diabetic retinopathy;Accuracy;Costs;Filtering;Medical services;Blindness;Predictive models;Retina;Convolutional neural networks;Reliability;Deep Learning;CNN;AI},
  doi={10.1109/ICCIAA65327.2025.11013768},
  ISSN={},
  month={April},}@INPROCEEDINGS{11031754,
  author={Jain, Saksham and J, Jayashree and Jain, Anushka and Jain, Harshit},
  booktitle={2025 International Conference on Data Science and Business Systems (ICDSBS)}, 
  title={Assisted Vision: a Transformative Web App for the Visually Impaired}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Blind and visually impaired are experiencing improved autonomy and mental engagement due to innovative approaches made possible by rapid advances in technology. Assisted Vision: A Cutting-Edge Web Application for Visually Impaired Individuals delivers instantaneous object recognition and user-friendly interaction through the use of machine learning and existing web-based technologies. The COCO-SSD object identification model is used by the ReactJS-created application to allow users to identify objects with their device's camera. The incorporation of useful sound feedback into this feature allows users to properly comprehend their surroundings. Voice-command capabilities deliver an effortless experience for users by optimizing interactions while simultaneously dealing with the necessity for traditional input techniques. This application incorporates cognitive tools, such as voice-activated games and quizzes, that increase mental engagement and offer enjoyment. In alignment with WCAG accessibility requirements, the application's versatile interfaces, adaptive designs, and screen reader support guarantee usability for an assortment of user groups. A backend governed by Node.js guarantees efficient processing, and SCSS strengthens the user interface for responsiveness and extensibility. Userfocused design, persistent enhancement, and inclusiveness are all highly regarded throughout the design and creation process. For the purpose of addressing obstacles including attaining accurate speech recognition and low-latency technology recognition of objects, sophisticated capabilities were employed. Potential advances are expected to involve bilingualism, portable accessibility, and superior recognition of objects. An example of how technology can encourage people who have visual disabilities, promote a welcoming environment, and set benchmarks for experimentation regarding digital accessibility is assisted vision.},
  keywords={Training;Visualization;Technological innovation;Speech recognition;Machine learning;User interfaces;Real-time systems;Object recognition;Usability;Low latency communication;Aided vision;object recognition;sight-impaired;online accessibility;artificial intelligence;COCO-SSD;ReactJS;vocal commands;WCAG guidelines;cognitive involvement;Node.js;SCSS},
  doi={10.1109/ICDSBS63635.2025.11031754},
  ISSN={},
  month={April},}@ARTICLE{10005817,
  author={Clarke, Roger},
  journal={IEEE Transactions on Technology and Society}, 
  title={The Re-Conception of AI: Beyond Artificial, and Beyond Intelligence}, 
  year={2023},
  volume={4},
  number={1},
  pages={24-33},
  abstract={The original conception of artificial intelligence (old-AI) was as a simulation of human intelligence. That has proven to be an ill-judged quest. It has led too many researchers repetitively down too many blind alleys, and embodies many threats to individuals, societies and economies. To increase value and reduce harm, it is necessary to re-conceptualise the field. A review is undertaken of old-AI’s flavours, operational definitions and important exemplars. The heart of the problem is argued to be an inappropriate focus on achieving substitution for human intelligence, either by replicating it in silicon or by inventing something functionally equivalent to it. Humankind instead needs its artefacts to deliver intellectual value different from human intelligence. By devising complementary artefact intelligence (CAI), and combining it with human intelligence, the mission becomes the delivery of augmented intelligence (new-AI). These alternative conceptions can serve the needs of the human race far better than either human or artefact intelligence can alone. The proposed re-conception goes a step further. Inferencing and decision-making lay the foundations for action. Old-AI has tended to compartmentalise discussion, with robotics considered as though it were a parallel or at best overlapping field of endeavour. Combining the intellectual with the physical leads to broader conceptions of far greater value: complementary artefact capability (CAC) and augmented capability (AC). These enable the re-orientation of research to avoid dead-ends and misdirected designs, and deliver techniques that serve real-world needs and amplify humankind’s capacity for responsible innovation.},
  keywords={Artificial intelligence;Robots;Software;Human intelligence;Cognition;Drones;Augmented reality;Inference algorithms;Decision making;Explainable AI;Artefact intelligence;augmented intelligence;inferencing;decision-making;action;robotics;artefact autonomy;explainability;accountability},
  doi={10.1109/TTS.2023.3234051},
  ISSN={2637-6415},
  month={March},}@INPROCEEDINGS{10960891,
  author={Thankappan, Manesh and Nataraj, Neetha K and K S, Divya and Jayaraj, Akshaya and Joby, Alen and Asokan, Abhilash},
  booktitle={2025 Emerging Technologies for Intelligent Systems (ETIS)}, 
  title={AI-Enabled Robotic Surveillance System for Critical Infrastructure}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Traditional monitoring approaches often experience blind spots and high resource utilization at such a crucial time of security. In our study, we design a small-scaled autonomous vehicle model with smart sensors that is to be equipped with an AI model in a Raspberry Pi for effective and reliable self-sustained monitoring of critical infrastructure to overcome the problems. The core AI model was specifically designed to detect in real-time anomalies such as a fire, a fight, vandalism, explosion, etc. The AI model is a modified version of an open source repository available on github named “Real-world Anomaly Detection in Surveillance Videos (pytorch) [11]”. This technology ensures speedy response time by stopping the vehicle, starting video recording, and forwarding the real-time footage to security personnel upon detecting an anomaly. This innovative solution allows for mobilization and cost-effectiveness, improving surveillance coverage, and diminished dependence on constant human oversight. Exploiting advances in artificial intelligence and robotics, the project delivers a scalable real-time monitoring system deployable into a wide range of critical infrastructure environments. This initiative represents practical application of AI towards improving public safety and building a stronger security posture in facilities that house critical infrastructure.},
  keywords={Accuracy;Surveillance;Real-time systems;Explosions;Critical infrastructure;Security;Anomaly detection;Robots;Intelligent sensors;Videos;R-CNN;CNN;One class detection;Mask R-CNN;Spatio Temporal autoencoders;Surveillance system;Anomaly detection},
  doi={10.1109/ETIS64005.2025.10960891},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10780266,
  author={Tresa Sangeetha, S. V. and Banu Jinnah, Asan and Kumar, Senthil and Porselvi, T. and Salim Alajmi, Abrar Abdulmohsin and Abdulmohsin Salim Alajmi, Bayan and Ali Alzadjali, Aisha Issa and Taaeeb Alhamdani, Aisha},
  booktitle={2024 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)}, 
  title={AI Model Integrated Omani Currency Detector for the Visually Impaired-A Comparative Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={In this research work a mobile application is created specifically for the blind and individuals with visual impairmentsfor enabling them to recognize Omani currenciesin real time independently. Also,people those who have a significant reduction in their ability to see, and it impacts their daily life usesvoice commandto recognize the Omani cash currencies. The proposed currency detector works by taking a picture of Omani currency by voice command, then the cash identification is done by using a teachable machine and personal image classifier AI models, at lastthe phone tells the value of currency correctly as a voice massage to the user, whether it is one riyal or five riyals or ten riyals or twenty riyals. This application can be installed both in IOS and Android devices. The proposed system aims to accurately identify and classify different denominations of Omani Rial (OMR) notes in real-time, enabling visually impaired individuals to confidently handle monetary transactions without any assistance. The application consists of two screens. Screen 1 set for teachable machine classifier AI model and Screen 2 as personal image classifier AI model through which a picture of the Omani currency is taken, identify the currencies and the user knows the cash value independently without others help by sound. The proposed workis tested by the teachable machine AI model for ten times and identified the Omani currencies correctlyfor 7 times,also tested by personal image classifier AI model for ten times and got the correct cash result for 8 times. From the test results,it is found that the personal image classifier works more accurate and better than teachable machine AI model.},
  keywords={Visualization;Accuracy;Detectors;Blindness;Control systems;Real-time systems;Mobile applications;Artificial intelligence;Currencies;Smart phones;MIT App inventor with Personal Image classifier Artificial intelligence (AI) model;Teachable MachineArtificial intelligence (AI) model;Android Mobile;Omani cash currencies},
  doi={10.1109/ICPECTS62210.2024.10780266},
  ISSN={},
  month={Oct},}@ARTICLE{9834143,
  author={Ghouali, S. and Onyema, EM. and Guellil, MS. and Wajid, M A. and Clare, O. and Cherifi, W. and Feham, M.},
  journal={IEEE Open Journal of Engineering in Medicine and Biology}, 
  title={Artificial Intelligence-Based Teleopthalmology Application for Diagnosis of Diabetics Retinopathy}, 
  year={2022},
  volume={3},
  number={},
  pages={124-133},
  abstract={Diabetic Retinopathy (DR) is one of the leading causes of blindness for people who have diabetes in the world. However, early detection of this disease can essentially decrease its effects on the patient. The recent breakthroughs in technologies, including the use of smart health systems based on Artificial intelligence, IoT and Blockchain are trying to improve the early diagnosis and treatment of diabetic retinopathy. In this study, we presented an AI-based smart teleopthalmology application for diagnosis of diabetic retinopathy. The app has the ability to facilitate the analyses of eye fundus images via deep learning from the Kaggle database using Tensor Flow mathematical library. The app would be useful in promoting mHealth and timely treatment of diabetic retinopathy by clinicians. With the AI-based application presented in this paper, patients can easily get supports and physicians and researchers can also mine or predict data on diabetic retinopathy and reports generated could assist doctors to determine the level of severity of the disease among the people.},
  keywords={Diabetes;Retinopathy;Deep learning;Artificial intelligence;Medical diagnostic imaging;Retina;Blindness;Internet of Medical Things;Smart healthcare;Deep learning;diabetic retinopathy;eye fundus images;tensorflow;artificial intelligence;smart health;IoT},
  doi={10.1109/OJEMB.2022.3192780},
  ISSN={2644-1276},
  month={},}@INPROCEEDINGS{10434747,
  author={Bhargavi, V.Sesha and Selvaraj, Krishnamoorthy and Koti, Valli Madhavi and Maheshwari, Rajeev Kumar and RadhaMahendran, S. and Chandra, Sukumar},
  booktitle={2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)}, 
  title={Artificial Intelligence Based Examination of the Field of Ophthalmology}, 
  year={2023},
  volume={10},
  number={},
  pages={1617-1623},
  abstract={Early and correct diagnosis is critical for optimal care and prevention of visual loss due to glaucoma, the leading cause of permanent blindness globally. New possibilities for better glaucoma identification and monitoring have emerged the progress that has been made in the field of AI and machine learning. Using a unique use of both SVM and CNN, or support vector machines and convolutional neural networks, to solve a problem., this study automates the examination of ophthalmological results relevant to glaucoma. In this paper, we introduce a hybrid SVM-CNN algorithm that draws from the most promising aspects of both existing these two approaches to machine learning. In order to lay a solid groundwork for further investigation, SVM is combined first- class classification and feature extraction. CNN is used to refine and improve classification accuracy since it is a potent deep learning architecture that can automatically learn complicated characteristics from raw data. The algorithm is educated on a large collection of ophthalmic pictures used in glaucoma diagnosis, such as fundus photos, examinations of the eye's visual acuity and OCT scans (optical coherence tomography). Noise is removed, contrast is increased, and the format is standardized in these preprocessed photos. The presence or absence of glaucoma is subsequently determined by feature extraction and classification using the SVM-CNN hybrid model. This study's findings support the use of the SVM-CNN hybrid algorithm for reliable detection of glaucoma-related pathology in ophthalmic pictures. The suggested AI-based methodology has various benefits over conventional manual diagnosis approaches, including increased efficiency, more consistency, and the possibility of earlier detection. It can also help ophthalmologists by offering automated preliminary assessments, so the specialists can devote their time and energy where it is most needed. This study concludes the promise of AI-based approaches, and in particular the SVM-CNN hybrid algorithm, to revolutionize ophthalmology, particularly glaucoma diagnosis and treatment. We can improve patient outcomes and decrease the burden of this disabling eye illness by combining the benefits of SVM and CNN to increase the accuracy and efficiency of glaucoma identification. This paper highlights the significance of ongoing research and development on the promising subject of AI use in ophthalmology.},
  keywords={Glaucoma;Support vector machines;Visualization;Ophthalmology;Classification algorithms;Convolutional neural networks;Reliability;Artificial Intelligence;Ophthalmology;Glaucoma;SVM (Support Vector Machines);CNN (Convolutional Neural Networks);etc},
  doi={10.1109/UPCON59197.2023.10434747},
  ISSN={2687-7767},
  month={Dec},}@INPROCEEDINGS{10961202,
  author={Megajai., K.Y and Nithish Kumar., P and Lakshmi Prabha., K},
  booktitle={2025 International Conference on Visual Analytics and Data Visualization (ICVADV)}, 
  title={A Study on Smart Assistive Stick for Visually Impaired Individuals}, 
  year={2025},
  volume={},
  number={},
  pages={1195-1200},
  abstract={Vision impairment heavily affects mobility, information access, employment, and independence for millions of people across the globe. Assistive technology for blind and visually impaired persons has grown from conventional white canes to high-tech smart devices with ultrasonic sensors, GPS, GSM modules, and artificial intelligence (AI). The current review compares development in smart assistive devices, such as smart blind sticks, wearable navigation systems, and IoT-based solutions. New technology like AI-based obstacle detection, image processing, and sensor fusion provide higher accuracy and realtime feedback. Despite this, prohibitive costs, short detection ranges, and issues in dynamic environments prevent its broad application. This research highlights major advancements, their drawbacks, and directions for future enhancement of accessibility, safety, and autonomy for the visually impaired. The suggested smart assistive stick prototype combines multi-sensor technology with AI-based capabilities to improve mobility and independence, overcoming current limitations in assistive navigation devices.},
  keywords={GSM;Accuracy;Navigation;Visual impairment;Real-time systems;Acoustics;Sensor systems;Artificial intelligence;Intelligent sensors;Global Positioning System;Visual impairment;assistive technology;Smart Blind Stick;real-time navigation;sensor integration;night visibility;scalable design},
  doi={10.1109/ICVADV63329.2025.10961202},
  ISSN={},
  month={March},}@INPROCEEDINGS{10029220,
  author={Shekar Goud, D. and Vigneshwari, M and Aparna, P and Vijayasekaran, G and Singh Yadav, Ajay and Kumar, Ashok},
  booktitle={2022 International Conference on Automation, Computing and Renewable Systems (ICACRS)}, 
  title={Text Localization and Recognition from Natural Scene Images using AI}, 
  year={2022},
  volume={},
  number={},
  pages={1153-1158},
  abstract={In computer vision systems, text detection and recognition (TDR) in natural scene images can be used for things like license plate recognition, automated street sign interpretation, and assisting blind people. Accordingly, finding text within an image is a time-based challenge in the field of computer vision. Because of factors like cluttered backgrounds, image blurring, partially obscured text, various fonts, noise, and fluctuating lighting, text identification in natural scenes has become a significant task with the increase in the use of actual vision systems. Images and videos with accompanying textual data can be leveraged for automatic annotation. This study provides a system for automatically identifying the text from images, and it discusses the methodology behind locating and recognizing text in images of natural scenes. This article handles the scene text recognition challenge from start to finish, breaking it down into text localization and recognition. The Maximally Stable Extremal Regions (MSER) technique is used to detect text and non-text regions in images for localization purposes. Convolutional Neural Networks (CNNs) and convolutional recurrent neural networks (CRNNs) are utilized for text recognition. By evaluating the accuracy, precision, and F1 score, the CRNN is determined to be the best.},
  keywords={Location awareness;Measurement;Computer vision;Image recognition;Text recognition;Speech recognition;Blindness;Text;Natural images;Resize;Enhancement;Localization;Accuracy;Deep Learning},
  doi={10.1109/ICACRS55517.2022.10029220},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9788361,
  author={Shruti, S and M, Shravya and Mohan, Spurthi and Y, Suhail and R C, Radha},
  booktitle={2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
  title={AI-based Solutions for ADAS}, 
  year={2022},
  volume={},
  number={},
  pages={1009-1012},
  abstract={In today's time, the Advanced Driver Assistance System (ADAS) has become indispensable to top car manufacturers. With the constant evolution of vehicle safety systems, automobile manufacturers and consumers can envision a world where vehicle collisions are a thing of the past. Object detection is a crucial part of ADAS for pedestrian and vehicle detection, and for enabling various functions like automatic emergency braking, blind spot monitoring and so on. This paper aims to develop a You Only Look Once version 4 (YOLOv4)-based object detection model. This model is trained on our custom dataset which has ten object classes that one would find on Indian roads.},
  keywords={Roads;Computational modeling;Vehicle detection;Object detection;Predictive models;Real-time systems;Automobiles;Advanced Driver Assistance System (ADAS);Object detection;You Only Look Once (YOLO);Artificial Intelligence;Deep Learning},
  doi={10.1109/ICICCS53718.2022.9788361},
  ISSN={2768-5330},
  month={May},}
