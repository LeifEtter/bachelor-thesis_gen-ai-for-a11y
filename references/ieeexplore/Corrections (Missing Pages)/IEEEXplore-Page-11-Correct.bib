@INPROCEEDINGS{10962599,
  author={Prasad, M. Lakshmi and Varshitha, Veeramalli and Goud, A. Praneeth and Bindu, R. and Srinivasulu, Ch. and Rao, S. Janardhana},
  booktitle={2025 International Conference on Emerging Systems and Intelligent Computing (ESIC)}, 
  title={AI-Driven Solutions with CNN and IoT for Enhancing Visual Impairments}, 
  year={2025},
  volume={},
  number={},
  pages={394-399},
  abstract={Visual impairment is a problem that affects millions of people worldwide, preventing them from walking independently and performing daily tasks. Some of the challenges that blind people face include crossing the street, reading, driving, and socializing. Despite the scientific community’s continuous efforts to treat the eye, these solutions have not yet been found. Traditional aids such as canes are useful but limited. Get in the groove. The main focus of the project is to integrate the YOLOv5 object detection model with a convolutional neural network (CNN). Known for its fast and accurate target detection, YOLOv5, together with CNN, improves the physical ability to detect and classify targets in difficult areas with high accuracy. The eye uses the camera module to continuously monitor the environment and identify various objects in real time; the detected data is converted into speech by text-to-speech (TTS) converters such as Google Text- to-Speech (gTTS) and pyttsx3 This description allows users to get an instant description of their surroundings, unlike helping the blind, it improves their ability to navigate independently rather than just providing references, but there are also good ways to work with better and more reliable assistive technologies.},
  keywords={YOLO;Cloud computing;Accuracy;Navigation;Face recognition;Visual impairment;Glass;Assistive technologies;Cameras;Convolutional neural networks;Feature extraction;Artificial intelligence;Open CV;Python;Face Recognition;Object Detection},
  doi={10.1109/ESIC64052.2025.10962599},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10621134,
  author={Fiandrino, Claudio and Gómez, Eloy Pérez and Fernández Pérez, Pablo and Mohammadalizadeh, Hossein and Fiore, Marco and Widmer, Joerg},
  booktitle={IEEE INFOCOM 2024 - IEEE Conference on Computer Communications}, 
  title={AIChronoLens: Advancing Explainability for Time Series AI Forecasting in Mobile Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1521-1530},
  abstract={Next-generation mobile networks will increasingly rely on the ability to forecast traffic patterns for resource management. Usually, this translates into forecasting diverse objectives like traffic load, bandwidth, or channel spectrum utilization, measured over time. Among the other techniques, Long-Short Term Memory (LSTM) proved very successful for this task. Unfortunately, the inherent complexity of these models makes them hard to interpret and, thus, hampers their deployment in production networks. To make the problem worsen, EXplainable Artificial Intelligence (XAI) techniques, which are primarily conceived for computer vision and natural language processing, fail to provide useful insights: they are blind to the temporal characteristics of the input and only work well with highly rich semantic data like images or text. In this paper, we take the research on XAI for time series forecasting one step further proposing AIChronoLens, a new tool that links legacy XAI explanations with the temporal properties of the input. In such a way, AIChronoLens makes it possible to dive deep into the model behavior and spot, among other aspects, the hidden cause of errors. Extensive evaluations with real-world mobile traffic traces pinpoint model behaviors that would not be possible to spot otherwise and model performance can increase by 32%.},
  keywords={Explainable AI;Computational modeling;Time series analysis;Semantics;Telecommunication traffic;Traffic control;Time measurement},
  doi={10.1109/INFOCOM52122.2024.10621134},
  ISSN={2641-9874},
  month={May},}@INPROCEEDINGS{10846743,
  author={Chen, Yi and Li, Yanshan and Yang, Jiahao},
  booktitle={2024 IEEE 17th International Conference on Signal Processing (ICSP)}, 
  title={The design of prompts driven by Chain of Thought in multicultural teaching}, 
  year={2024},
  volume={},
  number={},
  pages={258-262},
  abstract={Generative large language models (LLMs) show promise in multicultural teaching, but existing methods rely heavily on labeled data and resources, struggling to capture cultural nuances and generalize across various teaching contexts. To address the above issues, the paper introduces an innovative method called Multicultural Prompt Learning (MPL), which applies Chain of Thought (CoT) to prompt design. By gradually refining prompts, multicultural elements are integrated step by step, enabling AI to better understand and handle cultural nuances. This approach produces richer, more accurate, and diverse outputs, specifically aimed at enhancing students’ understanding of different cultures in multicultural teaching contexts. The experiment used a questionnaire, including image quality evaluation and a blind content comprehension, to compare Chain of Thought and traditional prompts. Results show Chain of Thought prompts outperform traditional methods in all dimensions, offering greater practical and educational value.},
  keywords={Surveys;Multiprotocol label switching;Design methodology;Education;Refining;Signal processing;Cultural differences;Prompt engineering;Standards;Optimization;Generative large language models(LLMs);Multicultural prompt learning;chain-of-thought(CoT)},
  doi={10.1109/ICSP62129.2024.10846743},
  ISSN={2164-5221},
  month={Oct},}@INPROCEEDINGS{9811197,
  author={Rahhal, Dania and Alhamouri, Rahaf and Albataineh, Iman and Duwairi, Rehab},
  booktitle={2022 13th International Conference on Information and Communication Systems (ICICS)}, 
  title={Detection and Classification of Diabetic Retinopathy Using Artificial Intelligence Algorithms}, 
  year={2022},
  volume={},
  number={},
  pages={15-21},
  abstract={Diabetic Retinopathy (DR) is considered as a sight-threatening complication of diabetes mellitus, the primary cause of blindness among working-age individuals. Ophthalmologists use fundus images to diagnose diabetic retinopathy and measure its severity by observing retinal lesions with high accuracy. However, diagnosing DR manually from fundus images requires a high level of expertise and effort from professional ophthalmologists. Early diagnosis of diabetes helps in saving the patient’s eye and preventing possible risky complications. In this context, the current paper proposed a model aims to detect DR using image processing and deep learning methods. A fully automatic diagnosis system that exceeds manual techniques to avoid misdiagnosis, reducing time, effort and cost were presented through this paper. A publicly available dataset of fundus images was used to apply the current paper’s proposed neural network model and transfer learning models, to classify each image into one of the five diabetic retinopathy stages. The simple proposed model achieved an accuracy of 66.68% in predicting the right label of the image. On the other hand, the second approach of fine-tuning the pre-trained models achieved higher testing accuracy (ranging from 93.13% to 100%,), which exceeds the current state-of-the-art results.},
  keywords={Solid modeling;Retinopathy;Transfer learning;Predictive models;Retina;Prediction algorithms;Diabetes;Diabetic Retinopathy;Deep Learning;Fundus;CNN;Classification;Transfer Learning},
  doi={10.1109/ICICS55353.2022.9811197},
  ISSN={2573-3346},
  month={June},}@INPROCEEDINGS{11031942,
  author={V, Sivadharshini and M, Sarnitha and G, Lathaselvi},
  booktitle={2025 International Conference on Data Science and Business Systems (ICDSBS)}, 
  title={AI-Driven Comic Narration System for Visually Impaired Users}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Comics, with their complex combination of visual storytelling and text-based dialogue, pose substantial accessibility issues for those with visual impairments. Current screen readers and accessibility solutions cannot handle the nonlinear and image-dependent nature of comic book content. The aim of this project is to design an automated system that provides individuals with visual impairments the opportunity to enjoy comic books through auditory storytelling. The system leverages a blend of computer vision, optical character recognition (OCR), and natural language processing (NLP) technologies to capture important details from comic book illustrations. This includes interpreting scene descriptions and text within speech bubbles. This information is compiled into a coherent narrative and transformed into speech using text-tospeech (TTS) technology. YOLOv11 is used for panel segmentation, ensuring the correct order of content is captured. After the segmentation, it analyses images, extracting text from speech bubbles, and providing detailed descriptions of scenes and characters, it provides a complete storytelling experience. This groundbreaking system offers a novel approach to making visual media, like comics, accessible for individuals with visual impairments by converting visual elements into an auditory format.},
  keywords={Visualization;Image segmentation;Art;Optical character recognition;Visual impairment;Machine learning;Immersive experience;Media;Lead;Natural language processing;Human-centered computing for accessibility;auditory storytelling;image processing;OCR;NLP;TTS;YOLOv11;scene descriptions;auditory experience},
  doi={10.1109/ICDSBS63635.2025.11031942},
  ISSN={},
  month={April},}@ARTICLE{10981719,
  author={Manzoni, Matteo and Mascetti, Sergio and Ahmetovic, Dragan and Crabb, Ryan and Coughlan, James M.},
  journal={IEEE Access}, 
  title={MapIO: A Gestural and Conversational Interface for Tactile Maps}, 
  year={2025},
  volume={13},
  number={},
  pages={84038-84056},
  abstract={For individuals who are blind or have low vision, tactile maps provide essential spatial information but are limited in the amount of data they can convey. Digitally augmented tactile maps enhance these capabilities with audio feedback, thereby combining the tactile feedback provided by the map with an audio description of the touched elements. In this context, we explore an embodied interaction paradigm to augment tactile maps with conversational interaction based on Large Language Models, thus enabling users to obtain answers to arbitrary questions regarding the map. We analyze the types of questions the users are interested in asking, engineer the Large Language Model’s prompt to provide reliable answers, and study the resulting system with a set of 10 participants, evaluating how the users interact with the system, its usability, and user experience.},
  keywords={Navigation;Inspection;Usability;Three-dimensional printing;Prototypes;Cameras;Roads;Computational modeling;Benchmark testing;Visualization;Assistive technologies;blind and low vision people;conversational interface;digitally augmented tactile maps;tactile maps},
  doi={10.1109/ACCESS.2025.3566286},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10882580,
  author={Chavan, Pranav Sandip and Musale, Jitendra and Thorat, Rahul and Rede, Gayatri Santosh and Bora, Disha Dnyanmal and Dalvi, Siddhi Mahesh},
  booktitle={2024 13th International Conference on System Modeling & Advancement in Research Trends (SMART)}, 
  title={Cyber-Bullying Detection on Social Medial Using Machine Learning}, 
  year={2024},
  volume={},
  number={},
  pages={144-150},
  abstract={In recent years, the rapid progress of artificial intelligence has captured the interest of numerous researchers in this field. Image captioning this study focuses on using image captioning methods to improve accessibility for those who are blind or visually impaired. Our project intends to automatically generate captions for photographs using deep learning techniques, so that blind people may comprehend visuals with ease. Image captioning is used in many contexts other than only accessibility; it's used in social media, education, and navigation, among others. Furthermore, this paper takes a close look at different ways of describing images, like using CNN-LSTM, scene features, and CNN-attention methods. We measure how well these methods work by checking scores like BLEU and ROUGE. We also talk about why different image collections are helpful for teaching computers how to understand pictures. By doing this, we hope to learn more about how to make images easier for people who can't see well. We also show how this technology can help with things like social media, school, and finding your way around. This paper hopes to help others learn more about making technology that's helpful for everyone, including people who can't see well.},
  keywords={Support vector machines;Visualization;Sentiment analysis;Accuracy;Navigation;Education;Cyberbullying;Reinforcement learning;Real-time systems;Monitoring;Cyber-bulling;Natural Language Processing (NLP);Machine Learning;Artificial Intelligence},
  doi={10.1109/SMART63812.2024.10882580},
  ISSN={2767-7362},
  month={Dec},}@INPROCEEDINGS{10772938,
  author={Aishani, Pandey and Arush, Sachdeva and Vivek, Mathur},
  booktitle={2024 ITU Kaleidoscope: Innovation and Digital Transformation for a Sustainable World (ITU K)}, 
  title={Unsight: Bridging The Educational Gap For Blind Learners}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Visually impaired learners face significant challenges in accessing quality education due to the lack of accessible educational materials and adaptive learning environments. Traditional educational methods often fall short in addressing the unique needs of these learners, leading to a gap in educational outcomes and opportunities. UnSight aims to enhance the educational experience for visually impaired learners through a virtual education platform that integrates advanced AI technologies. This platform leverages speech and text processing, image and text integration, and voice-to-action commands to address accessibility gaps. Emphasizing adaptive learning and personalized feedback, the proposed system contributes to SDG 4: Quality Education. This paper discusses the technical architecture, standardization efforts, scalability, and ethical considerations involved in developing and deploying UnSight.},
  keywords={Adaptive learning;Technological innovation;Ethics;Scalability;Face recognition;Digital transformation;Standardization;ITU;Speech processing;Text processing;Virtual Education;Accessibility;AI;Speech Processing;Image Recognition;Voice Commands;Visually Impaired;SDG 4},
  doi={10.23919/ITUK62727.2024.10772938},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10270742,
  author={Puri, Mahira and Arora, Saiyam and Rohan and Vinayak and Nagrath, Preeti},
  booktitle={2023 3rd Asian Conference on Innovation in Technology (ASIANCON)}, 
  title={Breaking the Silence: Empowering the Deaf, Mute & Blind Community through Sign Language Live Captioning Using Deep Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Gesture recognition is a rapidly advancing field in human-computer interaction, driven by the progress in artificial intelligence. This study focused on developing an efficient method for real-time translation of static and dynamic gestures in American Sign Language (ASL), British Sign Language (BSL), French Sign Language (FSL), and Indian Sign Language (ISL). Hand features were captured using Google's Mediapipe, while a custom dataset was created for experimentation. Long Short Term Memory (LSTM) was employed to recognize hand gestures, encompassing 70+ classes of signs and gestures. By training the LSTM model with spatiotemporal features extracted from the dataset, the resulting machine learning model facilitated real-time gesture prediction, live captioning, Text-to-Speech audio generation in multiple languages as well as text/speech conversion into ASL. This system has profound applications, benefiting individuals who are deaf, mute, or have difficulties in reading. With the proposed method tested on an 80% training and 20% test split, encouraging results with an accuracy of 96.77% were obtained for sign language gesture recognition.},
  keywords={Training;Technological innovation;Gesture recognition;Assistive technologies;Predictive models;Feature extraction;Real-time systems;American sign language;deep learning;hand gesture recognition;LSTM;Mediapipe;neural networks;sign language recognition},
  doi={10.1109/ASIANCON58793.2023.10270742},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11140985,
  author={Suresh, V. and S, Sabarivasan and S, Rohan V and M, Vignesh},
  booktitle={2025 8th International Conference on Computing Methodologies and Communication (ICCMC)}, 
  title={Audio Lens–Assistive Smart Glass for Blind}, 
  year={2025},
  volume={},
  number={},
  pages={1745-1749},
  abstract={Audio Lens is an assistive technology device to increase independence and access to printed materials and object recognition for those who have an inoperable vision impairment. Audio Lens is a Raspberry Pi platform that uses modern scanning and AI technologies to allow independence to be increased for the user. The user captures an image of printed material from books, menus, signage, etc., using a high-resolution camera, and sends the image to Tesseract OCR which processes the image into text that can be output audibly using a Text-to-Speech (TTS) program, so the user can produce content for consumption without the need of an able-bodied person to read to the individual. The device also has a real-time object detection process that utilizes the YOLO-model to allow someone to understand objects that exist in their world, providing situational awareness and safer navigation. The device audibly announces objects detected, subsequently providing another source of inputs useful for interaction in the environment. The device utilizes a rechargeable battery that allows portability and convenience for daily use. The device contains very simple controls that a user can access through basic button presses or detection gestures, allowing the technical and developmental controls to be simple enough to allow anyone to use at any age or technical level. Audio Lens developed a useful, wearable solution that empowers mobility, safety, and inclusion for people who are visually impaired by combining text-to-speech and object recognition technology, allowing them to close the accessibility gap and move confidently through the world.},
  keywords={YOLO;Presses;Optical character recognition;Visual impairment;Real-time systems;Text to speech;Safety;Object recognition;Lenses;Smart glasses;Text-to-Audio;Raspberry Pi;TTS;OCR;tesseract;Object Detection;YOLO},
  doi={10.1109/ICCMC65190.2025.11140985},
  ISSN={},
  month={July},}@INPROCEEDINGS{10051337,
  author={Akanji, Wasiu and Okey, Ogobuchi and Adelanwa, Saheed and Odesanya, Oluwafunsho and Olaleye, Taiwo and Amusu, Mary and Akinrinlola, Akinfolarin and Oladejo, Abiodun},
  booktitle={2022 5th Information Technology for Education and Development (ITED)}, 
  title={A blind steganalysis-based predictive analytics of numeric image descriptors for digital forensics with Random Forest & SqueezeNet}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Image steganalysis have been a prominent study in digital forensics and the data science use case of artificial intelligence has been widely adopted in conceptual frameworks. In existing studies, deep learners gain prominence for intrusion detection systems while other dissimilar modules are used for feature extraction. Hence, this study rather employs deep learners as image embedding networks aimed at feature extraction for a predictive analytics of image steganalysis. The extracted numeric image descriptors trains three learner algorithms for pattern recognition using a 10 fold cross-validation system. Experimental result indicates the ensemble of Random forest algorithm and SqueezeNet image embedder as the best for steganalysis in digital forensics while the size of the training set turns out to be insignificant for the supervised machine learning study.},
  keywords={Training;Machine learning algorithms;Digital forensics;Intrusion detection;Feature extraction;Prediction algorithms;Pattern recognition;Image steganalysis;Machine learning;Digital forensics;image embedding},
  doi={10.1109/ITED56637.2022.10051337},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10533951,
  author={V, Benedict Vinusha and V, Indhuja and Reddy, Medarametla Varshitha and Nikhitha, Nagalla and Pramila, R. Priyanka},
  booktitle={2024 International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS)}, 
  title={VisionSaver: Revolutionizing Ocular Disease Detection with AI-Powered Real-Time Screening}, 
  year={2024},
  volume={},
  number={},
  pages={590-596},
  abstract={Detecting ocular diseases is crucial for preventing vision impairment and blindness. However, various obstacles such as insufficient awareness, elevated healthcare expenses, and the necessity for technological advancements impede effective screening. This ground-breaking device, VisionSaver, presents a novel method of identifying eye pathologies. By employing a multi-layered deep Convolutional Neural Network, VisionSaver outperforms traditional models in the extraction of complex characteristics from medical images. VisionSaver stands out for its real-time detection capabilities, which are made possible by an easy-to-use Graphical User Interface. This feature gives medical professionals accurate results and makes it possible to make decisions quickly. The system’s adaptability guarantees precise identification of a broad range of ocular diseases, encompassing Diabetic Retinopathy, glaucoma, and even uncommon ocular conditions. VisionSaver enhances efficiency and retains crucial features by integrating sophisticated techniques like convolutional layers, pooling layers, dense layers, and VGG models. This innovative strategy holds the potential to transform eye healthcare, ushering in a proactive treatment era and significantly enhancing patient outcomes. VisionSaver combines data collection and preparation, data preprocessing, feature extraction, model selection, and model training to provide a robust solution for ocular disease detection achieving the accuracy of 91%. The system’s real-time capabilities, user-friendly interface, and adaptability make it a powerful tool in the fight against ocular diseases.},
  keywords={Glaucoma;Adaptation models;Pathology;Visual impairment;Medical services;Feature extraction;Real-time systems;Deep Learning;Real-time screening;medical imaging;Graphical User Interface (GUI);Diabetic retinopathy;Glaucoma;Convolutional Neural Network (CNN)},
  doi={10.1109/ICC-ROBINS60238.2024.10533951},
  ISSN={},
  month={April},}@INPROCEEDINGS{11005366,
  author={K., Vivek Balaji and R., Sugumar},
  booktitle={2025 International Conference on Advanced Computing Technologies (ICoACT)}, 
  title={Diagnosing Diabetic Retinopathy from Retinal Fundus Images Using Internet of Things and AI Deep Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={In recent times, the computer vision (CV) then Internet of Things (IoT) technologies have proven their worth across a range of applications, with a notable focus on healthcare. IoT-driven healthcare resolutions present intelligent strategies to significantly reduce costs and elevate the quality of healthcare services. Concurrently, Diabetic Retinopathy (DR) stands as a condition capable of causing permanent blindness and visual impairment in individuals afflicted by diabetes. The precise and initial identification of DR holds the possible to mitigate vision loss. To support experts in diagnosing DR, a Computer-Aided Analyses prototypical centered on retinal fundus images emerges as a potent instrument. In this domain, certain conventional Machine Learning (ML)-based DR diagnostic models have already been established. Nevertheless, recent strides in Deep Learning (DL) have demonstrated remarkable accomplishments compared to traditional ML algorithms in a variety of applications, rendering it a compelling choice for crafting DR diagnostic models. Fueled by this impetus, this paper introduces an innovative diabetic retinopathy diagnosis model (IDDR) that combines IoT and DL technologies while employing retinal fundus images. The IDDR approach harnesses IoT devices for data collection, followed by the transmission of this data to a cloud server for processing. Subsequently, the retinal fundus imageries undergo preprocessing to eliminate noise then enhance contrast levels. The identification of lesion regions within the fundus image is accomplished through the application of the Ant Colony Algorithm-based region-growing (ACORG) segmentation technique. Furthermore, an effective DR diagnosis is facilitated through a feature extractor based on the densely connected network (DenseNet) and a classifier relying on Long Short-Term Memory (LSTM). Toward fine-tune the parameters of the LSTM method, the Ant Colony Algorithm (ACA) is employed. To assess the enhanced diagnostic outcomes of the IDDR method, a comprehensive series of replications was conducted. A thorough comparative analysis revealed the superior recital of the proposed approach.},
  keywords={Deep learning;Diabetic retinopathy;Computational modeling;Noise;Medical services;Retina;Classification algorithms;Internet of Things;Servers;Long short term memory;Deep learning;LSTM;CNN;Computer vision;Ant_colony},
  doi={10.1109/ICoACT63339.2025.11005366},
  ISSN={},
  month={March},}@INPROCEEDINGS{10962741,
  author={Tasnia, Naima and Hosen, Md.Hamid and Nawar, Sadia and Amran, Mohammed and Chowdhury, Rituparna and Uddin, Altaf},
  booktitle={2024 IEEE International Conference on Biomedical Engineering, Computer and Information Technology for Health (BECITHCON)}, 
  title={Enhancing Glaucoma Diagnosis with Explainable AI Using Vision Transformers and Advanced Deep Learning Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={206-211},
  abstract={Glaucoma is a leading cause of irreversible blindness worldwide, necessitating early and accurate diagnosis to prevent vision loss. To ensure trustworthiness in the accuracy of diagnosis, interpretability, and understanding of risk factors associated with glaucoma are essential. This study proposes a comprehensive methodology for glaucoma classification using various deep learning architectures, including CNN, VGG16, VGG19, InceptionResNetV2, Xception, and Vision Transformers (ViTs) integrated with Explainability techniques. The dataset utilized for this research comprises retinal fundus images divided into training, validation, and testing sets. The approach involves applying various preprocessing techniques, like CLAHE, green channel conversion, and canny edge detection, to enhance feature extraction. Among the evaluated models, the ViT demonstrated superior performance, achieving an accuracy of 92% on the test set. This model’s ability to dynamically adjust attention weights based on the input image contributes to its improved classification capabilities. To provide insights into the model’s decision-making process, Grad-Cam facilitates better treatment planning for clinicians.},
  keywords={Glaucoma;Deep learning;Training;Computer vision;Accuracy;Explainable AI;Transformers;Retina;Planning;Testing;Glaucoma;Deep Learning;Explainable Artificial Intelligence (XAI);Vision Transformers;Grad CAM;CLAHE},
  doi={10.1109/BECITHCON64160.2024.10962741},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10941317,
  author={Hridya, G. and Suganya, S.S.},
  booktitle={2024 International Conference on Distributed Systems, Computer Networks and Cybersecurity (ICDSCNC)}, 
  title={Glaucoma Identification Using AI-based Deep Learning Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Early detection is key to effectively managing glaucoma, the leading cause of irreversible blindness. This study introduces a deep learning model based on Convolutional Neural Networks (CNNs) designed to automatically detect glaucoma using retinal images. The model's architecture includes several layers that help it learn and identify signs of glaucoma. To make the model more reliable and widely applicable, a lot of data was processed and enhanced. Key measures used to evaluate the model's performance included the AUC-ROC curve, F1-Score, accuracy, sensitivity, specificity, and precision. When tested on a separate dataset, the model outperformed traditional eye care methods and existing models, achieving an AUC-ROC of 0.978, sensitivity of 93.8%, specificity of 96.5%, precision of 94.1%, F1-Score of 94.0%, and accuracy of 95.2%. The proposed deep learning model offers a reliable, accurate, and scalable solution for glaucoma screening, with the potential to improve early detection and patient outcomes in clinical settings.},
  keywords={Glaucoma;Deep learning;Visualization;Accuracy;Sensitivity;Telemedicine;Retina;Data models;Reliability;Convolutional neural networks;intraocular pressure;optical coherence tomography;generative adversarial networks;receiver operating characteristic;gradient-weighted class activation mapping},
  doi={10.1109/ICDSCNC62492.2024.10941317},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11081362,
  author={Suganthi, N. Mohana and Vineela, Kanagala and Chandana, Malgireddy Siri and Chaitanya, J Shiva Sai},
  booktitle={2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={Eye Sight: Integrative AI for Non-Invasive Diabetic Retinopathy Detection using Pupillometry and Ensemble Deep Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1264-1271},
  abstract={Diabetic Retinopathy (DR) is the leading cause of blindness and vision impairment among people with diabetes and timely diagnosis is important to prevent severe consequences. Traditional DR screening methods are invasive and expensive collection techniques. Eyesight devises an innovative methodology to address this challenge by exploiting pupillometry, a noninvasive technique which measures the response of the pupil to light stimulus. Eye Sight combines this with ensemble deep learning models (ResNet, DenseNet, and EfficientNet) to classify DR severity across five stages: Mild DR, Moderate DR, Proliferative DR, and Severe DR. Early DR is detected by the system in an available, cheap, and accurate method. Eyesight is implemented on the Streamlit platform and provides clinicians the ability to upload pupillometry data to receive real time results with high confidence thus enabling timely intervention. Integrating pupillometry with advanced machine learning models improves the diagnostic efficiency and scalability and creates global access to early DR detection and treatment.},
  keywords={Deep learning;Diabetic retinopathy;Accuracy;Scalability;Visual impairment;Blindness;Noninvasive treatment;Real-time systems;Pupils;Diabetic Retinopathy;Pupillometry;Deep Learning;Ensemble Models;Non-Invasive Screening;ResNet;DenseNet;EfficientNet;Real-Time Classification},
  doi={10.1109/ICSSAS66150.2025.11081362},
  ISSN={},
  month={June},}@INPROCEEDINGS{11041855,
  author={Donga, Rishith and Agarwal, Manas and Deepa, R},
  booktitle={2025 Third International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)}, 
  title={VisionCare: AI-Driven Macular Degeneration Diagnosis using Fundus Images}, 
  year={2025},
  volume={},
  number={},
  pages={43-48},
  abstract={One of the main causes of vision loss and blindness, particularly in older persons, is macular degeneration. To stop serious sight decline, early detection is essential. The goal of this project is to create an automated system that uses fundus images to diagnose macular degeneration. Deep learning techniques will be used to improve the diagnostic' speed and accuracy. Fundus imaging is the best modality for identifying retinal abnormalities associated with macular degeneration because it offers detailed views of the retina. We suggest training VGG16 Convolutional Neural Networks (CNNs) on publicly accessible datasets of labelled fundus images in order to classify images. The model will be built to discriminate between retinal images that are normal and those that exhibit early, moderate, and severe stages of macular degeneration. The system will use data preparation methods like picture augmentation and normalization to improve performance. Classification metrics, such as accuracy, precision, recall, and AUC (Area Under the Curve), will be used to assess the final model. By offering rapid, accurate, and non-invasive screening for macular degeneration, this technology may help ophthalmologists by promoting early intervention and better patient outcomes.},
  keywords={Macular degeneration;Training;Deep learning;Accuracy;Sensitivity;Transfer learning;Retina;Robustness;Convolutional neural networks;Usability;Macular Degeneration;Fundus Images;Deep Learning;Convolutional Neural Networks (CNNs);Retinal Image Analysis;Automated Diagnosis;Medical Imaging;Transfer Learning;Ophthalmology},
  doi={10.1109/ICAISS61471.2025.11041855},
  ISSN={},
  month={May},}@INPROCEEDINGS{10739046,
  author={Shetty, Mangala and Ganesh, K N and Shetty, Spoorthi B},
  booktitle={2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT)}, 
  title={Design and Implementation of a Voice-Enabled Web Application for Customer Interaction Using Natural Language Processing}, 
  year={2024},
  volume={1},
  number={},
  pages={1-4},
  abstract={Web accessibility is crucial for inclusivity, particularly for users with diverse needs. This paper explores the integration of Dialogflow, a conversational AI platform, with Text-to-Speech (TTS) and Speech-to-Text (STT) technologies to enhance web accessibility. Our study aims to improve user interaction and accessibility features through this integration. We investigate the effectiveness of Dialogflow in creating a voice-enabled web accessibility solution and enhancing user experience and accessibility on the web.Our comparative analysis with existing methods demonstrates the effectiveness of our approach in improving customer interaction and satisfaction. Integrating Dialogflow with TTS and STT technologies provides a comprehensive solution for enhancing web accessibility, ensuring that all users, regardless of their abilities, can access and interact with web content effectively. This research contributes to the advancement of web accessibility and provides insights for future developments in this field.},
  keywords={Computers;Accuracy;User experience;Natural language processing;Text to speech;Reliability;Speech to text;voice interaction;dialogflow;text-to-speech;speech-to-text;web accessibility;user interaction},
  doi={10.1109/ICEECT61758.2024.10739046},
  ISSN={},
  month={Aug},}@ARTICLE{10510039,
  author={Zhan, Xianghao and Liu, Yuzhe and Cecchi, Nicholas J. and Callan, Ashlyn A. and Le Flao, Enora and Gevaert, Olivier and Zeineh, Michael M. and Grant, Gerald A. and Camarillo, David B.},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={AI-Based Denoising of Head Impact Kinematics Measurements With Convolutional Neural Network for Traumatic Brain Injury Prediction}, 
  year={2024},
  volume={71},
  number={9},
  pages={2759-2770},
  abstract={Objective: Wearable devices are developed to measure head impact kinematics but are intrinsically noisy because of the imperfect interface with human bodies. This study aimed to improve the head impact kinematics measurements obtained from instrumented mouthguards using deep learning to enhance traumatic brain injury (TBI) risk monitoring. Methods: We developed one-dimensional convolutional neural network (1D-CNN) models to denoise mouthguard kinematics measurements for tri-axial linear acceleration and tri-axial angular velocity from 163 laboratory dummy head impacts. The performance of the denoising models was evaluated on three levels: kinematics, brain injury criteria, and tissue-level strain and strain rate. Additionally, we performed a blind test on an on-field dataset of 118 college football impacts and a test on 413 post-mortem human subject (PMHS) impacts. Results: On the dummy head impacts, the denoised kinematics showed better correlation with reference kinematics, with relative reductions of 36% for pointwise root mean squared error and 56% for peak absolute error. Absolute errors in six brain injury criteria were reduced by a mean of 82%. For maximum principal strain and maximum principal strain rate, the mean error reduction was 35% and 69%, respectively. On the PMHS impacts, similar denoising effects were observed and the peak kinematics after denoising were more accurate (relative error reduction for 10% noisiest impacts was 75.6%). Conclusion: The 1D-CNN denoising models effectively reduced errors in mouthguard-derived kinematics measurements on dummy and PMHS impacts. Significance: This study provides a novel approach for denoising head kinematics measurements in dummy and PMHS impacts, which can be further validated on more real-human kinematics data before real-world applications.},
  keywords={Kinematics;Head;Strain;Noise reduction;Biomedical measurement;Convolutional neural networks;Brain injuries;Deep learning;head kinematics;instrumented mouthguard;traumatic brain injury},
  doi={10.1109/TBME.2024.3392537},
  ISSN={1558-2531},
  month={Sep.},}@ARTICLE{9731827,
  author={Génova, Gonzalo and Pelayo, Valentín Moreno and Martín, M. Rosario González},
  journal={IEEE Technology and Society Magazine}, 
  title={A Lesson From AI: Ethics Is Not an Imitation Game}, 
  year={2022},
  volume={41},
  number={1},
  pages={75-81},
  abstract={The title of the 2014 movie The Imitation Game tells us the life of Alan Turing, especially his outstanding participation in the decipherment of the German messages encrypted with the Enigma machine in the Bletchley Park Complex [1], [2]. The expression “the imitation game” is from Turing himself: these are the first words of his 1950 article, Computing Machinery and Intelligence [3]. It is also the name of a game played by the Victorian aristocracy, which consisted in a blind exchange of handwritten messages to try to guess whether the interlocutor was a woman or a man.},
  keywords={Ethics;Games;Motion pictures;Cryptography;Artificial intelligence},
  doi={10.1109/MTS.2022.3147531},
  ISSN={1937-416X},
  month={March},}@ARTICLE{10579706,
  author={Poli, L. and Rocca, P. and Rosatti, P. and Anselmi, N. and Salucci, M. and Yang, S. and Yang, F. and Massa, A.},
  journal={Radio Science}, 
  title={AI-assisted design of printed edge-fed non-uniform zig-zag antenna for mm-wave automotive radar}, 
  year={2024},
  volume={59},
  number={6},
  pages={1-20},
  abstract={In this paper, the design of a novel horizontally polarized single-layer antenna for 77 (GHz) automotive radar applications is 4 addressed. An innovative non-uniform zig-zag parametrization of the antenna layout is considered to enable a more flexible control on both the impedance matching in the working frequency band and the shaping of the radiated beam pattern with respect to a standard (uniform) one without compromising the linear (horizontal) polarization of the radiated field. Such a polarization guarantees a lower back-scattering from road pavements, resulting in a reduced amount of clutter and thus allowing a more robust target detection. Moreover, the single-layer layout has several advantages in terms of fabrication simplicity/costs and mechanical robustness to vibrations. The design of the proposed non-uniform zig-zag antenna (NZA) is performed through a customized implementation of the System-by-Design (SbD) approach that fruitfully combines machine learning and evolutionary optimization to efficiently deal with the computational complexity at hand. An extensive numerical validation, dealing with designs of different lengths, verifies the high performance of the NZA in terms of beam direction deviation (e.g., BDD < 1 (deg)), sidelobe level (e.g., SLL < −18.2 (dB)), and polarization ratio (e.g., PR > 20 (dB)) within the working frequency band H = [76 : 78] (GHz), as well as its superiority over competitive designs. Finally, the realization of a prototype and its experimental test, validate the proposed NZA concept for automotive mm-wave radar applications in advanced driver assistance systems and autonomous vehicles such as, for instance, adaptive cruise control, collision avoidance, and blind spot detection.},
  keywords={Antennas;Radar antennas;Electromagnetics;Substrates;Shape;Routing;Reliability},
  doi={10.1029/2023RS007912},
  ISSN={1944-799X},
  month={June},}@INPROCEEDINGS{10543550,
  author={Ajina, A. and Lochan, R and Saha, Mohini and Showghi, R B K and Harini, S},
  booktitle={2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS)}, 
  title={Vision Beyond Sight: An AI-Assisted Navigation System in Indoor Environments for the Visually Impaired}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This study introduces an innovative visual assistance technology that incorporates a cutting-edge live object recognition system to address the prevalent challenges faced by visually impaired individuals worldwide. In daily life, the visually impaired rely heavily on touch and auditory cues to navigate their surroundings, which underscores the urgent need for technological support. While existing solutions such as Screen Reading software and Braille devices fail to recognize crucial elements, rendering them ineffective in critical situations, our approach aims to revolutionize the lives of the blind and visually challenged on a global scale by providing a second set of eyes without external assistance. With a commitment to creating an inclusive environment, our research goes beyond current solutions by emphasizing assistive technology, which truly makes a difference. The proposed system leverages SSD MobileNet-v2 FPNlite-based object detection combined with an Android mobile application to empower visually impaired individuals through real-time voice-based guidance. By specifically addressing the challenges related to recognition and navigation, this project aims to significantly improve the independence and overall quality of life of those with visual disabilities, thereby contributing to the creation of a more welcoming and inclusive world.},
  keywords={Computer science;Visualization;Navigation;Object detection;Rendering (computer graphics);Real-time systems;Software;Real-time Object Detection;SSD MobileNet V2 FPNLite;Visually Impaired Navigation;Mobile-based Accessibility;Audio Guidance},
  doi={10.1109/ICETCS61022.2024.10543550},
  ISSN={},
  month={April},}@INPROCEEDINGS{10134901,
  author={Rajaraman, Sanjay and Shivapriya, Ps},
  booktitle={2023 3rd International conference on Artificial Intelligence and Signal Processing (AISP)}, 
  title={A-Eye: Computer vision and Deep learning based Smart-Glasses with Edge computing for Visually Impaired}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={India, where a third of the world’s blind people reside, has about 12 million blind people, especially in comparison to a total of 39 million worldwide, according to the National Programme for Control of Blindness (NPCB). Visually challenged people, including children and elderly people with low vision, still have trouble performing everyday tasks, even in this world of rapidly developing technologies. The existing solutions provide a straight forward solution using Artificial Intelligence technologies. To take this technological solution to the next level we propose a solution with all the AI features seamlessly integrated together and a dedicated mobile application gives full control and independency to their life. The A-Eye (smart glasses) system that is being proposed consists of a wearable smart glass solution, a mobile application, and a cloud-based data management platform. This project shows how Mobile Edge Computing (MEC) can be used to process data on the mobile phone other than to process the camera’s video stream in the cloud or locally at the system level. The project’s use of Mobile Edge Computing (MEC) expands the scope of its ability to add and update an endless number of features without being constrained by hardware. Therefore, the suggested smart glass system can minimize latency and be highly scalable while using heavy and effective models to effectively mitigate the problem and assist visually challenged individuals.},
  keywords={Multi-access edge computing;Blindness;Streaming media;Signal processing;Mobile handsets;Mobile applications;Artificial intelligence;Artificial intelligence;Internet of Things (IoT);computer vision;Deep learning;Mobile Edge Computing},
  doi={10.1109/AISP57993.2023.10134901},
  ISSN={2640-5768},
  month={March},}@INPROCEEDINGS{9719298,
  author={Zhai, Zhenzhen and Gao, Qi and Jiang, Yuan and Chai, Xinyu and Han, Wenjie},
  booktitle={2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications (ICPECA)}, 
  title={A Software for Rapid Annotation of Scene Objects Based on Saliency Object Ranking}, 
  year={2022},
  volume={},
  number={},
  pages={378-382},
  abstract={With the development of artificial intelligence (AI), a mainstream research line in building a non-invasive intelligent system for assisting people with visual disabilities is to capture images with a camera, and then using AI to identify the content of images and convert visual information into auditory information. The key point is to establish high-quality personalized datasets based on real open scenes for AI model training. However, most of existing annotation software is designed for face recognition, automatic driving and other tasks, whose annotation information type is monotonous. Also, when converting an image with multiple objects into audio information, the order of objects needs to be determined. Few studies have focused on this issue currently. Therefore, we develop an image annotation software for rapid construction of small sample databases of AI models that can be used in intelligent blindness assistance systems. The software can additionally mark information such as size and affiliation. Moreover, it has a database module to facilitate the management of multiple annotation tasks. Then, a saliency object ranking network based on multi-task cascade is designed. The object ranking results can provide a reference for the sequence of audio information. The Hit Rate, Relative Ranking accuracy and Saliency Object Ranking score of the ranking network are better than that existing saliency object network on the validation set. Finally, the network is deployed into the software to realize the AI-assisted annotation, and to quickly establish the personalized dataset.},
  keywords={Training;Visualization;Annotations;Databases;Blindness;Multitasking;Software;saliency object ranking;image annotation software;intelligent blindness assistance systems;personalized dataset},
  doi={10.1109/ICPECA53709.2022.9719298},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11189150,
  author={Gu, Chuqiao and Zhang, Wuyang and Huang, Zhenqian and Kou, Jieren and Liu, Zhenyao and Zhao, Chenjun and Liu, Chang and Zhang, Lifeng and Lin, Wenjie and Wang, Zhongda and Deng, Jianwei and Xie, Yuhuan and Huang, Guoxin and Zhang, Charles and Lu, Xiuyuan and Wang, Chengming and Zhang, Zejun and Yuan, Hao and Duan, Xiaoman and Fang, Yajun},
  booktitle={2024 7th International Conference on Universal Village (UV)}, 
  title={LENS: Layers of Evaluation of Hallucination in GenAI Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-85},
  abstract={Large Language Models (LLMs) and Vision-Language Models (VLMs) demonstrate remarkable capabilities but remain vulnerable to hallucinations—producing plausible yet factually incorrect content—with error rates ranging from as low as 1.47% in clinical applications [1] to as high as 75% in domain-specific queries [2]. Despite growing attention, existing hallucination evaluation frameworks remain insufficient to meet critical needs. Through a comprehensive survey of over 100 evaluation methods spanning six methodological paradigms (probe-based, adversarial testing, causal intervention, uncertainty-guided, internal state analysis, and online evaluation), we identify fundamental limitations: current approaches fall short of enabling objective model comparison, providing diagnostic insights into failure modes, supporting domain-evolving benchmark construction, and guiding targeted mitigation strategies. Our analysis reveals that evaluations remain fragmented, operating either horizontally—comparing models across tasks and domains—or vertically—probing reasoning chains within single outputs. This fragmentation limits holistic assessment: horizontal evaluations provide breadth but risk superficiality, while vertical assessments deliver depth but lack generalizability. Moreover, we identify five critical gaps: (1) dimensional poverty reducing hallucinations to binary metrics, (2) failure to integrate horizontal breadth with vertical depth, (3) metacognitive blind spots overlooking when models should seek external verification, (4) adaptability crisis from static benchmarks, and (5) transparency deficits providing scores without actionable insights. Beyond surveying the landscape, this paper articulates eight fundamental challenges confronting comprehensive evaluation—from epistemological difficulties in defining ground truth and computational complexity of scaling assessment, to attribution opacity obscuring causal mechanisms and dynamic knowledge evolution rendering benchmarks obsolete. These challenges span multimodal complexity, scale and diversity requirements, adversarial robustness, and human alignment considerations. We present LENS (Layers of Evaluation of Hallucination in GenAI Systems), a unified framework addressing these gaps through hierarchical, tree-based query decomposition. LENS transforms complex evaluation tasks into multi-layered assessment structures via a six-stage pipeline (task formulation, decomposition, tool-augmented execution, structured generation, multi-dimensional scoring, and trace analysis), enabling MRI-like scanning of inference processes to reveal where and why hallucinations originate. The framework introduces four key innovations: (1) Tool Necessity Detection and Selection (TND/TSA) – explicitly evaluating when models should consult external sources versus relying on parametric memory, addressing a fundamental hallucination source. (2) Multi-Dimensional Metrics – assessing degree (accuracy, faithfulness, tool appropriateness), quantity (coverage, completeness), stability (consistency, robustness), and risk (uncertainty quantification) beyond binary detection. (3) User-Centric Benchmark Construction – empowering organizations to design custom evaluations from their evolving knowledge bases while maintaining methodological rigor. (4) Actionable Error Attribution – providing hierarchical decomposition traces with causal attribution, evidence chains, and OpenTelemetry-based reproducibility for transparent auditing. Our systematic taxonomy unifies previously fragmented approaches across evaluation targets (task-specific, modality-based, hallucination-type, domain-specific), dimensions (factuality, faithfulness, consistency, robustness, causal reasoning, interpretability), and methodologies. We introduce unified metrics transcending individual dimensions and present mitigation-aware evaluation strategies integrating RAG, parameter-efficient fine-tuning, knowledge distillation, preference optimization, and temporal intervention approaches. By combining horizontal breadth (across domains and architectures) with vertical depth (into reasoning processes), LENS advances hallucination evaluation from post-hoc error detection to proactive risk assessment. Case studies in medical diagnosis, legal analysis, and financial reasoning demonstrate the framework’s transformative impact, enabling objective model comparison, informed selection, diagnostic insights, domain-evolving benchmarks, and targeted mitigation development—fostering calibrated trust in AI systems deployed in safety-critical applications where accuracy, interpretability, and accountability are indispensable.},
  keywords={Analytical models;Accuracy;Uncertainty;Prevention and mitigation;Large language models;Transforms;Benchmark testing;Cognition;Robustness;Lenses;Large language models;vision-language models;hallucination evaluation;hallucination mitigation;retrieval-augmented generation;comprehensive hallucination evaluation;horizontal vs vertical evaluation;generative AI evaluation;LENS framework;multi-dimensional metrics;multi-layered assessment structures;cross-domain evaluation;visual-textual misalignment;cross-modal inconsistencies;dynamic knowledge;evolving benchmarks;tool necessity detection;tool selection accuracy;hierarchical query decomposition;faithfulness evaluation;domain-specific benchmarks;user-centric benchmark construction;transparency and interpretability;surface-level accuracy;compositional complexity;ground truth ambiguity;reasoning failures;adversarial robustness;factuality assessment;uncertainty quantification;evidence grounding;tree-of-thoughts;reliability and robustness;causal reasoning capability;proactive risk assessment;computational efficiency;actionable error attribution;traceable evidence chains;post-hoc error detection;trustworthy AI;domain-specific queries;temporal drift;Universal Village;system theory;metacognitive evaluation;mechanistic interpretability;multimodal hallucination;causal attribution},
  doi={10.1109/UV63228.2024.11189150},
  ISSN={},
  month={Oct},}
