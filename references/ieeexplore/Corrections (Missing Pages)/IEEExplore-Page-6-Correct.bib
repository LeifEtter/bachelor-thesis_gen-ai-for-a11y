@INPROCEEDINGS{10931891,
  author={A, Boobal and J, Loyola Jasmine and Reddy, Chennama Charan Kesava and Reddy, Challa Akshay and Sai Rohith, Chaluvadi Bala Venkata},
  booktitle={2024 International Conference on Communication, Control, and Intelligent Systems (CCIS)}, 
  title={Real-Time Sign Language and Audio Conversion Using AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The appearance of audio conversion and the appearance of gesture language have greatly advanced communication technologies, especially for hearing impairment and blind. These double -purpose technologies promote communication between these groups, provide opportunities for expression and speech behind. The technology uses advanced artificial intelligence algorithms to provide real-time speech and sign language translation to improve accessibility and inclusivity. It is essential to protect data privacy, maintain the accuracy of AI models to avoid misinterpretations, and address ergonomic issues associated with long-term use. Technology converts audio entry into a gestic signal that can be seen by avatars using computer vision and natural language processing. There is a depth learning model that improves voice recognition and the acquisition of signs, and is fully adapted to many languages and sign dialects. Addressing such health and safety issues is essential to advancing the communication and optimization of this advanced technology.},
  keywords={Sign language;Computer vision;Translation;Communication systems;Computational modeling;Auditory system;Assistive technologies;Real-time systems;Natural language processing;Convolutional neural networks;Sign language translation;Real-time gesture recognition;Speech-to-sign language;Sign language to audio;Deep learning;Convolutional Neural Networks (CNNs);Natural language processing (NLP);Audio-to-sign language conversion;Machine learning;Gesture recognition;Computer vision in sign language;Django;MySQL;Indian sign language (ISL)},
  doi={10.1109/CCIS63231.2024.10931891},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10176033,
  author={Kumar Mishra, Amit},
  booktitle={2023 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)}, 
  title={A Propagation-model Empowered Solution for Blind-Calibration of Sensors}, 
  year={2023},
  volume={},
  number={},
  pages={01-05},
  abstract={Calibration of sensors is a major challenge especially in inexpensive sensors and sensors installed in inaccessible locations. The feasibility of calibrating sensors without the need for a standard sensor is called blind calibration. There is very little work in the open literature on totally blind calibration. In this work we model the sensing process as a combination of two processes, viz. propagation of the event through the environment to the sensor and measurement process in the sensor. Based on this, we propose a unique method for calibration in two flavours, viz semi-blind and completely-blind calibration. We show limited results based on simulation showing encouraging results.},
  keywords={Sensors;Calibration;Instrumentation and measurement;Standards;Sensors;Calibration;Blind-calibration;Sensor Network;AI},
  doi={10.1109/I2MTC53148.2023.10176033},
  ISSN={2642-2077},
  month={May},}@INPROCEEDINGS{10902945,
  author={Park, Woojin and An, Hyeyoung and Park, Soochang},
  booktitle={TENCON 2024 - 2024 IEEE Region 10 Conference (TENCON)}, 
  title={Smart Docent System for Visually Impaired People with Mobile-Based Artificial Intelligence of Things}, 
  year={2024},
  volume={},
  number={},
  pages={1541-1544},
  abstract={With the increasing demand for high-quality user experiences in museums and art galleries, interest in smart guidance systems has grown. These systems aim to provide detailed information about the artwork and deliver audio explanations related to the pieces of interest. Traditional guidance systems typically use techniques such as numbering, QR codes, and beacons. However, these methods can be challenging for visually impaired individuals, as they rely on visual cues. To address this issue, this paper introduces a novel smart docent system designed to enhance the gallery experience for visually impaired visitors. The system features two primary functional frameworks, supported by Artificial Intelligence (AI) models: user escorting and audio touring. These frameworks utilize mobile-based sensing, dynamic Bluetooth Low Energy (BLE) beaconing, and edge computing. The system employs an on-device object detection model to minimize communication delays caused by large amounts of visual data. Although BLE beacons are statically installed within the art gallery, they dynamically update location IDs. Edge computing helps correct positioning errors based on the user's movement history and direction. Moreover, the system adaptively provides an audio tour based on the user's behavior, such as approaching or passing by artwork, or leaving an exhibit while listening to audio descriptions.},
  keywords={Visualization;Art;Navigation;Computational modeling;QR codes;Object detection;Museums;Sensors;Artificial intelligence;Edge computing;AI-aided system;indoor navigation;gallery experience;blind},
  doi={10.1109/TENCON61640.2024.10902945},
  ISSN={2159-3450},
  month={Dec},}@INPROCEEDINGS{10053477,
  author={Aravindan, C and Vasuki, R},
  booktitle={2023 International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)}, 
  title={Detection and Classification of Early Stage Diabetic Retinopathy using Artificial Intelligence and Image Processing}, 
  year={2023},
  volume={},
  number={},
  pages={919-924},
  abstract={Diabetic retinopathy is the term used to describe the damage to the blood vessels in the retina of the human eye. The symptoms of diabetic retinopathy are blurriness, difficulty in vision and even blindness can occur. The blood vessels in the retina of the human eye have been damaged over time, which has an impact on the person’s ability to see. It is a cumulative problem in the modern world. Diabetic retinopathy has four stages, including mild, moderate, and severe non proliferative and proliferative. To reduce the effects of diabetic retinopathy are early diagnosis is necessary. Thus, by using artificial intelligence and image processing, the early stage of diabetic retinopathy can be detected. This leads to faster and easier screening of disorder for both the patients and ophthalmologists.},
  keywords={Training;Retinopathy;Image processing;Data visualization;Blood vessels;Retina;Diabetes;Diabetic retinopathy detection;artificial intelligence;earlier prediction;image processing;ophthalmologists},
  doi={10.1109/IDCIoT56793.2023.10053477},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10448193,
  author={Guo, Xuechen and Hu, Wenhao and Ni, Chiming and Chai, Wenhao and Li, Shiyan and Wang, Gaoang},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Blind Inpainting with Object-Aware Discrimination for Artificial Marker Removal}, 
  year={2024},
  volume={},
  number={},
  pages={1516-1520},
  abstract={Medical images often incorporate doctor-added markers that can hinder AI-based diagnosis. This issue highlights the need of inpainting techniques to restore the corrupted visual contents. However, existing methods require manual mask annotation as input, limiting the application scenarios. In this paper, we propose a novel blind inpainting method that automatically reconstructs visual contents within the corrupted regions without mask input as guidance. Our model includes a blind reconstruction network and an object-aware discriminator for adversarial training. The reconstruction network contains two branches that predict corrupted regions in images and simultaneously restore the missing visual contents. Leveraging the potent recognition capability of a dense object detector, the object-aware discriminator ensures markers undetectable after inpainting. Thus, the restored images closely resemble the clean ones. We evaluate our method on three datasets of various medical imaging modalities, confirming better performance over other state-of-the-art methods.},
  keywords={Training;Visualization;Detectors;Manuals;Robustness;Image restoration;Task analysis;Blind image inpainting;generative adversarial networks;image reconstruction;dense object detector},
  doi={10.1109/ICASSP48485.2024.10448193},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10040269,
  author={S, Zulaikha Beevi and P, Harish Kumar and S, Harish and J, Lakshan S},
  booktitle={2022 1st International Conference on Computational Science and Technology (ICCST)}, 
  title={Decision Making Algorithm for Blind Navigation Assistance using Deep Learning}, 
  year={2022},
  volume={},
  number={},
  pages={268-272},
  abstract={Blind people face several obstacles in their daily lives and technological interventions can help overcome these obstacles. In this research, we provide an AI-based autonomous assisting device that recognizes many objects and it will provide acoustic input to the user to help visually blind people to understand the surrounding better to understand their environment better. Multiple photos of objects relevant to visually impaired people were used to build a deep-learning model. Training photos are enhanced and manually annotated to improve the trained model's resilience. A distance-measuring sensor is included which recognise the objects using computer vision. The gadget is made more inclusive by recognizing the obstacles coming out of one place to another. After stage segmentation and obstacle detection, the aural information sent to the user is adj usted to get a lot of details in minimum time and speed up video processing.},
  keywords={Deep learning;Training;Navigation;Scientific computing;Computational modeling;Blindness;Libraries;Deep learning model;Image processing;Speech recognition;RCNN;Image recognition;Voice output},
  doi={10.1109/ICCST55948.2022.10040269},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10457245,
  author={Sungman, Hong},
  booktitle={2024 International Conference on Electronics, Information, and Communication (ICEIC)}, 
  title={Trade-off Experiments in Efficient Blind Video Quality Assessment: An Analysis on Sampling Strategies across CPU and GPU}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Currently, video data dominates mobile internet traffic. Aided by AI, promoting active research in Blind Video Quality Assessment (BVQA) for evaluating received video quality without the original video. However, AI methodologies using CNN employ all frames to maximize their predictive accuracy, may encounter long prediction times, hindering real-time applications like streaming services. This study explores accuracy and time trade-offs using frame sampling strategies across varying frame sampling cycles. We introduce the “Efficient BVQA coefficient” to gauge the balance between performance and time reduction, identifying optimal sampling points on both CPU and GPU with their characteristics. Our findings elucidate the impact of frame sampling strategies on BVQA's accuracy and computational time, paving the way for developing more efficient BVQA methodologies.},
  keywords={Graphics processing units;Streaming media;Real-time systems;Quality assessment;Computational efficiency;Artificial intelligence;Video recording;Blind video quality assessment;Frame sampling strategies;Trade-off experiments;Prediction accuracy;Time reduction},
  doi={10.1109/ICEIC61013.2024.10457245},
  ISSN={2767-7699},
  month={Jan},}@INPROCEEDINGS{10047784,
  author={Sangeetha, S.V.Tresa and Porselvi, T. and Venkateshwaran, A and Gokul, M and Sathmikan, I and Tharun Kumar, P},
  booktitle={2022 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)}, 
  title={Currency Detection App for Blind People Using MIT App Inventor}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={This paper helps the visually impaired people to identify the currency notes using an app built using MIT app inventor. The user has to press the shutter button in order to identify the amount of the currency being scanned. The output of the app is a voice command that will help the blind people to identify the amount. This paper makes use of the artificial intelligence tool to classify the image and gives the output. The artificial intelligence uses the image classifier to classify the image.Normally, the image classifier takes the input given the user and classifies the given input and gives the output.Here, the input given is the currency notes of different values so that the image classifier classifies the input and gives the output. This paper uses personal image classifier model to classify or detect the captured image.Here, the captured image in indian rupee note. This personal image classifier consists of four steps.},
  keywords={Presses;Blindness;Detectors;Control systems;Artificial intelligence;Currencies;Image classifier;MIT app inventor;Currency Detector},
  doi={10.1109/ICPECTS56089.2022.10047784},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10486214,
  author={Gowthami, M and Shreya, P and Sripriya, T and Yazhini, B},
  booktitle={2024 2nd International Conference on Computer, Communication and Control (IC4)}, 
  title={Cognitive Vision Companion: An AI-Enhanced Support System for the Visually Impaired}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In a world where a major portion of the populace suffer with daily challenges of blindness, this pioneering project introduces a comprehensive smart support system designed to aid the needs of people who are visually impaired. This advanced system is ingeniously integrated into a pair of headphones, combining various state-of-the-art technologies such as cameras, ultrasonic sensors, and cutting-edge machine learning algorithms. Its primary goals encompass recognizing objects, accurately estimating distances between objects and users, deciphering captured signs and indications, and offering real-time location-based navigation and directions. This multifunctional blind assistance device is designed to empower users by enhancing their spatial awareness and providing them the tools they need to navigate travel securely and on their own. It represents a remarkable leap forward in refining the overall quality of life and mobility for the blind people.},
  keywords={Headphones;Machine learning algorithms;Navigation;Refining;Estimation;Blindness;Cameras;visually impaired;machine learning;object detection;navigation;object distance estimation},
  doi={10.1109/IC457434.2024.10486214},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10266277,
  author={Joshi, Sunita and Gupta, Neha and Mitali and Yadav, Gautam},
  booktitle={2023 3rd International Conference on Pervasive Computing and Social Networking (ICPCSN)}, 
  title={A Machine Learning Approached Model to Identify the Object for Visually Impaired Person}, 
  year={2023},
  volume={},
  number={},
  pages={599-604},
  abstract={According to the World Health Organization (WHO), 253 million individuals worldwide are visually impaired, including 36 million who are blind and 217 million who have moderate to severe vision impairment. The objective of this study is to demonstrate an improved approach through a real-time working model that is used for the prediction of objects around a visually impaired person. The proposed model is based on object detection which aids in several aspects of object prediction such as accident reduction and managing daily routines while protecting themselves from hazards or obstacles. The proposed model has an accuracy of 72%. More datasets will be embedded in the future to conduct large-scale object prediction with greater accuracy.},
  keywords={Social networking (online);Computational modeling;Visual impairment;Object detection;Predictive models;Hazards;Object recognition;Machine Learning;Raspberry pi;Object Detection;Visually Blind Person;Artificial Intelligence;Convolution Neural Network;Image Processing;Deep Learning;Ultrasonic Sensor},
  doi={10.1109/ICPCSN58827.2023.00105},
  ISSN={},
  month={June},}@INPROCEEDINGS{10405085,
  author={Kumar, A. Rohith and Sanjay, K. and Praveen, M.},
  booktitle={2023 2nd International Conference on Automation, Computing and Renewable Systems (ICACRS)}, 
  title={EchoGuide: Empowering the Visually Impaired with IoT-Enabled Smart Stick and Audio Navigation}, 
  year={2023},
  volume={},
  number={},
  pages={1770-1774},
  abstract={Many blind and disabled people nowadays experience great hardship due to the numerous obstacles and the inherent dangers they must confront daily. The AIoT Blind Stick is a groundbreaking assistive device designed to empower visually impaired individuals with enhanced mobility and independence. This innovative solution integrates a Raspberry Pi Zero, high-resolution camera, MPU sensor, ultrasonic sensor, Buzzer, and speaker to create a comprehensive system that addresses the visually impaired community’s unique challenges. The device’s advanced features, including object recognition, obstacle detection, and orientation tracking, contribute to its effectiveness in providing real-time feedback to users. Through a combination of auditory alerts and voice prompts, the AIoT Blind Stick offers a multi-modal feedback system, catering to diverse user preferences. The detailed results and discussions on the device’s performance, highlighting its potential to revolutionize the way visually impaired individuals navigate their environments. The AIoT Blind Stick represents a significant advancement in assistive technology, potentially greatly improving the quality of life for visually impaired individuals worldwide. Further research and refinement are recommended to optimize its functionality across various real-world scenarios.},
  keywords={Navigation;Cameras;Acoustics;Safety;Internet of Things;Artificial intelligence;Testing;Smart stick;Navigation;Blind Assistive Device;Artificial intelligence of things},
  doi={10.1109/ICACRS58579.2023.10405085},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10854873,
  author={Julian, Trejos and Santos, Triana},
  booktitle={2024 Congreso Internacional de Innovación y Tendencias en Ingeniería (CONIITI)}, 
  title={Colour recognition algorithm for an inclusive experience (July 2018)}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={This study focused on developing a colour recognition algorithm in such a way as to privilege accuracy and efficiency without losing information in the images and evaluating colour detection techniques based on RGB space. The proposed approach combines colour detection techniques with the introduction of optimised parameters, comparing it with two traditional colour identification algorithms. A method involving image pre-processing and machine learning algorithms is employed, enhancing the images with matching and filtering techniques to increase the accuracy of colour detection.},
  keywords={Machine learning algorithms;Image recognition;Accuracy;Image color analysis;Filtering;Termination of employment;Robustness;Real-time systems;Blind;colour recognition;Quality of life;Artificial Intelligence},
  doi={10.1109/CONIITI64189.2024.10854873},
  ISSN={2539-4320},
  month={Oct},}@INPROCEEDINGS{10862684,
  author={Shehada, Dina and Turky, Ayad and Rabie, Tamer and Hussain, Abir},
  booktitle={2024 IEEE 12th Conference on Systems, Process & Control (ICSPC)}, 
  title={Enhanced Lightweight Facial Emotion Recognition Systems for Visually Impaired People}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Facial expressions are a vital component of human communication, conveying emotional information that enhances the social experience. However, for individuals with visual impairments, perceiving and interpreting facial expressions can be challenging, hindering their ability to engage fully in social interactions. This paper presents an enhanced facial emotion recognition system specifically designed to aid visually impaired individuals. Two enhanced approaches for facial emotion recognition systems are proposed. The first approach consolidates negative emotions (anger, fear, sadness, disgust) into a single class, while the second approach incorporates diverse training datasets from real-world environments. The negative emotion consolidation approach achieved $\mathbf{9 0. 9 \%}$ and $\mathbf{9 4. 7 \%}$ accuracy on the FER2013 and CK+ datasets, respectively. While, the diverse dataset integration approach achieved $\mathbf{8 4. 9 \%}$ and $\mathbf{7 0. 6 \%}$ accuracy on FER2013 and CK+ datasets, respectively. This is a significant improvement when benchmarked with other approaches available in the literature. The feasibility of the enhanced proposed systems as assistive technology for the visually impaired indicated its ability to be applied in real-world applications.},
  keywords={Training;Emotion recognition;Visualization;Accuracy;Face recognition;Visual impairment;Diversity reception;Robustness;Convolutional neural networks;Compounds;Convolutional Neural Network;Artificial intelligence;Blind;Visually Impaired;Emotion;Facial Detection},
  doi={10.1109/ICSPC63060.2024.10862684},
  ISSN={2769-7916},
  month={Dec},}@INPROCEEDINGS{10794115,
  author={Herbst, Elizabeth and Flannery, Sean and Bharat, Shyam and Sutton, Jonathan},
  booktitle={2024 IEEE Ultrasonics, Ferroelectrics, and Frequency Control Joint Symposium (UFFC-JS)}, 
  title={Automated Assessment of Uterine Coverage in Blind Sweep Obstetric Imaging Protocol}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Blind sweep ultrasound imaging protocols have increased the accessibility of obstetric ultrasound imaging in austere and low-resource environments due to its ease of adoption for midwives and novice users. Image processing algorithms can be used to extract key indicators of fetal and maternal health from blind sweep datasets, without expert interpretation. However, the accuracy of these indicators depends on markers of integrity in the dataset: for example, coverage of the uterine interior. Thus, automated assessment of blind sweep completeness would be beneficial to increase algorithm accuracy and consistency, and user confidence. In this paper, we describe the use of a support vector machine (SVM) model to classify blind sweep acquisitions as complete or incomplete, based on ground truth annotations from expert sonographers. Our results show that this model can be used to classify sweep completeness with an overall accuracy of 0.83, and a positive predictive value (PPV) of 0.80. These findings suggest that our model can adequately flag incomplete blind sweep acquisition. Next steps in this work include investigating the performance of downstream algorithms on the complete and incomplete data cohorts.},
  keywords={Support vector machines;Accuracy;Ultrasonic imaging;Protocols;Imaging;Predictive models;Prediction algorithms;Classification algorithms;Reliability;Obstetrics;ultrasound;obstetrics;blind sweep;machine learning;artificial intelligence},
  doi={10.1109/UFFC-JS60046.2024.10794115},
  ISSN={2375-0448},
  month={Sep.},}@INPROCEEDINGS{10467981,
  author={Gulshan and Chauhan, Vikas},
  booktitle={2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)}, 
  title={Diabetic Retinopathy Detection Using Artificial Intelligence: A Review}, 
  year={2024},
  volume={},
  number={},
  pages={1584-1588},
  abstract={Diabetic Retinopathy (DR) is a severe issue among the diabetic patients, which makes the patient blind and so an early stage detection should be held to prevent this permanent damage to our health. In this research study, the introduction section brief about the common diabetes disease and type of diabetes is highlighted in the next section. The third section illustrates the basic idea and terminology of AI along with how it helps in Diabetic Retinopathy. The comparative study in tabular form for a better understanding of the major work in this area is discussed.},
  keywords={Diabetic retinopathy;Terminology;Reviews;Learning (artificial intelligence);Internet of Things;Data communication;Diseases;Diabetes;Type 1 diabetes;Diabetic Retinopathy(DR);Artificial Intelligence;Deep learning;Machine learning},
  doi={10.1109/IDCIoT59759.2024.10467981},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10045533,
  author={Zhou, Guangtao and Sheng, Zhi},
  booktitle={2022 8th Annual International Conference on Network and Information Systems for Computers (ICNISC)}, 
  title={The Use of AI in the Elderly Care Industry in China}, 
  year={2022},
  volume={},
  number={},
  pages={794-799},
  abstract={The population aging in China has deepened rapidly in recent years. The most critical factor affecting elderly care is the labor problem with the population aging. Lack of labor poses a severe challenge to government governance. The overall labor force is insufficient, rural labor flows to cities, and the elderly become “intellectual blindness” to a certain extent, making the government face difficulties in population policy, Rural Revitalization, and data application. “Artificial intelligence” is closely related to human social life. The emergence of new things such as AI robots, AI terminals, and AI intelligent networks profoundly affects our daily lives. In such a background, intelligent elderly care entered the public view and attracted attention from all walks of life. Applying artificial intelligence to the elderly care industry can alleviate the shortage of elderly service human resources. The Chinese government is expected to encourage the R&D of artificial intelligence in elderly care services and build an artificial intelligence network to make the application of artificial intelligence in the elderly care industry more rapid and sustainable.},
  keywords={Systematics;Smart cities;Sociology;Government;Transportation;Symbols;Aging;elderly care;artificial intelligence (AI);smart services;population aging},
  doi={10.1109/ICNISC57059.2022.00159},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10306645,
  author={Patankar, Nikhil S and Haribhau, Bhushan and Dhorde, Prithviraj Shivaji and Pravin Patil, Harshal and Maind, Rohit Vijay and Deshmukh, Yogesh S},
  booktitle={2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={An Intelligent IoT Based Smart Stick For Visually Impaired Person Using Image Sensing}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Visual impairment is a global concern affecting millions of people worldwide, with a significant proportion classified as blind. The traditional blind stick, while widely used, presents limitations such as skill requirements, costs, and extensive training. However, recent technological advancements have paved the way for innovative solutions to aid visually impaired individuals in navigating their surroundings effectively. This paper explores the development and potential of smart blind sticks, which leverage sensors, cameras, and artificial intelligence algorithms to provide enhanced assistance to users. These devices offer obstacle detection, auditory and tactile feedback, and even GPS navigation, empowering visually impaired individuals and improving their mobility and quality of life. Several research initiatives worldwide are actively contributing to the design and development of user-friendly, affordable smart blind sticks that require minimal training. With the use of an IoT stick, this research hopes to create an image of opportunity, autonomy, and certainty. To swiftly complete their everyday tasks, the proposed smart stick is designed with an obstacle recognition module, a worldwide positioning system (GPS), pit and flight of stairs detection, water detection, and a global system for mobile communication (GSM).},
  keywords={GSM;Training;Visualization;Costs;Navigation;Visual impairment;Assistive technologies;Visual impairment;Smart Blind Stick;assistive technology;Obstacle detection;Artificial Intelligence;mobility;independence;accessibility;inclusive design},
  doi={10.1109/ICCCNT56998.2023.10306645},
  ISSN={2473-7674},
  month={July},}@INPROCEEDINGS{9893624,
  author={Khan, Ibrahim and Nguyen, Thai Van and Dai, Xincheng and Thawonmas, Ruck},
  booktitle={2022 IEEE Conference on Games (CoG)}, 
  title={DareFightingICE Competition: A Fighting Game Sound Design and AI Competition}, 
  year={2022},
  volume={},
  number={},
  pages={478-485},
  abstract={This paper presents a new competition-at the 2022 IEEE Conference on Games (CoG)- called DareFightingICE Competition. The competition has two tracks: a sound design track and an AI track. The game platform for this competition is also called DareFightingICE, a fighting game platform. DareFightingICE is a sound-design-enhanced version of FightingICE, used earlier in a competition at CoG until 2021 to promote artificial intelligence (AI) research in fighting games. In the sound design track, participants compete for the best sound design, given the default sound design of DareFightingICE as a sample, where we define a sound design as a set of sound effects combined with the source code that implements their timing-control algorithm. Participants of the AI track are asked to develop their AI algorithm that controls a character given only sound as the input (blind AI) to fight against their opponent; a sample deep-learning blind AI will be provided by us. Our means to maximize the synergy between the two tracks are also described. This competition serves to come up with effective sound designs for visually impaired players, a group in the gaming community which has been mostly ignored. To the best of our knowledge, DareFightingICE Competition is the first of its kind within and outside of CoG.},
  keywords={Codes;Games;Artificial intelligence;Sound Design Competition;AI Competition;Visually Impaired Players;DareFightingICE;FightingICE;Fighting Game},
  doi={10.1109/CoG51982.2022.9893624},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{9885356,
  author={Jakka, Surya Chaitanya and Sai, Yerragopu Venkata and A, Jesudoss and A, Viji Amutha Mary},
  booktitle={2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC)}, 
  title={Blind Assistance System using Tensor Flow}, 
  year={2022},
  volume={},
  number={},
  pages={1505-1511},
  abstract={E-mail is one of the most well-known types of human communication. Today, there is a multitude of classified and authentic data available. At the same time, approximately 253 million people have visual disabilities. The proposed blind assistance concepts were created to assist visually impaired people worldwide. These visually impaired people experience difficulties in reading an Email communication. The proposed system is divided into two levels based on the SSD algorithm and TensorFlow, which recognizes the objects not only for recognition but also for localization. It also tells you how far the person is from the object. Individuals with visual impairments may face difficulties as innovation advances step by step. This research work has proposed a novel framework by utilizing AI, which makes the framework more straightforward to use specifically for the individuals with visual impedances and to help the society. The main key aspect of the proposed system is identifying or naming the object detected, calculating the accurate distance between the user and objects and the voice over using Audio commands.},
  keywords={Location awareness;Visualization;Technological innovation;Tensors;Communication systems;Face recognition;Computational modeling;Virtual Assistance;Object Detection;Tensor Flow;SSD MobileNet},
  doi={10.1109/ICESC54411.2022.9885356},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10794008,
  author={Kalantari, Leila and Seth, Subhendu and Krishnan, Manikanda and Sutton, Jonathan and Jutras, Melanie and Rao, Anuradha},
  booktitle={2024 IEEE Ultrasonics, Ferroelectrics, and Frequency Control Joint Symposium (UFFC-JS)}, 
  title={Multiple Pregnancy Detection from Ultrasound Blind Sweeps}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Timely detection of high-risk pregnancies is of paramount interest to maternal-fetal health. With advances in AI and the advent of low-cost handheld ultrasound, automated detection of high-risk pregnancies, without the need for a trained sonographer in primary care setting, is a highly impactful yet feasible technology. The major challenge of building detection models for high risk pregnancies, such as multiple pregnancy (MP), fetal demise, placenta previa, is the scarcity of available data due to low-occurring nature of high-risk situations. Often model selection requires data hungry nested cross-validation. To mitigate the problem, we introduce a novel model selection criterion using non-nested cross-validation.We showcase the pipeline we built for developing a MP detection model with both deep learning and a less expressive machine learning (ML) model as components. The pipeline (A) efficiently uses limited multiple pregnancy data collected by blindly sweeping over maternal abdomen (blind to images), (B) can be deployed in a low-compute setting, and (C) is fully interpretable and hence inspectable for any alarming learned pattern.1},
  keywords={Pregnancy;Deep learning;Ultrasonic imaging;Placenta;Pipelines;Buildings;Data models;Acoustics;Frequency control;Abdomen},
  doi={10.1109/UFFC-JS60046.2024.10794008},
  ISSN={2375-0448},
  month={Sep.},}@INPROCEEDINGS{10635657,
  author={Castro-Macías, Francisco M. and Pérez-Bueno, Fernando and Vega, Miguel and Mateos, Javier and Molina, Rafael and Katsaggelos, Aggelos K.},
  booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
  title={Blind Color Deconvolution and Classification of Histological Images Using the Hyperbolic Secant Prior}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In this paper, we present a novel approach to Blind Color De-convolution, a stain separation technique useful for normalizing, augmenting, and automatically diagnosing histological images. To robustly estimate the stain colors and concentrations, we follow the Bayesian framework and introduce a Gaussian prior distribution on the color vectors and a Hyperbolic Secant prior on the concentrations, which is a seldom explored image model.We provide a comprehensive mathematical derivation of our inference procedure, outlining the underlying principles and assumptions. To demonstrate its effectiveness, we conduct two experiments. The first experiment assesses the fidelity of the reconstructed images to the underlying tissue characteristics. The second experiment uses it as a preprocessing step for a multicenter breast cancer classification task. Our results reveal the superior performance of the proposed method, underscoring its potential for advancing histological image processing and AI-assisted diagnosis.},
  keywords={GSM;Deconvolution;Image color analysis;Breast cancer;Vectors;Mathematical models;Data models;Blind Color Deconvolution;Bayesian modeling;histological images},
  doi={10.1109/ISBI56570.2024.10635657},
  ISSN={1945-8452},
  month={May},}@INPROCEEDINGS{10134298,
  author={Meenakshi, J. and Thailambal, G.},
  booktitle={2023 International Conference on Inventive Computation Technologies (ICICT)}, 
  title={Gender and Age Detection Techniques for Blind People using Principal Component Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={572-577},
  abstract={Predicting someone's gender and age-based solely on appearance is challenging for an AI model, as it requires recognizing and understanding complex patterns in images and videos. This kind of prediction is particularly used for blind people. Different kinds of techniques are applied to analyze and predict gender and age classification with the help of images. In the previous methods, accuracy and live prediction have different challenges such as accuracy and prediction rates. This work proposes a hybrid model to predict and analyze gender and age classification. This work consists of Haar Cascade, Histogram of Oriented Gradients, Hessian Filter and Principal Component Analysis. The Haar Cascade algorithm finds the face from the video images with constant speed and time. The Histogram-orientation extracts the features from the images, and the Hessian Filter finds the wrinkles and, based on that, finds the age. Principal Component Analysis is used to visualize and find the patterns from the images. This proposed hybrid work is implemented using 10137 images, and based on that, training and testing are performed. The proposed work is evaluated using recall, f1-score and precision. The proposed work achieved 87.30%. and 86.9% precision for gender classification and age classification respectively. This, compared to the previous work, produced effective results for gender and age detection.},
  keywords={Training;Analytical models;Computational modeling;Predictive models;Filtering algorithms;Feature extraction;Prediction algorithms;Age and Gender Detection;Hybrid Model;Principal Component Analysis},
  doi={10.1109/ICICT57646.2023.10134298},
  ISSN={2767-7788},
  month={April},}@INBOOK{10954456,
  author={Koley, Santanu and Sengupta, Shatadru and Biswas, Bipasha and Datta, Kankana and Jana, Manasi and Mitra, Apratim},
  booktitle={Artificial Intelligence-Enabled Businesses: How to Develop Strategies for Innovation}, 
  title={Applications of Artificial Intelligence and Machine Learning&#x2010;Enabled Businesses}, 
  year={2025},
  volume={},
  number={},
  pages={227-261},
  abstract={Summary <p>Developing intelligent business solutions, efficient in process implementation and effective in customer relationship management, has been well sought after since the advent of computing into businesses. After Artificial Intelligence, with its various branches, became one of the most major subject matters for study in the world, companies across all domains have been actively involved in the pursuit of assembling business processes that have the capability to adapt newer strategies based on dynamic and heavily analytical factors such as customer feedback, market dynamics, geographical forces, logistic issues, social trends, legal parameters &#x2013; to name but a few. Indeed, such adaptability arises only from intelligent reasoning with AI. Machine Learning, a branch of AI, is being used learn in unsupervised environments; Deep Learning in adapting to potential future market changes using technologies based on human neurons; Natural Language Processing is helping machines interact with human clients tirelessly and automatically without the burden of reaction thereby increasing rapport. Robotics, Expert Systems and Fuzzy Logic, find their respective uses in tasks like automatic process implementation, automated disease diagnosis and prophylaxis, and automated handling of uncertainties in logistic processes. Newer and more promising uses of AI in businesses are being evolved regularly.</p> <p>All this remarkable progress was originally aimed at achieving the maximum goals and raising the standards in man's life. However, it is important to bring forth the issues that must be tackled while business strategising, in order to maximise AI's achievements and minimise its nullifying effects. Citing real&#x2010;life and examples, we portray the various ways AI in businesses has improved and enhanced the society in general, yet some universal human values and some universally acknowledged human traits like intuition and instinct are slowly taking an exit route. This chapter shows how blind following of AI is leading to businesses going the &#x201c;safe and trendy way&#x201d; at the cost of genius. The examples try to show how the adverse effects of AI may be kept at bay while making business decisions. It is argued that in a properly designed environment, that is one in which the AI that is used is tuned in to benefit from human genius rather that to oppose it, AI will possibly emerge as the greatest gift of modern technology. It is, on the other hand, quite possible that AI that blunts the human effect and undermines the human effort, will prove to be the greatest technological scourge in human history.</p>},
  keywords={Artificial intelligence;Business;Machine learning;Machine learning algorithms;Deep learning;Prediction algorithms;Unsupervised learning;Supervised learning;Robots;Technological innovation},
  doi={10.1002/9781394234028.ch13},
  ISSN={},
  publisher={Wiley},
  isbn={9781394234004},
  url={https://ieeexplore.ieee.org/document/10954456},}@INPROCEEDINGS{11189127,
  author={Jasper, Surya and Luu, Minh and Pan, Evan and Tyagi, Aakash and Quinn, Michael and Hu, Jiang and Houngninou, David},
  booktitle={2025 ACM/IEEE 7th Symposium on Machine Learning for CAD (MLCAD)}, 
  title={BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={Hardware complexity continues to strain verification resources, motivating the adoption of machine learning (ML) methods to improve debug efficiency. However, ML-assisted debugging critically depends on diverse and scalable bug datasets, which existing manual or automated bug insertion methods fail to reliably produce. We introduce BugGen, a first of its kind, fully autonomous, multi-agent pipeline leveraging Large Language Models (LLMs) to systematically generate, insert, and validate realistic functional bugs in RTL. BugGen partitions modules, selects mutation targets via a closed-loop agentic architecture, and employs iterative refinement and rollback mechanisms to ensure syntactic correctness and functional detectability. Evaluated across five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional accuracy and achieved a throughput of 17.7 validated bugs per hour—over five times faster than typical manual expert insertion. Additionally, BugGen identified 104 previously undetected bugs in OpenTitan regressions, highlighting its utility in exposing verification coverage gaps. Compared against Certitude, BugGen demonstrated over twice the syntactic accuracy, deeper exposure of testbench blind spots, and more functionally meaningful and complex bug scenarios. Furthermore, when these BugGen-Generated datasets were employed to train ML-based failure triage models, we achieved high classification accuracy (88.1%–93.2%) across different IP blocks, confirming the practical utility and realism of generated bugs. BugGen thus provides a scalable solution for generating high-quality bug datasets, significantly enhancing verification efficiency and ML-assisted debugging.},
  keywords={Training;Solid modeling;Accuracy;Large language models;Computer bugs;Pipelines;Manuals;Machine learning;Syntactics;IP networks;design verification;bug insertion;large language model;machine learning},
  doi={10.1109/MLCAD65511.2025.11189127},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10018579,
  author={Cordero-Mendieta, Ma. Isabel and Pinos-Vélez, Eduardo and Buri-Abad, Edison and Coronel-Berrezueta, Roberto},
  booktitle={2022 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)}, 
  title={Support tool for presumptive diagnosis of Glaucoma using fundus image processing and artificial intelligence implementation}, 
  year={2022},
  volume={6},
  number={},
  pages={1-5},
  abstract={Blindness is a global health problem and glaucoma is one of the diseases that are considered of vital importance to treat since it is a neurodegenerative disease that causes irreversible blindness that still has no cure, however, it can be treated if detected early; Most people begin to feel symptoms when this disease is already in an advanced stage, therefore in this work we have developed a tool to support the medical diagnosis through digital image processing for it has been consulted in several databases for the study and classification of retinal images among these images we have healthy eyes, suspected glaucoma and diagnosed with glaucoma. The region of interest, we worked with was the optic disc since this is where the blood vessels are interconnected and it is an important area for analysis. We have made a tool to support the presumptive diagnosis of glaucoma applied several neural systems with different structures obtaining very good results of accuracy and sensitivity, this tool was developed free software so that it has free access to both treating physicians and students in the health area, in addition, it has been made in a very intuitive way for its easy use, in first instance allows the entry of images in JPG, JPEG and PNG color format, additionally the patient's data and the treating physician, the results are displayed in a PDF format document with all the information entered and the respective diagnosis, which makes it easier to keep a proper medical history.},
  keywords={Training;Sensitivity;Biomedical optical imaging;Databases;Computer architecture;Predictive models;Optical imaging;Glaucoma;blindness;visual impairment;optic disc;prediction and detection of glaucoma;neural networks;convolutional networks;presumptive diagnosis},
  doi={10.1109/ROPEC55836.2022.10018579},
  ISSN={2573-0770},
  month={Nov},}
