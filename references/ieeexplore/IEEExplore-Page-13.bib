@INPROCEEDINGS{10776226,
  author={Li, Yuexin and Zhang, Weiyue and Liu, Gaojie and Zhang, Qinjuan and Hu, Menghan},
  booktitle={2024 17th International Convention on Rehabilitation Engineering and Assistive Technology (i-CREATe)}, 
  title={Navigation Route Planning for Visually Impaired People Based on Ultra Wideband Indoor Positioning System}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In recent years, with the rapid development of science and technology, the assistive technology for the visually impaired, especially the navigation technology for the blind, has made significant progress. Traditional Global Positioning System (GPS) navigation has insufficient accuracy in indoor environments, which is difficult to meet the needs of visually impaired people. However, with breakthroughs in sensor technology, computer vision, and artificial intelligence, new navigation solutions have sprung up. Among them, UWB (Ultra Wideband) positioning system greatly improves the accuracy and reliability of navigation, and is the best choice for indoor positioning. Based on the use of UWB positioning system, this study compares the difference between straight and curve walking mode in blind navigation through experiments. The experimental results show that although the straight walking time is shorter, the route deviation is more. Although the curve walking takes a little longer, the visually impaired people have a better walking experience and a stronger sense of direction, which increases the safety of walking. These findings provide valuable optimization directions for blind navigation, that is, straight walking can be preferred on unfamiliar routes to improve efficiency, while curved walking can be adopted after familiarity to improve experience.},
  keywords={Legged locomotion;Visualization;Accuracy;Navigation;Assistive technologies;Planning;Optimization;Indoor positioning systems;Ultra wideband technology;Global Positioning System},
  doi={10.1109/i-CREATe62067.2024.10776226},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{11167288,
  author={Desai, Dhruv and Mehra, Aryan and M, Aruna},
  booktitle={2025 5th International Conference on Intelligent Technologies (CONIT)}, 
  title={Leveraging Multimodal Vision-Language Models for the Visually Impaired}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper presents a new assistive technology that can potentially enhance the autonomy and quality of life of the blind. The solution involves smart glasses equipped with a built-in camera that records real-time visual information and sends it to a mobile phone for processing. Utilizing advanced computer vision and artificial intelligence algorithms, the system offers a rich and accurate description of the user environment, hence enabling enhanced navigation for the blind. In addition, an interactive web portal has been incorporated to promote awareness of the daily life of the blind, as well as advocate for social integration. The study focuses on the development of an affordable, welfare-based assistive device within the reach of marginalized communities, without compromising high accuracy and reliability. Through the facilitation of an enhanced society and lowered technological barriers, the study hopes to improve the social integration and autonomy of people with disabilities. Empirical evidence and user experience highlight the effectiveness and potential influence of the solution in enhancing the quality of life of visually impaired users.},
  keywords={Computer vision;Visualization;Accuracy;Visual impairment;Real-time systems;User experience;Reliability;Artificial intelligence;Portals;Smart glasses;Assistive technology;smart glasses;visually impaired;computer vision;inclusive healthcare;real-time scene description},
  doi={10.1109/CONIT65521.2025.11167288},
  ISSN={},
  month={June},}@INPROCEEDINGS{10441545,
  author={Dubey, Reetika and Gajjar, Ruchi},
  booktitle={2023 9th International Conference on Signal Processing and Communication (ICSC)}, 
  title={Real-Time Image Super-Resolution using Drone through GFPGAN and Nvidia Jetson Nano}, 
  year={2023},
  volume={},
  number={},
  pages={440-444},
  abstract={Blind facial restoration generally uses facial priors, including a reference or a facial geometrical prior, to restore accurate and realistic details. The absence of severely low-quality inputs and the lack of high-quality references, however, significantly restrict its relevance to real-world situations. This work shows a GFPGAN model for blind face restoration that utilizes the rich and varied priors present in a pre-trained face GAN to enhance the quality and perspective of images captured by a drone in real-time. Implementing this model on an edge AI device illustrates the harmonious combination of realism and fidelity in face restoration. This is achieved by evaluating its performance on different images captured from various angles and distances. For processing on edge AI devices, the suggested system uses an Nvidia Jetson Nano, an affordable but powerful single-board computer. We present the results of a feasibility study demonstrating the capacity of GFPGAN to improve the caliber of real-time drone photography.},
  keywords={Superresolution;Transforms;Real-time systems;Nanoscale devices;Image restoration;Faces;Drones;Blind face restoration;Generation Facial Prior;Drone;Nvidia Jetson Nano},
  doi={10.1109/ICSC60394.2023.10441545},
  ISSN={2643-444X},
  month={Dec},}@INPROCEEDINGS{11069837,
  author={Sachin, A and Penukonda, Aryan and Naveen, M and Chitrapur, Pramod Gurunath and Kulkarni, Praveen and M, Chandrakala B},
  booktitle={2025 3rd International Conference on Inventive Computing and Informatics (ICICI)}, 
  title={NAVISIGHT: A Deep Learning and Voice-Assisted System for Intelligent Indoor Navigation of the Visually Impaired}, 
  year={2025},
  volume={},
  number={},
  pages={848-854},
  abstract={Indoor navigation is still a significant issue for the blind, restricting their mobility and independence. White canes and guide dogs are valuable aids but cannot communicate advanced spatial information. Over the last decade, advances in computer vision and AI have enabled more advanced navigation technology. This paper introduces an AI -based room and object recognition system that can facilitate the indoor navigation of blind and partially sighted people. Our method employs deep learning models, in this case, YOLO for object recognition, to categorize room types by identified objects. This research also have depth detection for estimating objects' distances from the user and voice output in real-time, allowing them to navigate better. Voice narration is also incorporated into the system for giving verbal descriptions, alerting users to obstacles and routes around them. This paper also present a well-organized dataset that is specifically designed for indoor scene classification with the best classification accuracy. The solution aims to fill the gap between current navigation assistance and complete autonomous support systems, providing an easy to use and practical device for visually impaired people.},
  keywords={Deep learning;Computer vision;Adaptation models;Indoor navigation;Depth measurement;Computational modeling;Real-time systems;Indoor environment;Object recognition;Artificial intelligence;Assistive technology;computer vision;object detection;room classification;deep learning},
  doi={10.1109/ICICI65870.2025.11069837},
  ISSN={},
  month={June},}@ARTICLE{10742585,
  author={Yang, Hui and Yu, Tiankuo and Liu, Wenxin and Yao, Qiuyan and Meng, Daqing and Vasilakos, Athanasios V. and Cheriet, Mohamed},
  journal={IEEE Communications Magazine}, 
  title={PAINet: An Integrated Passive and Active Intent Network for Digital Twins in Automatic Driving}, 
  year={2025},
  volume={63},
  number={3},
  pages={32-38},
  abstract={In recent years, the rapid development of artificial intelligence (AI) has accelerated the convergence of digital twin technology with the landscape of 6G automatic driving which is poised to exert profound and far-reaching effects on various aspects of human life. For example, it can offer solutions to prevent traffic accidents caused by blind spots, sudden malfunctions, and other factors. However, current digital twins used in automatic driving fail to meet the requirements for effective operation. This is mainly because the single view of vehicles is limited due to visual blind spots for object detection, while the information sensed by the network requires additional communication time, resulting in a lack of real-time updates and accurate labeling. To advance the level of digital twinning, this article proposes an integrated passive and active intent network (PAINet). This pioneering approach for communication amalgamates passive information sensed by vehicles and their behavioral data within vehicular networks. Through the extraction and detection of pertinent data at the intent layer, informed driving decisions are facilitated, particularly in response to unforeseen emergencies. Therefore, PAINet engenders a unified intelligent entity that seamlessly integrates communication, perception, computation, and control, ushering in a revolutionary safety paradigm for 6G automatic driving. We thoroughly constructed a vehicle intent dataset derived from conventional object detection datasets. In the context of our use case scenario, experimental findings strongly confirm the effectiveness of this methodology, revealing an impressive 96 percent precision in vehicle intent parse, along with a significant reduction of over 50 percent in processing delay. This innovative approach shows great potential for making significant strides in tackling the challenges of perception and decision-making in the realm of 6G automatic driving. We have uploaded the code to github.com/YU-spec-arch/PAINET.},
  keywords={Accuracy;Digital twins;Artificial intelligence;6G mobile communication;Visualization;Object detection;Decision making;Sensors;Reinforcement learning;Object recognition},
  doi={10.1109/MCOM.001.2400048},
  ISSN={1558-1896},
  month={March},}@INPROCEEDINGS{10225808,
  author={Al-Absi, Hamada R.H. and Muchori, Gilbert Njihia and Musleh, Saleh and Islam, Mohammad Tariqul and Pai, Anant and Alam, Tanvir},
  booktitle={2023 International Conference on Information Technology (ICIT)}, 
  title={DMEgrader: Android Mobile Application for Diabetic Macular Edema Grading Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={190-195},
  abstract={More than half a billion people worldwide are affected by diabetes, which is a prevalent non-communicable disease that can lead to critical health conditions, including vision loss. Diabetic Macular Edema (DME) is a primary cause of vision impairment and can eventually lead to blindness in diabetic patients. Early detection of DME and proper health management are crucial to controlling the disease. Retinal image-based AI-enabled diabetes diagnosis has gained significant attention as a non-invasive, fast, and reasonably accurate method for diagnosing DME. To make this technology accessible to underserved communities or areas lacking proper clinical facilities, a mobile application-based solution could have a significant impact. In this article, we describe how we transformed our previously published AI-enabled model into an Android-based mobile application, which is part of a two-phase research study. In the first phase, we developed a deep learning-based model that predicts DME grading using retinal images. In the second phase, we built a mobile application DMEgrader to make our model accessible via a mobile device. To the best of our knowledge, this is the first article to demonstrate necessary steps and code snippets to support developers in transforming deep learning models into Android based mobile applications for DME grading prediction.},
  keywords={Deep learning;Codes;Visual impairment;Predictive models;Retina;Mobile handsets;Diabetes;Diabetes;Diabetic Macular Edema;Deep Learning;Qatar Biobank},
  doi={10.1109/ICIT58056.2023.10225808},
  ISSN={2831-3399},
  month={Aug},}@INBOOK{10951729,
  author={Gehlot, Neha and Kuar, Amritpal},
  booktitle={Reshaping Intelligent Business and Industry: Convergence of AI and IoT at the Cutting Edge}, 
  title={Autonomous Vehicles}, 
  year={2024},
  volume={},
  number={},
  pages={459-473},
  abstract={Summary <p>Artificial intelligence is currently changing our world by transforming mobility. An autonomous car is a perfect example of the application convergence of artificial intelligence (AI) and the internet of things (IoT). A self&#x2010;driving car is a smart/intelligent vehicle that can perceive and sense its nearby world through the IoT. The task of creating an efficiently working self&#x2010;sufficient autonomous car is not very easy as it consists of many challenges. We know how difficult the task of driving cars can sometimes be, and teaching the same to a machine is even more difficult. There are lots of challenges which we need to overcome before deploying autonomous cars on public roads. These challenges include technical areas and some other very critical scenarios to be looked at.</p> <p>Many research studies and innovations are happening today in the area of self&#x2010;driving cars (SDCs), which have resulted in some working prototypes of these vehicles, but failure in the efficient working of these prototypes has made deploying them in public problematic. Therefore, it is important to acknowledge and analyze the areas of problems and challenges. In this chapter, we will look at the technical challenges and other critically challenging areas which SDCs might encounter. We also discuss the issue of blind spots and try to find a solution for the same. And lastly, we present the cyberattacks and hacks which are possible on the SDC. In&#x2010;depth analysis is necessary to address any possible loopholes in autonomous SDCs that can compromise their safety and security.</p>},
  keywords={Automobiles;Radar;Laser radar;Autonomous automobiles;Software;Artificial intelligence;Safety;Radar detection;Prototypes;Security},
  doi={10.1002/9781119905202.ch30},
  ISSN={},
  publisher={Wiley},
  isbn={9781119905196},
  url={https://ieeexplore.ieee.org/document/10951729},}@INPROCEEDINGS{11093334,
  author={Kim, Yoonjeon and Ryu, Soohyun and Jung, Yeonsung and Lee, Hyunkoo and Kim, Joowon and Yang, June Yong and Hwang, Jaeryong and Yang, Eunho},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Preserve or Modify? Context-Aware Evaluation for Balancing Preservation and Modification in Text-Guided Image Editing}, 
  year={2025},
  volume={},
  number={},
  pages={23474-23483},
  abstract={The development of vision-language and generative models has significantly advanced text-guided image editing, which seeks the preservation of core elements in the source image while implementing modifications based on the target text. However, existing metrics have a context-blindness problem, indiscriminately applying the same evaluation criteria on completely different pairs of source image and target text, biasing towards either modification or preservation. Directional CLIP similarity, the only metric that considers both source image and target text, is also biased towards modification aspects and attends to irrelevant editing regions of the image. We propose AugCLIP, a context-aware metric that adaptively coordinates preservation and modification aspects, depending on the specific context of a given source image and target text. This is done by deriving the CLIP representation of an ideally edited image, that preserves the source image with necessary modifications to align with target text. More specifically, using a multi-modal large language model, AugCLIP augments the textual descriptions of the source and target, then calculates a modification vector through a hyperplane that separates source and target attributes in CLIP space. Extensive experiments on five benchmark datasets, encompassing a diverse range of editing scenarios, show that AugCLIP aligns remarkably well with human evaluation standards, outperforming existing metrics. The code is available at https://github.com/augclip/augclip_eval.},
  keywords={Measurement;Visualization;Computer vision;Codes;Large language models;Benchmark testing;Vectors;Pattern recognition;Reliability;Standards;image editing;computer vision;multimodal;evaluation},
  doi={10.1109/CVPR52734.2025.02186},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10234986,
  author={Sharma, Gunjan and Anand, Vatsala and Gupta, Sheifali},
  booktitle={2023 World Conference on Communication & Computing (WCONF)}, 
  title={Harnessing the Strength of ResNet50 to Improve the Ocular Disease Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={Among the most prevalent eye conditions, cataract is the leading cause of blindness, impairing vision. A cataract is a condition that means clouding of the lens of the eye. Cataract-related blindness can be mainly prevented with early detection and prompt treatment. Artificial intelligence systems that grade cataracts based on fundus pictures are a practical way to help clinicians detect cataracts more accurately. For early detection of cataracts, Convolutional neural networks, also referred to as CNNs, have been reported to have a great deal of promise in several different domains, including the identification of many eye illnesses. In this research, a deep CNN model based on ResNet50 architecture has been proposed to classify the images into cataract-infected and normal classes. For fulfilling this task Ocular Disease Intelligent Recognition dataset has been chosen. This dataset contains real-time patient reports of both eyes. The model has shown a very good accuracy of 95.63% and 90.37% of validation accuracy while using SGD optimizer. The loss was 0.64 which is nominal and this model has shown very promising results in classifying the images. This model has a very innovative approach in the medical field so it can be used as a tool in the biomedical or healthcare field.},
  keywords={Cataracts;Computational modeling;Biological system modeling;Computer architecture;Blindness;Real-time systems;Convolutional neural networks;Transfer learning;Cataract Classification;ResNet50;Adam Optimizer;Sequential Model},
  doi={10.1109/WCONF58270.2023.10234986},
  ISSN={},
  month={July},}@INPROCEEDINGS{9888634,
  author={Ali, Hind Hadi and Al-Sultan, Ali Yakoob and Al-Saadi, Enas Hamood},
  booktitle={2022 5th International Conference on Engineering Technology and its Applications (IICETA)}, 
  title={Cataract Disease Detection Used Deep Convolution Neural Network}, 
  year={2022},
  volume={},
  number={},
  pages={102-108},
  abstract={Cataract is one of the most frequent eyesight problems that causes vision distortion. Normally, the lens converges light to the retina in most cases and the presence of the cloud structure (cataract) causes obstructing light and not reach the lens that cause in poor visual acuity. Cataracts can develop without causing any symptoms. Cataracts are rarely painful, but they can cause visual loss in the central vision and even blindness. Therefore, Accurate and quick detection of cataracts is the greatest method to avoid painful and costly procedures and, depending on the severity of the condition, to avoid blindness. Artificial intelligence-based cataract detection methods have gained a lot of attention in the scientific community. In this paper, Deep Convolution Neural Network (DCNN) used for detection cataract automatically in fundus images. The loss and activation functions are tuned to train the network with small kernels. Also used Adam optimizer and (Kaggle, ODIR) datasets to train the model. The suggested method beats state-of-the-art cataract detection systems with an average accuracy of 99.91%, precision 99%, sensitivity 99% and f1-score 99% for Kaggle dataset, and have a average accuracy of 100%, precision 100%, sensitivity 100% for ODIR dataset, according to experimental results.},
  keywords={Cataracts;Visualization;Sensitivity;Convolution;Neural networks;Blindness;Retina;cataract disease;Automatic Detection;Retinal fundus Images},
  doi={10.1109/IICETA54559.2022.9888634},
  ISSN={2831-753X},
  month={May},}@INPROCEEDINGS{10708076,
  author={Touati, Mohamed and Nana, Laurent and Benzarti, Faouzi},
  booktitle={2024 10th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
  title={Enhancing Diabetic Retinopathy Classification: A Fusion of ResNet50 with Attention Mechanism}, 
  year={2024},
  volume={},
  number={},
  pages={2929-2934},
  abstract={Diabetic Retinopathy (DR) is a critical health concern that, if diagnosed early, can significantly reduce the risk of blindness among individuals with diabetes. Leveraging the power of artificial intelligence, this paper introduces an innovative approach for DR detection using Deep Learning techniques. Our methodology employs Transfer Learning, adapting a modified ResNet50 model integrated with a self-attention mechanism. This novel strategy not only enhances detection efficiency but also reduces associated costs. The modified model is specifically tailored to address challenges stemming from unbalanced datasets, such as the APTOS 2019 Blindness Detection dataset. By incorporating a self-attention mechanism, the model can better focus on pertinent features, thereby improving classification and detection accuracy. Performance metrics demonstrate the efficacy of our approach, with a training accuracy of 98.24%, a test accuracy of 0.89, and a F1 Score of 0.94.},
  keywords={Training;Deep learning;Diabetic retinopathy;Adaptation models;Accuracy;Attention mechanisms;Transfer learning;Blindness;Security;Residual neural networks;Diabetic Retinopathy;attention mechanism;Resnet50;transfer learning;classification},
  doi={10.1109/CoDIT62066.2024.10708076},
  ISSN={2576-3555},
  month={July},}@INPROCEEDINGS{11135709,
  author={P X, Brijitto and Therese Bijo, Grace and P S, Jerin and M J, Joel and Francis, Ambily and K, Vidyamol},
  booktitle={2025 4th International Conference on Advances in Computing, Communication, Embedded and Secure Systems (ACCESS)}, 
  title={Detection of Diabetic Retinopathy Using Deep Learning and Raspberry PI}, 
  year={2025},
  volume={},
  number={},
  pages={72-77},
  abstract={Diabetic Retinopathy (DR) is a leading cause of preventable blindness globally, affecting individuals with diabetes. Early and accurate diagnosis of DR is critical to prevent vision loss, yet traditional screening methods are labor-intensive and susceptible to subjective variability. This paper presents an automated DR detection system utilizing Convolutional Neural Networks (CNNs) implemented in Python. The proposed method leverages deep learning to analyze retinal fundus images, classifying DR into its various stages. The system incorporates advanced pre-processing techniques, data augmentation, and transfer learning using a pre-trained model to address challenges such as dataset imbalance and variability in image quality. Experimental results demonstrate high accuracy and robustness across diverse datasets, validating the system’s effectiveness in both binary and multi-class DR classification tasks. Furthermore, the model’s lightweight architecture facilitates deployment on edge devices, such as Raspberry Pi. The study underscores the potential of deep learning to revolutionize DR screening, offering scalable, cost-effective solutions to combat blindness globally. This work contributes to bridging the gap between technology and healthcare, highlighting the role of artificial intelligence in enhancing diagnostic capabilities.},
  keywords={Deep learning;Image quality;Diabetic retinopathy;Accuracy;Computational modeling;Medical services;Blindness;Retina;Robustness;Convolutional neural networks;Diabetic Retinopathy (DR);Convolutional Neural Networks (CNNs);Deep Learning;Edge Devices},
  doi={10.1109/ACCESS65134.2025.11135709},
  ISSN={},
  month={June},}@INPROCEEDINGS{10985242,
  author={Ranjith, D. and Sakthivanitha, M.},
  booktitle={2025 International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
  title={A Novel Multi-Modal Deep Learning Framework for Early Detection of Ocular Diseases}, 
  year={2025},
  volume={},
  number={},
  pages={1340-1346},
  abstract={The leading causes of blindness across the globe are cataracts and glaucoma which require early and accurate detection. These conditions demonstrate the need for methods that can be employed on a large scale. In earlier methods of diagnosis mostly consisted of unmeasurable clinical evaluation and reporting which adds to the unreliability of these methods. This paper present a multi modal deep learning architecture with a new approach to combining information from various data sources such as OCT, fundus pictures, visual field exams, and clinical metadata. This multi-modal framework utilizes hybrid fusion approaches that integrate attention, late fusion and early fusion with graph neural networks (GNNs) to exploit feature specific and cross feature modalities. The use of explainable artificial intelligence methods such as Grad-CAM and SHAP increases clinician trust as they are able to offer clear model predictions. Using a dataset with 10000 multi-modal data, the architecture is able to achieve a 95% accuracy on cataract and 92% glaucoma detections, with AUC-ROC scores of 0.97 and 0.94 respectively. The model’s clarity ranked impressively with a cerca 4.5/5 satisfaction score. Other models were not able to compare with the versatility and accuracy of detections brought by the new frameworks. Although this work is an impressive first step towards further diagnosis, improving clinical methods in measuring blindness, and diagnosis cataracts and glaucoma conditions, further testing is required to cover real world challenges. This work mainly developing on a reliable, explainable and generalizable deep learning framework that can be advance the early detection of ocular diseases. By improving the gap in multi-modal data integration and model interpretability, our approach aims to lighten the differences in diagnosis, improve clinical decisions and improve the results of the patient’s treatment.},
  keywords={Glaucoma;Cataracts;Deep learning;Visualization;Accuracy;Data integration;Metadata;Graph neural networks;Testing;Diseases;Cataracts;Glaucoma;Multi-modal Deep Learning;Optical Coherence Tomography (OCT);Fundus Images;Graph Neural Networks (GNNs);Explainable AI;SHAP;Early Diagnosis},
  doi={10.1109/ICICCS65191.2025.10985242},
  ISSN={},
  month={March},}@INPROCEEDINGS{9716384,
  author={Josh, F.T. and Joseph, J.Jency and Morapakula, Sai Nandan and R., Meenal and Michael, Prawin Angel},
  booktitle={2022 4th International Conference on Smart Systems and Inventive Technology (ICSSIT)}, 
  title={Handy Reader - A Shopping Guide To Visionless Using RaspberryPi}, 
  year={2022},
  volume={},
  number={},
  pages={715-719},
  abstract={We all know how difficult it is to read, understand and interpret things in this world without eyes. Even though there are many new and emerging technologies trying to help them out such as assisted vision smart glasses, AI glasses, Finger reader etc, they are quite expensive and unaffordable to many. In the same way, the most famous technology for blind - Braille Technology has got its own disadvantages like the need to memorize many shortcuts, etc. With the help of strong software and hardware tools, in this paper we present a small prototype model using RFID tags in order to recognize the books a blind person takes in hand without the help of any other person. We have used Raspberry Pi and Python in developing the model.},
  keywords={Headphones;Prototypes;Blindness;Programming;Software;Libraries;Passive RFID tags;RFID Reader;RFID Tags;Raspberry pi;Python;ARMv7;Voice;Espeak;Conversion;SD Card},
  doi={10.1109/ICSSIT53264.2022.9716384},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11009560,
  author={Cheng, Ming and Gao, Lijie and Wang, Xiaofen},
  booktitle={2025 7th International Conference on Software Engineering and Computer Science (CSECS)}, 
  title={Integrated Sensory Platform: Driving Information Sharing in Multi-Level Autonomous Driving Mixed Traffic Mode}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={This article presents an innovative integrated solution called the Integrated Sensory Platform which combines driver and passenger behavior monitoring, vehicle blind spot detection, and V2X communication technology, all aimed at reducing traffic accidents in mixed-traffic scenarios involving multiple levels of automated driving. In this approach, deep learning visual AI technology is employed to identify the types of behaviors of occupants within the cabin, as well as the types of targets detected in blind spots. A three-branch algorithm framework is proposed, which enhances posture and temporal feature extraction, reduces the model's dependence on detailed features, and strengthens its ability to resist occlusion. Additionally, an expanded group channel reordering mechanism is designed to optimize the classification and recognition performance of the posture and temporal feature branches. The identified information is then broadcasted to surrounding vehicles via V2X communication, enabling deep information sharing among multiple vehicles in a spatial traffic environment. This information not only provides decision-making data for high-level automated driving vehicles but also offers operational guidance for fully human-driven vehicles, achieving the goal of collaborative decision-making among vehicles of varying automation levels and improving road traffic safety in complex mixed-traffic conditions.},
  keywords={Visualization;Decision making;Information sharing;Feature extraction;Road traffic;Safety;Vehicle-to-everything;Monitoring;Vehicles;Software engineering;V2X;Behavior monitoring;Blind spot monitoring;Machine vision;Mixed traffic},
  doi={10.1109/CSECS64665.2025.11009560},
  ISSN={},
  month={March},}@INPROCEEDINGS{10670827,
  author={Belgaum, Mohammad Riyaz and Pothureddy, Priyanka and Patnam, Sai Suraksha and Gadda, Parimala Deepthi},
  booktitle={2024 Third International Conference on Smart Technologies and Systems for Next Generation Computing (ICSTSN)}, 
  title={A Novel Framework for Preserving Patient Health Record in IoT Healthcare}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={As artificial intelligence, the Internet of Things (IoT), and next-generation mobile communication continue to evolve at a rapid pace, an increasingly popular application called the Internet of Medical Things (IoT) has arisen to provide ease and practicality in healthcare. Nonetheless, privacy preservation issues are brought up by accessing patient medical information inside the IoT framework, which calls for the creation of safe and private-preserving solutions. To address these issues, we provide in this study an IoT-Framework for preserving patient health record (FFPPHR) based on ELGamal blind signatures. We provide an enhanced privacy protection medical record searching technique (FFPPHR) based on the elliptic curve discrete logarithm problem (ECDLP) and point out security vulnerabilities in the current FFPPHR. Our security research shows that by utilizing the hard problem assumption of ECDLP, the FPMR guarantees identity privacy and accuracy. Furthermore, the use of Blind ECDSA Signatures improves data security. Performance assessments confirm the FFPPHR efficiency, and theoretical findings agree with simulations. The suggested FFPPHR approach yields lower time costs when compared to state-of-the-art techniques, which makes it a potential option for safe and effective medical record searches in IoT contexts.},
  keywords={Privacy;Costs;Elliptic curves;Data security;Internet of Medical Things;Search problems;Mobile communication;elliptic curve discrete logarithm problem (ECDLP);blind ECDSA signature;performance assessment;Internet of Things (IoT);privacy preservation;and medical record search},
  doi={10.1109/ICSTSN61422.2024.10670827},
  ISSN={},
  month={July},}@INPROCEEDINGS{11076528,
  author={Jancy, S. and Mary, A. Viji Amutha and Selvan, Mercy Paul and Grace, L. K. Joshila},
  booktitle={2025 7th International Conference on Intelligent Sustainable Systems (ICISS)}, 
  title={An Intelligent Chatbot for the Disabled People}, 
  year={2025},
  volume={},
  number={},
  pages={922-926},
  abstract={The creation of an intelligent chat bot with the express purpose of serving the needs of the blind, deaf, and dumb communities is presented in this study. For consumers that have trouble communicating through traditional channels, the chat bot provides an interactive and easily accessible communication tool. Through the use of artificial intelligence and natural language processing methods, the chat bot can communicate with people who are deaf or hard of hearing by using text-to-speech and speech recognition features. Additionally, it ensures accessibility for visually impaired users by combining an easy-to-use interface with a visually impaired-friendly design that includes options for large fonts and high contrast colors. Through time, the chat bot will learn from each user's preferences and demands in order to deliver a tailored and flexible experience. It can participate in meaningful discussions and offer pertinent information, help, and support for a range of daily tasks through intuitive and context-aware responses. The target community provided the system with positive feedback during user testing and evaluation, indicating that it has the potential to improve the quality of life for those who are blind, deaf, or visually impaired. In summary, our intelligent chat bot aims to close the communication gap that these communities experience by providing a customized solution that is inclusive, flexible, and approachable.},
  keywords={Visualization;Sign language;Speech recognition;Oral communication;Chatbots;People with disabilities;Information age;User experience;Text to speech;Testing;Intelligent Chatbot;natural language processing;speech recognition;text-to-speech},
  doi={10.1109/ICISS63372.2025.11076528},
  ISSN={},
  month={March},}@INPROCEEDINGS{11026142,
  author={A, Srinivasan and B, Meenakshi and R, Swathiha R},
  booktitle={2024 International Conference on Smart Technologies for Sustainable Development Goals (ICSTSDG)}, 
  title={Voice-Activated Email System: Enhancing Accessibility with Speech-to-Text Integration}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Email is a widely used tool for communication in today's society, particularly in professional settings where it is crucial. However, text reading and keyboard input are required for traditional email systems, which some users could find challenging. The voice-based email system proposed in this project allows voice commands to be used for email composition, sending, and receiving. The solution enables users to connect with their email accounts using simple voice commands thanks to speech-to-text technology. Furthermore, an embedded AI chatbot function improves user interaction by offering recommendations, helping with email management, and responding to questions about email content. For those who want or need hands-free communication, this innovative solution improves usability by eliminating the need for typing or visual focus. In addition, the system is made especially user-friendly for visually challenged users, allowing them to send and receive emails efficiently and independently. This solution provides a seamless email experience by utilizing AI chatbots and a Python-based architecture, enhancing accessibility and efficiency for all users–including the blind.},
  keywords={Training;Visualization;Keyboards;Chatbots;User experience;Electronic mail;Artificial intelligence;Usability;Sustainable development;Speech to text;Voice-based email;Speech-to-text;Accessibility;Visually impaired;Python;Hands-free communication;Email automation},
  doi={10.1109/ICSTSDG61998.2024.11026142},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10920618,
  author={Tong, Bohao and Hou, Xiaohan and Guo, Yali and Wang, Shuang and Bai, Jie and Chen, Yu},
  booktitle={2024 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)}, 
  title={Numerical Calculation and Experimental Reserch of Ultraviolet Photoelectron Yield Spectrum of AlGaN Photocathode Material}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={AlGaN photocathode can realize the “solar-blind” ultraviolet detection function with a wavelength response range below 280 nm due to its adjustable Al component. Based on the classical three-step emission theory and combined with the solid band theory, the theoretical models of ultraviolet photoelectron yield of reflective and transmissive photocathodes were established and the expressions were derived. The ultraviolet photoelectron yield spectra of AlGaN photocathode in two working modes were calculated and plotted and compared with the experimental results in the literature. The influence of some factors on the ultraviolet photoelectron yield of AlGaN photocathode was analyzed. At the same time, the ultraviolet photoelectron yield spectrum experimental test of single-layer AI0.27Ga0.73N crystal material was completed by simulating the emission layer of reflective AlGaN photocathode using ultraviolet photoelectron spectrum analyzer. The test results show that in the wavelength range of 220~310 nm, the ultraviolet photoelectron yield of single-layer AI0.27Ga0.73N crystal material is about 4%-6%, and it begins to drop sharply after the incident light wavelength reaches 310 nm.},
  keywords={Solid modeling;Data analysis;Spontaneous emission;Crystals;Solids;Wide band gap semiconductors;Photoelectricity;Photocathodes;Aluminum gallium nitride;Electrons;photocathode;AlGaN;photoelectron emission;photoelectron yield spectrum},
  doi={10.1109/ICSMD64214.2024.10920618},
  ISSN={},
  month={Oct},}@INBOOK{10952133,
  author={},
  booktitle={Uncertainty and Artificial Intelligence: Additive Manufacturing, Vibratory Control, Agro-composite, Mechatronics}, 
  title={New Intelligence Method for Machine Tool Energy Consumption Estimation}, 
  year={2023},
  volume={},
  number={},
  pages={1-17},
  abstract={Summary <p>This chapter applies an artificial intelligence method, called independent component analysis (ICA), to evaluate the power consumed during a peripheral milling operation. The artificial intelligence method ICA is applied to estimate the variable tangential and cutting efforts. ICA is an important method for the blind separation of sources. Kurtosis is a tool that finds non&#x2010;Gaussian components and their place in the field of frequency. The vibratory responses of the cutting system are obtained by solving the motion equation using the method of finite elements. The equation of motion is solved using the Newmark method coupled with the Newton&#x2013;Raphson method in order to estimate the temporal tooltip displacements. The method relies only on captors to identify the vibratory responses such as displacement or acceleration, and it then estimates the increased efforts of the machine tool.</p>},
  keywords={Machine tools;Energy consumption;Vehicle dynamics;Estimation;Artificial intelligence;Uncertainty;Mathematical models;Manufacturing;Inverse problems;Feeds},
  doi={10.1002/9781394255351.ch1},
  ISSN={},
  publisher={Wiley},
  isbn={9781394255337},
  url={https://ieeexplore.ieee.org/document/10952133},}@INPROCEEDINGS{10346704,
  author={Grover, Karanbir Singh and Kapoor, Nitika},
  booktitle={2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)}, 
  title={Detection of Glaucoma and Diabetic Retinopathy Using Fundus Images and Deep Learning}, 
  year={2023},
  volume={},
  number={},
  pages={407-412},
  abstract={Glaucoma and Diabetic Retinopathy are serious diseases that can lead to vision loss and blindness. With a growing number of patients affected by these diseases, it is crucial to find reliable and efficient methods for diagnosis and treatment. In this study, we aimed to compare the performance of five different models for diagnosing Glaucoma and Diabetic Retinopathy from images using AI. Specifically, we used the CNN-based models LeNet and AlexNet, as well as the inception and ResNet models. After training and testing each model, we compared their performance on accuracy and loss parameters. Our results showed that the AlexNet model outperformed the other models with an accuracy of 97.96% and Lenet performed the most consistent with all different parameters.},
  keywords={Glaucoma;Training;Analytical models;Diabetic retinopathy;Reliability;Task analysis;Optimization;Deep Learning;Fundus Images;Diabetic Retinopathy;Glaucoma},
  doi={10.1109/ICCCMLA58983.2023.10346704},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10968954,
  author={Mishra, Ashish and Choubey, Niyati and Jumde, Parnavi and Samiya, Ananya and Garg, Abhilasha and Kale, Ashlesha},
  booktitle={2025 IEEE 14th International Conference on Communication Systems and Network Technologies (CSNT)}, 
  title={Early Eye Disease Detection Using Machine Learning}, 
  year={2025},
  volume={},
  number={},
  pages={134-139},
  abstract={Early detection of eye diseases such as diabetic retinopathy, glaucoma, and cataracts is crucial in preventing severe vision impairment and blindness. This study explores the application of machine learning (ML) techniques, particularly Convolutional Neural Networks (CNNs), for the automated diagnosis of eye diseases. A deep learning model was trained on a labeled dataset of retinal images, utilizing CNNs for feature extraction and classification. The Tkinter-based Graphical User Interface (GUI) facilitates easy user interaction, enabling real-time diagnosis from uploaded eye images. Experimental results demonstrate a high accuracy rate of 91%, outperforming traditional diagnostic techniques. Additionally, explainable AI (XAI) methodologies were incorporated to enhance model interpretability, improving trust and usability in clinical settings. In the proposed methodology the findings highlight the potential of ML in ophthalmology, paving the way for cost-effective and scalable screening solutions.},
  keywords={Glaucoma;Cataracts;Visual impairment;Retina;Software;Real-time systems;Ophthalmology;Convolutional neural networks;Usability;Graphical user interfaces;Machine Learning;CNN;Ophthalmology;Neural Networks;Uveitis;Cataract;Glaucoma},
  doi={10.1109/CSNT64827.2025.10968954},
  ISSN={2473-5655},
  month={March},}@INPROCEEDINGS{10933190,
  author={Lanjewar, Rutuja Nitin and Pacharaney, Utkarsha},
  booktitle={2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)}, 
  title={Retinal Disease Classification: Binary and Multiclass Perspectives}, 
  year={2025},
  volume={},
  number={},
  pages={1672-1677},
  abstract={Retinal diseases, among which diabetic retinopathy, age-related macular degeneration, and glaucoma account for the greatest number of blindness cases worldwide, affect more than 338 million people. It is essential that early detection leads to timely intervention to preserve vision, but it is limited due to the ever-increasing demand for ophthalmic care and a shortage of specialists. Machine learning-driven systems, especially Convolutional Neural Networks (CNNs), have shown great promise in overcoming these challenges, achieving high accuracy in medical imaging tasks. This paper explores a unified framework for both multiclass and binary classification of retinal diseases, addressing issues like class imbalance and inter-class similarity while leveraging CNNs for robust feature extraction. Techniques like Grad-CAM also increase model interpretability, allowing clinicians to understand and trust AI-driven diagnostic processes.},
  keywords={Macular degeneration;Glaucoma;Sentiment analysis;Medical services;Retina;Feature extraction;Data models;Planning;Convolutional neural networks;Diseases;Retinal disease;Convolutional Neural Networks;Classification;multiclass classification;binary classification},
  doi={10.1109/ICSADL65848.2025.10933190},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{11167512,
  author={B, Priyadharsini and Femi P, Sharon and A, Kala and E, Stephena},
  booktitle={2025 3rd International Conference on Sustainable Computing and Data Communication Systems (ICSCDS)}, 
  title={A Review on the Analysis of Glaucoma and its Automated Detection Methods from Fundus Images using Machine Learning Techniques}, 
  year={2025},
  volume={},
  number={},
  pages={231-237},
  abstract={Glaucoma a long-term degenerative disease affecting the optic nerve, which leads to permanent vision impairment if not identified and treated early. As a major contributor to global blindness, it often progresses without symptoms until advanced stages. Conventional diagnostic techniques such as assessing Intraocular pressure (IOP), evaluating the optic disc, and performing visual field tests demand expert interpretation and are subject to variability. Consequently, the detection of Glaucoma using machine learning (ML) methods has become a significant research focus in the field of ophthalmology. This paper presents a survey of current ML-based approaches for detecting glaucoma using Fundus imagery. It covers Preprocessing steps, Feature extraction strategies, and Classification algorithms, offering a comparative analysis of their performance in clinical applications. Notable datasets such as RIM-ONE, ORIGA, DRISHTI-GS1, and REFUGE are discussed inoder to support reproducible research. The paper also addresses key challenges, including class imbalance, diverse image quality, and the transparency of AI models. Furthermore, emerging trends like Transfer learning, Federated learning, and Multi-modal systems are examined for their potential to enhance early detection and assist clinical decision-making.},
  keywords={Glaucoma;Integrated optics;Surveys;Visualization;Reviews;Visual impairment;Transfer learning;Optical imaging;Feature extraction;Ophthalmology;Glaucoma;Fundus Images;Machine Learning;Deep Learning;IOP;Feature Extraction;RIM-ONE;ORIGA;DRISHTI-GS1 and REFUGE},
  doi={10.1109/ICSCDS65426.2025.11167512},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10910807,
  author={R.T, Charulatha. and Nirmaladevi, G. and G, Vasumathi and Hemajothi, S. and Suresh, K. and C, Karthikeyan.},
  booktitle={2024 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)}, 
  title={A Robust Methodology to Detect Glaucoma Disease using Modified Deep Learning Principles with Image Processing Logics}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Glaucoma is a leading cause of irreversible blindness, requiring early and accurate diagnosis to prevent vision loss. This study proposes a robust methodology for detecting glaucoma using a hybrid approach that integrates deep learning (Autoencoder) with a machine learning classifier (Multi-Layer Perceptron). The methodology utilizes image processing techniques, including noise reduction and data augmentation, to enhance the quality of retinal fundus images. Feature extraction was performed using Convolutional Neural Networks (CNNs) and Gabor filters, followed by dimensionality reduction through Principal Component Analysis (PCA) and wavelet decomposition. The model was tested on multiple publicly available datasets, including RIGA, HRF, ACRIMA, and Kaggle. The hybrid model achieved an accuracy of 97.1% on the RIGA dataset, 95.6% on the HRF dataset, and 96.3% on the ACRIMA dataset. The model outperformed traditional classifiers such as Support Vector Machine (SVM) and Random Forest, which achieved accuracies of 92.3% and 94.5% on the RIGA dataset, respectively. The study demonstrates that combining deep learning with machine learning classifiers, along with effective preprocessing, significantly improves glaucoma detection accuracy. Furthermore, explainable AI techniques were employed to enhance model interpretability, ensuring that clinical practitioners can trust and understand the model's decisions.},
  keywords={Glaucoma;Deep learning;Support vector machines;Accuracy;Explainable AI;Computational modeling;Autoencoders;Wavelet analysis;Convolutional neural networks;Principal component analysis;Glaucoma Detection;Deep Learning;Autoencoder;Machine Learning;Image Processing;CNN;PCA;explainable AI},
  doi={10.1109/ICSES63760.2024.10910807},
  ISSN={},
  month={Dec},}
