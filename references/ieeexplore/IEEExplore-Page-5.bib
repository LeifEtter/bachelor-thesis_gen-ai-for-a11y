@INPROCEEDINGS{9826949,
  author={Farsi, Arman and Casaccia, Sara and Revel, Gian Marco},
  booktitle={2022 IEEE International Workshop on Metrology for Living Environment (MetroLivEn)}, 
  title={Assessment of normal and abnormal behaviour of people with dementia in living environment through non-invasive sensors and unsupervised AI}, 
  year={2022},
  volume={},
  number={},
  pages={71-75},
  abstract={The objective of this work is to define a measurement protocol to compute and distinguish abnormal from normal behavior of older people with early to middle stage dementia living alone at home using training artificial intelligence (AI) algorithms (in specific K-means, Agglomerative and Spectral Clustering Algorithms). Unlabeled activities of daily living (ADLs) databases were acquired from an AI-based sensor network composed of three Passive Infrared (PIR) motion sensors and two door sensors which are installed in voluntary participants’ houses in Italy, Netherlands, Switzerland and Norway. Our blind approach proposes an unsupervised learning-based solution, representing real-world use case where it is not possible to provide accurate labeling and annotation of the sensor data. Results indicate two out of three algorithms, which applied for 32 participants during 75 days of recording, are valid to create clustering groups for participants with acceptable Silhouette coefficient of 0.77 and 0.65 for K-means and Agglomerative clustering, respectively, and 0.13 for Spectral clustering method that makes this algorithm less reliable respect to others.},
  keywords={Training;Protocols;Clustering algorithms;Metrology;Motion detection;Recording;Reliability;AI-based sensors network;ADL;remote monitoring},
  doi={10.1109/MetroLivEnv54405.2022.9826949},
  ISSN={},
  month={May},}@INPROCEEDINGS{11019242,
  author={Bala, V Vijay and IE, Yogunth and Saravana, C Yugesh and Sivasakthi, T},
  booktitle={2025 International Conference on Computing and Communication Technologies (ICCCT)}, 
  title={AI-Driven Braille Display for Visually Impaired Individuals}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Online education has become an essential part of modern learning, yet visually impaired and blind students face significant barriers in accessing digital educational content. Traditional Braille books are expensive and cannot keep up with newly published materials, while existing electronic Braille solutions lack internet connectivity and real-time interaction. This paper presents an AI-powered electronic Braille system designed to enhance accessibility for visually impaired students. Our system integrates a refreshable Braille display with an AI chat bot, allowing students to read and interact with digital content through voice commands. Educators can upload study materials to a web-based platform, allowing real-time engagement between students and teachers. By combining artificial intelligence, IoT-based Braille displays, and cloud-based storage, this solution bridges the gap between visually impaired learners and modern web-based education..},
  keywords={Braille;Accuracy;Computational modeling;Scalability;Education;Retrieval augmented generation;Speech recognition;Oral communication;Chatbots;Real-time systems;AI Based Electronic Braille;Speech Recognition;Visually Impaired student;Retrieval-augmented generation Model},
  doi={10.1109/ICCCT63501.2025.11019242},
  ISSN={2995-3197},
  month={April},}@INPROCEEDINGS{10689820,
  author={Kamble, K.P. and Sahu, Ved and Deosthali, Gargi and Shende, Shambhavi and Bariya, Karan},
  booktitle={2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)}, 
  title={Voice Assisted Smart Blind Stick}, 
  year={2024},
  volume={},
  number={},
  pages={1565-1570},
  abstract={The concept of Smart Blind Stick develops an innovative electronic aid designed for blind individual, particularly in the detection of obstacles during navigation. The proposed system uses an Arduino UNO platform to provide artificial vision and real-time object detection abilities, mainly focused on sound-based assistance to enhance the independence of blind persons. The main idea behind this research is to challenge the level of flexibility that sighted people experience. The focus was given for creating a smart assistive device which includes object detection, artificial intelligence and integration of GPS module through Arduino Uno. The system integrates ultrasonic sensors, with auditory feedback providing information both static and dynamic, thereby allowing visually impaired persons to navigate independently. The objective is to provide a cost-effective and efficient navigation and obstacle detection device that presents artificial vision, helping independent mobility for the visually impaired in various environmental situations.},
  keywords={Navigation;Object detection;People with disabilities;Acoustics;Sensor systems;Real-time systems;Text to speech;Safety;Security;Intelligent sensors;Smart Blind Stick;Ultrasonic Sensor;Raspberry Pi;Buzzer;Google Text-to-Speech},
  doi={10.1109/ICESC60852.2024.10689820},
  ISSN={2996-5357},
  month={Aug},}@INPROCEEDINGS{10570607,
  author={Azzino, Tommy and Mezzavilla, Marco and Rangan, Sundeep and Wang, Yao and Rizzo, John-Ross},
  booktitle={2024 IEEE Wireless Communications and Networking Conference (WCNC)}, 
  title={5G Edge Vision: Wearable Assistive Technology for People with Blindness and Low Vision}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In an increasingly visual world, people with blindness and low vision (pBLV) face substantial challenges in navigating their surroundings and interpreting visual information. From our previous work, VIS4ION is a smart wearable that helps pBLV in their daily challenges. It enables multiple microservices based on artificial intelligence (AI), such as visual scene processing, navigation, and vision-language inference. These microservices require powerful computational resources and, in some cases, stringent inference times, hence the need to offload computation to edge servers. This paper introduces a novel video streaming platform that improves the capabilities of VIS4ION by providing real-time support of the microservices at the network edge. When video is offloaded wirelessly to the edge, the time-varying nature of the wireless network requires adaptation strategies for a seamless video service. We demonstrate the performance of our adaptive real-time video streaming platform through experimentation with an open-source 5G deployment based on open air interface (OAI). The experiments demonstrate the ability to provide microservices robustly in time-varying network conditions.},
  keywords={Visualization;Accuracy;5G mobile communication;Navigation;Microservice architectures;Blindness;Streaming media;5G;testbed;AI;assistive technology;e-health;wearable;edge computing;video streaming},
  doi={10.1109/WCNC57260.2024.10570607},
  ISSN={1558-2612},
  month={April},}@INPROCEEDINGS{9985659,
  author={Abhishek, S and Sathish, Harsha and K, Arvind Kumar and T, Anjali},
  booktitle={2022 4th International Conference on Inventive Research in Computing Applications (ICIRCA)}, 
  title={Aiding the Visually Impaired using Artificial Intelligence and Speech Recognition Technology}, 
  year={2022},
  volume={},
  number={},
  pages={1356-1362},
  abstract={Thousands of blind or visually handicapped people have valuable talents but are having trouble obtaining a job, or at least work that is comparable with their ability. Customer support and repair customer service, writers, product testing inspectors, customer service representatives, and curriculum professionals are just a few jobs where blind people have excelled. This paper gives an insight on the implementation of assistive technology software that employs voice recognition technologies to aid the visually handicapped in accessing computer applications and the internet. The software can also detect and rectify spelling and grammatical errors in content depending on the context of the entire phrase using an artificial intelligence-powered writing assistant called GingerIt.},
  keywords={Privacy;Customer services;Visual impairment;Speech recognition;Blindness;Writing;Maintenance engineering;Speech recognition;Artificial Intelligence Chatbot;GingerIt;Speech-to-Text;Voice Assistant},
  doi={10.1109/ICIRCA54612.2022.9985659},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10932061,
  author={A, Boobal and J, Loyola Jasmine and Charan Kesava Reddy, Chennama and Reddy, Challa Akshay and Bala Venkata Sai Rohith, Chaluvadi},
  booktitle={2024 International Conference on Communication, Control, and Intelligent Systems (CCIS)}, 
  title={Real-Time Sign Language and Audio Conversion Using AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The appearance of audio conversion and the appearance of gesture language have greatly advanced communication technologies, especially for hearing impairment and blind. These double -purpose technologies promote communication between these groups, provide opportunities for expression and speech behind. The technology uses advanced artificial intelligence algorithms to provide real-time speech and sign language translation to improve accessibility and inclusivity. It is essential to protect data privacy, maintain the accuracy of AI models to avoid misinterpretations, and address ergonomic issues associated with long-term use. Technology converts audio entry into a gestic signal that can be seen by avatars using computer vision and natural language processing. There is a depth learning model that improves voice recognition and the acquisition of signs, and is fully adapted to many languages and sign dialects. Addressing such health and safety issues is essential to advancing the communication and optimization of this advanced technology.},
  keywords={Sign language;Computer vision;Translation;Communication systems;Computational modeling;Auditory system;Assistive technologies;Real-time systems;Natural language processing;Convolutional neural networks;Sign language translation;Real-time gesture recognition;Speech-to-sign language;Sign language to audio;Deep learning;Convolutional Neural Networks (CNNs);Natural language processing (NLP);Audio-to-sign language conversion;Machine learning;Gesture recognition;Computer vision in sign language;Django;MySQL;Indian sign language (ISL)},
  doi={10.1109/CCIS63231.2024.10932061},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10931891,
  author={A, Boobal and J, Loyola Jasmine and Reddy, Chennama Charan Kesava and Reddy, Challa Akshay and Sai Rohith, Chaluvadi Bala Venkata},
  booktitle={2024 International Conference on Communication, Control, and Intelligent Systems (CCIS)}, 
  title={Real-Time Sign Language and Audio Conversion Using AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The appearance of audio conversion and the appearance of gesture language have greatly advanced communication technologies, especially for hearing impairment and blind. These double -purpose technologies promote communication between these groups, provide opportunities for expression and speech behind. The technology uses advanced artificial intelligence algorithms to provide real-time speech and sign language translation to improve accessibility and inclusivity. It is essential to protect data privacy, maintain the accuracy of AI models to avoid misinterpretations, and address ergonomic issues associated with long-term use. Technology converts audio entry into a gestic signal that can be seen by avatars using computer vision and natural language processing. There is a depth learning model that improves voice recognition and the acquisition of signs, and is fully adapted to many languages and sign dialects. Addressing such health and safety issues is essential to advancing the communication and optimization of this advanced technology.},
  keywords={Sign language;Computer vision;Translation;Communication systems;Computational modeling;Auditory system;Assistive technologies;Real-time systems;Natural language processing;Convolutional neural networks;Sign language translation;Real-time gesture recognition;Speech-to-sign language;Sign language to audio;Deep learning;Convolutional Neural Networks (CNNs);Natural language processing (NLP);Audio-to-sign language conversion;Machine learning;Gesture recognition;Computer vision in sign language;Django;MySQL;Indian sign language (ISL)},
  doi={10.1109/CCIS63231.2024.10931891},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10531451,
  author={Raghunathan, T and Mishra, Aditya and Mahur, Akhilesh Kumar and B, Balaji},
  booktitle={2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)}, 
  title={Multi-Modal AI/ML Integration for Precision Glaucoma Detection: A Comprehensive Analysis using Optical Coherence Tomography, Fundus Imaging, RNFL, and Vessel Density}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Glaucoma, a prevailing cause of irreversible blindness, necessitates early and accurate detection for optimal management. This paper introduces a novel approach integrating AI and ML to elevate glaucoma detection precision. Leveraging SD-OCT, fundus photography, RNFL, and vessel density, the proposed system embodies a holistic analysis. Preliminary results underscore the efficacy of this multimodal system, positioning it as a frontrunner in revolutionizing ocular health diagnostics.The conventional methods for glaucoma diagnosis are often limited by their reliance on single-modal imaging, prompting a paradigm shift towards comprehensive multi-modal integration. By fusing the capabilities of SDOCT, fundus photography, RNFL, and vessel density, our approach addresses the shortcomings of individual modalities, providing a more nuanced understanding of ocular health. The innovative combination of these imaging techniques, coupled with advanced AI and ML algorithms, establishes a robust foundation for glaucoma detection, heralding a new era in ocular health diagnostics. The suggested multi-modal system not only demonstrates superior accuracy but also showcases the potential for early detection and intervention. The intricate interplay of algorithms, illustrated in detailed block diagrams, underscores the synergy achieved by integrating diverse modalities. This paper presents a pioneering step towards a more comprehensive and accurate approach to glaucoma detection, opening avenues for further advancements at the intersection of AI, ML, and ophthalmic imaging.},
  keywords={Glaucoma;Photography;Technological innovation;Optical coherence tomography;Sociology;Refining;Medical services;Machine Learning;Glaucoma;Irreversible blindness;Early detection;Accurate detection;AI;Optimal management},
  doi={10.1109/AIMLA59606.2024.10531451},
  ISSN={},
  month={March},}@INPROCEEDINGS{10053477,
  author={Aravindan, C and Vasuki, R},
  booktitle={2023 International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)}, 
  title={Detection and Classification of Early Stage Diabetic Retinopathy using Artificial Intelligence and Image Processing}, 
  year={2023},
  volume={},
  number={},
  pages={919-924},
  abstract={Diabetic retinopathy is the term used to describe the damage to the blood vessels in the retina of the human eye. The symptoms of diabetic retinopathy are blurriness, difficulty in vision and even blindness can occur. The blood vessels in the retina of the human eye have been damaged over time, which has an impact on the person’s ability to see. It is a cumulative problem in the modern world. Diabetic retinopathy has four stages, including mild, moderate, and severe non proliferative and proliferative. To reduce the effects of diabetic retinopathy are early diagnosis is necessary. Thus, by using artificial intelligence and image processing, the early stage of diabetic retinopathy can be detected. This leads to faster and easier screening of disorder for both the patients and ophthalmologists.},
  keywords={Training;Retinopathy;Image processing;Data visualization;Blood vessels;Retina;Diabetes;Diabetic retinopathy detection;artificial intelligence;earlier prediction;image processing;ophthalmologists},
  doi={10.1109/IDCIoT56793.2023.10053477},
  ISSN={},
  month={Jan},}@ARTICLE{10472545,
  author={Gao, Zhaoqi and Guo, Meiqian and Li, Chuang and Li, Zhen and Gao, Jinghuai and Xu, Zongben},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Deep Learning Accelerated Blind Seismic Acoustic-Impedance Inversion}, 
  year={2024},
  volume={62},
  number={},
  pages={1-22},
  abstract={Blind seismic acoustic-impedance (AI) inversion is a technique for obtaining the AI of the subsurface medium without being given a wavelet. An effective way to solve the blind inversion problem is to split the multiparameter problem into two single-parameter subproblems and solve them in an alternative iteration way. However, this method becomes time-consuming when dealing with large-scale 3-D problems and faces challenges in selecting suitable regularization parameters. To overcome these shortcomings, we propose a deep learning accelerated blind seismic AI inversion (DLA-BSAII) method. It mainly has three steps. First, only a few 2-D profiles are selected from the whole 3-D data, and their corresponding AI models and wavelets are inverted using the conventional blind seismic AI inversion method. Second, the results of the first step are used to train deep networks to realize the nonlinear mapping from a 2-D seismic profile to AI and wavelet. In addition, the trained deep networks are used to generate predictions of AI models and wavelets for the remaining 2-D profiles. Third, benefiting from the predicted AI models and wavelets, a new alternative iteration method with fewer but more effective regularization terms is proposed to obtain the final inverted AI models and wavelets of the remaining 2-D profiles. It has the advantages of easier selection of regularization parameters and faster convergence speed. Synthetic and field data examples verify that DLA-BSAII outperforms conventional methods in terms of both efficiency and inversion accuracy.},
  keywords={Artificial intelligence;Data models;Three-dimensional displays;Deep learning;Predictive models;Optimization;TV;3-D;blind inversion;deep learning (DL);high efficiency;seismic acoustic-impedance (AI) inversion},
  doi={10.1109/TGRS.2024.3377274},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{10448193,
  author={Guo, Xuechen and Hu, Wenhao and Ni, Chiming and Chai, Wenhao and Li, Shiyan and Wang, Gaoang},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Blind Inpainting with Object-Aware Discrimination for Artificial Marker Removal}, 
  year={2024},
  volume={},
  number={},
  pages={1516-1520},
  abstract={Medical images often incorporate doctor-added markers that can hinder AI-based diagnosis. This issue highlights the need of inpainting techniques to restore the corrupted visual contents. However, existing methods require manual mask annotation as input, limiting the application scenarios. In this paper, we propose a novel blind inpainting method that automatically reconstructs visual contents within the corrupted regions without mask input as guidance. Our model includes a blind reconstruction network and an object-aware discriminator for adversarial training. The reconstruction network contains two branches that predict corrupted regions in images and simultaneously restore the missing visual contents. Leveraging the potent recognition capability of a dense object detector, the object-aware discriminator ensures markers undetectable after inpainting. Thus, the restored images closely resemble the clean ones. We evaluate our method on three datasets of various medical imaging modalities, confirming better performance over other state-of-the-art methods.},
  keywords={Training;Visualization;Detectors;Manuals;Robustness;Image restoration;Task analysis;Blind image inpainting;generative adversarial networks;image reconstruction;dense object detector},
  doi={10.1109/ICASSP48485.2024.10448193},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10040269,
  author={S, Zulaikha Beevi and P, Harish Kumar and S, Harish and J, Lakshan S},
  booktitle={2022 1st International Conference on Computational Science and Technology (ICCST)}, 
  title={Decision Making Algorithm for Blind Navigation Assistance using Deep Learning}, 
  year={2022},
  volume={},
  number={},
  pages={268-272},
  abstract={Blind people face several obstacles in their daily lives and technological interventions can help overcome these obstacles. In this research, we provide an AI-based autonomous assisting device that recognizes many objects and it will provide acoustic input to the user to help visually blind people to understand the surrounding better to understand their environment better. Multiple photos of objects relevant to visually impaired people were used to build a deep-learning model. Training photos are enhanced and manually annotated to improve the trained model's resilience. A distance-measuring sensor is included which recognise the objects using computer vision. The gadget is made more inclusive by recognizing the obstacles coming out of one place to another. After stage segmentation and obstacle detection, the aural information sent to the user is adj usted to get a lot of details in minimum time and speed up video processing.},
  keywords={Deep learning;Training;Navigation;Scientific computing;Computational modeling;Blindness;Libraries;Deep learning model;Image processing;Speech recognition;RCNN;Image recognition;Voice output},
  doi={10.1109/ICCST55948.2022.10040269},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10047784,
  author={Sangeetha, S.V.Tresa and Porselvi, T. and Venkateshwaran, A and Gokul, M and Sathmikan, I and Tharun Kumar, P},
  booktitle={2022 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)}, 
  title={Currency Detection App for Blind People Using MIT App Inventor}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={This paper helps the visually impaired people to identify the currency notes using an app built using MIT app inventor. The user has to press the shutter button in order to identify the amount of the currency being scanned. The output of the app is a voice command that will help the blind people to identify the amount. This paper makes use of the artificial intelligence tool to classify the image and gives the output. The artificial intelligence uses the image classifier to classify the image.Normally, the image classifier takes the input given the user and classifies the given input and gives the output.Here, the input given is the currency notes of different values so that the image classifier classifies the input and gives the output. This paper uses personal image classifier model to classify or detect the captured image.Here, the captured image in indian rupee note. This personal image classifier consists of four steps.},
  keywords={Presses;Blindness;Detectors;Control systems;Artificial intelligence;Currencies;Image classifier;MIT app inventor;Currency Detector},
  doi={10.1109/ICPECTS56089.2022.10047784},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10457245,
  author={Sungman, Hong},
  booktitle={2024 International Conference on Electronics, Information, and Communication (ICEIC)}, 
  title={Trade-off Experiments in Efficient Blind Video Quality Assessment: An Analysis on Sampling Strategies across CPU and GPU}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Currently, video data dominates mobile internet traffic. Aided by AI, promoting active research in Blind Video Quality Assessment (BVQA) for evaluating received video quality without the original video. However, AI methodologies using CNN employ all frames to maximize their predictive accuracy, may encounter long prediction times, hindering real-time applications like streaming services. This study explores accuracy and time trade-offs using frame sampling strategies across varying frame sampling cycles. We introduce the “Efficient BVQA coefficient” to gauge the balance between performance and time reduction, identifying optimal sampling points on both CPU and GPU with their characteristics. Our findings elucidate the impact of frame sampling strategies on BVQA's accuracy and computational time, paving the way for developing more efficient BVQA methodologies.},
  keywords={Graphics processing units;Streaming media;Real-time systems;Quality assessment;Computational efficiency;Artificial intelligence;Video recording;Blind video quality assessment;Frame sampling strategies;Trade-off experiments;Prediction accuracy;Time reduction},
  doi={10.1109/ICEIC61013.2024.10457245},
  ISSN={2767-7699},
  month={Jan},}@INPROCEEDINGS{10486214,
  author={Gowthami, M and Shreya, P and Sripriya, T and Yazhini, B},
  booktitle={2024 2nd International Conference on Computer, Communication and Control (IC4)}, 
  title={Cognitive Vision Companion: An AI-Enhanced Support System for the Visually Impaired}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In a world where a major portion of the populace suffer with daily challenges of blindness, this pioneering project introduces a comprehensive smart support system designed to aid the needs of people who are visually impaired. This advanced system is ingeniously integrated into a pair of headphones, combining various state-of-the-art technologies such as cameras, ultrasonic sensors, and cutting-edge machine learning algorithms. Its primary goals encompass recognizing objects, accurately estimating distances between objects and users, deciphering captured signs and indications, and offering real-time location-based navigation and directions. This multifunctional blind assistance device is designed to empower users by enhancing their spatial awareness and providing them the tools they need to navigate travel securely and on their own. It represents a remarkable leap forward in refining the overall quality of life and mobility for the blind people.},
  keywords={Headphones;Machine learning algorithms;Navigation;Refining;Estimation;Blindness;Cameras;visually impaired;machine learning;object detection;navigation;object distance estimation},
  doi={10.1109/IC457434.2024.10486214},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10467981,
  author={Gulshan and Chauhan, Vikas},
  booktitle={2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)}, 
  title={Diabetic Retinopathy Detection Using Artificial Intelligence: A Review}, 
  year={2024},
  volume={},
  number={},
  pages={1584-1588},
  abstract={Diabetic Retinopathy (DR) is a severe issue among the diabetic patients, which makes the patient blind and so an early stage detection should be held to prevent this permanent damage to our health. In this research study, the introduction section brief about the common diabetes disease and type of diabetes is highlighted in the next section. The third section illustrates the basic idea and terminology of AI along with how it helps in Diabetic Retinopathy. The comparative study in tabular form for a better understanding of the major work in this area is discussed.},
  keywords={Diabetic retinopathy;Terminology;Reviews;Learning (artificial intelligence);Internet of Things;Data communication;Diseases;Diabetes;Type 1 diabetes;Diabetic Retinopathy(DR);Artificial Intelligence;Deep learning;Machine learning},
  doi={10.1109/IDCIoT59759.2024.10467981},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10045533,
  author={Zhou, Guangtao and Sheng, Zhi},
  booktitle={2022 8th Annual International Conference on Network and Information Systems for Computers (ICNISC)}, 
  title={The Use of AI in the Elderly Care Industry in China}, 
  year={2022},
  volume={},
  number={},
  pages={794-799},
  abstract={The population aging in China has deepened rapidly in recent years. The most critical factor affecting elderly care is the labor problem with the population aging. Lack of labor poses a severe challenge to government governance. The overall labor force is insufficient, rural labor flows to cities, and the elderly become “intellectual blindness” to a certain extent, making the government face difficulties in population policy, Rural Revitalization, and data application. “Artificial intelligence” is closely related to human social life. The emergence of new things such as AI robots, AI terminals, and AI intelligent networks profoundly affects our daily lives. In such a background, intelligent elderly care entered the public view and attracted attention from all walks of life. Applying artificial intelligence to the elderly care industry can alleviate the shortage of elderly service human resources. The Chinese government is expected to encourage the R&D of artificial intelligence in elderly care services and build an artificial intelligence network to make the application of artificial intelligence in the elderly care industry more rapid and sustainable.},
  keywords={Systematics;Smart cities;Sociology;Government;Transportation;Symbols;Aging;elderly care;artificial intelligence (AI);smart services;population aging},
  doi={10.1109/ICNISC57059.2022.00159},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9893624,
  author={Khan, Ibrahim and Nguyen, Thai Van and Dai, Xincheng and Thawonmas, Ruck},
  booktitle={2022 IEEE Conference on Games (CoG)}, 
  title={DareFightingICE Competition: A Fighting Game Sound Design and AI Competition}, 
  year={2022},
  volume={},
  number={},
  pages={478-485},
  abstract={This paper presents a new competition-at the 2022 IEEE Conference on Games (CoG)- called DareFightingICE Competition. The competition has two tracks: a sound design track and an AI track. The game platform for this competition is also called DareFightingICE, a fighting game platform. DareFightingICE is a sound-design-enhanced version of FightingICE, used earlier in a competition at CoG until 2021 to promote artificial intelligence (AI) research in fighting games. In the sound design track, participants compete for the best sound design, given the default sound design of DareFightingICE as a sample, where we define a sound design as a set of sound effects combined with the source code that implements their timing-control algorithm. Participants of the AI track are asked to develop their AI algorithm that controls a character given only sound as the input (blind AI) to fight against their opponent; a sample deep-learning blind AI will be provided by us. Our means to maximize the synergy between the two tracks are also described. This competition serves to come up with effective sound designs for visually impaired players, a group in the gaming community which has been mostly ignored. To the best of our knowledge, DareFightingICE Competition is the first of its kind within and outside of CoG.},
  keywords={Codes;Games;Artificial intelligence;Sound Design Competition;AI Competition;Visually Impaired Players;DareFightingICE;FightingICE;Fighting Game},
  doi={10.1109/CoG51982.2022.9893624},
  ISSN={2325-4289},
  month={Aug},}@INBOOK{10954456,
  author={Koley, Santanu and Sengupta, Shatadru and Biswas, Bipasha and Datta, Kankana and Jana, Manasi and Mitra, Apratim},
  booktitle={Artificial Intelligence-Enabled Businesses: How to Develop Strategies for Innovation}, 
  title={Applications of Artificial Intelligence and Machine Learning&#x2010;Enabled Businesses}, 
  year={2025},
  volume={},
  number={},
  pages={227-261},
  abstract={Summary <p>Developing intelligent business solutions, efficient in process implementation and effective in customer relationship management, has been well sought after since the advent of computing into businesses. After Artificial Intelligence, with its various branches, became one of the most major subject matters for study in the world, companies across all domains have been actively involved in the pursuit of assembling business processes that have the capability to adapt newer strategies based on dynamic and heavily analytical factors such as customer feedback, market dynamics, geographical forces, logistic issues, social trends, legal parameters &#x2013; to name but a few. Indeed, such adaptability arises only from intelligent reasoning with AI. Machine Learning, a branch of AI, is being used learn in unsupervised environments; Deep Learning in adapting to potential future market changes using technologies based on human neurons; Natural Language Processing is helping machines interact with human clients tirelessly and automatically without the burden of reaction thereby increasing rapport. Robotics, Expert Systems and Fuzzy Logic, find their respective uses in tasks like automatic process implementation, automated disease diagnosis and prophylaxis, and automated handling of uncertainties in logistic processes. Newer and more promising uses of AI in businesses are being evolved regularly.</p> <p>All this remarkable progress was originally aimed at achieving the maximum goals and raising the standards in man's life. However, it is important to bring forth the issues that must be tackled while business strategising, in order to maximise AI's achievements and minimise its nullifying effects. Citing real&#x2010;life and examples, we portray the various ways AI in businesses has improved and enhanced the society in general, yet some universal human values and some universally acknowledged human traits like intuition and instinct are slowly taking an exit route. This chapter shows how blind following of AI is leading to businesses going the &#x201c;safe and trendy way&#x201d; at the cost of genius. The examples try to show how the adverse effects of AI may be kept at bay while making business decisions. It is argued that in a properly designed environment, that is one in which the AI that is used is tuned in to benefit from human genius rather that to oppose it, AI will possibly emerge as the greatest gift of modern technology. It is, on the other hand, quite possible that AI that blunts the human effect and undermines the human effort, will prove to be the greatest technological scourge in human history.</p>},
  keywords={Artificial intelligence;Business;Machine learning;Machine learning algorithms;Deep learning;Prediction algorithms;Unsupervised learning;Supervised learning;Robots;Technological innovation},
  doi={10.1002/9781394234028.ch13},
  ISSN={},
  publisher={Wiley},
  isbn={9781394234004},
  url={https://ieeexplore.ieee.org/document/10954456},}@INPROCEEDINGS{9885356,
  author={Jakka, Surya Chaitanya and Sai, Yerragopu Venkata and A, Jesudoss and A, Viji Amutha Mary},
  booktitle={2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC)}, 
  title={Blind Assistance System using Tensor Flow}, 
  year={2022},
  volume={},
  number={},
  pages={1505-1511},
  abstract={E-mail is one of the most well-known types of human communication. Today, there is a multitude of classified and authentic data available. At the same time, approximately 253 million people have visual disabilities. The proposed blind assistance concepts were created to assist visually impaired people worldwide. These visually impaired people experience difficulties in reading an Email communication. The proposed system is divided into two levels based on the SSD algorithm and TensorFlow, which recognizes the objects not only for recognition but also for localization. It also tells you how far the person is from the object. Individuals with visual impairments may face difficulties as innovation advances step by step. This research work has proposed a novel framework by utilizing AI, which makes the framework more straightforward to use specifically for the individuals with visual impedances and to help the society. The main key aspect of the proposed system is identifying or naming the object detected, calculating the accurate distance between the user and objects and the voice over using Audio commands.},
  keywords={Location awareness;Visualization;Technological innovation;Tensors;Communication systems;Face recognition;Computational modeling;Virtual Assistance;Object Detection;Tensor Flow;SSD MobileNet},
  doi={10.1109/ICESC54411.2022.9885356},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10794008,
  author={Kalantari, Leila and Seth, Subhendu and Krishnan, Manikanda and Sutton, Jonathan and Jutras, Melanie and Rao, Anuradha},
  booktitle={2024 IEEE Ultrasonics, Ferroelectrics, and Frequency Control Joint Symposium (UFFC-JS)}, 
  title={Multiple Pregnancy Detection from Ultrasound Blind Sweeps}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Timely detection of high-risk pregnancies is of paramount interest to maternal-fetal health. With advances in AI and the advent of low-cost handheld ultrasound, automated detection of high-risk pregnancies, without the need for a trained sonographer in primary care setting, is a highly impactful yet feasible technology. The major challenge of building detection models for high risk pregnancies, such as multiple pregnancy (MP), fetal demise, placenta previa, is the scarcity of available data due to low-occurring nature of high-risk situations. Often model selection requires data hungry nested cross-validation. To mitigate the problem, we introduce a novel model selection criterion using non-nested cross-validation.We showcase the pipeline we built for developing a MP detection model with both deep learning and a less expressive machine learning (ML) model as components. The pipeline (A) efficiently uses limited multiple pregnancy data collected by blindly sweeping over maternal abdomen (blind to images), (B) can be deployed in a low-compute setting, and (C) is fully interpretable and hence inspectable for any alarming learned pattern.1},
  keywords={Pregnancy;Deep learning;Ultrasonic imaging;Placenta;Pipelines;Buildings;Data models;Acoustics;Frequency control;Abdomen},
  doi={10.1109/UFFC-JS60046.2024.10794008},
  ISSN={2375-0448},
  month={Sep.},}@INPROCEEDINGS{10635657,
  author={Castro-Macías, Francisco M. and Pérez-Bueno, Fernando and Vega, Miguel and Mateos, Javier and Molina, Rafael and Katsaggelos, Aggelos K.},
  booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
  title={Blind Color Deconvolution and Classification of Histological Images Using the Hyperbolic Secant Prior}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In this paper, we present a novel approach to Blind Color De-convolution, a stain separation technique useful for normalizing, augmenting, and automatically diagnosing histological images. To robustly estimate the stain colors and concentrations, we follow the Bayesian framework and introduce a Gaussian prior distribution on the color vectors and a Hyperbolic Secant prior on the concentrations, which is a seldom explored image model.We provide a comprehensive mathematical derivation of our inference procedure, outlining the underlying principles and assumptions. To demonstrate its effectiveness, we conduct two experiments. The first experiment assesses the fidelity of the reconstructed images to the underlying tissue characteristics. The second experiment uses it as a preprocessing step for a multicenter breast cancer classification task. Our results reveal the superior performance of the proposed method, underscoring its potential for advancing histological image processing and AI-assisted diagnosis.},
  keywords={GSM;Deconvolution;Image color analysis;Breast cancer;Vectors;Mathematical models;Data models;Blind Color Deconvolution;Bayesian modeling;histological images},
  doi={10.1109/ISBI56570.2024.10635657},
  ISSN={1945-8452},
  month={May},}@INPROCEEDINGS{10134298,
  author={Meenakshi, J. and Thailambal, G.},
  booktitle={2023 International Conference on Inventive Computation Technologies (ICICT)}, 
  title={Gender and Age Detection Techniques for Blind People using Principal Component Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={572-577},
  abstract={Predicting someone's gender and age-based solely on appearance is challenging for an AI model, as it requires recognizing and understanding complex patterns in images and videos. This kind of prediction is particularly used for blind people. Different kinds of techniques are applied to analyze and predict gender and age classification with the help of images. In the previous methods, accuracy and live prediction have different challenges such as accuracy and prediction rates. This work proposes a hybrid model to predict and analyze gender and age classification. This work consists of Haar Cascade, Histogram of Oriented Gradients, Hessian Filter and Principal Component Analysis. The Haar Cascade algorithm finds the face from the video images with constant speed and time. The Histogram-orientation extracts the features from the images, and the Hessian Filter finds the wrinkles and, based on that, finds the age. Principal Component Analysis is used to visualize and find the patterns from the images. This proposed hybrid work is implemented using 10137 images, and based on that, training and testing are performed. The proposed work is evaluated using recall, f1-score and precision. The proposed work achieved 87.30%. and 86.9% precision for gender classification and age classification respectively. This, compared to the previous work, produced effective results for gender and age detection.},
  keywords={Training;Analytical models;Computational modeling;Predictive models;Filtering algorithms;Feature extraction;Prediction algorithms;Age and Gender Detection;Hybrid Model;Principal Component Analysis},
  doi={10.1109/ICICT57646.2023.10134298},
  ISSN={2767-7788},
  month={April},}@INPROCEEDINGS{11189127,
  author={Jasper, Surya and Luu, Minh and Pan, Evan and Tyagi, Aakash and Quinn, Michael and Hu, Jiang and Houngninou, David},
  booktitle={2025 ACM/IEEE 7th Symposium on Machine Learning for CAD (MLCAD)}, 
  title={BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={Hardware complexity continues to strain verification resources, motivating the adoption of machine learning (ML) methods to improve debug efficiency. However, ML-assisted debugging critically depends on diverse and scalable bug datasets, which existing manual or automated bug insertion methods fail to reliably produce. We introduce BugGen, a first of its kind, fully autonomous, multi-agent pipeline leveraging Large Language Models (LLMs) to systematically generate, insert, and validate realistic functional bugs in RTL. BugGen partitions modules, selects mutation targets via a closed-loop agentic architecture, and employs iterative refinement and rollback mechanisms to ensure syntactic correctness and functional detectability. Evaluated across five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional accuracy and achieved a throughput of 17.7 validated bugs per hour—over five times faster than typical manual expert insertion. Additionally, BugGen identified 104 previously undetected bugs in OpenTitan regressions, highlighting its utility in exposing verification coverage gaps. Compared against Certitude, BugGen demonstrated over twice the syntactic accuracy, deeper exposure of testbench blind spots, and more functionally meaningful and complex bug scenarios. Furthermore, when these BugGen-Generated datasets were employed to train ML-based failure triage models, we achieved high classification accuracy (88.1%–93.2%) across different IP blocks, confirming the practical utility and realism of generated bugs. BugGen thus provides a scalable solution for generating high-quality bug datasets, significantly enhancing verification efficiency and ML-assisted debugging.},
  keywords={Training;Solid modeling;Accuracy;Large language models;Computer bugs;Pipelines;Manuals;Machine learning;Syntactics;IP networks;design verification;bug insertion;large language model;machine learning},
  doi={10.1109/MLCAD65511.2025.11189127},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10018579,
  author={Cordero-Mendieta, Ma. Isabel and Pinos-Vélez, Eduardo and Buri-Abad, Edison and Coronel-Berrezueta, Roberto},
  booktitle={2022 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)}, 
  title={Support tool for presumptive diagnosis of Glaucoma using fundus image processing and artificial intelligence implementation}, 
  year={2022},
  volume={6},
  number={},
  pages={1-5},
  abstract={Blindness is a global health problem and glaucoma is one of the diseases that are considered of vital importance to treat since it is a neurodegenerative disease that causes irreversible blindness that still has no cure, however, it can be treated if detected early; Most people begin to feel symptoms when this disease is already in an advanced stage, therefore in this work we have developed a tool to support the medical diagnosis through digital image processing for it has been consulted in several databases for the study and classification of retinal images among these images we have healthy eyes, suspected glaucoma and diagnosed with glaucoma. The region of interest, we worked with was the optic disc since this is where the blood vessels are interconnected and it is an important area for analysis. We have made a tool to support the presumptive diagnosis of glaucoma applied several neural systems with different structures obtaining very good results of accuracy and sensitivity, this tool was developed free software so that it has free access to both treating physicians and students in the health area, in addition, it has been made in a very intuitive way for its easy use, in first instance allows the entry of images in JPG, JPEG and PNG color format, additionally the patient's data and the treating physician, the results are displayed in a PDF format document with all the information entered and the respective diagnosis, which makes it easier to keep a proper medical history.},
  keywords={Training;Sensitivity;Biomedical optical imaging;Databases;Computer architecture;Predictive models;Optical imaging;Glaucoma;blindness;visual impairment;optic disc;prediction and detection of glaucoma;neural networks;convolutional networks;presumptive diagnosis},
  doi={10.1109/ROPEC55836.2022.10018579},
  ISSN={2573-0770},
  month={Nov},}
