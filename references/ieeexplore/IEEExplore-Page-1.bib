@INPROCEEDINGS{10435266,
  author={Annamaneni, Nishikanth and Shankar, Vuppu and Hanumanthu, B. and Rao, V. Chandra Shekhar and Sujatha, M. and Vani, Bojja},
  booktitle={2023 1st International Conference on Optimization Techniques for Learning (ICOTL)}, 
  title={Voice Based Assistance About Surroundings of Blind and Visually Impaired People}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={People who are visually impaired or blind may find it difficult to comprehend what is going on around them. This project proposes an AI-based application that when the user gives a voice command, the application recognizes the voice command and takes the visual data (image) with an externally connected camera from the surroundings of blind or visually impaired people and based on the voice command and it will be sent to corresponding machine learning model API and then that model may generate the captions or textual description in natural language(English) or different kind of objects present in the image and finally convert the same into speech in natural language using Text to Speech API so that user can hear it. Now by using this AI application, blind and visually impaired people who are mostly dependent on volunteers or any assistive materials in moving around in their surroundings can be less dependent and also move around freely.},
  keywords={Visualization;Text recognition;Speech recognition;Object detection;Cameras;Data models;Optimization;Blind people;Image Captioning;Object detection;Speech Recognition;Speech Synthesis},
  doi={10.1109/ICOTL59758.2023.10435266},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10454680,
  author={Delnevo, Giovanni and Andruccioli, Manuel and Mirri, Silvia},
  booktitle={2024 IEEE 21st Consumer Communications & Networking Conference (CCNC)}, 
  title={On the Interaction with Large Language Models for Web Accessibility: Implications and Challenges}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The widespread diffusion of Large Language Models (LLMs) has ushered in a transformative era across numerous research domains, including web accessibility. In fact, they can potentially offer automated solutions for generating accessible content, performing accessibility testing, and enhancing the overall user experience for individuals with disabilities. In this paper, we investigate how LLMs can be successfully employed to evaluate and correct web accessibility. Then, we delve into the positive implications and the current challenges derived from the interaction between developers and LLMs in this specific context. Finally, we present some future directions that could be explored to ensure that web content remains accessible to all.},
  keywords={User experience;Testing;Web Accessibility;Large Language Models;Human-Computer Interaction;ChatGPT},
  doi={10.1109/CCNC51664.2024.10454680},
  ISSN={2331-9860},
  month={Jan},}@INPROCEEDINGS{10145230,
  author={Alqahtani, Reem Abdulmohsen and Alqahtani, Reema Saeed and Almutairi, Qamra Abdulrhman and Aloqayli, Reema Mohammad and Alkasem, Haifa Hamad},
  booktitle={2023 Sixth International Conference of Women in Data Science at Prince Sultan University (WiDS PSU)}, 
  title={Ayn: Assistive Tool to Describe Images for Blind and Visually Impaired}, 
  year={2023},
  volume={},
  number={},
  pages={25-30},
  abstract={Describing images on social networking sites is one of the main challenges for the blind and visually impaired people due the lack of necessary aids. Therefore, creating technical tools that aid them to describe images is a demand especially for Arabic speakers. This paper aims to build an application called Ayn for Android systems, which seeks to describe images published on the Twitter application by using artificial intelligence algorithm in the Arabic language through voice and writing. The Ayn application provides the blind and visually impaired people with image description in two ways, either by artificial intelligence or by volunteers registered in the system. Users can filter the descriptions of the images by two options, either by the highest-rated descriptions, or by the descriptions with the tallest comment.},
  keywords={Social networking (online);Blogs;Writing;Filtering algorithms;Data science;Artificial intelligence;Artificial Intelligence;Machine Learning;Alternative Text;Visually Impaired;Image processing},
  doi={10.1109/WiDS-PSU57071.2023.00018},
  ISSN={},
  month={March},}@INPROCEEDINGS{11108817,
  author={Bonda, Narasimha and Chenxin, Qin and Iwasaki, Yukiko and Iwata, Hiroyasu},
  booktitle={2025 9th International Conference on Robotics and Automation Sciences (ICRAS)}, 
  title={Hover-Bvi: Handover Vision-Language Embodied Robot System for Bvi Users}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={We present HOVER-BVI, a robot system designed to help blind and visually-impaired (BVI) users by dynamically planning and executing grasping tasks in indoor environments. Unlike traditional rigid assistive robots, our approach uses a large language model (LLM) as a flexible task planner combined with a hierarchical state machine system to sequentially execute action primitive functions. The system maintains an object memory, generates natural-language scene descriptions of the workspace, and interacts via speech. By leveraging an LLM for high-level planning, the system interprets diverse voice commands and compiles them into a sequence of primitives, making dynamic tasks possible. To validate the system, we designed experiments with blindfolded users to evaluate the system's effectiveness in tasks such as object search, description, and retrieval. Preliminary analysis suggests that in a semantically constrained indoor setting, the LLM-based planner can achieve high success rates while providing intuitive user interaction. We highlight the system's modularity and extensibility and discuss usability considerations for real-world blind users. This work takes LLM-task planning in general situations (e.g. in RoboCup GPSR) and adapts it to the BVI domain, exploiting the semantically rich, yet structured indoor context to improve reliability and user trust in assistive robotics.},
  keywords={Scalability;Large language models;Human-robot interaction;Search problems;Hardware;Planning;Indoor environment;Reliability;Usability;Robots;vision-guided assistive robotics;BVI interaction;human-robot interaction;LLM task planning},
  doi={10.1109/ICRAS65818.2025.11108817},
  ISSN={2694-3506},
  month={June},}@INPROCEEDINGS{10702085,
  author={Acosta-Vargas, Patricia and Acosta-Vargas, Gloria and Salvador-Acosta, Belén and Jadán-Guerrero, Janio},
  booktitle={2024 Tenth International Conference on eDemocracy & eGovernment (ICEDEG)}, 
  title={Addressing Web Accessibility Challenges with Generative Artificial Intelligence Tools for Inclusive Education}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={This article addresses the comprehensive evaluation of web accessibility in 20 generative artificial intelligence (AI) applications, such as ChatGPT and DALL-E, through a five-phase approach. Common issues were identified, including inadequate image descriptions, lack of semantic structures, and keyboard navigation challenges. Despite the inherent complexity of generative tools, the importance of evaluating their accessibility to ensure the inclusion of users with diverse abilities is emphasized. The WAVE automatic tool was used to identify issues, and future directions are proposed, such as improving image descriptions and optimizing keyboard navigation. The most significant accessibility challenges are linked to minimal contrast, representing 38%, followed by issues in easy-to-read font and text alternatives, both at 15%, associated with the perceptible principle. The discussion covers areas for improvement, ethical implications, and strategies to foster continuous enhancement in generative AI accessibility, highlighting the importance of balancing benefits and ethical challenges. Future research includes developing techniques for automatically generating consistent image descriptions, refining semantic structures, and optimizing keyboard navigation. Additionally, it is imperative to improve automatic accessibility evaluation tools to address the unique challenges of generative AI applications.},
  keywords={Ethics;Generative AI;Navigation;Semantics;Refining;Education;Keyboards;Chatbots;Complexity theory;generative artificial intelligence;inclusive user experience;web accessibility;WCAG},
  doi={10.1109/ICEDEG61611.2024.10702085},
  ISSN={2573-1998},
  month={June},}@INPROCEEDINGS{10928130,
  author={Huynh, Gia Ky and Lin, Wenjun},
  booktitle={2024 International Conference on Computer and Applications (ICCA)}, 
  title={SmartCaption AI - Enhancing Web Accessibility with Context-Aware Image Descriptions Using Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={The Internet provides vast amounts of information, services, and products. However, blind individuals and those with severe vision impairments face significant challenges in navi-gating web content, especially with understanding images. This paper introduces SmartCaption AI, an innovative solution that leverages Large Language Models (LLM) to generate descriptive text for images on web pages. By summarizing the content of a web page, SmartCaption AI provides relevant context for the LLM to produce accurate and meaningful image descriptions. These descriptions are seamlessly integrated into the web page's structure, allowing text- to-speech software to read them aloud to visually impaired users. SmartCaption AI offers several key contributions to web accessibility. It ensures the generated descriptions are contextually relevant, enhances the browsing experience by integrating real-time descriptions, and provides a universally accessible solution through a Chrome extension. This approach addresses the critical issue of missing or inadequate alternative text for images, thereby bridging the digital divide between sighted and visually impaired individuals. The results of our experiment demonstrated the effectiveness of SmartCaption AI, with an average score of 8.3/10, significantly outperforming state-of-art solutions: ImageToText (1.7/10) and AI-MCS (3.6/10). The source code of the tool is available on GitHub.},
  keywords={Large language models;Source coding;Visual impairment;Web pages;Information services;Software;Real-time systems;Internet;Faces;Software development management;image caption;Large Language Model;multi-agent;alternative text;alt text;vision impairment},
  doi={10.1109/ICCA62237.2024.10928130},
  ISSN={},
  month={Dec},}@ARTICLE{11146787,
  author={Salous, Mazen and Lange, Daniel and von Reeken, Timo and Wolters, Maria K. and Heuten, Wilko and Boll, Susanne and Abdenebaoui, Larbi},
  journal={IEEE Access}, 
  title={Semi-Automatic BVI Human-Centered Image Conversational Descriptions: Leveraging LLMs and Expert Refinements for Inclusive Visual Accessibility}, 
  year={2025},
  volume={13},
  number={},
  pages={156072-156090},
  abstract={Ensuring that blind and visually impaired (BVI) individuals can fully participate in today’s image-rich digital world remains a significant challenge. Current visual assistants often rely on annotations from sighted contributors, which may fail to capture BVI individuals’ preferences and expectations. Addressing this challenge, we explore how to develop optimal, conversational image descriptions that resonate with BVI individuals, and how to effectively scale their creation. We propose a semi-automatic approach that couples large language models (LLMs) with iterative, BVI-driven refinements. Starting with initial LLM-generated image descriptions, a small set of BVI experts refine them to better meet BVI individuals’ needs. These enhanced examples guide subsequent LLM outputs, which are then evaluated by BVI end-users to confirm improved quality and satisfaction. We contribute to constructing large-scale, BVI-centered training data, thereby advancing inclusive and conversational visual accessibility. Throughout our studies, the results show a significant improvement in BVI end-users’ satisfaction with image conversational descriptions when edited by BVI experts. Additionally, the results indicate promising improvements when the LLM re-generates descriptions using those edits as few-shot examples. Moreover, we got valuable insights from BVI participants’ focused mainly on optimizing clarity, relevance, and interaction patterns for image descriptions and conversational exchanges between AI and BVI individuals.},
  keywords={Visualization;Oral communication;Videos;Large language models;Iterative methods;Guidelines;Training;Standards;Annotations;Training data;Accessibility;digital image description;human–AI image conversation},
  doi={10.1109/ACCESS.2025.3605490},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11012288,
  author={R, Azhaguraja and Kumar, K S Ashvith and V, Paranthaman C and S, Arun Kumar},
  booktitle={2025 3rd International Conference on Advancements in Electrical, Electronics, Communication, Computing and Automation (ICAECA)}, 
  title={Assistive Technologies for Blind and Visually Impaired Individuals – A Short Review}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Globally, over 253 million people live with visual problems, with approximately 36 million being completely blind, as reported by the World Health Organization (WHO). The growing widespread presence of visual disabilities indicates the urgent need for assistive technologies that allow visually impaired people to navigate their environments and perform daily tasks more independently. The advancements in vision-based assistive devices are examined. Integrating artificial intelligence (AI), embedded systems, and machine learning has revolutionized these devices, enabling real-time scene recognition, object detection, text-to-speech conversion, voice-based user interaction and much more. This paper analyzes literature to explore the functions, advantages and disadvantages. This paper also identifies current trends, limitations, and opportunities for future advancements in assistive technologies to improve the quality of life for visually impaired individuals.},
  keywords={Visualization;Technological innovation;Reviews;Object detection;Machine learning;Speech recognition;Text to speech;Assistive devices;Reliability;Smart glasses;Assistive technologies;Artificial Intelligence;Machine learning;Embedded systems;Scene recognition;Object detection;Text-to-speech},
  doi={10.1109/ICAECA63854.2025.11012288},
  ISSN={},
  month={April},}@INPROCEEDINGS{10825638,
  author={Zhang, Jinghan and Liu, Kunpeng},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Thought Space Explorer: Navigating and Expanding Thought Space for Large Language Model Reasoning}, 
  year={2024},
  volume={},
  number={},
  pages={8259-8251},
  abstract={Recent advances in large language models (LLMs) have demonstrated their potential in handling complex reasoning tasks, which are usually achieved by constructing a thought chain to guide the model to solve the problem with multi-step thinking. However, existing methods often remain confined to previously explored solution spaces and thus overlook the critical blind spot within LLMs’ cognitive range. To address these issues, we design the Thought Space Explorer (TSE), a novel framework to expand and optimize thought structures to guide LLMs to explore their blind spots of thinking. By generating new reasoning steps and branches based on the original thought structure with various designed strategies, TSE broadens the thought space and alleviates the impact of blind spots for LLM reasoning. Experimental results on multiple levels of reasoning tasks demonstrate the efficacy of TSE. We also conduct extensive analysis to understand how structured and expansive thought can contribute to unleashing the potential of LLM reasoning capabilities.},
  keywords={Navigation;Large language models;Big Data;Cognition;Data models;Space exploration;language model;inference;generation;thought structure},
  doi={10.1109/BigData62323.2024.10825638},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10275753,
  author={Bimagambetova, Zhamilya and Rakhymzhanov, Dauren and Jaxylykova, Assel and Pak, Alexander},
  booktitle={2023 19th International Asian School-Seminar on Optimization Problems of Complex Systems (OPCS)}, 
  title={Evaluating Large Language Models for Sentence Augmentation in Low-Resource Languages: A Case Study on Kazakh}, 
  year={2023},
  volume={},
  number={},
  pages={14-18},
  abstract={Large language models (LLMs) have revolutionized natural language processing (NLP) and demonstrated exceptional performance in various NLP tasks for widely spoken languages. However, their efficacy in handling low-resource languages remains an area of concern. This study investigates the performance of LLMs, particularly GPT-3, in sentence augmentation tasks for a low-resource language, Kazakh. We employ a blind peer review methodology, where five native Kazakh annotators assess the quality of LLM-generated augmentations. The results reveal that LLMs excel in popular languages like English, Chinese, and German, but face challenges with low-resource languages due to limited training data. This work sheds light on the importance of improving LLMs' adaptability and relevance to address the unique needs of low-resource languages. Further research could enhance the augmentation capabilities of LLMs in scenarios with limited data sources, ensuring their effectiveness in promoting linguistic diversity and inclusivity. Furthermore, this study underscores the significance of cross-language transfer learning and data collection efforts to empower LLMs in supporting linguistic diversity and fostering inclusivity across the global language landscape.},
  keywords={Soft sensors;Transfer learning;Training data;Linguistics;Data collection;Natural language processing;Task analysis;Large language models;sentence augmentation;low-resource languages;Kazakh language},
  doi={10.1109/OPCS59592.2023.10275753},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10961841,
  author={T, Kumaresan and A, Geetha and G, Rajeshkumar and J, Benita Gracia Thangam and Eswaran, Malathi and Rajkumar, Thivya},
  booktitle={2024 First International Conference on Data, Computation and Communication (ICDCC)}, 
  title={Image to Audio Using LLM for Blind People}, 
  year={2024},
  volume={},
  number={},
  pages={610-614},
  abstract={Blind people face many difficulties while engaging with their surroundings. This project is all about creating innovative solutions for the blind people by providing the information in the format of audio. Image-to-Audio transformation system specifically designed for natural language understanding using Large Language Model (LLM).The system will capture a picture or video of the situation present in front of the blind person and then provide descriptions about the images. The objective is to provide guidelines for blind people by understanding their surroundings through an LLM model and provide audio response. The CLIP model analyzes the objects, scenes, text etc.. and provides relevant information. The extracted visual data is then passed through GTTs (Google-Text-To-Speech) to create rich audio descriptions. It provides accurate and detailed representation of the visual content, allowing blind people to understand their surroundings. The project involves integrating convolutional neural networks and Large Language Model (e.g GPT-4) technologies to create a real-time image-to-text system for blind individuals. The CNN model is used to calculate the distance of the object from the user. We are integrating the Google Assistants to open the app automatically by the blind people with help of their voice.},
  keywords={Visualization;Navigation;Large language models;Blindness;Real-time systems;Natural language processing;Internet;Convolutional neural networks;Multimedia communication;Monitoring;Large language model;blind people;Generation of audio;Distance calculation;Information extraction},
  doi={10.1109/ICDCC62744.2024.10961841},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11148483,
  author={Liu, Shenyu and Chen, Yu and Wang, Jieying},
  booktitle={2025 IEEE 20th Conference on Industrial Electronics and Applications (ICIEA)}, 
  title={A Lightweight Large Language Model for Personal Sleep Quality Estimation}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents a lightweight large language model (LLM) framework for personalized health advice generation, addressing critical limitations in resource-constrained scenarios. To resolve the inherent conflict between model performance and computational costs in existing solutions, we propose a threefold innovation: First, a structured prompting template dynamically integrates multimodal health data with semantic instructions. Raw sensor data undergoes anomaly detection and temporal aggregation. This process creates concise, semantically rich inputs for the LLM and eliminates the need for manual feature engineering. Second, we enable parameter-efficient fine-tuning of LLaMA 3.1 using Low-Rank Adaptation (LoRA). This approach injects rank-16 matrices into both attention and feed-forward layers, achieving single-GPU fine-tuning with only 7.93 GB memory overhead and sub-8-minute convergence. Third, we introduce an arena-style evaluation framework to assess health advice personalization. This framework uses third-party LLMs as neutral judges to compute ELO ratings through pairwise blind comparisons of eight candidate models, providing a quantifiable metric. Experimental results demonstrate that our optimized model attains a 36% ELO score improvement (from 1,072 to 1,389), surpassing some closed-source models as well as all open-source smaller models.},
  keywords={Performance evaluation;Training;Technological innovation;Computational modeling;Large language models;Semantics;Medical services;Manuals;Recommender systems;Monitoring;lightweight LLM;health informatics;parameter-efficient fine-tuning;multimodal data fusion;ELO evaluation},
  doi={10.1109/ICIEA65512.2025.11148483},
  ISSN={2158-2297},
  month={Aug},}@INPROCEEDINGS{10830245,
  author={Zhu, Yixin and Yue, Lishengsa and Sun, Jian},
  booktitle={2024 8th CAA International Conference on Vehicular Control and Intelligence (CVCI)}, 
  title={Modeling the Driver's Lane-Changing Decision Under the Influence of the Blind Zone Image of an Intelligent Vehicle: A Practice Based on Cognitive-Driven Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Lane-change blind zone image in intelligent vehicles can significantly improve driving safety, reducing the accident rate by 14%. However, inappropriate blind zone image design can lead to driver distraction and cognitive overload. This paper establishes a cognitive theory-driven large language model to address the interaction between the driver, blind zone image, and the road environment. The model is based on cognitive frame-works, integrating the powerful reasoning capabilities of large language models and a physiologically-based working memory module. This approach better simulates the cognitive decision-making processes in complex driver-machine-environment inter-actions, with improved interpretability and accuracy. The results show that the model achieves prediction accuracies of 96.59 % for driver decisions and 94.13 % for gaze points, effectively replicating driver cognitive decision-making. Additionally, the model can efficiently evaluate and optimize blind zone image design, revealing that improper lane-change blind zone image design significantly increases driver cognitive load and safety risks. Furthermore, it aids in optimizing HMI design to reduce cognitive load and enhance safety. This study lays the foundation for designing safer and more user-friendly intelligent vehicle HMI systems, promoting high-quality development in the automotive industry.},
  keywords={Accuracy;Intelligent vehicles;Large language models;Roads;Decision making;Cognitive load;Solids;Physiology;Safety;Load modeling;human-machine interaction;cognitive architecture;large language model;driving safety;cognitive load},
  doi={10.1109/CVCI63518.2024.10830245},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10609588,
  author={Oswal, Sushil K. and Oswal, Hitender K.},
  booktitle={2024 IEEE International Professional Communication Conference (ProComm)}, 
  title={Examining the Accessibility of Generative AI Website Builder Tools for Blind and Low Vision Users: 21 Best Practices for Designers and Developers}, 
  year={2024},
  volume={},
  number={},
  pages={121-128},
  abstract={Generative artificial intelligence tools are capturing the attention of the public and business since the introduction of ChatGPT. While this technology offers many productivity tools, their accessibility to screen reader users is little studied. Most of the technical and professional communication research about these tools focuses on their applied potential. This paper reports the findings of a mixed methods study on the user interfaces of three websites on which GenAI tools reside and the accessibility of their web editors. Studying the accessibility of these firms’ websites and their builder platforms is important because without accessibility features, blind customers cannot access these tools and create websites. This study found that none of the three builder websites, or the web editors for their tools, had WCAG 2.1 level AA accessibility. To improve these tools’ accessibility and usability, these website builders will need to invest resources to develop accessibility knowhow on web development and AI. Involvement of disabled users as co-designers and testers is essential to ensure accessibility in this AI infrastructure. This paper contributes 21 best practices for designing accessible interfaces for generative AI tools.},
  keywords={Visualization;Generative AI;Navigation;Layout;Prototypes;User interfaces;Artificial intelligence;Accessible generative AI;accessibility of web editors;AI website builders;best practices for generative AI tool design;documentation for large language models;web accessibility},
  doi={10.1109/ProComm61427.2024.00030},
  ISSN={2158-1002},
  month={July},}@ARTICLE{10398283,
  author={Joosten, Jan and Bilgram, Volker and Hahn, Alexander and Totzek, Dirk},
  journal={IEEE Engineering Management Review}, 
  title={Comparing the Ideation Quality of Humans With Generative Artificial Intelligence}, 
  year={2024},
  volume={52},
  number={2},
  pages={153-164},
  abstract={Traditionally, ideating new product innovations is primarily the responsibility of marketers, engineers, and designers. However, a rapidly growing interest lies in leveraging generative artificial intelligence (AI) to brainstorm new product and service ideas. This study conducts a comparative analysis of ideas generated by human professionals and an AI system. The results of a blind expert evaluation show that AI-generated ideas score significantly higher in novelty and customer benefit, while their feasibility scores are similar to those of human ideas. Overall, AI-generated ideas comprise the majority of the top-performing ideas, while human-generated ideas scored lower than expected. The executive's emotional and cognitive reactions were measured during the evaluation to check for potential biases and showed no differences between the idea groups. These findings suggest that, under certain circumstances, companies can benefit from integrating generative AI into their traditional idea-generation processes.},
  keywords={Technological innovation;Artificial intelligence;Generative AI;Creativity;Companies;Chatbots;Task analysis;AI-augmented innovation;artificial intelligence (AI);chatGPT;creativity;generative AI;idea generation;innovation;large language models (LLMs)},
  doi={10.1109/EMR.2024.3353338},
  ISSN={1937-4178},
  month={April},}@INPROCEEDINGS{11171137,
  author={Chaudhari, Isha and Wahane, Anjali and Rahate, Urshita and Bisen, Akanksha and Gupta, Anju},
  booktitle={2025 5th International Conference on Soft Computing for Security Applications (ICSCSA)}, 
  title={Colorbot: Interactive Smartphone Assistant for Color Blindness Detection and Realtime Assistance using LLM-based Chatbots and Machine Learning}, 
  year={2025},
  volume={},
  number={},
  pages={611-616},
  abstract={Color blindness, or color vision deficiency (CVD), impacts a large percentage of the population, making it difficult for them to differentiate between some colors. In addition to difficulties in everyday activities such as understanding traffic lights or recognizing colored objects, people with CVD usually experience social embarrassment, miscommunication, or even humiliation when incorrectly identifying colors in work, school, or social environments. This research puts forth an interactive smartphone app meant to grant color-blind individuals autonomy, self-esteem, and a dependable aid in facing everyday problems without needing the help of others. The application incorporates sophisticated color recognition and live help capabilities using the Ishihara test for customized CVD analysis. A tailored, interactive, real-time chatbot based on Gemini and Chat GPT-4 Vision models facilitates color recognition, while traffic light detection offers added road safety. The solution employs image processing techniques and machine learning models, developed using the Flutter framework and FastAPI backend, and is protected with Firebase Authentication. Using user testing and feedback, we assess the app's efficiency in offering accurate color detection and helpful support. The findings show that our method is a user-friendly, effective, and accessible solution for the color-blind, paving the way for future advancements in assistive technology.},
  keywords={Visualization;Accuracy;Image color analysis;Machine learning;Chatbots;Cameras;Real-time systems;Object recognition;Vision defects;Testing;Color blindness;realtime chatbot;android application;smartphone;AI assistant;Ishihara diagnosis;LLM},
  doi={10.1109/ICSCSA66339.2025.11171137},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10755614,
  author={Noreskal, Laura and Feuilloley, Guillaume and Charbel, Simon-Pierre},
  booktitle={2024 IEEE Thirteenth International Conference on Image Processing Theory, Tools and Applications (IPTA)}, 
  title={An AI Solution for Web Accessibility and Images Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This article details the research on web accessibility conducted at Capgemini's SogetiLabs. We introduce our project aimed at developing an automatic accessibility audit tool for website images. Our AI solution for web accessibility focuses on distinguishing between informative and decorative images in line with RGAA (Referenciel General d'Amelioration de I'Acessibilite) recommendations and then generating alternative text for informative images. To achieve this, we have established a comprehensive processing workflow. Additionally, we present initial experiments in image classification using Convolutional Neural Networks (CNNs) and YOLO's (You Only Look Once) model.},
  keywords={YOLO;Training;Analytical models;Computational modeling;Data models;Convolutional neural networks;Web sites;Artificial intelligence;Testing;Image classification;Machine Learning;Web Accessibility;RGAA;Computer Vision;Image processing;Classification;YOLO;CNN},
  doi={10.1109/IPTA62886.2024.10755614},
  ISSN={2154-512X},
  month={Oct},}@INPROCEEDINGS{10929822,
  author={Kim, Jee-Eun and Sahas, Gurung and Bessho, Masahiro},
  booktitle={2025 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={Toward Assisting Blind Individuals in Exploring Unfamiliar Indoor Environments Using Multimodal LLM and Smartphone LiDAR}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Blind individuals often face challenges accessing information about their surroundings when navigating unfamiliar indoor spaces. Recent advancements in artificial intelligence and smartphone-based sensing technology have opened up new possibilities for creating practical and cost-effective assistive solutions to enhance their environmental awareness and independence. In this paper, we investigate the integration of a cutting-edge large language model capable of processing multimodal inputs and outputs with smartphone-integrated LiDAR to assist blind travelers in scene understanding and obstacle avoidance in indoor spaces. We implement a smartphone application prototype that provides voice descriptions of user-captured photos and generates audio or vibrotactile feedback for nearby objects located in front of a smartphone camera. We present our initial findings from a preliminary study conducted with a blind expert in assistive technology, demonstrating our prototype's potential for enriching the indoor exploration experience of blind travelers. We also discuss opportunities and challenges regarding further improvements to our prototype.},
  keywords={Laser radar;Navigation;Large language models;Visual impairment;Prototypes;Assistive technologies;Indoor environment;Space exploration;Sensors;Usability;visual impairments;assistive technology;multimodal large language model;LiDAR;scene understanding;obstacle detection},
  doi={10.1109/ICCE63647.2025.10929822},
  ISSN={2158-4001},
  month={Jan},}@INPROCEEDINGS{10064545,
  author={Tiwary, Tejal and Mahapatra, Ranjendra Prasad},
  booktitle={2022 3rd International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT)}, 
  title={Web Accessibility Challenges for Disabled and Generation of Alt Text for Images in Websites using Artificial Intelligence}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={There are millions of visually impaired persons worldwide, and their numbers are continually increasing due to the ageing of the baby-boomer generation. The Internet may be extremely beneficial to blind individuals because it allows them to achieve things that they would otherwise be unable to complete without the assistance of others (such as reading mail or managing bank accounts). Education services and online shopping is also increasing day by day in the world of internet. So it is important that education websites, online shopping websites should be accessible to all regardless of their disabilities. This study describes challenges in accessibility of Indian Universities website especially for Visually Impaired. It also discusses its solutions by generating alternate text for images in websites through Artificial Intelligence techniques},
  keywords={Education;Production;Aging;Internet;Electronic commerce;Artificial intelligence;Task analysis;Artificial Intelligence;Web Accessibility;Person with Disability;WCAG;Alternative Texts;University website;Testing tool;Algorithms;optimization techniques;Deep Belief Network},
  doi={10.1109/ICICT55121.2022.10064545},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{11003300,
  author={Sung, Siyoon and Kim, Jemin and Kim, Junhyuk and Park, Sangeun and Jeong, Junho},
  booktitle={2025 17th International Conference on Knowledge and Smart Technology (KST)}, 
  title={A Study on a Framework for Initial Counseling for Vulnerable Populations in Welfare Blind Spots Based on LLM}, 
  year={2025},
  volume={},
  number={},
  pages={433-436},
  abstract={This study proposes a method to enhance the quality of counseling and automate initial counseling in welfare counseling systems by utilizing Large Language Models (LLMs) to correct errors occurring in the Speech-to-Text (STT) process. The Silver framework consists of an STT correction and evaluation model, a conversation model, and a summary model, aiming to simultaneously improve the flexibility and accuracy of counseling. In this study, the STT correction and evaluation model improved the quality of text correction, while the conversation model enabled natural conversation. Additionally, the summary model effectively organized and verified counseling content. Experimental results demonstrated that the STT correction model achieved 82 % accuracy in evaluation and 74 % accuracy in correction, while the conversation progress and summary models recorded 68 % and 92 % accuracy, respectively. These findings highlight the effective applicability of LLM-based technologies in welfare counseling.},
  keywords={Employee welfare;Silver;Accuracy;Large language models;Evaluation models;Oral communication;Chatbots;Speech to text;domain-specific chatbot;STT Correction;conversation summarization;natural language processing},
  doi={10.1109/KST65016.2025.11003300},
  ISSN={2473-764X},
  month={Feb},}@INPROCEEDINGS{10971607,
  author={Chen, Ziyu and Kelil, Selam B. and Naser, Mohammad Y. M. and Metcalfe, Jason S. and Bhattacharya, Sylvia},
  booktitle={SoutheastCon 2025}, 
  title={Human-Guided Artificial Intelligence (HGAI), a Framework for Overcoming AI's Blind Spots}, 
  year={2025},
  volume={},
  number={},
  pages={1417-1423},
  abstract={Artificial Intelligence (AI) systems frequently exhibit systematic blind spots, often referred to as hallucinations in Large Language Models (LLMs), posing risks in high-stakes applications such as autonomous systems, security, and military operations. This paper explores how human intuitive responses, along with conscious reasoning processes, can be integrated to mitigate AI blind spots and enhance decision-making effectiveness within Human-AI teams. We introduce Human-Guided Artificial Intelligence (HGAI) as a framework for achieving this goal. Specifically, we examine the role of System-1 intuitive processing, as captured through physiological signals such as electroencephalography (EEG) and electrocardiography (ECG), System-2 reasoning-based decision-making, and Multi-Modal Fusion (MMF) mechanisms while assessing the knowledge required to develop more reliable, context-aware, and ethically aligned intelligent decision-making systems for highly complex environments.},
  keywords={Systematics;Large language models;Decision making;Electrocardiography;Physiology;Electroencephalography;Reliability;Security;Artificial intelligence;Research and development;Artificial Intelligence (AI);Human-AI Teams;Physiological Signals;Multi-Modal Fusion (MMF)},
  doi={10.1109/SoutheastCon56624.2025.10971607},
  ISSN={1558-058X},
  month={March},}@INPROCEEDINGS{10989928,
  author={Faraji, Nastaran and Ansari, Sam and Alnajjar, Khawla A. and Albreem, Mahmoud A. and Mahmoud, Soliman and Hussain, Abir},
  booktitle={2025 IEEE 22nd International Multi-Conference on Systems, Signals & Devices (SSD)}, 
  title={Leveraging Generative Artificial Intelligence for Enhanced Detection and Classification of Diabetic Retinopathy}, 
  year={2025},
  volume={},
  number={},
  pages={1236-1241},
  abstract={Diabetic retinopathy (DR) is a severe microvascular complication of diabetes mellitus, causing progressive retinal damage and, if left untreated, leading to permanent blindness. Early detection is critical for effective treatment; however, the current manual diagnostic process performed by ophthalmologists is both time-intensive and reliant on highly trained professionals. This approach faces numerous challenges, including its inefficiency in detecting early-stage DR and the growing global prevalence of the disease. To address these limitations, the adoption of automated, efficient, and accurate detection methods is imperative. Recent advancements in deep learning (DL) algorithms have significantly improved the performance of commercial DR detection systems, enabling more precise diagnoses. This study examines the challenges associated with traditional DR detection and classification, particularly in the early stages, and explores the transformative potential of generative artificial intelligence (GAI). By leveraging technologies like chat generative pre-trained transformers (e.g., ChatGPT), GAI can enhance clinical workflows by generating detailed medical reports, educating patients, and analyzing extensive datasets to uncover actionable insights. This research highlights the integration of GAI as a promising avenue for revolutionizing DR detection and management.},
  keywords={Deep learning;Diabetic retinopathy;Generative AI;Manuals;Chatbots;Transformers;Retina;Classification algorithms;Medical diagnostic imaging;Image classification;chatgpt;deep learning algorithms;diabetic retinopathy detection;generative artificial intelligence;medical image classification},
  doi={10.1109/SSD64182.2025.10989928},
  ISSN={2474-0446},
  month={Feb},}@ARTICLE{11027076,
  author={Pedemonte, Giacomo and Leotta, Maurizio and Ribaudo, Marina},
  journal={IEEE Access}, 
  title={Improving Web Accessibility With an LLM-Based Tool: A Preliminary Evaluation for STEM Images}, 
  year={2025},
  volume={13},
  number={},
  pages={107566-107582},
  abstract={Ensuring equitable access to web-based visual content in Science, Technology, Engineering, and Mathematics (STEM) disciplines remains a significant challenge for visually impaired users. This preliminary study explores the use of Large Language Models (LLMs) to automatically generate high-quality alternative texts for complex web images in these domains, contributing to the development of an accessibility tool. First, we analyzed the outputs of various LLM-based image-captioning systems, selected the most suitable one (Gemini), and developed a browser extension, AlternAtIve, capable of generating alternative descriptions at varying verbosity levels. To evaluate AlternAtIve, we assessed its perceived usefulness in a study involving 35 participants, including a blind user. Additionally, we manually compared the quality of the outputs generated by AlternAtIve with those provided by two state-of-the-practice tools from the Google Web Store, using a custom metric that computes the quality of the descriptions considering their correctness, usefulness, and completeness. The results show that the descriptions generated with AlternAtIve achieved high quality scores, almost always better than those of the other two tools. Although conveying the meaning of complex images to visually impaired users through descriptions remains challenging, the findings suggest that AI-based tools, such as AlternAtIve, can significantly improve the web navigation experience for screen reader users.},
  keywords={Data visualization;Visualization;Dolphins;Guidelines;Browsers;STEM;Dogs;Anatomy;Transformers;Social networking (online);Web accessibility;image captioning;large language models;empirical evaluation},
  doi={10.1109/ACCESS.2025.3577519},
  ISSN={2169-3536},
  month={},}@ARTICLE{10226197,
  author={Chen, Yenming J. and Lin, Lung-Chang and Yang, Shu-Ting and Hwang, Kao-Shing and Liao, Chia-Te and Ho, Wen-Hsien},
  journal={IEEE Access}, 
  title={High-Reliability Non-Contact Photoplethysmography Imaging for Newborn Care by a Generative Artificial Intelligence}, 
  year={2023},
  volume={11},
  number={},
  pages={90801-90810},
  abstract={Long-term wiring on a newborn patient could be a disguise scene for parents. Unobtrusive and reliable monitoring without wiring can be a euphoric alternative for newborns and parents in obstetrics and gynecology (OB/GYN) incubation rooms. However, reliable and continuous non-contact surveillance in an incubation room is challenging. Therefore, a novel photoplethysmography imaging (PPGi) is developed specifically for baby skins through predictive adversarial adaptation and risk-sensitive generative synchronizer. Our artificial intelligence approach does not take blind guesses from input-output pairs. We apply an intelligent step to decouple the influence of fluctuated illumination through a generative algorithm of artificial intelligence. To boost skin detection performance, we capture those pixels with periodic variations and maximize the coherence of the extraction algorithm by the generative synchronizer. The periodic variations are matched by a synthesized pulse from the output PPGi signals through the control of a risk-sensitive filter to not over-compensate the illuminate variation. Based on the sensed pulsation, we synthesize the corresponding pulsation signals on the flight to identify the living skin in a spatiotemporal image sequence. We find that our skin classifier in risk-sensitive generative synchronizer effectively improves the quality of the resulting non-contact PPGi signal. Our algorithm produces substantial accuracy in the performance of PPGi reconstruction in the critical environment of newborn care. In the limited illustration of the incubation room, our non-contact PPGi can still achieve an average accuracy of 96.62%.},
  keywords={Skin;Pediatrics;Biomedical imaging;Image color analysis;Lighting;Synchronization;Filtering algorithms;Photoplethysmography;Surveillance;Prediction methods;Risk management;Thresholding (Imaging);Photoplethysmogram imaging (PPGi);non-contact surveillance;predictive adversarial adaptation;homographic filter;risk-sensitive generation;adaptive thresholding},
  doi={10.1109/ACCESS.2023.3307637},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11021747,
  author={Islam, Md. Repon and Sadi, Muhammad Sheikh},
  booktitle={2024 27th International Conference on Computer and Information Technology (ICCIT)}, 
  title={Adapting Open-source Multimodal Large Language Models for Enhanced Vision Assistants}, 
  year={2024},
  volume={},
  number={},
  pages={3560-3565},
  abstract={This paper presents a novel wearable system designed to enhance the lives of visually impaired individuals by integrating advanced vision understanding through Multimodal Large Language Models (MLLMs) and Optical Character Recognition (OCR). An open-source 8 billion parameters MLLM has been deployed on a consumer-grade GPU-enabled server which excels in interpreting visual data and delivering intelligent responses. The wearable device, powered by an Orange Pi 5B single-board computer (SBC), incorporates advanced OCR, Automatic Speech Recognition (ASR), Text-to-Speech (TTS), and an on-device 0.5B Large Language Model (LLM) for real-time interaction. Text recognition is achieved using PaddleOCR lite, optimized through a series of algorithms for edge performance. The on-device LLM handles offline question-answer or summarization tasks, while server integration provides a more powerful, multimodal understanding of the environment, based on user queries. This hybrid approach balances low-latency, real-time processing with the computational demands of advanced multimodal models, offering a scalable, cost-effective solution. Experimental evaluations demonstrate character recognition accuracy of 95.55% for OCR and efficient response generation at 18 tokens per second for LLM on the edge. The deployed MLLM scores 9.1 out of 10 on a carefully curated subset of the VizWiz VQA dataset, which consists of real-world image-question pairs captured by blind individuals across the globe based on their actual needs. The system represents a significant advancement in assistive technologies for the visually impaired community, leveraging a hybrid computing framework and state-of-the-art LMMs for robust, real-time support.},
  keywords={Visualization;Accuracy;Large language models;Optical character recognition;Assistive technologies;Real-time systems;Servers;Character recognition;Wearable devices;Thermal stability;Visually impaired;vision assistant;optical character recognition;multimodal LLM;PaddleOCR},
  doi={10.1109/ICCIT64611.2024.11021747},
  ISSN={2474-9656},
  month={Dec},}
