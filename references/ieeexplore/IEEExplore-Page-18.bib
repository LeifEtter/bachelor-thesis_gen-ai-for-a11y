@ARTICLE{10930450,
  author={Ali, Rashid and Khan, Fiaz Gul and Rehman, Zia Ur and Kwak, Daehan and Ali, Farman},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Enhanced Diabetic Retinopathy Detection: An Explainable Semi-Supervised Approach Using Contrastive Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={Diabetic retinopathy (DR) is a leading cause of blindness and represents a critical challenge to global vision health. Early detection is essential to preventing irreversible eye damage. Automated medical image analysis plays a pivotal role in enabling timely diagnosis. However, the development of robust diagnostic models is challenged by the scarcity of labeled data and the prevalence of imbalanced and unlabeled datasets. Semi-supervised learning offers a potential solution by leveraging unlabeled data to enhance model performance. However, it is often limited by challenges such as unreliable pseudo-labeling, the exclusion of low-confidence data, and biases introduced by imbalanced datasets. To address these limitations, we propose a novel semi-supervised learning framework for DR detection that combines similarity and contrastive learning. Our approach utilizes class prototypes and an ensemble of classifiers to generate reliable pseudo-labels for unlabeled data. Unlike traditional methods that discard unreliable samples, our framework integrates them into the training process using contrastive learning. This allows us to extract valuable features and improve overall performance. Furthermore, we enhance the model's transparency and interpretability by incorporating the explainable AI technique GradCAM, which provides insights into the model's predictions for specific images. We evaluated the proposed method on the publicly available Kaggle DR dataset for diabetic retinopathy classification. Experimental results demonstrate that our approach achieves improved performance compared to existing semi-supervised learning methods. It also effectively leverages unreliable samples, highlighting its potential to advance DR diagnosis.},
  keywords={Feature extraction;Diabetic retinopathy;Data models;Accuracy;Predictive models;Semisupervised learning;Reliability;Contrastive learning;Supervised learning;Medical diagnostic imaging;Diabetic Retinopathy;Semi-Supervised Learning (SSL);Contrastive Learning;Pseudo-Labeling;Medical Image Analysis},
  doi={10.1109/JBHI.2025.3551696},
  ISSN={2168-2208},
  month={},}@ARTICLE{9767546,
  author={Zhao, Zhongyong and Chen, Yu and Liu, Jiangnan and Cheng, Yingying and Tang, Chao and Yao, Chenguo},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Evaluation of Operating State for Smart Electricity Meters Based on Transformer–Encoder–BiLSTM}, 
  year={2023},
  volume={19},
  number={3},
  pages={2409-2420},
  abstract={The reliable operating state of smart electricity meters is significant in industrial applications. Faulty meters or meters in a poor measurement state will seriously impact both customers and stakeholders. However, the maintenance personnel now still use the manually periodic sampling inspection from the batch of smart electricity meters to evaluate the state of the entire batch, which has limitations of blindness, poor real time, inadequate, and insufficient examination. With the population of smart meters, it is feasible to evaluate the health condition of these devices with big data and artificial intelligence technology. One significant contribution of this article is first proposing an operating state evaluation method of smart electricity meters based on the transformer–encoder and bidirectional long-term and short-term memory. The evaluation indicators and preprocess of meters’ data are carefully selected. A deep neural network is constructed and trained, the experimental verification is carried out, and the performance of the proposed method is compared with that of other traditional methods. The results show that the average classification accuracy of the proposed neural network model is 99.5%. Besides, compared with conventional machine learning and deep learning models, the proposed model is suitable for the operation state evaluation of smart electricity meters. From the experimental result, the potential benefit of the proposed method is that it could improve the accuracy and robustness of state evaluation.1},
  keywords={Meters;Transformers;Inspection;Logic gates;Smart meters;Reliability;Real-time systems;Bidirectional long-term and short-term memory (BiLSTM);deep learning;neural network;smart electricity meters;state evaluation;transformer–encoder},
  doi={10.1109/TII.2022.3172182},
  ISSN={1941-0050},
  month={March},}@ARTICLE{9921304,
  author={Leonardo, Ricardo and Gonçalves, João and Carreiro, André and Simões, Beatriz and Oliveira, Tiago and Soares, Filipe},
  journal={IEEE Access}, 
  title={Impact of Generative Modeling for Fundus Image Augmentation With Improved and Degraded Quality in the Classification of Glaucoma}, 
  year={2022},
  volume={10},
  number={},
  pages={111636-111649},
  abstract={Glaucoma is a heterogeneous group of diseases characterised by cupping of the optic nerve head and visual field damage, starting with a progressive loss of vision that leads to permanent blindness. When diagnosed in time, Glaucoma can be delayed by adequate treatment. More efficient processes for diagnosis are being proposed, and the role of artificial intelligence in the field is growing. This work presents a pipeline to evaluate the impact of generative modelling in Computer-Aided Diagnosis (CADx) of Glaucoma based on Deep Learning, particularly focused on the optic disc region. The methodology relies on transforming retinal fundus images to improve and degrade their quality to augment the training data and assess the diagnostic performance. The objective evaluation of the proposed model based on Generative Adversarial Networks revealed quantitative and qualitative improvements in image quality. To support this, we propose a new model to evaluate the quality of fundus images, which can also be used within the pipeline to reject samples with lower image quality for diagnosis. Its performance surpassed related work, achieving a balanced accuracy of 0.929. Concerning Glaucoma CADx, the results obtained in public datasets point to a considerable gain in Sensitivity, Specificity, and Accuracy, achieving scores of 0.883 (+0.054), 0.957 (+0.019), and 0.931 (+0.031), respectively, after image data augmentation when compared with previous work targeted at offline inference in mobile devices. Considering the restriction of choosing simpler backbone networks that can run on edge devices, our findings support the importance of image quality diversity and realistic augmentation.},
  keywords={Optical imaging;Retina;Adaptive optics;Image quality;Biomedical optical imaging;Image segmentation;Deep learning;Computer aided diagnosis;Convolutional neural networks;Generative adversarial networks;Computer-aided diagnosis;convolutional neural networks;deep learning;fundus images;generative adversarial networks;glaucoma;image quality;image classification;retina},
  doi={10.1109/ACCESS.2022.3215126},
  ISSN={2169-3536},
  month={},}@ARTICLE{10309125,
  author={Jagadesh, B. N. and Karthik, M. Ganesh and Siri, D. and Shareef, S. K. Khaja and Mantena, Srihari Varma and Vatambeti, Ramesh},
  journal={IEEE Access}, 
  title={Segmentation Using the IC2T Model and Classification of Diabetic Retinopathy Using the Rock Hyrax Swarm-Based Coordination Attention Mechanism}, 
  year={2023},
  volume={11},
  number={},
  pages={124441-124458},
  abstract={Diabetic Retinopathy (DR) evaluations are increasingly being automated using artificial intelligence. Diabetes-related retinal vascular disease is a major cause of blindness and visual impairment worldwide. Therefore, automated DR detection devices would greatly aid in reducing visual impairment due to DR through early screening and treatment. Researchers have provided many techniques for picking out abnormalities in retinal images during the past several years. In the past, automated methods for diagnosing diabetic retinopathy required a human to extract information from retinal images before passing them on to a classifier. This study takes a novel two-pronged approach to automated DR classification to solve the issues. Due to the low positive instance percentage of existing asymmetric, we segment O.D.s and B.V.s with an enhanced version of an improved contoured convolutional transformer (IC2T). We develop a contoured optical disc (OD), a blood vessels (BV) detection module, and a dual convolutional transformer block that combines local and global contexts to make trustworthy associations. A second-stage Improved Coordination Attention Mechanism (ICAM) network is trained to recognize retinal biomarkers for DR such as microaneurysms (M.A.), haemorrhages (H.M.), and exudates (EX). With an average accuracy of 96%, 97%, and 98% on EyePACS-1, Messidor-2, and DIARETDB0, respectively, the suggested technique has proven itself to be at the field’s cutting edge. Comprehensive testing and comparisons to established methods support the proposed strategy.},
  keywords={Retina;Diabetic retinopathy;Lesions;Image segmentation;Transformers;Visualization;Diseases;Convolutional neural networks;Contoured detection module;diabetic retinopathy;dual convolutional transformer block;improved coordination attention mechanism;improved contoured convolutional transformer;optic disk segmentation},
  doi={10.1109/ACCESS.2023.3330436},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10417352,
  author={K.H, Wijesinghe. and U.K.T, Dilshan and K.B.G.L, Dilshan and M.A.U, Tharupathi and Kasthuriarachchi, Sanvitha and Rajapaksha, Samantha},
  booktitle={2023 5th International Conference on Advancements in Computing (ICAC)}, 
  title={Identification of Diabetic Related Eye Diseases Using Deep Learning}, 
  year={2023},
  volume={},
  number={},
  pages={786-791},
  abstract={Visual disability seems to be increasing and widespread worldwide. Current diagnostics require manual expertise for diagnosis. The advancement of Artificial Intelligence research for medical applications has been in focus in recent years. Fundus images and oct images are valuable sources of information to help ophthalmologists diagnose vision disorders or eye diseases. Early detection can improve the chances of recovery and prevent blindness. The application of an intelligent computer-based approach to classifying different eye disorders is extremely beneficial for both diagnosis and prevention. The proposed expert system for diagnosing diabetic macular edema, diabetic retinopathy, cataracts, and glaucoma diseases is discussed in this research study. It is based on deep learning image processing techniques. For the detection of eye diseases, software used retinal and OTC images. This study presents Yolo, UNET, customized CNN, and Auto ML model-based system for detecting eye diseases. For the tasks of disease identification and grading, the suggested methods each obtained more than 87% accuracy. The primary objective of this study is to achieve automated identification of eye disease images. This is accomplished through the implementation of a deep learning model alongside advanced image processing techniques, enabling the automatic classification of eyes into either a healthy or diseased category.},
  keywords={Deep learning;Diabetic retinopathy;Visualization;Image processing;Tomography;Eye diseases;Task analysis;diabetic retinopathy (DR);glaucoma;cataracts;optical coherence tomography (OCT);diabetic macular edema (DME);deep learning;retinal images;diabetes eye disease},
  doi={10.1109/ICAC60630.2023.10417352},
  ISSN={2837-5424},
  month={Dec},}@INPROCEEDINGS{10493798,
  author={K, Madhu and John, Shinu Mathew and Joseph, Asha and Abraham, Bejoy},
  booktitle={2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE)}, 
  title={A study on Retinopathy of Prematurity(ROP) Screening Using Deep Learning Approaches and a Blood Vessel Segmentation Framework for ROP Affected Eyes}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={The retina contains millions of photoreceptive cells called cones, rods, and blood vessels that nourish these cells. Retinopathy of prematurity is an eye disease affecting the retina of a prematurely born infant and is one of the leading causes of childhood blindness at a global level. Pediatric fundus images captured using retina imaging devices such as Retcams have been used by doctors for analyzing the retina and detecting various abnormalities present in the retina. Multiple stages in a single ROP image, underdeveloped retina vessels, immaturity of the macula and fovea,image quality issues like illuminations and artifacts present in an image make ROP disease detection, classification, and segmentation a difficult task for both an experienced practitioner and a computer-aided diagnostic system(CAD). Nowadays automated image analysis using artificial intelligence techniques is gaining popularity among practitioners and researchers. The objective of this survey is to do a systematic review of published works that use deep learning approaches for disease detection, classification, and segmentation in the context of Retinopathy of prematurity (ROP). This work presents a performance analysis and comparison of state-of-the-art deep learning architectures us ed for the above tasks. We also provide potential avenues for future research in ROP detection, classification, and segmentation using deep learning. After the study we proposed a segmentation framework called ROP-seg using a publicly available dataset for blood vessel segmentation that uses the combined power of Unet and Efficientnet architectures. The proposed system shows promising results in blood vessel segmentation of ROP affected eyes.},
  keywords={Deep learning;Surveys;Image segmentation;Pediatrics;Retinopathy;Cells (biology);Blood vessels;ROP;CNN;YOLO;ICROP;ET-ROP;A-ROP},
  doi={10.1109/ic-ETITE58242.2024.10493798},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10543549,
  author={R G, Devika and Ramachandran, Sivakumar and Shine, Linu and C V, Jiji},
  booktitle={2024 IEEE 9th International Conference for Convergence in Technology (I2CT)}, 
  title={A CNN-Based Framework with Focal Loss for Glaucoma Detection on Retinal Fundus Images}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Optic cup-to-disc ratio (CDR) evaluation using ophthalmoscopy is the most widely used diagnostic method for detecting Glaucoma, a leading cause for blindness. CDR is often determined manually by ophthalmologists, giving rise to variability due to differences in expertise, experience, and different interpretations of subtle image features. This paper presents a novel methodology for the efficient detection of early glaucoma by harnessing image features within a precisely defined Region of Interest (ROI) centered around the optic disc. The method first segments the data to localize the optic disc using a pre-trained U-Net architecture. The generated optic disc mask is then used to detect the optic disc region in the color fundus images, which are then used to train the modified ResNet50 architecture for glaucoma screening. The proposed model was trained on the images acquired from the Artificial Intelligence for Robust Glaucoma Screening (AIROGS) challenge, conducted during ISBI 2022. The trained model is then tested on three publicly available datasets, namely ACRIMA, Drishti−GS, and RIM-ONE. The obtained results demonstrate the efficacy of our model to generalize to unseen data distributions, as evidenced by its exemplary performance on the three diverse datasets that were completely unseen during training. We also addressed the class imbalance problem in the training dataset by integrating the focal loss function into the proposed pipeline. Moreover, the proposed model has shown comparable performance with the other state-of-the-art techniques in identifying glaucomatous eyes from normal, healthy images.},
  keywords={Glaucoma;Training;Optical losses;Solid modeling;Atmospheric modeling;Pipelines;Optical imaging;Glaucoma;U-Net;Supervised Learning;Optic Disc;ResNet50;Focal Loss},
  doi={10.1109/I2CT61223.2024.10543549},
  ISSN={},
  month={April},}@INPROCEEDINGS{10423472,
  author={Das, Soma and Ghosh, Sagarika and Kumar, Sourav and Ganguly, Gaurav and Devi, G Uma},
  booktitle={2023 7th International Conference on Electronics, Materials Engineering & Nano-Technology (IEMENTech)}, 
  title={Diabetes Detection System using Machine Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The increasing prevalence of diabetes in contemporary lifestyles underscores the need for swift and accurate diagnostic methods. Early detection is pivotal for effective management and prevention of complications, such as heart attacks, blindness, and kidney diseases, associated with diabetes mellitus. This research explores the application of machine learning in expediting the diagnosis of diabetes, offering a preliminary assessment that not only aids individuals but also supports governmental efforts in surveying diabetes prevalence in specific regions. Traditional diagnostic procedures often entail prolonged waiting periods and visits to diagnostic centers, impeding timely intervention. Leveraging machine learning and artificial intelligence, this study proposes an automated approach to analyze medical datasets, facilitating early detection and diagnosis of diabetes. The integration of advanced technologies promises to revolutionize diagnostic processes, offering a more efficient and timely response compared to conventional manual methods. This study demonstrates that the GridSearchCV-XGBClassifier performs better than other machine learning models, including K-Nearest Neighbours, Support Vector Machine, Random Forest, AdaBoost, Gradient Boosting, Voting Classifier-XGB-SVM, and XGBoost, with an accuracy of 91.41%.},
  keywords={Support vector machines;Machine learning;Manuals;Diabetes;Medical diagnosis;Medical diagnostic imaging;Random forests;Machine Learning;Supervised Learning;K Nearest Neighbor;Healthcare;Diabetes;Hyperparameter tuning;XGBoost},
  doi={10.1109/IEMENTech60402.2023.10423472},
  ISSN={2767-9934},
  month={Dec},}@INPROCEEDINGS{10863344,
  author={Imran, Shaik Mohammed and Geetha, Angelina},
  booktitle={2024 First International Conference for Women in Computing (InCoWoCo)}, 
  title={Evaluating the Effectiveness of Smote for Imbalanced Data Expansion and Its Impact on Classification Accuracy}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Big data classification technology has emerged together with artificial intelligence, offering valuable support for studies on auxiliary diagnostics in medicine. Medical big data is frequently unbalanced because of the various conditions in the many sample collections. Many popular learning methods have their classification performance hindered by the class-imbalance problem. While the SMOTE algorithm's random sample point generation feature could lead to an increase in the imbalance rate, the blindness of parameter selection and marginalization formation make its deployment problematic. This research seeks to remedy the situation by presenting a normal distribution-based SMOTE algorithm that is superior to its predecessor. The extra sample points will be distributed more fairly, which will prevent larger data portions from being underrepresented. Experiments show that when applied to imbalanced datasets like Pima, WDBC, WPBC, Ionosphere, and Breast Cancer, the new method outperforms the original SMOTE algorithm in terms of classification performance. This was demonstrated in Wisconsin. Our thorough testing also revealed that maintaining the distribution properties of the original data through the selection of appropriate parameters in the suggested method results in the best classification impact.},
  keywords={Learning systems;Accuracy;Reviews;Medical services;Big Data;Predictive models;Classification algorithms;Medical diagnostic imaging;Ionosphere;Testing;Evaluating;Effectiveness;Synthetic Minority Over-sampling Technique (SMOTE);Imbalanced Data Expansion;Classification Accuracy},
  doi={10.1109/InCoWoCo64194.2024.10863344},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10913675,
  author={Chakour, El-Mehdi and Mansouri, Anass and Ahaitouf, Ali},
  booktitle={2024 3rd International Conference on Embedded Systems and Artificial Intelligence (ESAI)}, 
  title={Transfer Learning for Severity and Stages Detection of Diabetic Retinopathy}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Diabetic retinopathy (DR) is a frequent consequence linked to diabetes mellitus, characterized by progressive damage to the retina that can result in visual impairment and, if not detected and addressed promptly, can lead to blindness. Depending on its severity, DR can be categorized into four stages, from mild to proliferative. The diagnosis of DR is typically based on the analysis of retinal photographs, a procedure that may be complex, lengthy, and susceptible to mistakes. The design of an automated solution for DR detection could be a helpful and interesting solution for accelerated diagnosis and improved accuracy. In this article, an artificial intelligence-based method using the powerful Deep learning models, VGG-16 and ResNet-50, leveraging the power of transfer learning, is proposed to detect DR into five categories: no DR, mild NPDR, moderate NPDR, severe non-proliferative diabetic retinopathy (NPDR), and proliferative diabetic retinopathy (PDR). An appropriate preprocessing chain has been first developed to improve retinal image quality. The data augmentation technique is then applied to enhance the amount of training data, improve model resilience, and reduce overfitting. Three separate datasets—APTOS, EyePACS, and IDRiD have been chosen to evaluate the proposed approach. The results are very promising; in fact when the developed model was fine-tuned. VGG-16 achieved an accuracy of $\mathbf{8 8 . 2 3 \%}$, while ResNet- 50 obtained a score of $\mathbf{8 7 . 7 4 \%}$. These findings emphasize the relevance and reliability of our proposed method for a precise classification of diabetic retinopathy stages, offering promising prospects for its future integration into innovative ophthalmological clinical applications.},
  keywords={Deep learning;Diabetic retinopathy;Accuracy;Transfer learning;Visual impairment;Training data;Retina;Data augmentation;Reliability;Resilience;Diabetic Retinopathy;Deep Learning;Transfer Learning;Image Preprocessing;Data Augmentation},
  doi={10.1109/ESAI62891.2024.10913675},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10724872,
  author={Roopashree, R and Ghosh, Rajkumari and Gaur, Murli Manohar},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={Exploring the Potential of Deep Learning in Diagnosing Eye Diseases}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Deep gaining knowledge is a powerful artificial intelligence approach that has been applied to numerous clinical areas, which include computer imagination and prescient, herbal language processing, medical picture analysis, and now, eye ailment diagnosis. Even as still in its infancy, deep studying has begun to show capability for assisting docs and medical practitioners in diagnosing eye diseases quickly and appropriately. With the aid of the use of deep knowledge of models to extract meaningful functions from patient-specific eye imaging, medical doctors are capable of making decisions with multiplied self-belief and accuracy. This text will speak about the contemporary state of research in the region of deep getting to know and its ability to improve the diagnosis of eye sicknesses. Deep getting-to-know architectures, especially convolutional neural networks (CNNs), have been used to effectively stumble on and classify diverse types of eye sicknesses in retinal pix. A CNN is a synthetic neural network that simplifies the technique of photo recognition and classification by extracting features inclusive of edges and patterns. These capabilities are used to make predictions approximately the affected person’s eye fitness. Every other utility of deep mastering is inside the analysis of diabetic retinopathy (DR). DR is a situation caused by excessive blood sugar stages and is the main motive of blindness among people with diabetes.},
  keywords={Deep learning;Neural networks;Sensitivity and specificity;Eye diseases;Feature extraction;Retina;Pattern recognition;Convolutional neural networks;Prognostics and health management;Medical diagnostic imaging;Knowledge;Powerful;Numerous;Clinical;Architectures;Excessive},
  doi={10.1109/ICCCNT61001.2024.10724872},
  ISSN={2473-7674},
  month={June},}@INPROCEEDINGS{11176473,
  author={Chempavathy, B and Kumari, M. Rethina and Mukilan, P. and Pandi, C. and Samsudeen Shaffi, S and Lawrence, Jinsha},
  booktitle={2025 8th International Conference on Circuit, Power & Computing Technologies (ICCPCT)}, 
  title={Enhanced Medical Image Analysis for Diabetic Eye Disease Detection Using RAM-Net Architecture}, 
  year={2025},
  volume={},
  number={},
  pages={1394-1399},
  abstract={Diabetic Retinopathy (DR) diagnosis is often automated with the use of artificial intelligence. One of the most prevalent and significant contributors to blindness and vision impairment is diabetes-related retinal vascular disease. In order to reduce vision loss brought on by DR and to support early screening and treatment, automated DR detection is extremely beneficial. In this study, retinal fundus images are used to classify diabetic eye condition using an automated deep learning framework. Adaptive Savitzky–Golay (ASG) filter based preprocessing is used to improve picture quality, while Mean Shift Segmentation is used to identify pertinent retinal regions. For reliable feature extraction, Directional Local Binary Pattern (DLBP) is used, which captures directional texture patterns that are essential for DR identification. The final classification is carried out by a Residual Attention MobileNet (RAM-Net) classifier, which combines attention processes, residual connections, and dilated convolutions for better feature representation. With a high classification accuracy of $94.37 \%$, the model was tested and deployed using Python on Diabetic Eye Disease Detection dataset. The proposed model demonstrates high efficiency and accuracy in distinguishing between DR and non-DR cases, making it a potential tool for automated retinal disease screening.},
  keywords={Image segmentation;Diabetic retinopathy;Accuracy;Computational modeling;Visual impairment;Retina;Feature extraction;Eye diseases;Reliability;Medical diagnostic imaging;Diabetic Retinopathy;Adaptive Savitzky–;Golay Filter;Mean Shift Segmentation;Directional Local Binary Pattern;RAM-Net},
  doi={10.1109/ICCPCT65132.2025.11176473},
  ISSN={},
  month={Aug},}@ARTICLE{11149662,
  author={Arrieta-Rodríguez, Eugenia and Araque-Gallardo, José and Peñaloza Barrios, Natalia and Luis Teherán-Forero, Oscar and Claudia Bonfante, María and de-la-Hoz-Franco, Emiro and Gamarra, Margarita and Escorcia-Gutierrez, José},
  journal={IEEE Access}, 
  title={Deep Learning for Glaucoma Classification and Grading: A Comprehensive Review on Fundus Imaging Approaches}, 
  year={2025},
  volume={13},
  number={},
  pages={163699-163730},
  abstract={Glaucoma is one of the leading causes of blindness worldwide and is characterized by progressive visual field loss due to optic nerve damage. Early detection is fundamental, yet it is often hindered by the asymptomatic nature of the disease in its initial stages. In response to this challenge, advanced techniques such as Deep Learning (DL) and computer vision are emerging as potential tools to revolutionize glaucoma diagnosis. This article aims to systematically evaluate the current state of artificial intelligence approaches for fundus image-based glaucoma detection and to identify trends, challenges, and opportunities. Following the PRISMA methodology, we conducted a comprehensive systematic review examining a total of 63 publications available in Scopus, ScienceDirect, IEEE, and Web of Science databases, available in English and published during the study period between 2020 and 2024. The review revealed key techniques in the critical stages of automated glaucoma detection. Convolutional neural networks dominated recent literature, with ResNet architectures achieving optimal performance (accuracy range: 82.37%-98.48%). For localization and segmentation, U-Net variants, attention-guided networks, and advanced ensemble approaches were prominent. In feature extraction, methods exploiting structural and textural metrics, wavelet-based transformations, and attention mechanisms showed considerable potential. Classification tasks benefited from Convolutional Neural Network (CNN) optimizations, attention-based architectures, hybrid models, and transformer frameworks, demonstrating high accuracy for both binary and multiclass glaucoma detection. Deep learning (DL) approaches have demonstrated significant potential for both binary and multiclass glaucoma classification from color fundus images. Key findings from this study include: 1) attention mechanisms and transformer architectures show superior performance in capturing subtle disease features, with accuracies exceeding 95%; 2) hybrid approaches combining multiple techniques achieve better generalization across datasets; 3) curriculum learning strategies improve multiclass severity grading accuracy; and 4) challenges persist in standardizing evaluation metrics and managing variations in data quality. Future research should focus on developing more robust architectures that can handle diverse image qualities and incorporate clinical knowledge into the learning process. Additionally, systematic analysis revealed critical implementation barriers: only 30%-40% of studies included external validation, with significant performance degradation on independent datasets; approximately 60%-70% relied on structure-only approaches, excluding essential visual field correlation for clinical decision-making.},
  keywords={Glaucoma;Optical imaging;Systematic literature review;Optical fibers;Integrated optics;Visualization;Optical fiber sensors;Diseases;Computer architecture;Image color analysis;Deep learning;color fundus images;glaucoma;image processing;ocular diseases;PRISMA},
  doi={10.1109/ACCESS.2025.3605819},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11031643,
  author={Ravindra, Dupaguntla and B, Sakthi Karthi Durai and S, Alex David and R, Madonna Arieth and B, Prabhu Shankar and M.A.Y, Peer Mohamed Appa},
  booktitle={2025 International Conference on Data Science and Business Systems (ICDSBS)}, 
  title={Enhancing Cataract Screening with Deep Learning Techniques}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Cataract is one of the primary causes of blindness. It is highly essential to diagnose them properly and in early stages for an efficient treatment procedure. Manual diagnostic methods applied by an ophthalmologist are not only time-consuming but also subjective in nature. This research proposes to develop an accurate and automated cataract diagnosis system by applying the deep learning methodology. The CNNs, including architectures like VGG16 and ResNet, are used to classify the medical eye images with high precision and efficiency. This paper considers several deep learning models to check which one can be the most efficient for the detection of cataract. The study's performance metrics for assessing the models’ dependability include accuracy, sensitivity, and specificity. The study offers a reliable, scalable, and effective method for early disease identification that can be extended into more extensive ophthalmological applications in the future. It also helps integrate artificial intelligence in medical diagnostics.},
  keywords={Cataracts;Deep learning;Accuracy;Sensitivity and specificity;Real-time systems;Reliability;Convolutional neural networks;Artificial intelligence;Medical diagnostic imaging;Standards;Deep Learning;Convolutional Neural Network;Cataract Detection;Medical Image Analysis;VGG16;ResNet;AI in Healthcare},
  doi={10.1109/ICDSBS63635.2025.11031643},
  ISSN={},
  month={April},}@ARTICLE{10006813,
  author={Zakaria, Noor Jannah and Shapiai, Mohd Ibrahim and Ghani, Rasli Abd and Yassin, Mohd Najib Mohd and Ibrahim, Mohd Zamri and Wahid, Nurbaiti},
  journal={IEEE Access}, 
  title={Lane Detection in Autonomous Vehicles: A Systematic Review}, 
  year={2023},
  volume={11},
  number={},
  pages={3729-3765},
  abstract={One of the essential systems in autonomous vehicles for ensuring a secure circumstance for drivers and passengers is the Advanced Driver Assistance System (ADAS). Adaptive Cruise Control, Automatic Braking/Steer Away, Lane-Keeping System, Blind Spot Assist, Lane Departure Warning System, and Lane Detection are examples of ADAS. Lane detection displays information specific to the geometrical features of lane line structures to the vehicle’s intelligent system to show the position of lane markings. This article reviews the methods employed for lane detection in an autonomous vehicle. A systematic literature review (SLR) has been carried out to analyze the most delicate approach to detecting the road lane for the benefit of the automation industry. One hundred and two publications from well-known databases were chosen for this review. The trend was discovered after thoroughly examining the selected articles on the method implemented for detecting the road lane from 2018 until 2021. The selected literature used various methods, with the input dataset being one of two types: self-collected or acquired from an online public dataset. In the meantime, the methodologies include geometric modeling and traditional methods, while AI includes deep learning and machine learning. The use of deep learning has been increasingly researched throughout the last four years. Some studies used stand-alone deep learning implementations for lane detection problems. Meanwhile, some research focuses on merging deep learning with other machine learning techniques and classical methodologies. Recent advancements imply that attention mechanism has become a popular combined strategy with deep learning methods. The use of deep algorithms in conjunction with other techniques showed promising outcomes. This research aims to provide a complete overview of the literature on lane detection methods, highlighting which approaches are currently being researched and the performance of existing state-of-the-art techniques. Also, the paper covered the equipment used to collect the dataset for the training process and the dataset used for network training, validation, and testing. This review yields a valuable foundation on lane detection techniques, challenges, and opportunities and supports new research works in this automation field. For further study, it is suggested to put more effort into accuracy improvement, increased speed performance, and more challenging work on various extreme conditions in detecting the road lane.},
  keywords={Lane detection;Road traffic;Machine learning;Autonomous vehicles;Feature extraction;Safety;Deep learning;Traffic control;Lane detection;autonomous vehicle;systematic literature review;geometric modelling;deep learning;machine learning},
  doi={10.1109/ACCESS.2023.3234442},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10210243,
  author={Perera, Pethigamage},
  booktitle={2023 IEEE/ACIS 23rd International Conference on Computer and Information Science (ICIS)}, 
  title={Uncovering the Malpractices in Publishing: A Global Review System Using Disruptive Technologies}, 
  year={2023},
  volume={},
  number={},
  pages={48-54},
  abstract={In recent years, traditional peer review has faced criticism due to the rise of fake peer-reviewers and a lack of transparency and accountability in the review process. This has led to instances where research ideas are stolen or published elsewhere while authors wait for reviewer feedback. Moreover, the competitive research culture has led to an increase in misconduct, making the need for a more effective peer review process even more crucial. While double-blind peer review has been suggested as a possible solution, it has also been found to have its flaws in practice (Thomas, 2015). As a result, there is a need to explore new theoretical and technological solutions that can improve the peer review process. One such solution is the use of disruptive technologies such as AI and Blockchain, which can “open up” peer review process to global review process by identifying potential expert reviewers in academia and industry. As a result, the risks of non-expert reviewers engaging in publishing malpractices can be mitigated. Additionally, there are a wide variety of ways in which this new process can be implemented, offering a range of options to improve the peer review process for the better. This paper proposes a technological solution to address publishing malpractices in review process.},
  keywords={Training;Industries;Information science;Ethics;Publishing;Blockchains;Disruptive technologies;Publishing;Open Access;Global Review;Publishing Malpractices;Fake Reviewers;Open Review},
  doi={10.1109/ICIS57766.2023.10210243},
  ISSN={},
  month={June},}@ARTICLE{10301678,
  author={Cao, Guodong and Wang, Zhibo and Feng, Yunhe and Dong, Xiaowei and Zhang, Zhifei and Qin, Zhan and Ren, Kui},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={Task-Free Fairness-Aware Bias Mitigation for Black-Box Deployed Models}, 
  year={2024},
  volume={21},
  number={4},
  pages={3390-3405},
  abstract={With AI systems widely deployed in societal applications, the fairness of these models is of increasing concern, for instance, hiring systems should recommend applicants impartially from different demographic groups, and risk assessment systems must eliminate racial inequity in the criminal justice system. Therefore, ensuring fairness in these models is crucial. In this paper, we propose Task-Free Fairness-Aware Adversarial Perturbation (TF-FAAP), a flexible approach for improving the fairness of black-box deployed models by adding perturbations on input samples that blind their fairness-related attribute information without modifying the model's parameters or structures. The proposed TF-FAAP consists of a discriminator and a generator to create universal fairness-aware perturbations for a variety of tasks. The former aims to distinguish fairnessrelated attributes, and the latter generates perturbations to make the discriminator's prediction distribution of fairness-related attributes uniform. To preserve the utility of perturbed samples, we maximize the mutual information between their representations and corresponding original samples, retaining more original samples' information. In addition, the perturbation generated by TF-FAAP has a high transferability, i.e., the perturbations learned on one dataset can also alleviate the unfairness of a model trained on a different dataset. The extensive experimental evaluation demonstrated the effectiveness and superior performance of our method.},
  keywords={Perturbation methods;Task analysis;Data models;Training;Closed box;Training data;Computational modeling;Adversarial learning;data utility;fairness;fairness-related attributes;mutual information},
  doi={10.1109/TDSC.2023.3328663},
  ISSN={1941-0018},
  month={July},}@INPROCEEDINGS{10183607,
  author={Baranwal, Amritanshu and Rohilla, Ajeet and Mishra, Asha Rani and Chauhan, Sansar Singh},
  booktitle={2023 International Conference on Computational Intelligence and Sustainable Engineering Solutions (CISES)}, 
  title={Pixellayer- A novel approach for stitching digital Images and Videos}, 
  year={2023},
  volume={},
  number={},
  pages={112-117},
  abstract={Image stitching and video stitching are the techniques to solve the problem of overlapping fields of view in images and videos using AI technologies. The creations of panoramic images using computer vision are primarily focused on creating panoramas using a library of images. The basic idea of this research is to propose a suitable method to meet the needs of stitching two or more images and further generate a 360° visualization of that stitched image. This method performs stitching of real-time images using live wireless cameras. Proposed approach ‘Pixellayer’ also removes the usage of a wide-angle wireless camera for stitching which makes this approach efficient in terms of cost. It includes the feature of frames per second and captures the live movement of an object in a real-time environment. It also solves the problem of blind spots of security wireless cameras and creating cinematic effects.},
  keywords={Wireless communication;Wireless sensor networks;Computer vision;Visualization;Streaming media;Cameras;Real-time systems;Image stitching;Panoramic Image;Feature Extraction;Image Blending;Homography Matrix;SURF;SIFT;Video stitching},
  doi={10.1109/CISES58720.2023.10183607},
  ISSN={},
  month={April},}@INPROCEEDINGS{10453757,
  author={Kamal, Ahmed Mohamed and Al-Ayyoub, Mahmoud and Deriche, Mohamed},
  booktitle={2023 24th International Arab Conference on Information Technology (ACIT)}, 
  title={A UAV-Assisted Multirobot Network for Surveillance and Tracking Applications: Design and Implementation}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={The developments of AI in the security sector and related fields have made it possible to design and implement advanced robotics systems working with UAVs (or Drones) for applications such as surveillance, monitoring, and scanning for intrusions or breaches. However, such systems usually exhibit some challenges, difficulties and limitations in real-life implementation as these need to perform pre-mapping by scanning the area first and learning the paths, then encoding the resulting maps. This costs time and is complicated to implement. In this work, we design, implement, and test a system of drone and ground vehicles connected over a wireless communication network. The system performs different monitoring tasks paired with detailed analysis of regions of concern by the robots. The integration of a UAV in a form of a drone, gives the system an advantage of a moving bird-eye view of the area to be scanned. This advantage helps detects breaches that might be blocked as well as eliminating the blind spot limitations of regular CCTVs. When such a breach is detected, it locates itself and sends the location to the nearest ground vehicle which will in turn automatically navigate to that location for more insight into the type of breach or fault. With this system, we provide an efficient solution to the complex problem of localization and path planning while maintaining accuracy and fast response time. The developed system is capable of detecting intrusions and reporting breaches.},
  keywords={Location awareness;Wireless communication;Costs;Surveillance;Land vehicles;Time factors;Multi-robot systems;multirobot communication network;TCP/IP;localization;routing},
  doi={10.1109/ACIT58888.2023.10453757},
  ISSN={2831-4948},
  month={Dec},}@INPROCEEDINGS{11114299,
  author={Kallabis, Leonie and Bertram, Timo and Rupp, Florian},
  booktitle={2025 IEEE Conference on Games (CoG)}, 
  title={Deceptive Game Design? Investigating the Impact of Visual Card Style on Player Perception}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={The visual style of game elements considerably contributes to the overall experience. Aesthetics influence player appeal, while the abilities of game pieces define their in-game functionality. In this paper, we investigate how the visual style of collectible cards influences the players' perception of the card's actual strength in the game. Using the popular trading card game Magic: The Gathering, we conduct a single-blind survey study that examines how players perceive the strength of AI-generated cards that are shown in two contrasting visual styles: cute and harmless, or heroic and mighty. Our analysis reveals that some participants are influenced by a card's visual appearance when judging its in-game strength. Overall, differences in style perception are normally distributed around a neutral center, but individual participants vary in both directions: some generally perceive the cute style to be stronger, whereas others believe that the heroic style is better.},
  keywords={Surveys;Visualization;Games;Gaussian distribution;Game perception;cognitive bias;visual style;collectible card games;games user research},
  doi={10.1109/CoG64752.2025.11114299},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{11171290,
  author={Karthikeyan, S. and Nithish, U. and Sanjay, S. and Sibiraj, T. and Vishnu, J.},
  booktitle={2025 5th International Conference on Soft Computing for Security Applications (ICSCSA)}, 
  title={Real-Time Detection of Heavy Vehicles on Hairpin Bends using YOLO Algorithm}, 
  year={2025},
  volume={},
  number={},
  pages={2170-2175},
  abstract={Hairpin curves along hilly roads are accident-prone areas with less visibility, particularly where heavy transport vehicles such as buses and trucks come from the other direction. The Ministry of Road Transport and Highways (MoRTH) reports that more than 11% of road accidents in India take place on sharp turns and ghat roads, and a major percentage is due to blind spot collisions. This article suggests a real-time crash prevention system through image processing based on the YOLO (You Only Look Once) object detection model. A camera placed strategically in front of the upward slope senses vehicles approaching, while an LCD display on the downward slope warns drivers if heavy vehicles like buses or trucks are approaching. As opposed to cars or motorcycles, these vehicles tend to encroach on the other lane when taking sharp turns, making them more susceptible to collision. Apart from automatic warning systems, there is an emergency switch positioned at the curve to allow passengers or onlookers to alert nearby control rooms in case of an accident. The system facilitates a proactive safety mechanism that couples AI-driven vision and human-activated emergency response, substantially eliminating the possibility and consequences of such accidents in high-risk areas.},
  keywords={YOLO;Road accidents;Prevention and mitigation;Switches;Cameras;Real-time systems;Liquid crystal displays;Road safety;Vehicles;Testing;YOLO Algorithm;Hairpin Bend Safety;Raspberry Pi;Real-Time Vehicle Detection;Road Accident Prevention;Embedded System;Traffic Alert System;Heavy Vehicle Detection;LCD Display Alerts;Emergency Stop Switch},
  doi={10.1109/ICSCSA66339.2025.11171290},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10730216,
  author={Kanjolia, Aryan and Tiwari, Anshu and Kumar, Bhishek and Mehra, Pawan Singh},
  booktitle={2024 1st International Conference on Advanced Computing and Emerging Technologies (ACET)}, 
  title={Harnessing ChatGPT in Financial Services: Opportunities and Challenges}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The introduction of ChatGPT in the banking sector has resulted in a massive revolution of technology engagement. However, the wide use of the technology calls for great ethical concerns. Let this piece be the window through which you peer into these policy issues: framing impartial results, creation of fake information to head financial decisions, invasion of privacy (and its bedfellow security), operations tacitness (and non-transparency due to GPT), job losses and not forgetting the legal jargon tsunami. This document emphasises action and not merely words; it challenges financial institutions to deal with the teething troubles before they are acted upon and consider laws enactment. The importance of an ethical framework gets highlighted—with events that range from continental drift, which might appear to be promising not only academic pastures on AI’s unexplored attachment with money but also bringing into focus blind spots in the real world. One of the most crucial factors of this kind is decreasing the bias by including more variables in the training datasets and measuring more frequently, which minimises the possible negative outcomes.},
  keywords={Training;Ethics;Privacy;Law;Banking;Chatbots;Tsunami;Security;Financial services;LLM;GPT 4.0;ChatGPT;NLM;GLBA},
  doi={10.1109/ACET61898.2024.10730216},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10911431,
  author={Sailaja, N. Venkata and Vinay, Alladi Shiva and Reddy, Devi and Reddy, Siddartha and Goud, Maturi Abhinay and Mustaq, Mohammed Suleman},
  booktitle={2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG)}, 
  title={Visual Narratives: Bridging Images and Stories Using NLP}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={In today’s visual world, the power of turning images into narrative experiences is still an untapped market. The problem is that there lacks a simple yet powerful system to automatically translate images into engaging, contextually rich stories — until now this has been the goal of "Image to Story Narration" work. The system uses state-of-the-art AI algorithms based on the deep learning and modern natural-language processing mechanisms to comprehend both textual input data from users as well visual content in order to build consistent, creative stories. It has an easy to use interface where users can upload images and pick a genre for their stories such as Horror, Action, Romance, Comedy, Historical or Sci-fi.The image-to-caption module uses deep learning to generate textual descriptions and the caption-to-story-module employs a GPT-2 model. This twin-module design guarantees tales that are contextually perfect yet also imaginative compelling. Preliminary results suggest the system is competitive in generating a variety of engaging narratives, spanning multiple genres. The resulting stories, both in terms of accuracy and creativity outperform plain factual captions showing a high level satisfaction towards the successful translation from visual to narrative form. In early testing, the system has been adept at producing original science fiction stories and highly factual historic tales. Applications for this "Image to Story Narration" work are broad, from improving sighted multimedia teaching aids and presentations to creating immersive storytelling experiences for the blind. The proposed system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.},
  keywords={Deep learning;Visualization;Translation;Image recognition;Government;Turning;Natural language processing;Artificial intelligence;Creativity;Testing;Image-to-Story Generation;Narrative AI;Deep Learning;Natural Language Processing (NLP);Image Captioning;Story Generation;Genre-Based Storytelling;Visual Content Analysis;Contextual Storytelling;Creative AI},
  doi={10.1109/ICTBIG64922.2024.10911431},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11176567,
  author={Sreehari, S and Menon, Arya R and Malavika, D and Navaneeth, C K and Devrag, R M and Rashmi, Manazhy},
  booktitle={2025 8th International Conference on Circuit, Power & Computing Technologies (ICCPCT)}, 
  title={VISTA: Vision-Integrated Smart Tracking Assistant}, 
  year={2025},
  volume={},
  number={},
  pages={1311-1317},
  abstract={Indoor navigation can be a serious challenge for visually impaired persons, so they require rather sophisticated assistive technology. VISTA: Vision-Integrated Smart Tracking Assistant is an up-and-coming technology that will address indoor way finding, object identification, real-time hazard detection, and scheduling for blind users. In addition, through computer vision, machine learning, and TTS technologies, this system could give an experience of being safer and a lot more independent. The primary features of VISTA include, but are not limited to, realtime object detection, hazard detection, voice-guided navigation, and dynamic reading of text. The system utilizes a camera-based system to find any obstacles and important landmarks while providing the user with auditory feedback in real time. In addition, a real-time reading module of signboard and danger signal has been integrated into the system to identify and read written text into speech thus warning the user for relevant announcements, room numbers, or warnings. An integrated keyboard-based schedule and reminder system has been produced for the benefit of daily users’ life management. This system, developed in Python, employs pyttsx 3 for offline text-to-speech conversion, the keyboard library for detection of user input, and the date time module for time-related retrieval. Users are also easily facilitated to their schedules, reminders, and notifications through very simple key press operations-the easy access and efficiency conferred on them. The system was tested to have real-world repeatability in accuracy, usability, and response time. Future enhanced features will include AI-powered obstacle detection, customized voice alerts, and cloud data synchronization for enhanced usability. System integrates QR based indoor navigation for precise and exact location tracking and management within buildings.},
  keywords={YOLO;Schedules;Indoor navigation;Optical character recognition;Feature extraction;Real-time systems;Hazards;Text to speech;Time factors;Usability;Text to Speech;Optical Character Recognition (OCR);Open-Source Routing Machine (OSRM) API;You Look Only Once (YOLO)},
  doi={10.1109/ICCPCT65132.2025.11176567},
  ISSN={},
  month={Aug},}@ARTICLE{11017464,
  author={Lewandowska, Emilia and Jezierska, Anna and Węsierski, Daniel},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Streamed optical flow adaptation from synthetic to real dental scenes}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Optical flow is integral to numerous video processing tasks such as stabilization. Although recent advancements in optical flow estimation have shown significant efficacy in general scenes, their applicability to challenging medical scenarios, which exhibit unique domain-specific visual phenomena, remains limited. Supervised learning methods facilitate the robust training of motion estimators. However, the absence of ground truth optical flow in many medical video-assisted applications poses a significant barrier to their progress. This is particularly evident in Video-Assisted Dentistry (VAD), where enhanced and continual vision could improve the educational, training, and fully operational dental workflows. Addressing this gap, we propose a new method for optical flow adaptation on real videos aided by a large dataset of synthetic dental videos with corresponding ground truth optical flows. Notably, the method employs a novel blind evaluation measure for unsupervised learning and incrementally fine-tunes optical flow neural networks. It facilitates their transition from synthetic to real dynamic dental scenes characterized by variable illumination, specular reflections, fluid dynamics, and sparse textures. In effect, this study demonstrates the effective streamed adaptation of multiple state-of-the-art optical flow networks to real medical scenes of video-assisted conservative dental treatments. Our code is available at https://github.com/camalab-ai/sofa-flow.},
  keywords={Optical flow;Dentistry;Training;Streaming media;Cameras;Estimation;Teeth;Lighting;Reflection;Adaptation models;label enhancement;instance selection;optical flow;synthetic video;unsupervised learning;blind evaluation;noisy labels;video-assisted dentistry},
  doi={10.1109/TMI.2025.3573265},
  ISSN={1558-254X},
  month={},}
