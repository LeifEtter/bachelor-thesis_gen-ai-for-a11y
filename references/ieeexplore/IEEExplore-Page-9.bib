@INPROCEEDINGS{10474275,
  author={Tyagi, Shruti and Pingulkar, Shriya and Tiwary, Aryaman},
  booktitle={2023 IEEE International Carnahan Conference on Security Technology (ICCST)}, 
  title={Detecting Diabetic Retinopathy using ResNet50 and Explainable AI}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Diabetic retinopathy, a common complication of diabetes, can lead to vision impairment and blindness if not detected early. Current diagnostic methods, involving manual examination of fundus images, can be time-consuming. This paper proposes an innovative approach that combines deep learning models with Explainable AI techniques, such as the SHAP method, to enhance the interpretability of diabetic retinopathy detection. This combination allows for more accurate identification of the disease stage and provides insights into the model's decision-making process, which can guide treatment and interventions. The proposed approach aims to improve the accuracy, fairness, transparency, and outcomes of diabetic retinopathy detection, fostering trust in AI models and enabling informed decision-making in healthcare settings.},
  keywords={Deep learning;Diabetic retinopathy;Explainable AI;Visual impairment;Medical services;Blindness;Predictive models;Diabetic Retinopathy;Deep learning;SHAP model;Explainable AI;Convolutional Neural Network (CNN);healthcare;Vision impairment},
  doi={10.1109/ICCST59048.2023.10474275},
  ISSN={2153-0742},
  month={Oct},}@INPROCEEDINGS{10452456,
  author={Hemavathy, J and Sabarika Shree, A and Priyanka, S and Subhashree, K},
  booktitle={2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)}, 
  title={AI Based Voice Assisted Object Recognition for Visually Impaired Society}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={Blind individuals face the significant challenge of not being able to clearly identify objects in their environment, relying heavily on their other senses to navigate the world. The absence of sight limits their ability to independently experience and understand the physical world around them. To address this fundamental issue, we propose the development of a voice-assisted application that aims to provide technical support and assistance to blind individuals through their smartphones. Unlike existing methods that often require in-person assistance, our application seeks to bridge the gap in the visually impaired community’s access to information and independence, leveraging current technological advancements. Our object detection model system combines multiple components, including optical character recognition and object and human recognition utilizing obstacle detection sensors. When the smartphone’s camera captures an image, it is then processed through a dataset that we have curated. The processing involves a matching process using the K-nearest neighbor algorithm in machine learning to identify and provide information about the objects and people within the image. This comprehensive approach not only aids in object identification but also enhances the user’s understanding of their surroundings. The primary goal of this application is to empower blind individuals to lead more independent lives. By providing real-time information about their environment, it equips them with the tools they need to make informed decisions and navigate the world with greater confidence. Ultimately, our voice-assisted application represents a significant step toward breaking down the barriers faced by the blind community, leveraging technology to enhance their quality of life and foster greater autonomy.},
  keywords={Navigation;Speech recognition;Sensor systems;Real-time systems;Object recognition;Optical sensors;Smart phones;Visually Impaired;Technology;Open-CV;CNN;Object recognition;Images},
  doi={10.1109/ICDSAAI59313.2023.10452456},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10679411,
  author={Johnson, Saul and Hassing, Remco and Pijpker, Jeroen and Loves, Rob},
  booktitle={2024 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={A Modular Generative Honeypot Shell}, 
  year={2024},
  volume={},
  number={},
  pages={387-394},
  abstract={In this work, we present Limbosh, a generative honeypot shell written in Python that places attackers in a conversation with a large language model (LLM) configured to behave like a real shell. The use of generative AI in place of traditional honeypot shell software admits the development of arbitrary honeypot configurations by adjusting the prompt used to seed the LLM context. Key features of Limbosh include: a flexible prompt generation system based on text templating and reusable prompt fragments; the ability to make use of arbi-trary LLMs; sophisticated prompt injection mitigation measures; and a highly modular and configurable architecture permitting straightforward expansion of its feature set and enhancement of its capabilities. To demonstrate its utility and practicality, we ran a single-blind, within-subjects study of the interaction of four cybersecurity professionals with Limbosh compared to a control shell. We find that Limbosh is capable of convincingly emulating real shell software, even when faced with professional users. We present our experimental results, and make the Limbosh software itself open-source and freely available.},
  keywords={Prevention and mitigation;Large language models;Computer architecture;Oral communication;Software;Software measurement;Prompt engineering;honeypot;shell;generative artificial intelligence;large language models},
  doi={10.1109/CSR61664.2024.10679411},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11019512,
  author={Abdul Syed, S. Syed and Karthiga, I. and Preeti and Nazreen, A. and Sulthana rashya Begam, A. and Joy Kirupa, P.},
  booktitle={2025 International Conference on Computing and Communication Technologies (ICCCT)}, 
  title={AI-IoT Based Smart Helmet with Frequency Based Vehicle Control}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents an AI - IoT based smart helmet system with machine learning enhancements aimed at improving motorbike safety. The helmet integrates the LeNet Convolutional Neural Network (CNN) to monitor rider fatigue by analyzing facial features in real-time, issuing alerts when signs of drowsiness, such as frequent blinking, are detected. The system also includes a vibration sensor to detect accidents and initiate emergency protocols, as well as a blind spot detection system using ultrasonic sensors to monitor a 180-degree area for potential collisions. Communication between the helmet and vehicle is facilitated by Zigbee technology, ensuring reliable real-time data exchange. This integrated solution addresses critical safety issues such as driver fatigue, accident detection, and blind spot awareness, offering a comprehensive approach to enhance rider safety.},
  keywords={Vibrations;Head;Fatigue;Acoustics;Road safety;Safety;Convolutional neural networks;Monitoring;Vehicles;Accidents;Smart helmet;IoT;machine learning;LeNet CNN;driver fatigue detection;accident detection;vibration sensor;ultrasonic sensor;blind spot detection},
  doi={10.1109/ICCCT63501.2025.11019512},
  ISSN={2995-3197},
  month={April},}@INPROCEEDINGS{10533927,
  author={Selvan, Shirley and Stella, Johnsi and B, Keerthana and Nikitha, Nimmagadda V Gayathri Sai},
  booktitle={2024 International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS)}, 
  title={Smart Shopping Trolley based on IoT and AI for the Visually Impaired}, 
  year={2024},
  volume={},
  number={},
  pages={132-138},
  abstract={A smart shopping trolley can help the visually impaired and blind customers to shop without the need of assistance. Shopping malls that provide such trolleys attract more customers. The smart shopping trolley proposed in this research emphasize on Artificial intelligence and is embedded with RFID technology. Tesseract optical character recognition algorithm extracts the name and cost of the product. The E-speak library reads the name and cost of the product to the visually impaired as an audio message. The smart trolley makes use of cameras, Ultrasonic (US) sensors, DC motor, Motor Drivers, interfaced with a Raspberry Pi to make the system work. An US sensor finds the distance of an obstacle, if any, in the person’s path and alerts the user frequently through an audio. This shopping trolley assists blind people making it easier for them to shop and indeed enjoy shopping. The system saves customer time thereby promoting business sales.},
  keywords={Headphones;Presses;Optical switches;Optical character recognition;Blindness;Cameras;Sensor systems;OCR;E-speak;Tesseract algorithm;visually impaired;Raspberry Pi},
  doi={10.1109/ICC-ROBINS60238.2024.10533927},
  ISSN={},
  month={April},}@INPROCEEDINGS{10620973,
  author={Ghose, Sanchita and Roy, Krishna and Prevost, John J.},
  booktitle={2024 19th Annual System of Systems Engineering Conference (SoSE)}, 
  title={SoundEYE: AI-Driven Sensory Augmentation System for Visually Impaired Individuals through Natural Sound}, 
  year={2024},
  volume={},
  number={},
  pages={147-152},
  abstract={Sensory augmentation is required for the visually impaired person (VIP) to compensate for the loss of vision and enhance their perception of the environment. Existing solutions for navigation aids have limited flexibility to respond to real-time changes in actions happening in a visual scene, limiting the blind individual to perceive the surroundings comprehensively and spontaneously. In this paper, we introduce an AI-driven sensory augmentation system (SoundEYE) for VIPs that will produce sound from visuals utilizing the advantages of employing natural auditory guidance which is readily comprehensible to the user. The proposed SoundEYE system incorporates a novel sound generation AI model combining visual action recognition and adversarial audio synthesis networks that can automatically generate representable sound for critical situations. This leads to a significant impact on VIPs to comprehend and deal with real-world events. Unlike the existing sound aid solutions, SoundEYE is capable of analyzing a complex scene with the presence of multiple sound sources and generating representable synchronous sound through capturing audio-visual synchronization with moving objects. In addition, we propose a comprehensive evaluation methodology to assess the performance of the system including an empirical study plan to analyze the impact of natural sound over vocal auditory cues for the VIP.},
  keywords={Deep learning;Visualization;Limiting;Navigation;Real-time systems;Synchronization;Modeling;sensory augmentation;deep learning;sound generation;visually impaired person;video recognition},
  doi={10.1109/SOSE62659.2024.10620973},
  ISSN={2835-3161},
  month={June},}@ARTICLE{10942479,
  author={Yang, Yuzhi and Zhang, Zhaoyang and Chen, Zirui and Yang, Zhaohui and Liu, Lei and Huang, Chongwen and Debbah, Mérouane},
  journal={IEEE Transactions on Wireless Communications}, 
  title={A Hybrid Inference Architecture Incorporating Neural Network With Belief Propagation for AI Receivers}, 
  year={2025},
  volume={24},
  number={8},
  pages={6365-6381},
  abstract={Conventional wireless communication receivers guided by Bayesian inference methods need to know the exact statistical relationship among variables, which is hard to obtain accurately in wireless contexts, thus limiting the system performance. The recently emerging artificial intelligence (AI)-empowered algorithms have shown striking performances in exploring the implicit relationship among variables with specially designed Neural Networks (NNs). Therefore, it is preferable to integrate NNs with BPs in receiver design. Such approaches also leverage NNs’ lack of reasoning ability in large state spaces and traditional BPs’ lack of reasoning depth. However, conventional receiver modules are usually designed based on explicit mathematical derivations, which cannot be easily substituted with data-driven NNs as they may break the overall inner relationship of the algorithm. In this paper, we investigate how to beneficially incorporate NNs into the existing Belief Propagation (BP)-based framework, taking the traditional semi-blind estimation problem in an Orthogonal Frequency-Division Multiplexing (OFDM) receiver as an example. Unlike existing deep-unfolding approaches, we simply utilize NNs as embedded functional units rather than duplicate denoising modules. Through qualitative discussions and numerical results, we illustrate the characteristics, principles, and differences of our proposed architecture compared to the traditional BP framework and show the dramatic performance improvements brought by incorporating NNs with BP in this well-investigated problem. Recalling that the state evolution of NNs is different from that of traditional BP methods, we give some new insights and design principles which are somehow counterfactual. We also raise some open issues on the incorporated framework.},
  keywords={Artificial neural networks;Estimation;Iterative methods;Transceivers;Receivers;MIMO;Cognition;Artificial intelligence;Wireless sensor networks;Correlation;Artificially intelligent (AI) receiver;belief propagation;neural networks;deep unfolding;semi-blind estimation},
  doi={10.1109/TWC.2025.3552818},
  ISSN={1558-2248},
  month={Aug},}@INPROCEEDINGS{11102467,
  author={Reddiar Seetharaman, Karthika Murugandi},
  booktitle={2025 International Conference on Networks and Cryptology (NETCRYPT)}, 
  title={Automated Eye Disease Detection of Diabetic Retinopathy Using Artificial Intelligence on Fundus Images}, 
  year={2025},
  volume={},
  number={},
  pages={590-595},
  abstract={Diabetic retinopathy (DR) is a diabetes-related eye disease that is a major contributor to blindness in diabetic people. This paper introduces an AI-powered automated method for DR identification utilizing the publicly available MESSIDOR dataset. The dataset contains 1200 fundus pictures, 654 DR cases spanning different severity levels, and 546 normal photos. The approach included a number of preprocessing processes to prepare the images for disease feature extraction, including scaling, cropping, noise reduction, CLAHE-based contrast enhancement, and segmentation. To address class imbalance, SMOTE was applied, ensuring uniform class distribution. A ResNet50 deep learning model was proposed and fine-tuned using hyperparameter optimization, including the Adam optimizer and categorical cross-entropy loss. A model's performance was evaluated and compared with existing models, such as InceptionV3, KNN, AlexNet, DenseNet-201, and CNN, using accuracy, sensitivity, and specificity metrics. The proposed ResNet50 model outperformed others, achieving a remarkable accuracy of 99.02 %, sensitivity of 98.32 %, and specificity of 99.04 %, demonstrating its strong potential in accurately detecting and classifying DR stages for clinical applications.},
  keywords={Diabetic retinopathy;Image segmentation;Accuracy;Sensitivity;Noise reduction;Sensitivity and specificity;Nearest neighbor methods;Eye diseases;Hyperparameter optimization;Residual neural networks;Eye Disease detection;diabetic retinopathy;fundus images;Image Segmentation;DL;MESSIDOR dataset},
  doi={10.1109/NETCRYPT65877.2025.11102467},
  ISSN={},
  month={May},}@ARTICLE{10539012,
  author={Ahnaf Alavee, Kazi and Hasan, Mehedi and Hasnayen Zillanee, Abu and Mostakim, Moin and Uddin, Jia and Silva Alvarado, Eduardo and de la Torre Diez, Isabel and Ashraf, Imran and Abdus Samad, Md},
  journal={IEEE Access}, 
  title={Enhancing Early Detection of Diabetic Retinopathy Through the Integration of Deep Learning Models and Explainable Artificial Intelligence}, 
  year={2024},
  volume={12},
  number={},
  pages={73950-73969},
  abstract={Humans can carry various diseases, some of which are poorly understood and lack comprehensive solutions. Such a disease can exists in human eye that can affect one or both eyes is diabetic retinopathy (DR) which can impair function, vision, and eventually result in permanent blindness. It is one of those complex complexities. Therefore, early detection of DR can significantly reduce the risk of vision impairment by appropriate treatment and necessary precautions. The primary aim of this study is to leverage cutting-edge models trained on diverse image datasets and propose a CNN model that demonstrates comparable performance. Specifically, we employ transfer learning models such as DenseNet121, Xception, Resnet50, VGG16, VGG19, and InceptionV3, and machine learning models such as SVM, and neural network models like (RNN) for binary and multi-class classification. It has been shown that the proposed approach of multi-label classification with softmax functions and categorical cross-entropy works more effectively, yielding perfect accuracy, precision, and recall values. In particular, Xception achieved an impressive 82% accuracy among all the transfer learning models, setting a new benchmark for the dataset used. However, our proposed CNN model shows superior performance, achieving an accuracy of 95.27% on this dataset, surpassing the state-of-the-art Xception model. Moreover, for single-label (binary classifications), our proposed model achieved perfect accuracy as well. Through exploration of these advances, our objective is to provide a comprehensive overview of the leading methods for the early detection of DR. The aim is to discuss the challenges associated with these methods and highlight potential enhancements. In essence, this paper provides a high-level perspective on the integration of deep learning techniques and machine learning models, coupled with explainable artificial intelligence (XAI) and gradient-weighted class activation mapping (Grad-CAM). We present insights into their respective accuracy and the challenges they face. We anticipate that these insights will prove valuable to researchers and practitioners in the field. Our ambition is that this in-depth study and comparison of models will inform and inspire future research endeavors, ultimately leading to enhanced disease detection in medical imaging and thereby assisting healthcare professionals.},
  keywords={Retina;Support vector machines;Diabetic retinopathy;Biological system modeling;Diseases;Deep learning;Computational modeling;Diabetic retinopathy;transfer learning;CNN;Xception;inception;Grad-CAM},
  doi={10.1109/ACCESS.2024.3405570},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11064275,
  author={Narendra, Lalam and Babu, Bommina Surendra and Naga Durga Vinay, Bavaraju Venkata and Tirumani, Gopikanth and Manohar, Chanamolu},
  booktitle={2025 International Conference on Electronics, Computing, Communication and Control Technology (ICECCC)}, 
  title={Smart Glasses with Voice Assistance and GPS for Independent Mobility of the Blind People}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={The study establishes smart glasses embedded with AI voice assistance that covers obstacle detection and object detection, OCR (Optical Character Recognition), and GPS tracking for visually impaired individuals. The smart glasses improve mobility and independence by providing real- time obstacle and object detection. The audio-guided navigation in the glasses helps the users overcome different environments, whether indoor, public transport, or city streets. These glasses use the internal sensors to scan the surroundings for possible hazards and inform the user to take action when navigating. GPS tracking allows monitoring a location, enhancing safety. The AI voice assistance makes the interaction intuitive and easy, and OCR detects text in images, allowing the reader to read signs or documents through audio feedback. This innovative technology enables visually impaired persons to traverse new and difficult settings with minimal assistance, promoting confidence and autonomy. The great deal of new input methods integrated into wearable technology pushes the assistive technology field forward in terms of greatly enhancing the quality of life for users that are visually impaired. With the integration of AI voice assistance, GPS tracking, Optical Character Recognition, Object detection, and Obstacle detection, Smart Glasses provide real-time navigation to improve accessibility and promote independence for the users.},
  keywords={Accuracy;Navigation;Optical character recognition;Visual impairment;Object detection;Assistive technologies;Real-time systems;Artificial intelligence;Smart glasses;Global Positioning System;Smart glasses;Visual impairment;voice assistance;GPS navigation;Obstacle detection;Object detection autonomous mobility;Assistive technology;Real-time guidance;accessibility},
  doi={10.1109/ICECCC65144.2025.11064275},
  ISSN={},
  month={May},}@ARTICLE{11153825,
  author={Sun, Ruijie and Hamilton-Fletcher, Giles and Faizal, Sahil and Feng, Chen and Hudson, Todd E. and Rizzo, John-Ross and Chan, Kevin C.},
  journal={IEEE Open Journal of Engineering in Medicine and Biology}, 
  title={Training Indoor and Scene-Specific Semantic Segmentation Models to Assist Blind and Low Vision Users in Activities of Daily Living}, 
  year={2025},
  volume={6},
  number={},
  pages={533-539},
  abstract={Goal: Persons with blindness or low vision (pBLV) face challenges in completing activities of daily living (ADLs/IADLs). Semantic segmentation techniques on smartphones, like DeepLabV3+, can quickly assist in identifying key objects, but their performance across different indoor settings and lighting conditions remains unclear. Methods: Using the MIT ADE20K SceneParse150 dataset, we trained and evaluated AI models for specific indoor scenes (kitchen, bedroom, bathroom, living room) and compared them with a generic indoor model. Performance was assessed using mean accuracy and intersection-over-union metrics. Results: Scene-specific models outperformed the generic model, particularly in identifying ADL/IADL objects. Models focusing on rooms with more unique objects showed the greatest improvements (bedroom, bathroom). Scene-specific models were also more resilient to low-light conditions. Conclusions: These findings highlight how using scene-specific models can boost key performance indicators for assisting pBLV across different functional environments. We suggest that a dynamic selection of the best-performing models on mobile technologies may better facilitate ADLs/IADLs for pBLV.},
  keywords={Semantic segmentation;Training;Smart phones;Biomedical imaging;Accuracy;Visualization;Assistive technologies;Artificial intelligence;YOLO;Computational modeling;Basic and instrumental activities of daily living;mobile technology;persons with blindness or low vision;semantic segmentation models;visual assistive technology},
  doi={10.1109/OJEMB.2025.3607816},
  ISSN={2644-1276},
  month={},}@INPROCEEDINGS{11042688,
  author={Bhuvanachandran, Binoy Sugatha and Bamini, A. M Anusha and Brindha, D.},
  booktitle={2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)}, 
  title={AI-Driven Diabetic Retinopathy Detection: IoT and Deep Learning for Retinal Fundus Image Analysis}, 
  year={2025},
  volume={1},
  number={},
  pages={1-6},
  abstract={Diabetic retinopathy is one of the main causes of blindness in diabetics and early detection and classification are essential for both prevention and successful treatment. The subjective and time-consuming nature of traditional diagnostic techniques emphasizes the need for automated, accurate, and efficient systems. Using IoT devices that capture real-time retinal fundus images and combining them with patient metadata including age, gender, and blood glucose levels, this study suggests a novel deep learning-based method for the detection and classification of DR. The suggested approach combines a number of advanced techniques, including U-Net for image segmentation, InceptionV3 for feature extraction, and Vision Transformer for classification. To improve quality and eliminate noise, the images are preprocessed using Median Filtering and CLAHE. Evaluation was conducted using the IDRiD, which consists of 516 fundus images with DR grades ranging from 0 (no DR) to 4 (proliferative DR). With an accuracy of 98.55%, precision of 97.89%, recall of 98.01%, and F1 score of 97.94%, the experimental results show the effectiveness of the model. This study offers a possible way to prevent blindness in diabetic patients by demonstrating the potential of integrating deep learning with IoT devices for the early and precise diagnosis of DR.},
  keywords={Deep learning;Diabetic retinopathy;Image segmentation;Computer vision;Accuracy;Retina;Transformers;Feature extraction;Real-time systems;Internet of Things;Diabetic retinopathy;Internet of Things;Deep learning;Severity;Accuracy;Retinal fundus images},
  doi={10.1109/ICTEST64710.2025.11042688},
  ISSN={},
  month={April},}@INPROCEEDINGS{10335803,
  author={Baig, Muhammad Abrar and Hussain, Talat and Khan, Muhammad Dildar and Malik, Abdul and Akram, Faraz},
  booktitle={2023 International Conference on IT and Industrial Technologies (ICIT)}, 
  title={Smartphone-Based AI Detection of Ocular Diseases}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={Eye diseases are a major global health issue that necessitates prompt detection and treatment to avoid irreversible vision loss. In this research, we describe a unique method for the automatic detection of ocular diseases using a smartphone. Our technology makes advantage of a smartphone’s integrated flash and camera for real-time detection, making it available and practical for consumers. Our strategy intends to address cataracts’ high incidence as Pakistan’s main source of blindness. First, the haar cascade classifier is used in the detection process to identify the eye region from facial images taken by the smartphone. To isolate the important features for further processing, this eye region is then further segmented out. A custom Convolution Neural Network (CNN) is created and trained on a diverse annotated image data set in order to categorize diseases. The CNN model is capable of identifying and classifying ocular conditions like cataracts, conjunctivitis, and healthy eyes. We have created an Android app using Java that incorporates the trained CNN model for remote and real-time applications. Users receive fast feedback on the state of their ocular health thanks to the application’s real-time eye image capturing and instant disease classification. After a thorough examination, our approach was found to be very accurate and effective at detecting eye diseases, addressing the urgent problem of cataract-related blindness in Pakistan.},
  keywords={Cataracts;Image segmentation;Visualization;Java;Sensitivity;Neural networks;Blindness;Convolutional Neural Network;Eye Segmentation;haar cascade classifier;Android app;Eye diseases;ocular diseases;smartphone},
  doi={10.1109/ICIT59216.2023.10335803},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10257110,
  author={Ren, Jiaxuan and Wang, Shaorong and Yang, Youhang and Zhang, Yanyan and Wei, Zheng and Zheng, Chao and Li, Jingsong and Chen, Yunan},
  booktitle={2023 3rd International Conference on Energy Engineering and Power Systems (EEPS)}, 
  title={Intelligent Operation and Inspection Software Based on Edge Computing Concept and Artificial Intelligence Technology}, 
  year={2023},
  volume={},
  number={},
  pages={879-885},
  abstract={A new intelligent operation and inspection software based on the concept of edge computing and artificial intelligence technology is proposed for the physical and operational architecture of substation electrical equipment intelligent inspection, and a human-computer interaction interface is designed for the convenience of operation and maintenance personnel, which effectively avoids the problems of poor real-time and monitoring blind areas of off-line inspection and conventional energized inspection, and solves the defects of cloud computing and manual inspection. The key technologies included in the realization of intelligent operation and inspection software functions are: the design of physical and operational architecture, and the analysis of structured and unstructured charged inspection data. The intelligent operation and inspection software is stable and reliable, which can greatly improve the efficiency of inspection and the intelligence level of the substation.},
  keywords={Substations;Neural networks;Computer architecture;Inspection;Maintenance engineering;Software;Power system reliability;intelligent operation and inspection software;edge computing;artificial intelligence technology;structured data;unstructured data},
  doi={10.1109/EEPS58791.2023.10257110},
  ISSN={},
  month={July},}@INPROCEEDINGS{10333170,
  author={Bertram, Timo and Fürnkranz, Johannes and Müller, Martin},
  booktitle={2023 IEEE Conference on Games (CoG)}, 
  title={Weighting Information Sets with Siamese Neural Networks in Reconnaissance Blind Chess}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Research in Game Artificial Intelligence distinguishes between fully observable, perfect-information games and imperfect-information games, which hide part of the game’s full information. In games with imperfect information, all possible game states that are consistent with a player’s currently available information about the progress of the game are called the information set for that player. This information set can be used for multiple purposes such as determining the expected outcome of a certain move by evaluating it on all possible states in the information set. While in theory there is no way to distinguish states within an information set, players can use experience and other context information to estimate which states are the most likely. In this paper, we estimate a probability distribution over an information set from historic data such that we can assign a weight to each individual state. We achieve this by training a Siamese neural network with triplets of comparisons between different states in the information set given the context of the previously obtained information. A first evaluation in the game of Reconnaissance Blind Chess shows that we can learn to identify the one true game state in a large information set with high probability. In addition, when used within a naively constructed RBC agent, this approach shows promising gameplay performance. At the time of writing, a simple agent based on the Siamese neural network is ranked #6 of all agents on the public RBC leaderboard.},
  keywords={Training;Neural networks;Games;Reconnaissance;Writing;Probability distribution;Artificial intelligence;Siamese Neural Networks;Imperfect Information Games;Artificial Intelligence;Reconnaissance Blind Chess},
  doi={10.1109/CoG57401.2023.10333170},
  ISSN={2325-4289},
  month={Aug},}@ARTICLE{10670470,
  author={Pang, Lihui and Tang, Yilong and Jiang, Kaili and Zhang, Wenwei},
  journal={IEEE Transactions on Aerospace and Electronic Systems}, 
  title={Sinusoidal Similarity Among Electromagnetic Reconnaissance Signals and Its Application in Blind Signal Separation}, 
  year={2025},
  volume={61},
  number={1},
  pages={1012-1033},
  abstract={In nowadays information era, the types of communication or radar electronic equipment for both military and civilian applications are increasing significantly, which leads to various communication and radar signals overcrowded in the time domain, overlapped in the frequency domain, and intertwined in the space domain, shaping a more complicated electromagnetic environment. In order to accurately capture the interested signal or discover the interference signal, separating these time-frequency overlapped signals and extracting the information implied in the useful signal has become a research task with significance for the electromagnetic surveillance domain. This article investigated the sinusoidal similarity among electromagnetic reconnaissance signals and proposed a time–frequency overlapped signals separation method based on sinusoidal similarity for complex electromagnetic monitoring environments. First, we mathematically derived the sinusoidal similarity among electromagnetic reconnaissance signals for three different mixed scenarios, to be specific, digital modulation communication signals mixing scene, digital modulation communication signal(s) and pulse repetition interval radar signal(s) mixing scene, and pulse repetition interval radar signals mixing scene. Then, we formulated a blind signal separation (BSS) objective/cost function utilizing sinusoidal similarity (SSim) among electromagnetic reconnaissance signals. In addition to the SSim constraint, we introduced another constraint term into the objective function, accounting for the unique characteristics of source signals in various mixed scenarios. After that, we successfully optimized the constructed objective function by leveraging advanced machine learning optimization algorithms. Then, we substantiated the validity of the proposed method through simulation experiments, showcasing its performance from various perspectives, and we underscored its advantages by conducting comparisons with classical methods such as fast Independent component analysis and joint approximative diagonalization of eigenmatrices. Furthermore, the proposed BSS method has broader applications in the realm of artificial intelligence. It can be employed for intelligent tasks such as speech signal separation and biomedical signal separation, showcasing its versatility in diverse domains.},
  keywords={Cost function;Blind source separation;Time-frequency analysis;Frequency modulation;Digital modulation;Vectors;Radar applications;Artificial intelligence;blind signal separation (BSS);electromagnetic reconnaissance;sinusoidal similarity (SSim);time–frequency overlapped signal},
  doi={10.1109/TAES.2024.3456094},
  ISSN={1557-9603},
  month={Feb},}@INPROCEEDINGS{11008249,
  author={Hanaa, Oulad Hamdaoui and Abdelmajid, Bybi and Hilal, Drissi Lahssini and Imane, Assalhi and Hassna, Khalfi},
  booktitle={2025 5th International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)}, 
  title={Wearable Assistive Technologies for Blind People}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Wearable assistive technologies enhance autonomy and quality of life for blind individuals. This paper reviews the current state-of-the-art, considering advantages, limitations, and the growing impact of AI. It explores how machine learning and multimodal interfaces personalize and improve device efficiency, enabling more intuitive navigation, object recognition, and communication. The paper provides a concise overview and showcases representative works, highlighting both benefits like improved mobility and usability limitations.},
  keywords={Costs;Accuracy;Navigation;Reviews;Blindness;Assistive technologies;Object recognition;Reliability;Wearable devices;Usability;wearable assistive technologies;blind individuals;advantages;limitations;and artificial intelligence},
  doi={10.1109/IRASET64571.2025.11008249},
  ISSN={},
  month={May},}@INPROCEEDINGS{10779857,
  author={Abraham, Naveen Philip and George, Joppen and Chandy, Jonathan and Sajan, Kevin and Issac, Bini M.},
  booktitle={2024 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES)}, 
  title={EchoLink : A Voice Based Email Assistance For Blind}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={EchoLink is a revolutionary Django web application tailored for visually impaired users, offering seamless email management through voice commands. Integrated with the Gmail API, EchoLink enables intuitive inbox navigation, email composition, and management of key features like filtering and starring. The application ensures user security through advanced face recognition and voice print authentication. EchoLink’s inclusive design enhances user independence and productivity, providing tailored voice assistance similar to popular AI platforms. The project also incorporates AI-driven summarization for emails and personalized content generation. EchoLink redefines accessible digital communication, offering a secure and efficient platform for visually impaired individuals.},
  keywords={Systematics;Face recognition;Visual impairment;Authentication;Digital communication;User experience;Electronic mail;Security;Usability;Spectrogram;Speech Recognition;Email Summarization;Visually Impaired;Facial Recognition;Voice Print Authentication;Email Generation},
  doi={10.1109/SPICES62143.2024.10779857},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11156562,
  author={Wankhede, Pranav and Bhusari, Tannu and Tammewar, Vedika and Agrawal, Rahul and Dhule, Chetan and Morris, Nekita Chavan},
  booktitle={2025 12th International Conference on Emerging Trends in Engineering & Technology - Signal and Information Processing (ICETET - SIP)}, 
  title={IoT & AI-ML Based Portable Non-Contact Retinal Imaging for Glaucoma and Eye Disease Monitoring at Home}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={A major contributor to irreversible blindness, glaucoma necessitates early detection for efficient treatment. Traditional diagnostic methods rely on specialized clinical equipment, limiting accessibility. This paper introduces a Non-Contact Portable Glaucoma Detection Device that incorporates machine learning (ML) and Internet of Things (IoT) for real-time automated diagnosis. It employs a 20D Double Aspheric Lens to enlarge the retina and the Raspberry Pi Camera takes an image that is processed. A YOLO11x-cls model on Raspberry Pi diagnoses the image and puts it into one of four categories: Normal, Diabetic Retinopathy, Cataract, or Glaucoma. The output is presented on a Touchscreen LCD for immediate feedback to the user. Training was conducted on 5,897 retinal images with TensorFlow, PyTorch, and Ultralights Hub, and performance was optimized using transfer learning and hyperparameter optimization. Testing yielded 94.6 accuracy with 95 precision and 100 recall at 50% confidence threshold. The proposed technique is a cost-effective, portable AI driven system for the detection of early glaucoma with home screening and reduced dependence on specialty centers. Upgradation plans incorporate cloud integration, self-enhancement of images, and wider disease classification to make the diagnosis more accessible.},
  keywords={Glaucoma;Visualization;Accuracy;Touch sensitive screens;Retina;Cameras;Real-time systems;Internet of Things;Medical diagnostic imaging;Lenses;Glaucoma detection;Non-Contact Diagnosis;Retinal Imaging;YOLO11x-cls Model},
  doi={10.1109/ICETETSIP64213.2025.11156562},
  ISSN={2157-0485},
  month={Aug},}@INPROCEEDINGS{11019582,
  author={P, Ramalakshimi and A, Senthil G. and S, Geerthik and M, Preethi and S, Rexlin Felix},
  booktitle={2025 International Conference on Computing and Communication Technologies (ICCCT)}, 
  title={AI-Driven Methods Cutting-Edge Deep Learning Techniques for Eye Fundus Disease Detection and Segmentation}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Eye fundus diseases are critical conditions that can lead to severe vision impairment and even permanent blindness if not diagnosed and treated promptly. Manual diagnosis of these diseases is time-consuming and heavily reliant on the expertise of ophthalmologists. This proposed methodology research aims to develop an efficient and accurate diagnostic system for eye fundus disease classification and segmentation using Deep Learning techniques such as (SVM), (ANN), (CNN) and (U-Net) Model. The study involves the compilation of a comprehensive dataset of eye fundus images, encompassing various types of diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and others. Each image is accompanied by corresponding ground-truth annotations provided by expert ophthalmologists for segmentation. The experimental results demonstrate the effectiveness and reliability of the proposed system in accurately classifying eye fundus diseases and segmenting affected regions within the images. The AI-Deep Learning algorithms achieve high accuracy and provide valuable insights into the presence and extent of various fundus diseases as the proposed methodology of SVM is 85%, ANN is 88%, CNN 93% and U-Net is 97%.},
  keywords={Deep learning;Support vector machines;Macular degeneration;Image segmentation;Accuracy;Manuals;Ophthalmology;Convolutional neural networks;Reliability;Diseases;Eye Fundus Disease;Artificial Intelligence;Deep Learning;Disease Classification;Segmentation;Convolutional Neural Networks;U-Net;Ophthalmology;Medical Imaging;Diagnostic System},
  doi={10.1109/ICCCT63501.2025.11019582},
  ISSN={2995-3197},
  month={April},}@INPROCEEDINGS{10708276,
  author={Haddad, Zainab and Zgolli, Hsouna and Sidibé, Désiré and Tabia, Hedi and Khlifa, Nawres},
  booktitle={2024 10th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
  title={Explainable AI For Retinal Pathology Detection In OCT Images}, 
  year={2024},
  volume={},
  number={},
  pages={2401-2406},
  abstract={Diabetic macular edema (DME) and Age-Related Macular Degeneration (AMD) are two of the most common disorders that can cause blindness in a population and primarily cause retinal degradation. The application of multiple deep learning algorithms on Optical Coherence Tomography OCT) images to detect these disorders demonstrates excellent performance. However, because these algorithms include black box features, medical professionals are hesitant to fully trust the results. To address these challenges, we present a modified convolutional neural network based on the xception architecture for diagnosing DME and AMD using optical coherence tomography (OCT) images. To demonstrate the model’s transparency and trustworthiness, we used the Grad-CAM technique, which incorporates Explainable AI into the research and improves model interpretability. This technique assists medical specialists in demystifying deep learning algorithms and obtaining more information about the critical areas in OCT images used for prediction. The proposed model achieved an accuracy of 99.87%, a precision of 99.67%, and a recall of 98.29% on a dataset of 934 images.},
  keywords={Deep learning;Pathology;Visualization;Explainable AI;Optical coherence tomography;Predictive models;Retina;Prediction algorithms;Real-time systems;Medical diagnostic imaging;Optical Coherence Tomography;Artificial intelligence;Deep learning;Convolutional neural networks;Classification;Interpretability;Grad-CAM},
  doi={10.1109/CoDIT62066.2024.10708276},
  ISSN={2576-3555},
  month={July},}@INPROCEEDINGS{10927966,
  author={Deo, Arpit and Gupta, Priyasha and Rajput, Ajeet Singh and Sil, Devyani and Chandorikar, Chirag and Pawar, Mirth},
  booktitle={2025 International Conference on Computational, Communication and Information Technology (ICCCIT)}, 
  title={A Transparent Diagnosis Model for Diabetic Retinopathy Using Explainable AI}, 
  year={2025},
  volume={},
  number={},
  pages={200-208},
  abstract={Diabetic retinopathy is the leading cause of vision problems and partial blindness, and its signs are complicated and variable, making detection difficult. Standard diagnosis is mainly reliant on expert interpretation of retinal images, which can lead to bias and mistake, highlighting the need for improved diagnostic approaches. Despite the fact that conventional AI technologies considerably improve diagnostic skills, their black box aspect limits healthcare practitioners' confidence and optimizes their usage in practice. Grad-CAM++ is a new technique that addresses the black box problem by improving model knowledge, allowing users to grasp the reasoning behind specific decisions. Using XAI approaches, this work creates a diagnostic model for diagnosing diabetic retinopathy, hence improving transparency. The proposed model's convergent plots validate the learning process, which results in 94.4% better diagnostic precision than previous approaches.},
  keywords={Training;Diabetic retinopathy;Accuracy;Explainable AI;Computational modeling;Medical services;Retina;Information and communication technology;History;Standards;diabetic retinopathy;artificial intelligence;Gradient-Weighted Class Activation Mapping (Grad-CAM++);convolutional neural network},
  doi={10.1109/ICCCIT62592.2025.10927966},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{11024855,
  author={Sarker, Shuvashis},
  booktitle={2024 13th International Conference on Electrical and Computer Engineering (ICECE)}, 
  title={Fuzzy Rank-Based Ensemble Learning for Eye Disease Classification Using Retinal Images: A Bangladeshi-Specific Dataset with Explainable AI Integration}, 
  year={2024},
  volume={},
  number={},
  pages={615-620},
  abstract={This study focuses on identifying and categorizing ocular disorders affecting critical components of the eye, including the retina, optic nerve and blood vessels, which can lead to severe outcomes such as visual impairment or blindness if untreated. In Bangladesh, where healthcare resources are scarce, especially in rural regions, early identification of ocular illnesses is crucial to avert permanent vision impairment. To address the deficiency of region-specific datasets, this research employs retinal scans from Bangladeshi healthcare facilities to develop a transfer learning-based solution. Multiple transfer learning models, including DenseNet, EfficientNet, GoogleNet, MobileNet, ResNet50, VGG16, VGG19 and ShuffleNet, have been assessed to determine the best effective architecture for eye illness categorization. A fuzzy rank-based ensemble model was proposed, integrating predictions from these models using the Gompertz function, which adaptively adjusts the weight of each model’s predictions to improve classification accuracy. The ensemble model attained an overall accuracy of $96.16 \%$, demonstrating remarkable precision for illnesses such as Diabetic Retinopathy ($99.62 \%$) and Disc Oedema ($99.63 \%$). Moreover, Explainable AI (XAI) tools like LIME have been included to improve the transparency and interpretability of the model’s decision-making process, allowing medical practitioners to comprehend and validate predictions. By providing a reliable and interpretable AIbased solution, this research significantly improves eye disease diagnosis in Bangladesh, particularly for underserved areas.},
  keywords={Adaptation models;Accuracy;Explainable AI;Visual impairment;Transfer learning;Medical services;Predictive models;Retina;Eye diseases;Medical diagnostic imaging;Retinal Disease Classification;Transfer Learning (TL);Ensemble Learning (EL);Explainable AI (XAI);Fuzzy Rank-Based Ensemble;Gompertz Function},
  doi={10.1109/ICECE64886.2024.11024855},
  ISSN={2771-7917},
  month={Dec},}@ARTICLE{9766027,
  author={Kamal, Md. Sarwar and Dey, Nilanjan and Chowdhury, Linkon and Hasan, Syed Irtija and Santosh, KC},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Explainable AI for Glaucoma Prediction Analysis to Understand Risk Factors in Treatment Planning}, 
  year={2022},
  volume={71},
  number={},
  pages={1-9},
  abstract={Glaucoma causes irreversible blindness. In 2020, about 80 million people worldwide had glaucoma. Existing machine learning (ML) models are limited to glaucoma prediction, where clinicians, patients, and medical experts are unaware of how data analysis and decision-making are handled. Explainable artificial intelligence (XAI) and interpretable ML (IML) create opportunities to increase user confidence in the decision-making process. This article proposes XAI and IML models for analyzing glaucoma predictions/results. XAI primarily uses adaptive neuro-fuzzy inference system (ANFIS) and pixel density analysis (PDA) to provide trustworthy explanations for glaucoma predictions from infected and healthy images. IML uses sub-modular pick local interpretable model-agonistic explanation (SP-LIME) to explain results coherently. SP-LIME interprets spike neural network (SNN) results. Using two different publicly available datasets, namely fundus images, i.e., coherence tomography images of the eyes and clinical medical records of glaucoma patients, our experimental results show that XAI and IML models provide convincing and coherent decisions for clinicians/medical experts and patients.},
  keywords={Neurons;Firing;Decision making;Computational modeling;Biological system modeling;Adaptation models;Optical imaging;Explainable artificial intelligence (XAI);fuzzy inference system (FIS);interpretable machine learning (IML);pixel density analysis (PDA);sub-modular pick local interpretable model-agonistic explanation (SP-LIME)},
  doi={10.1109/TIM.2022.3171613},
  ISSN={1557-9662},
  month={},}@ARTICLE{10256175,
  author={Aurangzeb, Khursheed and Alharthi, Rasha Sarhan and Haider, Syed Irtaza and Alhussein, Musaed},
  journal={IEEE Access}, 
  title={Systematic Development of AI-Enabled Diagnostic Systems for Glaucoma and Diabetic Retinopathy}, 
  year={2023},
  volume={11},
  number={},
  pages={105069-105081},
  abstract={With the rapid advancements in artificial intelligence, particularly in machine learning and deep learning, automated disease diagnosis is becoming increasingly feasible. Generating larger databases is crucial for training and validating the performance of models for chronic diseases such as glaucoma and diabetic retinopathy, which progress slowly and unnoticed. Automated procedures for retinal vessel segmentation and optic cup/disk localization are preferred for large-scale screening of the public, contributing to the early detection and treatment of eye diseases, preventing blindness, and improving public health. This paper focuses on the challenges involved in segmenting the retinal vessels from fundus images and presents a modified ColonSegNet model for retinal vessel segmentation that includes efficient methods for locating the true vessels and applies data augmentation to overcome the issue of fewer graded images. The paper uses the optimal values for the contrast enhancement of retinal fundus images using intelligent evolution algorithms. The central vessel reflex, bifurcation, crossover, thin vessels, and lesion presence are highlighted as significant challenges in retinal vessel segmentation. The proposed method achieves high sensitivity, specificity, and accuracy, {0.839, 0.979, 0.966}, {0.865, 0.979, 0.971}, and {0.867, 0.981, 0.972}, segmenting retinal vessels on DRIVE, CHASE_DB, and STARE. The work is crucial in developing automated systems for the early detection and treatment of eye diseases, thereby improving public health.},
  keywords={Diseases;Glaucoma;Eye diseases;Retinal vessels;Image segmentation;Optical imaging;Diabetic retinopathy;Deep learning;Medical diagnosis;Deep learning;disease diagnosis;diabetic retinopathy;glaucoma;image classification;retinal vessels;cup to disc ratio},
  doi={10.1109/ACCESS.2023.3317348},
  ISSN={2169-3536},
  month={},}
