@INPROCEEDINGS{10463430,
  author={Kuzdeuov, Askat and Mukayev, Olzhas and Nurgaliyev, Shakhizat and Kunbolsyn, Alisher and Varol, Huseyin Atakan},
  booktitle={2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={ChatGPT for Visually Impaired and Blind}, 
  year={2024},
  volume={},
  number={},
  pages={722-727},
  abstract={According to the World Health Organization (WHO), hundreds of million people have some type of visual disability. Vision impairment has a personal impact with lifelong consequences because more than 80 % of our perception, cognition, learning, and daily activities are mediated through vision. Moreover, in the era of rapid advancements in artificial intelligence (AI), visually impaired and blind people face challenges at work and in education because of inaccessibility to AI technologies. In this regard, we present an assistive mobile application with an intuitive user interface (UI) for visually impaired and blind people to interact with ChatGPT via natural conversation. The app employs automatic speech recognition (ASR), text-to-speech (TTS), keyword spotting (KWS), voice activity detection (VAD), and a convenient UI to interact with ChatGPT effortlessly. We have made the source code, pre-trained models, and VI publicly available at https://github.com/IS2AI/talk-llm to stimulate the development of assistive mobile applications.},
  keywords={Voice activity detection;Visualization;Source coding;Oral communication;Blindness;User interfaces;Chatbots},
  doi={10.1109/ICAIIC60209.2024.10463430},
  ISSN={2831-6983},
  month={Feb},}@INPROCEEDINGS{10332217,
  author={Fu, Hanyuan and Renaudin, Valerie and Bonis, Thomas and Zhu, Ni},
  booktitle={2023 13th International Conference on Indoor Positioning and Indoor Navigation (IPIN)}, 
  title={Investigating the Impact of Outfits on AI-Based Pedestrian Dead Reckoning with a Wearable Inertial Sensor Placed in the Pocket}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In this article, we explore the impact of outfits on AI-based Pedestrian Dead Reckoning (PDR) with a pocket-worn inertial sensor. This PDR mode faces significant variability due to the countless choices of outfits available. We observe significant variations in the inertial signals captured by a pocket-worn device, which are highly influenced by the outfit being worn. To address this, we propose a 2-category classification of outfits as tight or loose, based on their impact on the inertial signals. Notably, AI models trained on tight outfits exhibit poor generalization with loose outfits and vice versa. We highlight this phenomenon by implementing a data-low-cost PDR algorithm based on Support Vector Regression (SVR) and assess its performance on two healthy volunteers and a senior and blind volunteer wearing tight and loose outfits, on real-life situation test tracks spanning approximately 200 to 400 meters.},
  keywords={Support vector machines;Meters;Pedestrians;Dead reckoning;Inertial sensors;Indoor navigation;Classification algorithms;Indoor positioning;inertial sensors;pedestrian navigation;Pedestrian Dead Reckoning;Machine Learning},
  doi={10.1109/IPIN57070.2023.10332217},
  ISSN={2471-917X},
  month={Sep.},}@INPROCEEDINGS{10717137,
  author={Joy, Alen and Nazeer, Naja Abdul and Gregory, Neha Maria and Davis, Jasmy},
  booktitle={2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS)}, 
  title={VEye-AI Vision Assistant}, 
  year={2024},
  volume={1},
  number={},
  pages={1385-1390},
  abstract={This paper aims to deal with the difficulties faced by blind while interacting with their surroundings. The proposed system introduces VEye - An AI Vision Assistant, a wearable device with an integrated camera and an ear-piece, using AI algorithms to convert real-time visual input from camera to audio description. It includes other features such as obstacle detection, text recognition, face recognition for authorized individuals, object identification, GPS integration and an emergency button. This device assists the visually challenged people in their day- to-day activities and develop a feeling of independence in them.},
  keywords={Visualization;Text recognition;Visual impairment;Assistive technologies;Cameras;Real-time systems;Object recognition;Artificial intelligence;Wearable devices;Global Positioning System;Obstacle Detection;Text Recognition;Face Recognition;Object Identification;GPS Integration;Artificial Intelligence;Computer Vision;Ergonomic Design;Voice Activation;Real-time Assistance},
  doi={10.1109/ICACCS60874.2024.10717137},
  ISSN={2575-7288},
  month={March},}@INPROCEEDINGS{11011636,
  author={S, Swaminathan and D, Nagendra Kumar and Hari S, Santhana and B, Manorohith and S, Haris Balaji},
  booktitle={2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)}, 
  title={Leveraging AI for Translating English-Captioned YouTube Videos into Regional Languages}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={YouTube is often the best player in the world of video for entertainment, education, and information. However, most users experience problems in aspects such as downloading videos to perform offline analysis or simply due to poor Internet connectivity or academic purposes. There is also a growing need for automated captioning translation and multilingual access for non-native speakers and non-local users. The research proposes, therefore, the development of an adaptive translation system that deals with YouTube movies extracting captions from video, translating the captions to a specific language, and synchronizing audio video with the translated text along with the original captions. Most of the public existing tools would fail because YouTube sources keep frequently changing. Most of these problems have minimal performance accuracy in captioning or fail to download the captions. The proposed methodology has been imagined to overcome these using AI and ensure performance improvement across various environments It will helpful application for blind users, users seeking knowledge from inclusive education, and those wishing to learn using their mother tongue or overcome language barriers. Advancing toward the vision of national language interaction ensures that people from different languages can enjoy video content regardless of their mother tongue. The future of this application will be a blend of AI-based media processing with a well-designed architecture for future-proofing to improve digital accessibility and to provide a mean scaled-down replacement for real-time language translation in video content.},
  keywords={Translation;Video on demand;Tongue;Adaptive systems;Education;Media;Real-time systems;Multilingual;Web sites;Artificial intelligence;Automated captioning translation;Multilingual access;Adaptive translation system;Natural language processing;AI-based media processing;Real-time language translation},
  doi={10.1109/ICDSAAI65575.2025.11011636},
  ISSN={},
  month={March},}@INPROCEEDINGS{10782045,
  author={Srikanth, D. and Gurung, J. and Deepika, N. S. and Joshi, V. and Giri, L. and Vaddavalli, P. and Jana, S.},
  booktitle={2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Blindness and other eye diseases are a global health concern, particularly in low- and middle-income countries like India. In this regard, during the COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi attachment for smartphone-based eye imaging gained in use. However, user-captured images do not often possess complete clinical information for decision making, leading to delays. In this backdrop, we propose an AI-based quality assessment system with instant feedback mimicking clinicians’ judgments and tested on patient-captured images. Dividing the complex problem hierarchically, here we tackle a nontrivial part, and demonstrate a proof of the concept.},
  keywords={Image quality;Pandemics;Decision making;Imaging;Eye diseases;Quality assessment;Delays;Clinical diagnosis;Engineering in medicine and biology;Next generation networking;Teleophthalmology;Automated image quality assessment;Smartphone-based teleconsultation},
  doi={10.1109/EMBC53108.2024.10782045},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10774872,
  author={Arun, Bhingardive Akshada and Bansode, B. N.},
  booktitle={2024 8th International Conference on Computing, Communication, Control and Automation (ICCUBEA)}, 
  title={Visionary AI: Transforming Diabetic Retinopathy Discovery through Advanced Deep Learning Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Leveraging AI for DR detection can transform patient outcomes by enabling early intervention and preventing vision loss. Diabetic Retinopathy (DR) is a significant contributor to visual impairment and blindness globally. It predominantly affects patients who have lived with diabetes for an extended duration. The work aims to develop an effective representation for retinal images, enhancing the recognition. This paper discusses the implementation of a transfer learning-based method for DR discovery with a deep Convolutional Neural Network (CNN) using Inception-v3, VGG-16, Alexnet, Googlenet and ResNet 18 architecture on the APTOS 2019 Dataset and metrics like the precession, confusion matrix, f1- score and recall evaluated. In summary, these studies underscore the transformative potential of deep learning in revolutionizing the judgement of DR.},
  keywords={Deep learning;Measurement;Diabetic retinopathy;Pathology;Visual impairment;Refining;Computer architecture;Transforms;Retina;Convolutional neural networks;DR (Diabetic Retinopathy);AI (Artificial Intelligence);Convolutional Neural Network(CNN);Transfer Learning;F1 score;Recall;Precision;Confusion Matrix},
  doi={10.1109/ICCUBEA61740.2024.10774872},
  ISSN={2771-1358},
  month={Aug},}@INPROCEEDINGS{11030961,
  author={Shaik, Amjan and Flower, K. Little and Veerabhadraiah and Nandini, A. and Kiran, Ch. Sai and Yashwanth Goud, K.},
  booktitle={2025 International Research Conference on Smart Computing and Systems Engineering (SCSE)}, 
  title={AI-Driven Approach for Measuring and Classifying Diabetic Retinopathy Severity}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Diabetic Retinopathy is one of the most common complications affecting people with diabetes and causes blindness in the world. Advanced technological methods through image analysis and artificial neural networks have become big assets in handling the escalating problem of DR. This paper discusses some of the approaches of implementing automation in DR detection with relation to the image acquisition and preprocessing, feature extraction and the actual classification employing the aid of AI. We review the utility of these systems in relation to costs, benefits, and performance, as well as some of the problems tied to the data, model interpretability and regulatory requirements. It demonstrates that automation holds the key to delivering higher patient-impact opportunities in clinical applications such as screening programs and telemedicine. Last, we discuss directions for future research and utilization in the community with an emphasis on the value of highly controlled, naturalistic designs and the translation of the findings into clinical practice.},
  keywords={Diabetic retinopathy;Automation;Accuracy;Translation;Telemedicine;Scalability;Feature extraction;Systems engineering and theory;Loss measurement;Artificial intelligence;retinal disease;classification;ophthalmology;medical imaging;diabetic;artificial intelligence},
  doi={10.1109/SCSE65633.2025.11030961},
  ISSN={2997-7363},
  month={April},}@INPROCEEDINGS{10772912,
  author={Shailendra, Sagar and Megha, Agarwal},
  booktitle={2024 ITU Kaleidoscope: Innovation and Digital Transformation for a Sustainable World (ITU K)}, 
  title={AI-Driven Early Prediction Of Eye Disorder}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Diabetic retinopathy (DR) arises when diabetes mellitus damages the blood vessels in the retina, the crucial part of the eye responsible for image capture. Without timely intervention, this damage can lead to vision impairment or even blindness. Given the cost and time constraints associated with manual examination of fundus images by trained medical professionals, there’s a pressing need for automated DR detection systems. AI-enabled teleophthalmology leveraging 5G/6G technology can enhance eye care accessibility, particularly in remote regions. Transfer learning plays a pivotal role in medical image analysis by facilitating the development of accurate and efficient models, especially when data and computational resources are limited. This paper presents a modification of the VGG16 network for DR detection, incorporating a feature selection technique. By fine-tuning network parameters and selecting superior features, our proposed approach significantly enhances performance. Classification using machine learning models yields promising results over a benchmark DR dataset, with our modified VGG16 network achieving an impressive accuracy of 98.4%, surpassing the standard VGG16 and ResNet18 network along with state-of-the-art methods.},
  keywords={Technological innovation;Accuracy;Computational modeling;Visual impairment;Transfer learning;Retina;ITU;Time factors;Standards;Biomedical imaging;Artificial Intelligence;Disease Prediction;Retinal Disease;VGG16},
  doi={10.23919/ITUK62727.2024.10772912},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10583994,
  author={Kartasheva, Anna and Tomiltseva, Daria},
  booktitle={2024 IEEE Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)}, 
  title={AI Systems and the Possibility of Moral Decision-making in Smart Cities}, 
  year={2024},
  volume={},
  number={},
  pages={185-187},
  abstract={Technologies and algorithms combine quantitative data about people and deep insights into the causes of human actions, empowering people to create new artificial agents. The article raises the challenges of implementing normativity in urban spaces equipped with technical systems with artificial intelligence. The improvement of urban life is come through access to technology also the emergence of smart cities is, but not everyone welcomes the predictability and transparency it brings. The introduced artificial agents influence the existing system of normativity in the city, abolishing any norms, transforming, and generating others. Such influence cannot always predict accurately, so it is not possible to expect cultural and religious organizations, individual scientists, or government institutions to finally provide a definite list of moral norms and social rules, which need to implement into technical systems. It is worth noting that if existing norms and regulations are insufficient, then citizens develop their own models of interaction. In addition to rational and effective decision-making, it needs for the technical system needs to earn the trust of the citizens whose daily lives it regulates. The article outlines three caveats for morally correct decision-making mechanisms. First, they should be ongoing measures against distortion. Second, prevention of “social blindness” on the part of developers. Third, controlling the use of techniques that attract users’ attention while being addictive or negatively affecting users in general. These three caveats can be the basis for engineers, social researchers, and urban communities to begin working together conceptually to form morally correct smart city solutions.},
  keywords={Heart;Ethics;Smart cities;Prevention and mitigation;Decision making;Government;Regulation;smart city;moral city;normativity;moral norms;embodied AI systems},
  doi={10.1109/USBEREIT61901.2024.10583994},
  ISSN={2769-3635},
  month={May},}@INPROCEEDINGS{10883357,
  author={Sharma, Ankita and Gupta, Sheifali},
  booktitle={2025 6th International Conference on Mobile Computing and Sustainable Informatics (ICMCSI)}, 
  title={AI-Driven Eye Disease Detection: A VGG19-Based Approach to Retinal Image Classification}, 
  year={2025},
  volume={},
  number={},
  pages={1325-1329},
  abstract={Eye disease is also a very severe condition that affects the human eye it has several disorders, including cataracts, glaucoma, and diabetic retinopathy. These disorders can affect the eye in different ways, some of them can cause permanent blindness to the eye. This research addresses the Artificial Intelligence and Deep Learning approaches, focusing on Convolutional Neural Networks, to automate the classification of eye illnesses. Utilizing the VGG19 model, a well-established Deep Neural Network the study tries to classify retinal images into two basic categories, Normal and Cataract. The VGG19 model is a pre-trained model trained on a large data set that works well on the image data set. Deep learning methods work well in the field of healthcare, and medical sciences. Due to their pre-training, these models are also called transfer learning models. The VGG19 model displayed a remarkable accuracy of 95% with 0.055 loss, demonstrating its usefulness in eye illness classification. This research stresses the critical significance of CNNs in aiding the timely detection and treatment of eye disease, ultimately leading to techniques for reducing vision loss.},
  keywords={Cataracts;Glaucoma;Deep learning;Diabetic retinopathy;Accuracy;Medical services;Blindness;Retina;Data models;Artificial intelligence;Artificial Intelligence;Deep Learning;CNN;Neural Networks;Innovation;Mortality Rates;Myopia;Glaucoma;Cataracts;Eye Disease},
  doi={10.1109/ICMCSI64620.2025.10883357},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{9988614,
  author={Ananth, Christo and Thenmozhi, P. and Vadakkan, Densy John and Kavitha, S. and Kumar, T. Ananth},
  booktitle={2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)}, 
  title={Artificial Intelligence based Visual Aid with Live Tracking of Visually Impaired People}, 
  year={2022},
  volume={},
  number={},
  pages={573-577},
  abstract={Blind people cannot do things like navigate on their own, recognize objects, or avoid obstacles because they cannot see where they are going or what is around them. In the future, there will be an article about a new visual aid for people who are completely deaf or blind. As a result, the Raspberry Pi 3 Model B+ was chosen because it is cheap, small, and easy to work with other systems. This model was made to show how the prototype would work and how it would look. The system's obstacle avoidance configuration includes a camera and other sensors and object detection and tracking algorithms to help it avoid things. People can use ultrasonic technology and a camera to figure out how far away they are from a specific object or person, so they can figure out how far away they are. As an extra layer of protection, the GPS system tracks people's movements who cannot see. The products will be lightweight and easy to move, making them easy to move. Regular eyeglasses can be used as a base for these glasses so that you do not have to spend extra money or make things more complicated. Using this idea to make eyeglasses would be easy, and it would not make them more expensive or more complicated. The price or complexity of eyeglasses would not change if we made one of these products, so it would be easy to put one in.},
  keywords={Visualization;Tracking;Navigation;Prototypes;Object detection;Cameras;Sensor systems;Artificial Intelligence;Global Positioning System;Internet of Things (IoT);Raspberry Pi-3B},
  doi={10.1109/ICTACS56270.2022.9988614},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10798155,
  author={Wu, Yu-Chun and Jhang, Bo-Yu and Lee, Shuenn-Yuh and Chen, Ju-Yi},
  booktitle={2024 IEEE Biomedical Circuits and Systems Conference (BioCAS)}, 
  title={Fully Convolution Based Denoise Autoencoder AI Accelerator for ECG Arrhythmia Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The electrocardiogram (ECG) is a reliable indicator for arrhythmias detection. This paper introduces an arrhythmia classification system designed for identifying cardiovascular diseases, following the international standards presented by the Association for the Advancement of Medical Instrumentation (AAMI). Arrhythmic conditions are categorized into five major classes. The proposed deep learning accelerator (DLA) combines noise reduction and disease classification features to enhance accuracy. The noise reduction process utilizes a fully convolutional denoising autoencoder (DAE) model that reconstructs clean ECG data. The denoising performance, measured by signal-to-noise ratio (SNR), reaches 20.19 dB. By applying transfer learning, the encoder of DAE is integrated with the R-peak interval model through model fusion, enabling the simultaneous learning of single-window ECG features and long-term heart rate variations. To validate the model’s robustness, test data from the MIT-BIH and AHA databases were randomly selected before model training. Ultimately, the model achieves 97.8% accuracy in blind testing. Additionally, this study presents an AI accelerator implemented using Taiwan Semiconductor Manufacturing Company (TSMC) 180-nm CMOS technology. The accelerator incorporates dynamic zero-gating process elements, reducing power consumption by up to 55.8%. The chip utilizes 14 KB of static random-access memory (SRAM), three FIFO data buffers, and eight processing elements, each with three MACs. The total power consumption is 0.533 mW, and the accelerator achieves inference in just 55 ms.},
  keywords={Training;Semiconductor device modeling;Power demand;Accuracy;Databases;Arrhythmia;Noise reduction;Electrocardiography;Data models;Testing;Arrhythmia;Fully Convolution Neural Network;Denoise Autoencoder;AI Accelerator},
  doi={10.1109/BioCAS61083.2024.10798155},
  ISSN={2766-4465},
  month={Oct},}@INPROCEEDINGS{11172231,
  author={Almas, Arqam Bin and Rahman, Md. Nahul and Nanjeeba, Maisha and Zishan, Md. Rubaet Kabir and Sattar, Md Abdus and Akhtaruzzaman, M.},
  booktitle={2025 International Conference on Quantum Photonics, Artificial Intelligence, and Networking (QPAIN)}, 
  title={AI Integrated Automated Door Lock System Through Face Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Traditional security systems in organizations lack automation, while available facial recognition access control systems are expensive, complex, ineffective under varying lighting environments, and inaccessible for visually impaired users. Therefore, a smart, low-cost AI-based facial recognition system designed for real-time access control is proposed aiming to solve the issues. The system utilizes Haar cascade classifiers for face detection and Dlib's ResNet-34 architecture-based model for face recognition, implemented using OpenCV and Python while incorporating ESP32-CAM and a solenoid lock with a buzzer mechanism for blind users. Outcomes suggest solution delivers high accuracy of $\mathbf{9 6. 2 3 \%}$ considering $\mathbf{4 0 \%}$ pixel modification under varying lighting and facial occlusion conditions maintaining enhanced robustness. Thus, combination of affordable hardware and advanced features of this system provides the world of smart access control with a realistic and accessible solution.},
  keywords={Access control;Accuracy;Face recognition;Lighting;Robustness;Real-time systems;Face detection;Artificial intelligence;Solenoids;Testing;ESP32-CAM;Arduino UNO R3;LDR;Buzzer;Haar Cascade;Deep Learning;Face Detection;Face Recognition},
  doi={10.1109/QPAIN66474.2025.11172231},
  ISSN={},
  month={July},}@INPROCEEDINGS{10687636,
  author={Roge, Ankita and Sontakke, Amisha and Wyawahare, Nikhil P},
  booktitle={2024 OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 4.0}, 
  title={Visual Ease: Empowering The Visually Impaired With AI Pill Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Managing medication is a major worry for visually impaired people, among many other everyday challenges.It can be difficult and prone to error to identify medications on your own, which could pose health hazards. The suggested Drug Pill Recognition System (DPRS) uses artificial intelligence toautomatically recognize and classify medications in order to solve this. This inventive strategy gives visually impaired peoplea reliable means to properly manage their dailyhealth routine while boosting independence while decreasing the chance of prescription accidents. The DPRS is an effective and useful solution to a major problem that the blind and visually impairedencounter on a daily basis.},
  keywords={Drugs;Visualization;Technological innovation;Boosting;Hazards;Fourth Industrial Revolution;Reliability;Deep Learning;Image processing;CNN;Healthcare},
  doi={10.1109/OTCON60325.2024.10687636},
  ISSN={},
  month={June},}@INPROCEEDINGS{10235120,
  author={Abirami, A and Kavitha, R},
  booktitle={2023 World Conference on Communication & Computing (WCONF)}, 
  title={Diabetic Retinopathy Image Recognition Using Artificial Intelligence}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={DR is health condition which is caused by DM and it is primary cause which is being involved for blindness in adults due to disfigurement of human retina. detection of MAs is challenging. Lesions are manually detected for fundus photographs which is considered as tedious. Many ML and DL methods have been applied on data sets of DR neuron-wise architecture are applied using a CNN and training was done with a DR in the dataset. It was noticed that the NN can capture lesions colours and textures in diagnosis, which mimics human thinking ability. NN employ a 2-channel architecture (GTB, RTB) to segment 4 DR lesion combined. Experimental outcomes of the network could be attributed with GTB as well as RTB.},
  keywords={Training;Image segmentation;Diabetic retinopathy;Image recognition;Image color analysis;MIMICs;Computer architecture;Diabetic Retinopathy;Machine learning;Deep learning;Green channel image processing;Convolutional neural network},
  doi={10.1109/WCONF58270.2023.10235120},
  ISSN={},
  month={July},}@INPROCEEDINGS{10568753,
  author={T M, Inbamalar and K, Sreenidhi and M, Supriya Shree and P, Vernisha Shree},
  booktitle={2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM)}, 
  title={Artificial Intelligence Powered Eye for Visually Challenged People}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={More than 2.2 billion individuals globally are experiencing blindness or visual impairments. The primary goal of this endeavor is to empower individuals with visual impairments, enabling them to engage in their daily activities independently and ultimately improving their overall quality of life. To achieve this, the project incorporates the use of the ESP32-CAM Microcontroller, and the system accomplishes real-time visual data recording through an ESP32-CAM module equipped with a camera. By implementing image processing and identification algorithms, the device aids those with visual impairments in identifying text, faces, and significant elements in their surroundings. Furthermore, object identification contributes to heightened environmental awareness, and facial recognition enhances social context by identifying specific individuals. The technology communicates information through audio signals, providing individuals with visual impairments with a more comprehensive understanding of their surroundings.},
  keywords={Visualization;Navigation;Microcontrollers;Visual impairment;Virtual reality;Real-time systems;Mathematics;Artificial Intelligence;Object Recognition;Image Processing;Visual impairment},
  doi={10.1109/ICONSTEM60960.2024.10568753},
  ISSN={},
  month={April},}@INPROCEEDINGS{10073186,
  author={Banerjee, Kakoli and Harsha, K G and Kumar, Pradeep and Vats, Indira and Vinooth, P and Dasila, Piyush and Akhtar, Naved and Kumar, Ajay and Gautam, Prajjwal},
  booktitle={2022 5th International Conference on Contemporary Computing and Informatics (IC3I)}, 
  title={A review on Artificial Intelligence based Sign Language Recognition Techniques}, 
  year={2022},
  volume={},
  number={},
  pages={2195-2201},
  abstract={The World Wide Web Consortium describes web accessibility as the ability to access any information online regardless of the technology used and interacting with the interface of the web if required. Over the last few decades, significant efforts have been made in the field of web accessibility. Successful tools were developed leveraging technologies such as Artificial Intelligence, Speech Recognition systems and motion detection using Computer Vision. Sign Language is a form of non-verbal, hand movement or gesture based mode communicating thoughts and ideas. It is the primary way of communication for many people, majorly including differently abled people, such as people with speech-impairments. Despite being popular amongst people who are unable to speak or hear, there exists a great disparity in the knowledge of Sign Language in the general population consisting of able-bodied individuals. This necessitates the existence of a system that bridges this gap, and allows for efficient interpretation of Sign Language. Sign Language recognition is a growing field of interest in the domain of Artificial Intelligence, and many tools have been created to automate this process using image recognition techniques. These techniques are used to classify sign language and map them into their corresponding meaning in the English language. The steps required in this include the acquisition of data, extraction of relevant features followed by the training of the classifier. This paper gives an overview of the level of work done in this field, and delineates the scope of some models created for this purpose. It also discusses the challenges that are being faced and the scope of work that is yet to be done.},
  keywords={Training;Technological innovation;Image recognition;Training data;Gesture recognition;Speech recognition;Assistive technologies;Artificial Intelligence;Machine Learning;Computer Vision;Sign Language Detection;Hand Gestures},
  doi={10.1109/IC3I56241.2022.10073186},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10073000,
  author={Banerjee, Kakoli and Vats, Indira and Akhtar, Naved and G, Harsha K and P, Vinooth and Kumar, Ajay and Kumar, Pradeep and Dasila, Piyush and Gautam, Prajjwal},
  booktitle={2022 5th International Conference on Contemporary Computing and Informatics (IC3I)}, 
  title={A review on Artificial Intelligence based Sign Language Recognition Techniques}, 
  year={2022},
  volume={},
  number={},
  pages={2195-2201},
  abstract={The World Wide Web Consortium describes web accessibility as the ability to access any information online regardless of the technology used and interacting with the interface of the web if required. Over the last few decades, significant efforts have been made in the field of web accessibility. Successful tools were developed leveraging technologies such as Artificial Intelligence, Speech Recognition systems and motion detection using Computer Vision. Sign Language is a form of non-verbal, hand movement or gesture based mode communicating thoughts and ideas. It is the primary way of communication for many people, majorly including differently abled people, such as people with speech-impairments. Despite being popular amongst people who are unable to speak or hear, there exists a great disparity in the knowledge of Sign Language in the general population consisting of able-bodied individuals. This necessitates the existence of a system that bridges this gap, and allows for efficient interpretation of Sign Language. Sign Language recognition is a growing field of interest in the domain of Artificial Intelligence, and many tools have been created to automate this process using image recognition techniques. These techniques are used to classify sign language and map them into their corresponding meaning in the English language. The steps required in this include the acquisition of data, extraction of relevant features followed by the training of the classifier. This paper gives an overview of the level of work done in this field, and delineates the scope of some models created for this purpose. It also discusses the challenges that are being faced and the scope of work that is yet to be done.},
  keywords={Training;Technological innovation;Image recognition;Training data;Gesture recognition;Speech recognition;Assistive technologies;Artificial Intelligence;Machine Learning;Computer Vision;Sign Language Detection;Hand Gestures},
  doi={10.1109/IC3I56241.2022.10073000},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10993523,
  author={Elankavi, R. and Surekha, R. and Nuthalapati, Urekha and Priya, Vadala Chandana and Sathwik, Aramadaka and Babu, Matta Raveendar},
  booktitle={2024 International Conference on Communication, Computing and Energy Efficient Technologies (I3CEET)}, 
  title={Gradient Based Insights Into Eye Disease Identification With AI}, 
  year={2024},
  volume={},
  number={},
  pages={1026-1032},
  abstract={Eye diseases represent a significant public health challenge, particularly in regions with limited access to specialized medical care. In India, where millions suffer from curable blindness, the need for early detection and intervention is critical. This paper proposes a novel approach leveraging deep learning techniques, including Convolutional Neural Networks (CNNs) such as ResNet, VGG16, and VGG19, to automate the identification of eye diseases based on visually observable symptoms. By analyzing digital images of eyes, our model categorizes and detects conditions. The integration of CNNs enables accurate and efficient analysis, facilitating early disease detection and prompting patients to seek timely medical attention. This automated system holds promise for improving healthcare outcomes and reducing the burden of preventable blindness worldwide.},
  keywords={Deep learning;Digital images;Blindness;Eye diseases;Energy efficiency;Telecommunication computing;Convolutional neural networks;Indexes;Public healthcare;Biomedical imaging;Index Terms -Eye Diseases;ResNet;VGG16;VGG19;CNN},
  doi={10.1109/I3CEET61722.2024.10993523},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11022403,
  author={Muntaha, Samia and Salam, Sayeda Sabiha and Mustakim, Naosher},
  booktitle={2024 27th International Conference on Computer and Information Technology (ICCIT)}, 
  title={An Explainable AI-based Deep Learning Model for Classification of Diabetic Retinopathy Stages Using Retinal Fundus Images}, 
  year={2024},
  volume={},
  number={},
  pages={3010-3015},
  abstract={Diabetic Retinopathy (DR) is a vision-related illness in humans that causes retinal degeneration and can eventually lead to blindness. Diabetic retinopathy is one of the main causes of vision impairment globally and a major consequence of diabetes mellitus. Diabetic retinopathy detection in early stage is a must, in order to prevent total blindness. Numerous physical examinations, including visual clarity assessment, pupil dilation, and optical coherence tomography, are used to diagnose diabetic retinopathy; nevertheless, they are time-consuming and affect patients. This research outlines an innovative approach using Convolutional Neural Networks (CNN) for the detection of Diabetic Retinopathy (DR). This study builds a diversified dataset of retinal fundus images in order to detect five phases of Diabetic Retinopathy (DR) using a customized deep CNN architecture. After considerable research and parameter fine-tuning, the CNN outperformed existing approaches in terms of accuracy, sensitivity, and specificity in diagnosing DR stages. This study uses 3 publicly available datasets, previously imbalanced images across 5 classes, was oversampled to new images, resolving the imbalance issue. The suggested CNN architecture attained a testing accuracy of 95.25%, with testing precision, recall, and F1 score values of 96%, 95%, and 95%, respectively, across the three datasets. However, as a black-box model, its operation is difficult to explain, raising worries about the reliability of its results. To address this issue, we employed Gradient-Weighted Class Activation Map (Grad-CAM) for elucidation, which emphasizes significant areas within the photographs. This approach delivers classification results as well as insights, which help medical practitioners make decisions. An interpretability study revealed important visual regions for decision-making. This method not only advances diagnostic systems, but it also shows promise in early DR detection, potentially assisting in timely identification in clinical settings. It underscores the capabilities of convolutional neural networks (CNNs) in medical image processing, offering the prospect of enhanced accuracy in diabetic retinopathy (DR) screening, hence benefiting diabetic patients and diminishing healthcare expenses.},
  keywords={Diabetic retinopathy;Visualization;Accuracy;Computational modeling;Visual impairment;Blindness;Retina;Convolutional neural networks;Testing;Residual neural networks;Diabetic Retinopathy;CNN;EyePACS;IDRiD},
  doi={10.1109/ICCIT64611.2024.11022403},
  ISSN={2474-9656},
  month={Dec},}@INPROCEEDINGS{9851037,
  author={Samundeswari, S. and Lalitha, V and Archana, V and Sreshta, K},
  booktitle={2022 International Virtual Conference on Power Engineering Computing and Control: Developments in Electric Vehicles and Energy Sector for Sustainable Future (PECCON)}, 
  title={OPTICAL CHARACTER RECOGNITION FOR VISUALL YCHALLENGED PEOPLE WITH SHOPPING CART USING AI}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Disability is defined as a situation in which one must rely on others help to meet daily needs. Visual impairment is one of them. Several approaches to improving the quality to live for visually impaired and blind people have been proposed so far, buying things in the store without the help of others is always a difficult undertaking for them. A camera-based assistive text reading framework is being developed as a solution to aid blind individuals in reading text labels and product packaging from hand-held objects in their shopping store. To distinguish the object from crowded backdrops or other nearby objects in the camera view, we used an old, active motion-based method to focus a region of interest (ROI) in the picture by asking the user to shake the object. Based on real-time situations, the voice instructions will aid the user inside the supermarket. The maj or goal of this system is to eliminate other people's assistance in shopping for visually impaired persons by providing them with a convenient, smart environment. When this prototype method is implemented, that makes shopping and product purchasing easier for blind people, saves time for customers, and increases business sales. Where user interface difficulties are investigated and the algorithm's robustness in extracting and reading text from objects with complicated backgrounds is evaluated.},
  keywords={Power engineering computing;Visual impairment;Optical character recognition;Prototypes;Blindness;User interfaces;Packaging},
  doi={10.1109/PECCON55017.2022.9851037},
  ISSN={},
  month={May},}@INPROCEEDINGS{11068389,
  author={Marchiori, Francesco and Conti, Mauro},
  booktitle={2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S)}, 
  title={Leaving No Blind Spots: Toward Automotive Cybersecurity}, 
  year={2025},
  volume={},
  number={},
  pages={230-232},
  abstract={The increasing connectivity and autonomy of modern vehicles have drastically expanded their attack surface, introducing interdependent cybersecurity risks. However, existing security mechanisms often focus on isolated threats, failing to address their interplay within complex vehicle ecosystems. As vehicles become increasingly dependent on AI-driven control, electric powertrains, and networked architectures, ensuring resilience across multiple attack vectors requires a holistic security approach. This work proposes a unified three-layer security framework that integrates (i) physical-layer protection through battery authentication and side-channel resilience, (ii) AI-layer robustness against adversarial attacks on perception and intrusion detection, and (iii) communication-layer security for in-vehicle network protection. By leveraging cross-domain security principles, including cyber-physical security analysis, adversarial ML defenses, and in-vehicle network protection, this framework provides a cohesive and scalable methodology for securing next-generation automotive systems.},
  keywords={Mechanical power transmission;Vectors;Robustness;Batteries;Sensors;Protection;Artificial intelligence;Next generation networking;Resilience;Automotive engineering;Automotive Security;Adversarial Machine Learning;Cyber-Physical Systems},
  doi={10.1109/DSN-S65789.2025.00018},
  ISSN={2833-292X},
  month={June},}@INPROCEEDINGS{10927291,
  author={Wisdom, Daniel Dauda and Balogun, Oluwatobi Azeez and Vincent, Olufunke Rebecca and Ajayi, Taiwo David and Ayetuoma, Isaac Oniovosah and Garba, Alpha Baba},
  booktitle={2024 IEEE 5th International Conference on Electro-Computing Technologies for Humanity (NIGERCON)}, 
  title={A Hybrid Artificial Intelligence System for the Visually Impaired}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Accessible Artificial Intelligence (AI) for the visually impaired is an important problem faced by individuals with visual disabilities. These challenges include difficulties in reading written text and documents, navigating and recognizing surroundings, converting printed text into digital formats, and limited accessibility of smartphones for daily tasks. Thus, this paper developed a hybrid AI solution that enhances independence, accessibility, and overall quality of life for visually impaired individuals across diverse aspects of daily living. The research successfully achieved its objectives by delivering three functional software systems that address key areas of human-computer interaction: object recognition, text-to-speech, and speech-to-text—and integrated these systems known as an intelligent Hybrid AI System for visually impaired people with 92% accuracy. The research designed to assist visually impaired individuals in interacting more effectively with their environment and digital devices. The application was developed using programming languages such as Python, JavaScript, PHP, MySQL, Flask, and APIs such as Microsoft Azure and vision computing. The results proved that these solutions significantly enhance the ability of visually impaired users to recognize objects and navigate digital content using voice commands and screen readers with superior performance.},
  keywords={Human computer interaction;Visualization;Navigation;Text recognition;Software systems;Text to speech;Object recognition;Artificial intelligence;Speech to text;Python;AI;Python;JavaScript;PHP;MySQL;Flask},
  doi={10.1109/NIGERCON62786.2024.10927291},
  ISSN={2377-2697},
  month={Nov},}@INPROCEEDINGS{10009591,
  author={Shinghal, Deepti and Shinghal, Kshitij and Saxena, Shuchita and Saxena, Amit and Saxena, Nishant and Sharma, Amit},
  booktitle={2022 International Conference on Advances in Computing, Communication and Materials (ICACCM)}, 
  title={Artificial Intelligence Based Visually Impaired Assist System}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={In present work, a system is proposed which is unique in a way that there is a requirement of visually impaired friendly buildings. In current scenario when a visually impaired person enters a building which is Visually Impaired (VI) friendly, an attendant hands him over braille based navigation chart or electronic guide system The proposed system automatically detects a visually impaired person makes an announcement, generates an alert message from the basket where VI person enabled braille based guide maps are kept. The system was tested and it is able to detect blind persons with good accuracy.},
  keywords={Braille;Navigation;Buildings;Blindness;Artificial intelligence;Visually Impaired;Artificial intelligence. Machine learning;Raspberry pi-4},
  doi={10.1109/ICACCM56405.2022.10009591},
  ISSN={2642-7354},
  month={Nov},}@INPROCEEDINGS{10389880,
  author={Als, Adrian and Norville, Tizelia and Layne, Roshawn},
  booktitle={2023 IEEE Gaming, Entertainment, and Media Conference (GEM)}, 
  title={VisualEyez – Assisting The Visually Impaired with Barbadian Banknote Detection}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The International Agency for the Prevention of Blindness's Vision Atlas reports that there are 1.1 billion people globally with vision loss and this is expected to rise to 1.7 billion by 2050. Moreover, much of this vision loss is driven by inequality with higher rates occurring in low- and middle-income countries such as those within the Caribbean region. Four percent of the Barbadian population is estimated to be visually impaired (VI) and of this, one percent is totally blind. Although this is a small percentage, it represents over ten thousand people who have the right to live independently in an inclusive society. Until December 05, 2022, this right was inhibited by the lack of a mechanism for members of the VI community to distinguish among the various local banknotes. The introduction of unique tactile symbols on each denomination was welcomed by the community. However, as the older notes are still widely circulated, the issue persists. VisualEyez, an artificial intelligence (AI) mobile application was developed to address the matter. This application identifies the different denominations of Barbadian banknotes and communicates the detected value using either speech or vibration modes. In this work, the Systems Usability Survey (SUS) instrument was employed on a purposive sample of 6 members from the Barbados Association for the Blind and Deaf (BABD) to quantify the user experience (UX). Results showed an average rating of 79.6 which indicates that users are very satisfied with the application. Speech was the generally preferred operating mode, with vibration mode being preferred in public spaces. This research shows that the VisualEyez application will be beneficial to those who are blind or visually impaired.},
  keywords={Vibrations;Surveys;Sociology;Symbols;User experience;Artificial intelligence;Usability;Visually Impaired;Assistive Technology;Smartphone;Artificial Intelligence Application;Barbadian Banknote Detector},
  doi={10.1109/GEM59776.2023.10389880},
  ISSN={2766-6530},
  month={Nov},}
