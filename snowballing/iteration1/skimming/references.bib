@inproceedings{10.1145/3544548.3581494,
author = {Huh, Mina and Yang, Saelyne and Peng, Yi-Hao and Chen, Xiang 'Anthony' and Kim, Young-Ho and Pavel, Amy},
title = {AVscript: Accessible Video Editing with Audio-Visual Scripts},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581494},
doi = {10.1145/3544548.3581494},
abstract = {Sighted and blind and low vision (BLV) creators alike use videos to communicate with broad audiences. Yet, video editing remains inaccessible to BLV creators. Our formative study revealed that current video editing tools make it difficult to access the visual content, assess the visual quality, and efficiently navigate the timeline. We present &nbsp;AVscript, an accessible text-based video editor. &nbsp;AVscript enables users to edit their video using a script that embeds the video’s visual content, visual errors (e.g., dark or blurred footage), and speech. Users can also efficiently navigate between scenes and visual errors or locate objects in the frame or spoken words of interest. A comparison study (N=12) showed that &nbsp;AVscript significantly lowered BLV creators’ mental demands while increasing confidence and independence in video editing. We further demonstrate the potential of &nbsp;AVscript through an exploratory study (N=3) where BLV creators edited their own footage.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {796},
numpages = {17},
keywords = {accessibility, authoring tools, video},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3588029.3595471,
author = {Tanaka, Kengo and Fushimi, Tatsuki and Tsutsui, Ayaka and Ochiai, Yoichi},
title = {Text to Haptics: Method and Case Studies of Designing Tactile Graphics for Inclusive Tactile Picture Books by Digital Fabrication and Generative AI},
year = {2023},
isbn = {9798400701535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588029.3595471},
doi = {10.1145/3588029.3595471},
abstract = {In this case study, we explore the possibilities between Generative AI and tactile graphics for inclusivity in computer graphics communities. The use of Generative AI in the design of tactile graphics has made it possible to support the processes used by publishers and tactile graphics designers. In addition, the idea of printing tactile graphics on transparent sheets with a 3D printer through digital fabrication technology allows the creation of inclusive tactile picture books that can be read and enjoyed together by sighted and visually impaired people in a single picture book.},
booktitle = {ACM SIGGRAPH 2023 Labs},
articleno = {10},
numpages = {2},
keywords = {3D printing, generative ai, tactile graphics, tactile picture books, visual impairments},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3613904.3642839,
author = {Van Daele, Tess and Iyer, Akhil and Zhang, Yuning and Derry, Jalyn C and Huh, Mina and Pavel, Amy},
title = {Making Short-Form Videos Accessible with Hierarchical Video Summaries},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642839},
doi = {10.1145/3613904.3642839},
abstract = {Short videos on platforms such as TikTok, Instagram Reels, and YouTube Shorts (i.e. short-form videos) have become a primary source of information and entertainment. Many short-form videos are inaccessible to blind and low vision (BLV) viewers due to their rapid visual changes, on-screen text, and music or meme-audio overlays. In our formative study, 7 BLV viewers who regularly watched short-form videos reported frequently skipping such inaccessible content. We present &nbsp;ShortScribe, a system that provides hierarchical visual summaries of short-form videos at three levels of detail to support BLV viewers in selecting and understanding short-form videos. ShortScribe allows BLV users to navigate between video descriptions based on their level of interest. To evaluate &nbsp;ShortScribe, we assessed description accuracy and conducted a user study with 10 BLV participants comparing &nbsp;ShortScribe to a baseline interface. When using ShortScribe, participants reported higher comprehension and provided more accurate summaries of video content.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {895},
numpages = {17},
keywords = {Accessibility, Short-Form Video, Summaries, Video Description},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3663547.3759758,
author = {Tang, Kaiyuan and Chen, Kerui and Qing, Tian and Jiang, Zhou and Lin, Qiuyu and Xu, Tianming},
title = {EchoMall: Prototyping GenAI-powered Vision-Free Online Shopping Experience for Blind and Low-Vision Customers},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3759758},
doi = {10.1145/3663547.3759758},
abstract = {Conventional online shopping platforms typically prioritise user experience design centred around visual elements, heavily relying on visual cues for product information and user navigation. This design pattern presents barriers for Blind and Low-Vision (BLV) customers. Existing accommodations focus primarily on ensuring compatibility of the platforms with assistive software such as screen readers. While assistive software enable BLV customers to access online shopping services, they often involve compromises in user experience during the conversion of visual elements to auditory ones. With the rapid advancement of Generative AI (GenAI), particularly voice- and dialogue-based interactions driven by Large Language Models (LLMs), we prototyped an innovative demo, EchoMall, aiming to explore the design of an enjoyable vision-free online shopping experience. By blending multiple LLMs and GenAI techniques, EchoMall potentially allows BLV customers to engage in an intuitive online shopping process without assistive software and creates new market opportunities.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {133},
numpages = {5},
keywords = {Blind and Low-vision Assistance; Auditory User Interface; Generative User Interface; Generative AI; Large Language Models; Conversational Interaction},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3663547.3759708,
author = {Hayward, Jayne and Vincenzi, Beatrice and Karpodini, Christina and Theil, Arthur},
title = {How Do Disabled Visual Artists Use GenAI Tools for Business Administration Tasks?},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3759708},
doi = {10.1145/3663547.3759708},
abstract = {This paper investigates how disabled visual artists use GenAI tools to manage administrative and business tasks essential to sustaining creative careers. Whilst previous research has focused on GenAI's emerging role in artistic work, our study highlights its potential in supporting non-creative aspects of artistic practice, which are often overlooked yet critical for workload management of disabled artists. Through in-depth semi-structured interviews with 5 disabled visual artists, we explore the perceived benefits and barriers of using GenAI, particularly in terms of accessibility, cognitive demands, ethical concerns, and financial constraints. Our findings suggest that whilst GenAI can increase productivity, reduce cognitive load, and promote equity, it also presents challenges related to transparency, bias, interface complexity, and job displacement fears. We conclude by calling for more inclusive and affordable GenAI tools, improved accessible interfaces, ethical governance, and personalised support to enable disabled creatives in their professional practices.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {151},
numpages = {5},
keywords = {Accessibility, Art Business, Assistive Technology, Creative Work, Disability Employment Gap, Disabled Workers, Generative Artificial Intelligence},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3663547.3746362,
author = {Zhang, Zhuohao (Jerry) and Li, Haichang and Yu, Chun Meng and Faruqi, Faraz and Xie, Junan and Kim, Gene S-H and Fan, Mingming and Forbes, Angus and Wobbrock, Jacob O. and Guo, Anhong and He, Liang},
title = {A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3746362},
doi = {10.1145/3663547.3746362},
abstract = {Building 3-D models is challenging for blind and low-vision (BLV) users due to the inherent complexity of 3-D models and the lack of support for non-visual interaction in existing tools. To address this issue, we introduce A11yShape, a novel system designed to help BLV users who possess basic programming skills understand, modify, and iterate on 3-D models. A11yShape leverages LLMs and integrates with OpenSCAD, a popular open-source editor that generates 3-D models from code. Key functionalities of A11yShape include accessible descriptions of 3-D models, version control to track changes in models and code, and a hierarchical representation of model components. Most importantly, A11yShape employs a cross-representation highlighting mechanism to synchronize semantic selections across all model representations—code, semantic hierarchy, AI description, and 3-D rendering. We conducted a multi-session user study with four BLV programmers, where, after an initial tutorial session, participants independently completed 12 distinct models across two testing sessions, achieving results that aligned with their own satisfaction. The result demonstrates that participants were able to comprehend provided 3-D models, as well as independently create and modify 3-D models—tasks that were previously impossible without assistance from sighted individuals.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {84},
numpages = {20},
keywords = {3-D Modeling, Assistive Technologies, AI, LLM, Blind and Low-vision},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3663547.3746356,
author = {Xiao, Lan and Bandukda, Maryam and Li, Franklin Mingzhe and Colley, Mark and Holloway, Catherine},
title = {Understanding the Video Content Creation Journey of Creators with Sensory Impairment in Kenya},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3746356},
doi = {10.1145/3663547.3746356},
abstract = {Video content creation offers vital opportunities for expression and participation, yet remains largely inaccessible to creators with sensory impairments, especially in low-resource settings. We conducted interviews with 20 video creators with visual and hearing impairments in Kenya to examine their tools, challenges, and collaborative practices. Our findings show that accessibility barriers and infrastructural limitations shape video creation as a staged, collaborative process involving trusted human partners and emerging AI tools. Across workflows, creators actively negotiated agency and trust, maintaining creative control while bridging sensory gaps. We discuss the need for flexible, interdependent collaboration models, inclusive human-AI workflows, and diverse storytelling practices. This work broadens accessibility research in HCI by examining how technology and social factors intersect in low-resource contexts, suggesting ways to better support disabled creators globally.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {42},
numpages = {14},
keywords = {Video Content Creation, Accessibility, Sensory Impairment, LMICs},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3597638.3614490,
author = {Chheda-Kothary, Arnavi and Rios, David A and Smith, Kynnedy Simone and Reyna, Avery and Zhang, Cecilia and Smith, Brian A.},
title = {Understanding Blind and Low Vision Users' Attitudes Towards Spatial Interactions in Desktop Screen Readers},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614490},
doi = {10.1145/3597638.3614490},
abstract = {Desktop screen readers as a web navigation mechanism for BLV users are tedious and frozen in time, especially in the face of richer ways of presenting spatial information such as tactile and touchscreen devices. In our work, we consider what it means to create and evaluate systems that can present a similarly rich, spatial interaction mechanism plugged into existing screen reader paradigms. We present a formative study conducted with SpaceNav, a custom screen reader that utilizes spatial input and output to navigate two different web applications. We present results from this study, and discuss a new browser extension we are implementing based on our formative study feedback to more robustly test spatial interactions in the context of real world websites. To close, we describe our goals for evaluating the new web extension in a future study.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {83},
numpages = {5},
keywords = {Blind or low vision users, accessibility, desktop web applications, human-computer interaction, screen readers, spatialized layout},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3597638.3608418,
author = {Zhang, Zhuohao and Kim, Gene S-H and Wobbrock, Jacob O.},
title = {Developing and Deploying a Real-World Solution for Accessible Slide Reading and Authoring for Blind Users},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608418},
doi = {10.1145/3597638.3608418},
abstract = {Presentation software like Microsoft PowerPoint and Google Slides remains largely inaccessible for blind users because screen readers are not well suited to 2-D “artboards” that contain different objects in arbitrary arrangements lacking any inherent reading order. To investigate this problem, prior work by Zhang \& Wobbrock (2023) developed multimodal interaction techniques in a prototype system called A11yBoard, but their system was limited to a single artboard in a self-contained prototype and was unable to support real-world use. In this work, we present a major extension of A11yBoard that expands upon its initial interaction techniques, addresses numerous real-world issues, and makes it deployable with Google Slides. We describe the new features developed for A11yBoard for Google Slides along with our participatory design process with a blind co-author. We also present two case studies based on real-world deployments showing that participants were able to independently complete slide reading and authoring tasks that were not possible without sighted assistance previously. We conclude with several design guidelines for making accessible digital content creation tools.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {47},
numpages = {15},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3544548.3580655,
author = {Zhang, Zhuohao (Jerry) and Wobbrock, Jacob O.},
title = {A11yBoard: Making Digital Artboards Accessible to Blind and Low-Vision Users},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580655},
doi = {10.1145/3544548.3580655},
abstract = {Digital artboards, which hold objects rather than pixels (e.g., Microsoft PowerPoint and Google Slides), remain largely inaccessible for blind and low-vision (BLV) users. Building on prior findings about the experiences of BLV users with digital artboards, we present a novel tool called A11yBoard, an interactive multimodal system that makes interpreting and authoring digital artboards accessible. A11yBoard combines a web-based drawing canvas paired with a mobile touch screen device such as a tablet. The mobile device displays the same canvas and enables risk-free spatial exploration of the artboard via touch and gesture. Speech recognition, non-speech audio, and keyboard-based commands are also used for input and output. Through a series of pilot studies and formal task-based user studies with BLV participants, we show that A11yBoard provides (1) intuitive spatial reasoning about two-dimensional objects, (2) multimodal access to objects’ properties and relationships, and (3) eyes-free creating and editing of objects to establish their desired properties and positions.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {55},
numpages = {17},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3715336.3735685,
author = {Cheema, Maryam and Seifi, Hasti and Fazli, Pooyan},
title = {Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735685},
doi = {10.1145/3715336.3735685},
abstract = {Audio descriptions (AD) make videos accessible for blind and low vision (BLV) users by describing visual elements that cannot be understood from the main audio track. AD created by professionals or novice describers is time-consuming and offers little customization or control to BLV viewers on description length and content and when they receive it. To address this gap, we explore user-driven AI-generated descriptions, enabling BLV viewers to control both the timing and level of detail of the descriptions they receive. In a study, 20 BLV participants activated audio descriptions for seven different video genres with two levels of detail: concise and detailed. Our findings reveal differences in the preferred frequency and level of detail of ADs for different videos, participants’ sense of control with this style of AD delivery, and its limitations. We discuss the implications of these findings for the development of future AD tools for BLV users.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {458–474},
numpages = {17},
keywords = {audio description, online videos, accessibility, multimodal large language models, blind and low vision},
location = {
},
series = {DIS '25}
}

@inproceedings{10.1145/3491102.3502092,
author = {Zhang, Mingrui Ray and Zhong, Mingyuan and Wobbrock, Jacob O.},
title = {Ga11y: An Automated GIF Annotation System for Visually Impaired Users},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502092},
doi = {10.1145/3491102.3502092},
abstract = {Animated GIF images have become prevalent in internet culture, often used to express richer and more nuanced meanings than static images. But animated GIFs often lack adequate alternative text descriptions, and it is challenging to generate such descriptions automatically, resulting in inaccessible GIFs for blind or low-vision (BLV) users. To improve the accessibility of animated GIFs for BLV users, we provide a system called Ga11y (pronounced “galley”), for creating GIF annotations. Ga11y combines the power of machine intelligence and crowdsourcing and has three components: an Android client for submitting annotation requests, a backend server and database, and a web interface where volunteers can respond to annotation requests. We evaluated three human annotation interfaces and employ the one that yielded the best annotation quality. We also conducted a multi-stage evaluation with 12 BLV participants from the United States and China, receiving positive feedback.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {197},
numpages = {16},
keywords = {GIF, accessibility., blind, crowdsourcing, human annotation, images, low vision, text description},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@INPROCEEDINGS{11236096,
  author={Dang, Khang and Lee, Sooyeon},
  booktitle={2025 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Personalized Conversational Audio Descriptions in 360° Virtual Reality for Blind and Low-Vision Users}, 
  year={2025},
  volume={},
  number={},
  pages={961-962},
  keywords={Headphones;Visualization;Pipelines;Speech recognition;Glass;Real-time systems;Museums;Encoding;Text to speech;Delays;Conversational AI;virtual reality;accessibility;audio description},
  doi={10.1109/ISMAR-Adjunct68609.2025.00268}}

@inproceedings{10.1145/3587281.3587293,
author = {Knaeble, Merlin and Sailer, Gabriel and Chen, Zihan and Schwarz, Thorsten and Yang, Kailun and Nadj, Mario and Stiefelhagen, Rainer and Maedche, Alexander},
title = {AutoChemplete - Making Chemical Structural Formulas Accessible},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587281.3587293},
doi = {10.1145/3587281.3587293},
abstract = {Despite their interests, blind and low vision students avoid STEM subjects. Research attributes this to a lack of accessible material. Annotating STEM content such as chemical structural formulas requires expert domain knowledge, is time consuming, and frustrating. We conduct interviews with blind and low vision chemists and accessibility professionals to derive requirements for tool support. On this basis, we develop AutoChemplete, an interactive labeling tool for chemical structural formulas. It ingests images and uses machine learning to predict the molecule. With a similarity search in the solution space, we enable even novices to simply pick from options. From this we are able to generate accessible representations. We conduct fifteen think-aloud sessions with participants of varying domain expertise and find support of different annotation styles simultaneously. Not only does AutoChemplete strike a balance in skill-support, participants even find it entertaining.},
booktitle = {Proceedings of the 20th International Web for All Conference},
pages = {104–115},
numpages = {12},
keywords = {SMILES, STEM, accessibility, autocomplete, chemistry, interactive labeling, structural formula},
location = {Austin, TX, USA},
series = {W4A '23}
}

@inproceedings{10.1145/3493612.3520470,
author = {Silva, Jorge Sassaki Resende and Freire, Andr\'{e} Pimenta and Cardoso, Paula Christina Figueira},
title = {When headers are not there: design and user evaluation of an automatic topicalisation and labelling tool to aid the exploration of web documents by blind users},
year = {2022},
isbn = {9781450391702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493612.3520470},
doi = {10.1145/3493612.3520470},
abstract = {Using headers to grasp a document's structure has been one of the main strategies employed by blind users on web documents when using screen readers. However, when headers are not available or not appropriately marked up, they can cause serious difficulties. This paper presents the design and evaluation of a tool for automatically generating headers for screen readers with topicalisation and labelling algorithms. The proposed tool uses Natural Language Processing techniques to divide a web document into topic segments and label each segment based on its content. We conducted an initial user study with eight blind and partially-sighted screen reader users. The evaluation involved tasks with questions answered by participants with information from texts with and without automatically generated headers. Results provided preliminary indicators of improvement in performance and reduction of cognitive load. The findings contribute to knowledge to improve tools to aid in text exploration. It also provides initial empirical evidence to be further explored to analyze the impact of automatically-generated headings in improving performance and reducing cognitive load for blind users.},
booktitle = {Proceedings of the 19th International Web for All Conference},
articleno = {18},
numpages = {11},
keywords = {screen readers, natural language processing, automatic topicalisation and labelling, accessibility},
location = {Lyon, France},
series = {W4A '22}
}

@InProceedings{10.1007/978-3-032-04999-5_3,
author="Oliveira, Alberto Dumont Alves
and Aljedaani, Wajdi
and Eler, Marcelo Medeiros",
editor="Ardito, Carmelo
and Diniz Junqueira Barbosa, Simone
and Conte, Tayana
and Freire, Andr{\'e}
and Gasparini, Isabela
and Palanque, Philippe
and Prates, Raquel",
title="Assessing Visual Impairment Feedback in Mobile Applications",
booktitle="Human-Computer Interaction -- INTERACT 2025",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="36--59",
abstract="Mobile software development emphasizes frequent deliveries by incorporating improvements based on user and stakeholder feedback. In this context, user reviews posted on platforms such as the Google Play Store contain important information about various quality aspects, including accessibility. Although prior studies have investigated accessibility issues through user reviews, few have explicitly mapped this feedback to established accessibility standards or guidelines, which could reinforce the relevance of such standards and illustrate the real impact of accessibility shortcomings on users with disabilities. This study addresses this gap by examining reviews related to visual impairments and ocular conditions and linking them to accessibility guidelines. We used a public dataset of 4,999 accessibility-related reviews of Android apps, applying manual content analysis to associate feedback with the BBC Mobile Accessibility Guidelines. Findings highlight frequent issues with ``Adjustability'', ``Colour Contrast'', and ``Content Resizing''.",
isbn="978-3-032-04999-5"
}

@article{10.1016/j.infsof.2025.107821,
author = {Vera-Amaro, Guillermo and Rojano-C\'{a}ceres, Jos\'{e} Rafael},
title = {Towards accessible website design through artificial intelligence: A systematic literature review},
year = {2025},
issue_date = {Oct 2025},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {186},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2025.107821},
doi = {10.1016/j.infsof.2025.107821},
journal = {Inf. Softw. Technol.},
month = oct,
numpages = {31},
keywords = {Web accessibility, Systematic literature review, Artificial intelligence, Wcag, Machine learning, Large language models}
}

@InProceedings{10.1007/978-3-032-04999-5_6,
author="Penna, Giuseppe Della
and Buzzi, Marina
and Leporini, Barbara",
editor="Ardito, Carmelo
and Diniz Junqueira Barbosa, Simone
and Conte, Tayana
and Freire, Andr{\'e}
and Gasparini, Isabela
and Palanque, Philippe
and Prates, Raquel",
title="Generative AI as a New Assistive Technology for Web Interaction",
booktitle="Human-Computer Interaction -- INTERACT 2025",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="91--107",
abstract="For users who are unfamiliar with technology or rely on assistive tools such as screen readers, interacting with a web page can be challenging. Ensuring a seamless experience requires a well-designed user interface (UI) that prioritizes accessibility and usability. However, achieving this target demands specialized expertise from developers and can involve significant effort. In this context, Generative Artificial Intelligence (GAI) has become a valuable aid for improving access to information and facilitating interaction with web interfaces. To effectively enhance user interaction---such as accessing services or specific functionalities---AI-driven tools must first be capable of understanding the structure and content of a web page. This study investigates if GAIs can be exploited to assist the user when navigating through a website, describing the site contents, explaining the interface structure and interactive elements, and suggesting actions or procedures to follow to perform a certain task or accomplish a specific goal. This kind of assistive technology can benefit not only visually impaired people but also persons with cognitive impairment and, more generally, people that are not ``skilled'' with modern web applications, like seniors. Specifically, thirteen popular websites were analyzed by asking Copilot one hundred questions. Results suggest that GAIs have the potential to assist people in web tasks. However, limitations have still been detected, with 20{\%} of completely erroneous answers received from the navigation and interaction questions and 15{\%} for those related to structure, mainly detected in pages having scarce accessibility and sites having a complex HTML structure, respectively.",
isbn="978-3-032-04999-5"
}

@article{Nino06062025,
author = {Juan Nino and Jocelyne Kiss and Frédérique Poncet and Walter Wittich and Geoffreyjen Edwards and Ernesto Morales},
title = {Toward improving internet navigation for visually impaired screen Reader users: Co-designing an open-source assistive technology system},
journal = {Assistive Technology},
volume = {0},
number = {0},
pages = {1--12},
year = {2025},
publisher = {Taylor \& Francis},
doi = {10.1080/10400435.2025.2509699},
note ={PMID: 40478980},
URL = {https://doi.org/10.1080/10400435.2025.2509699},
eprint = {https://doi.org/10.1080/10400435.2025.2509699},
abstract = { Visually impaired individuals, estimated at 285 million globally, rely heavily on-screen readers for internet access. However, much of the visually available information, such as the relationship between webpage elements, does not translate well to its textual representation and must be always kept in memory, limiting contextual interactions. To address this challenge, we developed Touch Matrix Assistive Technology Navigator (TOMAT), an open-source system that works alongside screen readers to provide an interactive, audio-tactile representation of webpage structure and enable contextual interactions. Our study employed a participatory design approach, involving visually impaired users, healthcare professionals, engineers, and community organizations in co-design sessions, prototype demonstrations, and focus groups. The resulting system extracts and presents non-linear web information at multiple levels of detail, allowing users to dynamically adjust granularity and efficiently navigate and interact with web content. Participants reported that TOMAT enhanced their understanding of webpage structure and provided an intuitive complement to screen reader software. The findings suggest TOMAT has the potential to improve the internet navigation experience for visually impaired users, fostering greater independence and digital participation. To support further development and collaboration, TOMAT’s source files have been released under an open-source license. }
}

@article{10.1007/s10209-020-00777-w,
author = {Aqle, Aboubakr and Al-Thani, Dena and Jaoua, Ali},
title = {Can search result summaries enhance the web search efficiency and experiences of the visually impaired users?},
year = {2022},
issue_date = {Mar 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {1},
issn = {1615-5289},
url = {https://doi.org/10.1007/s10209-020-00777-w},
doi = {10.1007/s10209-020-00777-w},
abstract = {There are limited studies that are addressing the challenges of visually impaired (VI) users when viewing search results on a search engine interface by using a screen reader. This study investigates the effect of providing an overview of search results to VI users. We present a novel interactive search engine interface called InteractSE to support VI users during the results exploration stage in order to improve their interactive experience and web search efficiency. An overview of the search results is generated using an unsupervised machine learning approach to present the discovered concepts via a formal concept analysis that is domain-independent. These concepts are arranged in a multi-level tree following a hierarchical order and covering all retrieved documents that share maximal features. The InteractSE interface was evaluated by 16 legally blind users and compared with the Google search engine interface for complex search tasks. The evaluation results were obtained based on both quantitative (as task completion time) and qualitative (as participants’ feedback) measures. These results are promising and indicate that InteractSE enhances the search efficiency and consequently advances user experience. Our observations and analysis of the user interactions and feedback yielded design suggestions to support VI users when exploring and interacting with search results.},
journal = {Univers. Access Inf. Soc.},
month = mar,
pages = {171–192},
numpages = {22},
keywords = {Web accessibility, User interaction, Usability, Interface evaluation, Visually impaired users, Search result summarization, Search engine interface}
}

@inproceedings{10.1145/3491102.3517695,
author = {Zhang, Lotus and Shao, Jingyao and Liu, Augustina Ao and Jiang, Lucy and Stangl, Abigale and Fourney, Adam and Morris, Meredith Ringel and Findlater, Leah},
title = {Exploring Interactive Sound Design for Auditory Websites},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517695},
doi = {10.1145/3491102.3517695},
abstract = {Auditory interfaces increasingly support access to website content, through recent advances in voice interaction. Typically, however, these interfaces provide only limited audio styling, collapsing rich visual design into a static audio output style with a single synthesized voice. To explore the potential for more aesthetic and intuitive sound design for websites, we prompted 14 professional sound designers to create auditory website mockups and interviewed them about their designs and rationale. Our findings reveal their prioritized design considerations (aesthetics and emotion, user engagement, audio clarity, information dynamics, and interactivity), specific sound design ideas to support each consideration (e.g., replacing spoken labels with short, memorable audio expressions), and challenges with applying sound design practices to auditory websites. These findings provide promising direction for how to support designers in creating richer auditory website experiences.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {222},
numpages = {16},
keywords = {audio display, interaction design, voice interaction},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@InProceedings{10.1007/978-3-032-05005-2_17,
author="Usabaev, Bela
and B{\"u}hrich, Nicola
and Ta{\c{s}}, Tutku Can
and Weber, Gerhard
and Ondrusek, Petra",
editor="Ardito, Carmelo
and Diniz Junqueira Barbosa, Simone
and Conte, Tayana
and Freire, Andr{\'e}
and Gasparini, Isabela
and Palanque, Philippe
and Prates, Raquel",
title="Integrating Touch, Gestures and Speech for Multi-modal Conversations with an Audio-Tactile Graphics Reader",
booktitle="Human-Computer Interaction -- INTERACT 2025",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="323--332",
abstract="Screen readers present one cell of a spreadsheet at a time and provide only a limited overview on a single table such as its dimensions. Screen reader users struggle, for example, in recognizing labels to explain another cell's purpose. In a Wizard-of-Oz study with two wizards as voice agents generating speech feedback we explore a novel audio-tactile graphics reader with tactile grid-based overlays and spoken feedback for touch to enable screen reader users to engage in a conversation about the spreadsheet with a voice assistant and utilize a screen reader to solve spreadsheet calculation tasks. In a pilot with 3 and a main study with 8 BLV students, we identify multi-modal interaction patterns and confirm the importance of two separate roles of speakers: a voice assistant and a screen reader. The conversation is driven by user's multi-modal speech input and hand gestures provided sequentially or in parallel. Verbal references to cells by spoken addresses, values, and formulas can be embodied as tangible objects to unify tactile and verbal representations.",
isbn="978-3-032-05005-2"
}

@inproceedings{10.1145/3746059.3747797,
author = {Peng, Yi-Hao and Li, Dingzeyu and Bigham, Jeffrey P and Pavel, Amy},
title = {Morae: Proactively Pausing UI Agents for User Choices},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747797},
doi = {10.1145/3746059.3747797},
abstract = {User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {198},
numpages = {14},
keywords = {Agents; User Interface Agents; Proactive Agents; Human-Agent Interaction; Accessibility; Generative UI},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3706598.3714125,
author = {Chheda-Kothary, Arnavi and Sharif, Ather and Rios, David Angel and Smith, Brian A.},
title = {"It Brought Me Joy": Opportunities for Spatial Browsing in Desktop Screen Readers},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714125},
doi = {10.1145/3706598.3714125},
abstract = {Blind or low-vision (BLV) screen-reader users have a significantly limited experience interacting with desktop websites compared to non-BLV, i.e., sighted users. This digital divide is exacerbated by the incapability to browse the web spatially—an affordance that leverages spatial reasoning, which sighted users often rely on. In this work, we investigate the value of and opportunities for BLV screen-reader users to browse websites spatially (e.g., understanding page layouts). We additionally explore at-scale website layout understanding as a feature of desktop screen readers. We created a technology probe, WebNExt, to facilitate our investigation. Specifically, we conducted a lab study with eight participants and a five-day field study with four participants to evaluate spatial browsing using WebNExt. Our findings show that participants found spatial browsing intuitive and fulfilling, strengthening their connection to the design of web pages. Furthermore, participants envisioned spatial browsing as a step toward reducing the digital divide.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {974},
numpages = {18},
keywords = {Blind or low-vision users; accessibility; desktop web applications; spatial awareness},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3597638.3614548,
author = {Glazko, Kate S and Yamagami, Momona and Desai, Aashaka and Mack, Kelly Avery and Potluri, Venkatesh and Xu, Xuhai and Mankoff, Jennifer},
title = {An Autoethnographic Case Study of Generative Artificial Intelligence's Utility for Accessibility},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614548},
doi = {10.1145/3597638.3614548},
abstract = {With the recent rapid rise in Generative Artificial Intelligence (GAI) tools, it is imperative that we understand their impact on people with disabilities, both positive and negative. However, although we know that AI in general poses both risks and opportunities for people with disabilities, little is known specifically about GAI in particular. To address this, we conducted a three-month autoethnography of our use of GAI to meet personal and professional needs as a team of researchers with and without disabilities. Our findings demonstrate a wide variety of potential accessibility-related uses for GAI while also highlighting concerns around verifiability, training data, ableism, and false promises.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {99},
numpages = {8},
keywords = {ableism, accessibility, auto-ethnography, generative artificial intelligence},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3663547.3759727,
author = {Raman, Gayatri},
title = {“Somewhere Between Images and Art”: How Blind Creators Engage with AI Image Generators},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3759727},
doi = {10.1145/3663547.3759727},
abstract = {Blind and low-vision (BLV) artists working in visual media navigate systemic barriers shaped by ableist norms that question their creative legitimacy. While AI image generators like Midjourney are gaining prominence in visual art practice, little is known about how BLV artists engage with these tools. This study explores how six BLV artists across diverse media interacted with Midjourney through guided prompting and interviews. Participants approached the tool with curiosity, investigating its potential for inspiration, evaluation, and fragment generation for multisensory works, while drawing clear boundaries around authorship and creative agency. Digital artists expressed particular vulnerability to assumptions of overreliance on AI, while others saw limited opportunities to embed embodied, adaptive, and intersectional elements central to their practice. We contribute to HCI by foregrounding BLV artists’ perspectives and identifying design opportunities for generative tools that prioritize accessibility, support dialogic meaning-making, and validate BLV artistic authorship in visual fields.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {148},
numpages = {5},
keywords = {Blind art, disability, generative AI, visual impairment},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3663547.3759725,
author = {Cao, Xinyun and Ju, Kexin Phyllis and Li, Chenglin and Potluri, Venkatesh and Jain, Dhruv},
title = {Demo of RAVEN: Realtime Accessibility in Virtual ENvironments for Blind and Low-Vision People},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3759725},
doi = {10.1145/3663547.3759725},
abstract = {As virtual 3D environments become prevalent, enabling presence and spatial exploration, equitable access is crucial for blind and low-vision (BLV) users who face challenges with spatial awareness, navigation, and interaction. To address these, previous work explored enhancing visual information or supplementing it with auditory and haptic modalities. However, these methods are static and might risk steep learning curves. In this work, we present RAVEN, a system that responds to query or modification prompts from BLV users to improve the accessibility of a 3D virtual scene at runtime. The system integrates LLMs with semantic scene data and runtime code generation to support iterative, dialogue-based user interactions. We evaluated the system with eight BLV people, uncovering key insights into the strengths and shortcomings of generative AI-driven accessibility in virtual 3D environments.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {168},
numpages = {5},
keywords = {Accessibility, blind and low-vision, virtual 3D environment, generative AI.},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3746059.3747597,
author = {Tang, Yilin and Fang, Yuyang and Wang, Tianle and Sun, Lingyun and Chen, Liuqing},
title = {"This is My Fault", Really? Understanding Blind and Low-Vision People’s Perception of Hallucination in Large Vision Language Models},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747597},
doi = {10.1145/3746059.3747597},
abstract = {Visual question-answering (VQA) tools powered by large visual language models (LVLMs) are used to assist blind and low-vision (BLV) individuals in overcoming visual challenges, raising concerns about hallucinations and associated risks. Existing literature overlooks the variations of hallucinations across distinct usage scenarios and types in the context of VQA for BLV people, resulting in limited understanding of their perceptions and insufficient guidance for targeted mitigation strategies. By analyzing 3,467 real-world VQA cases from BLV users, we developed a manifestation-scenario-based dual-dimensional hallucination typology, uncovering eight scenarios and five types of hallucinations. Through interviews with 16 BLV users, we examined their awareness levels, detection strategies, mental models of hallucinations, and their tolerance of associated risks, identifying key gaps between their perceptions and real situations. By designing with 12 BLV users, we uncovered their expectations for hallucination-mitigating solutions, including enhanced information provision, transparency in processing, verification strategies, and feedback mechanisms.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {44},
numpages = {20},
keywords = {Large visual language models (LVLMs), Artificial intelligence (AI), Hallucination, Blind and low vision, Human-centered AI},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3733155.3737910,
author = {Leporini, Barbara and Buzzi, Marina and Della Penna, Giuseppe},
title = {A Preliminary Evaluation of Generative AI Tools for Blind Users: Usability and Screen Reader Interaction},
year = {2025},
isbn = {9798400714023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733155.3737910},
doi = {10.1145/3733155.3737910},
abstract = {The increasing use of Generative Artificial Intelligence (GAI) tools such as ChatGPT, Copilot, Perplexity and Gemini opens up new possible scenarios for supporting work and everyday activities. For people who are blind, the usability of such tools through screen readers is crucial to ensure their use of such AI-based technologies. In this study, we explore the accessibility and usability of the interfaces of four popular AI-based tools via screen readers through a combination of semi-automated evaluations and inspections conducted by both sighted and blind accessibility experts and screen readers with more than 20 years of experience. Navigation, labeling of control elements, feedback mechanisms, and prompt handling were considered in the study. The results point to usability difficulties in all tools, particularly in navigation structure, clarity of feedback and interactive elements. Although this work empirically explores the accessibility of AI-based tools it brings out the first critical issues that deserve further investigation. However, they are based on a small group of experts and thus should be considered preliminary and useful for future studies.},
booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {562–568},
numpages = {7},
keywords = {Accessibility, Blind users, ChatGPT, Copilot, Gemini, Generative AI, Perplexity, screen reader interaction},
location = {
},
series = {PETRA '25}
}

@inproceedings{10.1145/3706598.3714121,
author = {Atcheson, Alex and Khan, Omar and Siemann, Brian and Jain, Anika and Karahalios, Karrie},
title = {"I'd Never Actually Realized How Big An Impact It Had Until Now": Perspectives of University Students with Disabilities on Generative Artificial Intelligence},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714121},
doi = {10.1145/3706598.3714121},
abstract = {Prior research on the experiences of students with disabilities in higher education has surfaced a number of barriers that prevent full inclusion. Generative artificial intelligence (GenAI) has begun to attract interest for its potential to address longstanding barriers to access. However, little is known about the impact of these tools on the living and learning experiences of post-secondary students with disabilities. As a mixed-abilities team, we investigated student experiences with GenAI tools by collecting survey and interview responses from 62 and 21 students with disabilities, respectively, across two universities to measure students’ use of GenAI tools and their perspectives on the impact of these tools in ways related to disability, university support, and sense of belonging. Despite concerns over potential risks of GenAI and unclear university policies, students described GenAI tools as a useful resource for personalizing learning, promoting self-care, and assisting with important self-advocacy work. Guidance demonstrating safe, acceptable uses of GenAI tools, along with clear policies and resources that acknowledge diverse student needs, were desired. We discuss implications of these tools for accessibility and inclusion in higher education.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {42},
numpages = {22},
keywords = {Students with Disabilities, Higher Education, Generative Artificial Intelligence, Student Perspectives},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3490099.3511126,
author = {Ferdous, Javedul and Lee, Hae-Na and Jayarathna, Sampath and Ashok, Vikas},
title = {InSupport: Proxy Interface for Enabling Efficient Non-Visual Interaction with Web Data Records},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511126},
doi = {10.1145/3490099.3511126},
abstract = {Interaction with web data records typically involves accessing auxiliary webpage segments such as filters, sort options, search form, and multi-page links. As these segments are usually scattered all across the screen, it is arduous and tedious for blind users who rely on screen readers to access the segments, given that content navigation with screen readers is predominantly one-dimensional, despite the available support for skipping content via either special keyboard shortcuts or selective navigation. The extant techniques to overcome inefficient web screen reader interaction have mostly focused on general web content navigation, and as such they provide little to no support for data record-specific interaction activities such as filtering and sorting – activities that are equally important for enabling quick and easy access to the desired data records. To fill this void, we present InSupport, a browser extension that: (i) employs custom-built machine learning models to automatically extract auxiliary segments on any webpage containing data records, and (ii) provides an instantly accessible proxy one-stop interface for easily navigating the extracted segments using basic screen reader shortcuts. An evaluation study with 14 blind participants showed significant improvement in usability with InSupport, driven by increased reduction in interaction time and the number of key presses, compared to state-of-the-art solutions.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {49–62},
numpages = {14},
keywords = {Blind, Data records, Screen reader, Visual impairment, Web accessibility},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.1145/3664639,
author = {Prakash, Yash and Nayak, Akshay Kolgar and Sunkara, Mohan and Jayarathna, Sampath and Lee, Hae-Na and Ashok, Vikas},
title = {All in One Place: Ensuring Usable Access to Online Shopping Items for Blind Users},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {EICS},
url = {https://doi.org/10.1145/3664639},
doi = {10.1145/3664639},
abstract = {Perusing web data items such as shopping products is a core online user activity. To prevent information overload, the content associated with data items is typically dispersed across multiple webpage sections over multiple web pages. However, such content distribution manifests an unintended side effect of significantly increasing the interaction burden for blind users, since navigating to-and-fro between different sections in different pages is tedious and cumbersome with their screen readers. While existing works have proposed methods for the context of a single webpage, solutions enabling usable access to content distributed across multiple webpages are few and far between. In this paper, we present InstaFetch, a browser extension that dynamically generates an alternative screen reader-friendly user interface in real-time, which blind users can leverage to almost instantly access different item-related information such as description, full specification, and user reviews, all in one place, without having to tediously navigate to different sections in different webpages. Moreover, InstaFetch also supports natural language queries about any item, a feature blind users can exploit to quickly obtain desired information, thereby avoiding manually trudging through reams of text. In a study with 14 blind users, we observed that the participants needed significantly lesser time to peruse data items with InstaFetch, than with a state-of-the-art solution.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {257},
numpages = {25},
keywords = {Blind, Online shopping, Screen reader, Visual impairment, Web usability}
}

@article{10.1145/3664639,
author = {Prakash, Yash and Nayak, Akshay Kolgar and Sunkara, Mohan and Jayarathna, Sampath and Lee, Hae-Na and Ashok, Vikas},
title = {All in One Place: Ensuring Usable Access to Online Shopping Items for Blind Users},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {EICS},
url = {https://doi.org/10.1145/3664639},
doi = {10.1145/3664639},
abstract = {Perusing web data items such as shopping products is a core online user activity. To prevent information overload, the content associated with data items is typically dispersed across multiple webpage sections over multiple web pages. However, such content distribution manifests an unintended side effect of significantly increasing the interaction burden for blind users, since navigating to-and-fro between different sections in different pages is tedious and cumbersome with their screen readers. While existing works have proposed methods for the context of a single webpage, solutions enabling usable access to content distributed across multiple webpages are few and far between. In this paper, we present InstaFetch, a browser extension that dynamically generates an alternative screen reader-friendly user interface in real-time, which blind users can leverage to almost instantly access different item-related information such as description, full specification, and user reviews, all in one place, without having to tediously navigate to different sections in different webpages. Moreover, InstaFetch also supports natural language queries about any item, a feature blind users can exploit to quickly obtain desired information, thereby avoiding manually trudging through reams of text. In a study with 14 blind users, we observed that the participants needed significantly lesser time to peruse data items with InstaFetch, than with a state-of-the-art solution.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {257},
numpages = {25},
keywords = {Blind, Online shopping, Screen reader, Visual impairment, Web usability}
}

@article{10.1145/3519032,
author = {Barbosa, Nat\~{a} M. and Hayes, Jordan and Kaushik, Smirity and Wang, Yang},
title = {“Every Website Is a Puzzle!”: Facilitating Access to Common Website Features for People with Visual Impairments},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7228},
url = {https://doi.org/10.1145/3519032},
doi = {10.1145/3519032},
abstract = {Navigating unfamiliar websites is challenging for users with visual impairments. Although many websites offer visual cues to facilitate access to pages/features most websites are expected to have (e.g., log in at the top right), such visual shortcuts are not accessible to users with visual impairments. Moreover, although such pages serve the same functionality across websites (e.g., to log in, to sign up), the location, wording, and navigation path of links to these pages vary from one website to another. Such inconsistencies are challenging for users with visual impairments, especially for users of screen readers, who often need to linearly listen to content of pages to figure out how to access certain website features. To study how to improve access to main website features, we iteratively designed and tested a command-based approach for main features of websites via a browser extension powered by machine learning and human input. The browser extension gives users a way to access high-level website features (e.g., log in, find stores, contact) via keyboard commands. We tested the browser extension in a lab setting with 15 Internet users, including 9 users with visual impairments and 6 without. Our study showed that commands for main website features can greatly improve the experience of users with visual impairments. People without visual impairments also found command-based access helpful when visiting unfamiliar, cluttered, or infrequently visited websites, suggesting that this approach can support users with visual impairments while also benefiting other user groups (i.e., universal design). Our study reveals concerns about the handling of unsupported commands and the availability and trustworthiness of human input. We discuss how websites, browsers, and assistive technologies could incorporate a command-based paradigm to enhance web accessibility and provide more consistency on the web to benefit users with varied abilities when navigating unfamiliar or complex websites.},
journal = {ACM Trans. Access. Comput.},
month = jul,
articleno = {19},
numpages = {35},
keywords = {website commands, intelligent personal assistants, Website accessibility}
}

@inproceedings{10.1145/3532106.3533522,
author = {Jung, Ju Yeon and Steinberger, Tom and Kim, Junbeom and Ackerman, Mark S.},
title = {“So What? What's That to Do With Me?” Expectations of People With Visual Impairments for Image Descriptions in Their Personal Photo Activities},
year = {2022},
isbn = {9781450393584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532106.3533522},
doi = {10.1145/3532106.3533522},
abstract = {People with visual impairments (PVI) access photos through image descriptions. Thus far, research has studied what PVI expect in these descriptions mostly regarding functional purposes (e.g., identifying an object) and when engaging with online, publicly available images. Extending this research, we interviewed 30 PVI to understand their expectations for image descriptions when viewing, taking, searching, and reminiscing with personal photos on their own devices. We show how their expectations varied across photo activities and often went well beyond identifying objects in photos. Based on our findings, we propose design opportunities for generating and providing image descriptions for personal photo use by PVI. The design opportunities for PVI also point to novel support for the sighted for using image descriptions to enrich their experience of photos.&nbsp;},
booktitle = {Proceedings of the 2022 ACM Designing Interactive Systems Conference},
pages = {1893–1906},
numpages = {14},
location = {Virtual Event, Australia},
series = {DIS '22}
}

@inproceedings{10.1145/3544548.3581532,
author = {Kim, Jiho and Srinivasan, Arjun and Kim, Nam Wook and Kim, Yea-Seul},
title = {Exploring Chart Question Answering for Blind and Low Vision Users},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581532},
doi = {10.1145/3544548.3581532},
abstract = {Data visualizations can be complex or involve numerous data points, making them impractical to navigate using screen readers alone. Question answering (QA) systems have the potential to support visualization interpretation and exploration without overwhelming blind and low vision (BLV) users. To investigate if and how QA systems can help BLV users in working with visualizations, we conducted a Wizard of Oz study with 24 BLV people where participants freely posed queries about four visualizations. We collected 979 queries and mapped them to popular analytic task taxonomies. We found that retrieving value and finding extremum were the most common tasks, participants often made complex queries and used visual references, and the data topic notably influenced the queries. We compile a list of design considerations for accessible chart QA systems and make our question corpus publicly available to guide future research and development.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {828},
numpages = {15},–
keywords = {Accessibility, Design Considerations, Human-Subjects Qualitative Studies, Question Answering, Visualization},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1007/978-3-031-97207-2_29,
author = {Piro, Ludovica and Di Fede, Giulia and Pucci, Emanuele and Tolomeo, Stefano and Matera, Maristella},
title = {Leveraging LLMs for Voice-Based form Filling on the Web: The ConWeb Approach},
year = {2025},
isbn = {978-3-031-97206-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-97207-2_29},
doi = {10.1007/978-3-031-97207-2_29},
abstract = {Web forms are among the most challenging components for individuals using assistive technologies. Improper coding practices can significantly impede screen readers, preventing them from correctly interpreting or even accessing input fields. Conversational interaction presents an opportunity to increase form accessibility. This demonstration presents ConWeb, a conversational platform with which users can interact with web forms using natural language interaction. Our work aims at introducing an alternative and inclusive paradigm to visual web browsing.},
booktitle = {Web Engineering: 25th International Conference, ICWE 2025, Delft, The Netherlands, June 30 – July 3, 2025, Proceedings},
pages = {370–373},
numpages = {4},
keywords = {Voice interaction, Conversational form filling, Accessibility},
location = {Delft, The Netherlands}
}

@inproceedings{10.1145/3764687.3764694,
author = {Palani, Hari and Adnin, Rudaiba and Nagar, Shivangee},
title = {Kanak: Automating the Generation of Accessible STEM Materials for Blind and Low-vision Students},
year = {2025},
isbn = {9798400720161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3764687.3764694},
doi = {10.1145/3764687.3764694},
abstract = {Blind and low-vision (BLV) students face significant barriers to timely access of visual STEM materials. Accessibility practitioners, such as braille transcribers and teachers of the visually impaired (TVIs), rely on manual and labor-intensive processes to produce accessible formats (e.g., braille, tactile graphics) of visual educational materials. To complement this effort and address ongoing accessibility gaps, we developed Kanak, an AI system that leverages generative intelligence to automate the generation of accessible STEM content, including text, math equations, and graphics. Through a comparative study with seven practitioners, we examined how Kanak supports traditional transcribing workflows and augments expert judgment. Our findings reveal how generative tools could complement practitioner expertise by providing context-aware formatting, proofreading support, and on-demand graphics generation. We conclude with design considerations for human-AI collaboration in transcribing work, positioning generative intelligence not as a replacement for expert insights, but as a catalyst for expanding equitable access.},
booktitle = {Proceedings of the 37th Australian Conference on Human-Computer Interaction},
pages = {287–299},
numpages = {13},
keywords = {Accessibility, STEM Education, Blind and Low-Vision (BLV), Human-AI Collaboration, Generative AI, Tactile Graphics, Inclusive Design},
location = {
},
series = {OzCHI '25}
}

@inproceedings{10.1145/3544549.3583909,
author = {Balasubramanian, Harshadha and Morrison, Cecily and Grayson, Martin and Makhataeva, Zhanat and Marques, Rita Faia and Gable, Thomas and Perez, Dalya and Cutrell, Edward},
title = {Enable Blind Users’ Experience in 3D Virtual Environments: The Scene Weaver Prototype},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583909},
doi = {10.1145/3544549.3583909},
abstract = {Three-dimensional virtual environments are currently inaccessible to people who are blind, as current screen-reading solutions for 2D content are not fully extensible to achieve the needed embodied spatial presence. Forefronting perceptual agency as key to any access approach for users who are blind, we offer Scene Weaving as an interactional metaphor that allows users to choose how and when they perceive the environment and the people in it. We illustrate how this metaphor can be implemented in an example prototype system. In this interactivity, users can control how and when they perceive a virtual museum environment and people within it through a range of interaction mechanisms.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {447},
numpages = {4},
keywords = {Accessibility, Interaction Metaphor, Perceptual Agency, Virtual Environments, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3586183.3606830,
author = {Jain, Gaurav and Hindi, Basel and Courtien, Connor and Xu, Xin Yi Therese and Wyrick, Conrad and Malcolm, Michael and Smith, Brian A.},
title = {Front Row: Automatically Generating Immersive Audio Representations of Tennis Broadcasts for Blind Viewers},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606830},
doi = {10.1145/3586183.3606830},
abstract = {Blind and low-vision (BLV) people face challenges watching sports due to the lack of accessibility of sports broadcasts. Currently, BLV people rely on descriptions from TV commentators, radio announcers, or their friends to understand the game. These descriptions, however, do not allow BLV viewers to visualize the action by themselves. We present Front Row, a system that automatically generates an immersive audio representation of sports broadcasts, specifically tennis, allowing BLV viewers to more directly perceive what is happening in the game. Front Row first recognizes gameplay from the video feed using computer vision, then renders players’ positions and shots via spatialized (3D) audio cues. User evaluations with 12 BLV participants show that Front Row gives BLV viewers a more accurate understanding of the game compared to TV and radio, enabling viewers to form their own opinions on players’ moods and strategies. We discuss future implications of Front Row and illustrate several applications, including a Front Row plug-in for video streaming platforms to enable BLV people to visualize the action in sports videos across the Web.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {39},
numpages = {17},
keywords = {Visual impairments, accessibility, computer vision, sports},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3746059.3747791,
author = {Xu, Shuchang and Jin, Xiaofu and Zhang, Wenshuo and Qu, Huamin and Yan, Yukang},
title = {Branch Explorer: Leveraging Branching Narratives to Support Interactive 360° Video Viewing for Blind and Low Vision Users},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747791},
doi = {10.1145/3746059.3747791},
abstract = {360° videos enable users to freely choose their viewing paths, but blind and low vision (BLV) users are often excluded from this interactive experience. To bridge this gap, we present Branch Explorer, a system that transforms 360° videos into branching narratives—stories that dynamically unfold based on viewer choices—to support interactive viewing for BLV audiences. Our formative study identified three key considerations for accessible branching narratives: providing diverse branch options, ensuring coherent story progression, and enabling immersive navigation among branches. To address these needs, Branch Explorer employs a multi-modal machine learning pipeline to generate diverse narrative paths, allowing users to flexibly make choices at detected branching points and seamlessly engage with each storyline through immersive audio guidance. Evaluation with 12 BLV viewers showed that Branch Explorer significantly enhanced user agency and engagement in 360° video viewing. Users also developed personalized strategies for exploring 360° content. We further highlight implications for supporting accessible exploration of videos and virtual environments.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {48},
numpages = {18},
keywords = {Blind, Low Vision, Visual Impairment, 360° Video, Panorama Video, Audio Description, Interactive Storytelling, Virtual Reality},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3663547.3759702,
author = {Yee, Diana and Kobenova, Amina and Jhangiani, Rohan and Stickler, Piper and Kurniawan, Sri},
title = {“Better Than Nothing” or Not Enough? User-Centered Reflections on AI-Generated Audio Descriptions Across Media Formats},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3759702},
doi = {10.1145/3663547.3759702},
abstract = {AI-generated audio descriptions (ADs) offer scalable solutions for making visual media accessible to blind and low-vision (BLV) audiences. Nevertheless, little is known about how BLV users experience and evaluate these descriptions across emerging platforms. In this qualitative study, we conducted semi-structured interviews with ten (N=10) BLV participants, recruited based on divergences in prior survey ratings, to explore their perceptions of both human- and AI-generated ADs in contexts ranging from traditional film to short-form video and livestreams. Thematic analysis revealed five key themes: (1) information prioritization and genre-sensitive details, (2) the “better-than-nothing” consensus tempered by emotional and contextual gaps in AI-generated ADs, (3) the social dynamics of shared viewing, (4) accessibility deserts on new media platforms, and (5) the artistry-precision dilemma. Our findings highlight the need for adaptive, transparent, and user-informed AD systems that balance narrative resonance with efficiency. We conclude with design recommendations for co-designing AI-assisted accessibility tools in partnership with BLV communities.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {121},
numpages = {4},
keywords = {Audio Description, Blind and Low-Vision, Generative AI, Video Accessibility, New Media},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3706599.3719849,
author = {Gupta, Chitralekha and Ram, Ashwin and Sridhar, Shreyas and Jouffrais, Christophe and Nanayakkara, Suranga},
title = {Scene-to-Audio: Distant Scene Sonification for Blind and Low Vision People},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719849},
doi = {10.1145/3706599.3719849},
abstract = {Awareness of distant environmental scenes, or Vista scenes, is necessary for comprehending one’s surroundings and enjoying their beauty in leisurely settings. Current tools for Blind and Low Vision people (BLV) typically use spoken descriptions but lack support for an enjoyable experience of visually pleasing scenes. We propose a Scene-to-Audio framework that generates comprehensible and enjoyable non-verbal sounds representing distant scenes using generative models informed by psycho-acoustics, audio scene analysis, and Foley sound synthesis. We conducted a user study with eleven BLV participants evaluating strategies to utilize our scene-to-audio framework. We found that these sounds, in combination with speech, provide a significantly more memorable and immersive experience of the scenes, compared to only spoken descriptions. Our work is a step towards bridging the gap between a visual and an auditory experience of a scene, addressing the aesthetic needs of BLVs.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {471},
numpages = {9},
keywords = {Blind and Low Vision, People with Visual Impairments, Vista Scenes, Spatial Cognition, Scene Experience, Cognitive and Aesthetic Needs, Sonification, Generative Models for Audio},
location = {
},
series = {CHI EA '25}
}
@inproceedings{10.1145/3663548.3675618,
author = {Dang, Khang and Burke, Grace and Korreshi, Hamdi and Lee, Sooyeon},
title = {Towards Accessible Musical Performances in Virtual Reality: Designing a Conceptual Framework for Omnidirectional Audio Descriptions},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675618},
doi = {10.1145/3663548.3675618},
abstract = {Our research focuses on making musical performance experience in virtual reality (VR) settings non-visually accessible for Blind and Low Vision (BLV) individuals by designing a conceptual framework for omnidirectional audio descriptions (AD). We address BLV users’ prevalent challenges in accessing effective AD during VR musical performances. Employing a two-phased interview methodology, we initially collected qualitative data about BLV AD users’ experiences, followed by gathering insights from BLV professionals who specialize in AD. This approach ensures that the developed solutions are both user-centric and practically feasible. The study devises strategies for three design concepts of omnidirectional AD (Spatial AD, View-dependent AD, and Explorative AD) tailored to different types of musical performances, which vary in their visual and auditory components. Each design concept offers unique benefits; collectively, they enhance accessibility and enjoyment for BLV audiences by addressing specific user needs. Key insights highlight the crucial role of flexibility and user control in AD implementation. Based on these insights, we propose a comprehensive conceptual framework to enhance musical experiences for BLV users within VR environments.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {6},
numpages = {17},
keywords = {Accessibility, Audio Description, Blind and Low Vision Users, Musical Performances, Virtual Reality},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3544548.3580921,
author = {Peng, Yi-Hao and Chi, Peggy and Kannan, Anjuli and Morris, Meredith Ringel and Essa, Irfan},
title = {Slide Gestalt: Automatic Structure Extraction in Slide Decks for Non-Visual Access},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580921},
doi = {10.1145/3544548.3580921},
abstract = {Presentation slides commonly use visual patterns for structural navigation, such as titles, dividers, and build slides. However, screen readers do not capture such intention, making it time-consuming and less accessible for blind and visually impaired (BVI) users to linearly consume slides with repeated content. We present Slide Gestalt, an automatic approach that identifies the hierarchical structure in a slide deck. Slide Gestalt computes the visual and textual correspondences between slides to generate hierarchical groupings. Readers can navigate the slide deck from the higher-level section overview to the lower-level description of a slide group or individual elements interactively with our UI. We derived side consumption and authoring practices from interviews with BVI readers and sighted creators and an analysis of 100 decks. We performed our pipeline with 50 real-world slide decks and a large dataset. Feedback from eight BVI participants showed that Slide Gestalt helped navigate a slide deck by anchoring content more efficiently, compared to using accessible slides.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {829},
numpages = {14},
keywords = {Accessibility, Multimodal correspondence and alignment, Presentation, Screen reader, Slide deck},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3587281.3587284,
author = {Sharif, Ather and Zhang, Andrew M. and Reinecke, Katharina and Wobbrock, Jacob O.},
title = {Understanding and Improving Drilled-Down Information Extraction from Online Data Visualizations for Screen-Reader Users},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587281.3587284},
doi = {10.1145/3587281.3587284},
abstract = {Inaccessible online data visualizations can significantly disenfranchise screen-reader users from accessing critical online information. Current accessibility measures, such as adding alternative text to visualizations, only provide a high-level overview of data, limiting screen-reader users from exploring data visualizations in depth. In this work, we build on prior research to develop taxonomies of information sought by screen-reader users to interact with online data visualizations granularly through role-based and longitudinal studies with screen-reader users. Utilizing these taxonomies, we extended the functionality of VoxLens—an open-source multi-modal system that improves the accessibility of data visualizations—by supporting drilled-down information extraction. We assessed the performance of our VoxLens enhancements through task-based user studies with 10 screen-reader and 10 non-screen-reader users. Our enhancements “closed the gap” between the two groups by enabling screen-reader users to extract information with approximately the same accuracy as non-screen-reader users, reducing interaction time by 22\% in the process.},
booktitle = {Proceedings of the 20th International Web for All Conference},
pages = {18–31},
numpages = {14},
keywords = {Data visualization, accessibility, blind, screen reader, voice assistant.},
location = {Austin, TX, USA},
series = {W4A '23}
}

@inproceedings{10.1145/3587281.3587284,
author = {Sharif, Ather and Zhang, Andrew M. and Reinecke, Katharina and Wobbrock, Jacob O.},
title = {Understanding and Improving Drilled-Down Information Extraction from Online Data Visualizations for Screen-Reader Users},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587281.3587284},
doi = {10.1145/3587281.3587284},
abstract = {Inaccessible online data visualizations can significantly disenfranchise screen-reader users from accessing critical online information. Current accessibility measures, such as adding alternative text to visualizations, only provide a high-level overview of data, limiting screen-reader users from exploring data visualizations in depth. In this work, we build on prior research to develop taxonomies of information sought by screen-reader users to interact with online data visualizations granularly through role-based and longitudinal studies with screen-reader users. Utilizing these taxonomies, we extended the functionality of VoxLens—an open-source multi-modal system that improves the accessibility of data visualizations—by supporting drilled-down information extraction. We assessed the performance of our VoxLens enhancements through task-based user studies with 10 screen-reader and 10 non-screen-reader users. Our enhancements “closed the gap” between the two groups by enabling screen-reader users to extract information with approximately the same accuracy as non-screen-reader users, reducing interaction time by 22\% in the process.},
booktitle = {Proceedings of the 20th International Web for All Conference},
pages = {18–31},
numpages = {14},
keywords = {Data visualization, accessibility, blind, screen reader, voice assistant.},
location = {Austin, TX, USA},
series = {W4A '23}
}

@inproceedings{10.1145/3656650.3656739,
author = {Pucci, Emanuele and Piro, Ludovica and Andolina, Salvatore and Matera, Maristella},
title = {From Conversational Web to Inclusive Conversations with LLMs},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656739},
doi = {10.1145/3656650.3656739},
abstract = {This paper explores how a library of conversational patterns defined for Web browsing can be adopted to improve the accessibility of prompt interfaces for Large Language Models. It reports preliminary findings from a formative evaluation with blind and visually impaired users. It also discusses how these findings support transferring the benefits of voice-based Web browsing to the inclusivity of prompt-based interfaces.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {87},
numpages = {3},
keywords = {Accessibility, Information Retrieval, Prompt-based Interaction},
location = {Arenzano, Genoa, Italy},
series = {AVI '24}
}

@INPROCEEDINGS{10463430,
  author={Kuzdeuov, Askat and Mukayev, Olzhas and Nurgaliyev, Shakhizat and Kunbolsyn, Alisher and Varol, Huseyin Atakan},
  booktitle={2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={ChatGPT for Visually Impaired and Blind}, 
  year={2024},
  volume={},
  number={},
  pages={722-727},
  keywords={Voice activity detection;Visualization;Source coding;Oral communication;Blindness;User interfaces;Chatbots},
  doi={10.1109/ICAIIC60209.2024.10463430}}

@misc{chen2025envisionvrsceneinterpretationtool,
      title={EnVisionVR: A Scene Interpretation Tool for Visual Accessibility in Virtual Reality}, 
      author={Junlong Chen and Rosella P. Galindo Esparza and Vanja Garaj and Per Ola Kristensson and John Dudley},
      year={2025},
      eprint={2502.03564},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2502.03564}, 
}

@inproceedings{10.1145/3663548.3688498,
author = {Collins, Jazmin and Nicholson, Kaylah Myranda and Khadir, Yusuf and Stevenson Won, Andrea and Azenkot, Shiri},
title = {An AI Guide to Enhance Accessibility of Social Virtual Reality for Blind People},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3688498},
doi = {10.1145/3663548.3688498},
abstract = {The rapid growth of virtual reality (VR) has led to increased use of social VR platforms for interaction. However, these platforms lack adequate features to support blind and low vision (BLV) users, posing significant challenges in navigation, visual interpretation, and social interaction. One promising approach to these challenges is employing human guides in VR. However, this approach faces limitations with a lack of availability of humans to serve as guides, or the inability to customize the guidance a user receives from the human guide. We introduce an AI-powered guide to address these limitations. The AI guide features six personas, each offering unique behaviors and appearances to meet diverse user needs, along with visual interpretation and navigation assistance. We aim to use this AI guide in the future to help us understand BLV users’ preferences for guide forms and functionalities.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {130},
numpages = {5},
keywords = {VR, accessibility, blind, low vision},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@article{10.1109/TVCG.2024.3456358,
author = {Reinders, Samuel and Butler, Matthew and Zukerman, Ingrid and Lee, Bongshin and Qu, Lizhen and Marriott, Kim},
title = {When Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis with Touch and Speech},
year = {2025},
issue_date = {Jan. 2025},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {31},
number = {1},
issn = {1077-2626},
url = {https://doi.org/10.1109/TVCG.2024.3456358},
doi = {10.1109/TVCG.2024.3456358},
abstract = {Despite the recent surge of research efforts to make data visualizations accessible to people who are blind or have low vision (BLV), how to support BLV people's data analysis remains an important and challenging question. As refreshable tactile displays (RTDs) become cheaper and conversational agents continue to improve, their combination provides a promising approach to support BLV people's interactive data exploration and analysis. To understand how BLV people would use and react to a system combining an RTD with a conversational agent, we conducted a Wizard-of-Oz study with 11 BLV participants, where they interacted with line charts, bar charts, and isarithmic maps. Our analysis of participants' interactions led to the identification of nine distinct patterns. We also learned that the choice of modalities depended on the type of task and prior experience with tactile graphics, and that participants strongly preferred the combination of RTD and speech to a single modality. In addition, participants with more tactile experience described how tactile images facilitated a deeper engagement with the data and supported independent interpretation. Our findings will inform the design of interfaces for such interactive mixed-modality systems.},
journal = {IEEE Transactions on Visualization and Computer Graphics},
month = jan,
pages = {864–874},
numpages = {11}
}