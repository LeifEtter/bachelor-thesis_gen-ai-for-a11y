@inproceedings{10.1145/3544548.3581494,
author = {Huh, Mina and Yang, Saelyne and Peng, Yi-Hao and Chen, Xiang 'Anthony' and Kim, Young-Ho and Pavel, Amy},
title = {AVscript: Accessible Video Editing with Audio-Visual Scripts},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581494},
doi = {10.1145/3544548.3581494},
abstract = {Sighted and blind and low vision (BLV) creators alike use videos to communicate with broad audiences. Yet, video editing remains inaccessible to BLV creators. Our formative study revealed that current video editing tools make it difficult to access the visual content, assess the visual quality, and efficiently navigate the timeline. We present &nbsp;AVscript, an accessible text-based video editor. &nbsp;AVscript enables users to edit their video using a script that embeds the video’s visual content, visual errors (e.g., dark or blurred footage), and speech. Users can also efficiently navigate between scenes and visual errors or locate objects in the frame or spoken words of interest. A comparison study (N=12) showed that &nbsp;AVscript significantly lowered BLV creators’ mental demands while increasing confidence and independence in video editing. We further demonstrate the potential of &nbsp;AVscript through an exploratory study (N=3) where BLV creators edited their own footage.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {796},
numpages = {17},
keywords = {accessibility, authoring tools, video},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3588029.3595471,
author = {Tanaka, Kengo and Fushimi, Tatsuki and Tsutsui, Ayaka and Ochiai, Yoichi},
title = {Text to Haptics: Method and Case Studies of Designing Tactile Graphics for Inclusive Tactile Picture Books by Digital Fabrication and Generative AI},
year = {2023},
isbn = {9798400701535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588029.3595471},
doi = {10.1145/3588029.3595471},
abstract = {In this case study, we explore the possibilities between Generative AI and tactile graphics for inclusivity in computer graphics communities. The use of Generative AI in the design of tactile graphics has made it possible to support the processes used by publishers and tactile graphics designers. In addition, the idea of printing tactile graphics on transparent sheets with a 3D printer through digital fabrication technology allows the creation of inclusive tactile picture books that can be read and enjoyed together by sighted and visually impaired people in a single picture book.},
booktitle = {ACM SIGGRAPH 2023 Labs},
articleno = {10},
numpages = {2},
keywords = {3D printing, generative ai, tactile graphics, tactile picture books, visual impairments},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3613904.3642839,
author = {Van Daele, Tess and Iyer, Akhil and Zhang, Yuning and Derry, Jalyn C and Huh, Mina and Pavel, Amy},
title = {Making Short-Form Videos Accessible with Hierarchical Video Summaries},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642839},
doi = {10.1145/3613904.3642839},
abstract = {Short videos on platforms such as TikTok, Instagram Reels, and YouTube Shorts (i.e. short-form videos) have become a primary source of information and entertainment. Many short-form videos are inaccessible to blind and low vision (BLV) viewers due to their rapid visual changes, on-screen text, and music or meme-audio overlays. In our formative study, 7 BLV viewers who regularly watched short-form videos reported frequently skipping such inaccessible content. We present &nbsp;ShortScribe, a system that provides hierarchical visual summaries of short-form videos at three levels of detail to support BLV viewers in selecting and understanding short-form videos. ShortScribe allows BLV users to navigate between video descriptions based on their level of interest. To evaluate &nbsp;ShortScribe, we assessed description accuracy and conducted a user study with 10 BLV participants comparing &nbsp;ShortScribe to a baseline interface. When using ShortScribe, participants reported higher comprehension and provided more accurate summaries of video content.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {895},
numpages = {17},
keywords = {Accessibility, Short-Form Video, Summaries, Video Description},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3663547.3759758,
author = {Tang, Kaiyuan and Chen, Kerui and Qing, Tian and Jiang, Zhou and Lin, Qiuyu and Xu, Tianming},
title = {EchoMall: Prototyping GenAI-powered Vision-Free Online Shopping Experience for Blind and Low-Vision Customers},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3759758},
doi = {10.1145/3663547.3759758},
abstract = {Conventional online shopping platforms typically prioritise user experience design centred around visual elements, heavily relying on visual cues for product information and user navigation. This design pattern presents barriers for Blind and Low-Vision (BLV) customers. Existing accommodations focus primarily on ensuring compatibility of the platforms with assistive software such as screen readers. While assistive software enable BLV customers to access online shopping services, they often involve compromises in user experience during the conversion of visual elements to auditory ones. With the rapid advancement of Generative AI (GenAI), particularly voice- and dialogue-based interactions driven by Large Language Models (LLMs), we prototyped an innovative demo, EchoMall, aiming to explore the design of an enjoyable vision-free online shopping experience. By blending multiple LLMs and GenAI techniques, EchoMall potentially allows BLV customers to engage in an intuitive online shopping process without assistive software and creates new market opportunities.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {133},
numpages = {5},
keywords = {Blind and Low-vision Assistance; Auditory User Interface; Generative User Interface; Generative AI; Large Language Models; Conversational Interaction},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3663547.3759708,
author = {Hayward, Jayne and Vincenzi, Beatrice and Karpodini, Christina and Theil, Arthur},
title = {How Do Disabled Visual Artists Use GenAI Tools for Business Administration Tasks?},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3759708},
doi = {10.1145/3663547.3759708},
abstract = {This paper investigates how disabled visual artists use GenAI tools to manage administrative and business tasks essential to sustaining creative careers. Whilst previous research has focused on GenAI's emerging role in artistic work, our study highlights its potential in supporting non-creative aspects of artistic practice, which are often overlooked yet critical for workload management of disabled artists. Through in-depth semi-structured interviews with 5 disabled visual artists, we explore the perceived benefits and barriers of using GenAI, particularly in terms of accessibility, cognitive demands, ethical concerns, and financial constraints. Our findings suggest that whilst GenAI can increase productivity, reduce cognitive load, and promote equity, it also presents challenges related to transparency, bias, interface complexity, and job displacement fears. We conclude by calling for more inclusive and affordable GenAI tools, improved accessible interfaces, ethical governance, and personalised support to enable disabled creatives in their professional practices.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {151},
numpages = {5},
keywords = {Accessibility, Art Business, Assistive Technology, Creative Work, Disability Employment Gap, Disabled Workers, Generative Artificial Intelligence},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3663547.3746362,
author = {Zhang, Zhuohao (Jerry) and Li, Haichang and Yu, Chun Meng and Faruqi, Faraz and Xie, Junan and Kim, Gene S-H and Fan, Mingming and Forbes, Angus and Wobbrock, Jacob O. and Guo, Anhong and He, Liang},
title = {A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3746362},
doi = {10.1145/3663547.3746362},
abstract = {Building 3-D models is challenging for blind and low-vision (BLV) users due to the inherent complexity of 3-D models and the lack of support for non-visual interaction in existing tools. To address this issue, we introduce A11yShape, a novel system designed to help BLV users who possess basic programming skills understand, modify, and iterate on 3-D models. A11yShape leverages LLMs and integrates with OpenSCAD, a popular open-source editor that generates 3-D models from code. Key functionalities of A11yShape include accessible descriptions of 3-D models, version control to track changes in models and code, and a hierarchical representation of model components. Most importantly, A11yShape employs a cross-representation highlighting mechanism to synchronize semantic selections across all model representations—code, semantic hierarchy, AI description, and 3-D rendering. We conducted a multi-session user study with four BLV programmers, where, after an initial tutorial session, participants independently completed 12 distinct models across two testing sessions, achieving results that aligned with their own satisfaction. The result demonstrates that participants were able to comprehend provided 3-D models, as well as independently create and modify 3-D models—tasks that were previously impossible without assistance from sighted individuals.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {84},
numpages = {20},
keywords = {3-D Modeling, Assistive Technologies, AI, LLM, Blind and Low-vision},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3663547.3746356,
author = {Xiao, Lan and Bandukda, Maryam and Li, Franklin Mingzhe and Colley, Mark and Holloway, Catherine},
title = {Understanding the Video Content Creation Journey of Creators with Sensory Impairment in Kenya},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3746356},
doi = {10.1145/3663547.3746356},
abstract = {Video content creation offers vital opportunities for expression and participation, yet remains largely inaccessible to creators with sensory impairments, especially in low-resource settings. We conducted interviews with 20 video creators with visual and hearing impairments in Kenya to examine their tools, challenges, and collaborative practices. Our findings show that accessibility barriers and infrastructural limitations shape video creation as a staged, collaborative process involving trusted human partners and emerging AI tools. Across workflows, creators actively negotiated agency and trust, maintaining creative control while bridging sensory gaps. We discuss the need for flexible, interdependent collaboration models, inclusive human-AI workflows, and diverse storytelling practices. This work broadens accessibility research in HCI by examining how technology and social factors intersect in low-resource contexts, suggesting ways to better support disabled creators globally.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {42},
numpages = {14},
keywords = {Video Content Creation, Accessibility, Sensory Impairment, LMICs},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3597638.3614490,
author = {Chheda-Kothary, Arnavi and Rios, David A and Smith, Kynnedy Simone and Reyna, Avery and Zhang, Cecilia and Smith, Brian A.},
title = {Understanding Blind and Low Vision Users' Attitudes Towards Spatial Interactions in Desktop Screen Readers},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614490},
doi = {10.1145/3597638.3614490},
abstract = {Desktop screen readers as a web navigation mechanism for BLV users are tedious and frozen in time, especially in the face of richer ways of presenting spatial information such as tactile and touchscreen devices. In our work, we consider what it means to create and evaluate systems that can present a similarly rich, spatial interaction mechanism plugged into existing screen reader paradigms. We present a formative study conducted with SpaceNav, a custom screen reader that utilizes spatial input and output to navigate two different web applications. We present results from this study, and discuss a new browser extension we are implementing based on our formative study feedback to more robustly test spatial interactions in the context of real world websites. To close, we describe our goals for evaluating the new web extension in a future study.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {83},
numpages = {5},
keywords = {Blind or low vision users, accessibility, desktop web applications, human-computer interaction, screen readers, spatialized layout},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3597638.3608418,
author = {Zhang, Zhuohao and Kim, Gene S-H and Wobbrock, Jacob O.},
title = {Developing and Deploying a Real-World Solution for Accessible Slide Reading and Authoring for Blind Users},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608418},
doi = {10.1145/3597638.3608418},
abstract = {Presentation software like Microsoft PowerPoint and Google Slides remains largely inaccessible for blind users because screen readers are not well suited to 2-D “artboards” that contain different objects in arbitrary arrangements lacking any inherent reading order. To investigate this problem, prior work by Zhang \& Wobbrock (2023) developed multimodal interaction techniques in a prototype system called A11yBoard, but their system was limited to a single artboard in a self-contained prototype and was unable to support real-world use. In this work, we present a major extension of A11yBoard that expands upon its initial interaction techniques, addresses numerous real-world issues, and makes it deployable with Google Slides. We describe the new features developed for A11yBoard for Google Slides along with our participatory design process with a blind co-author. We also present two case studies based on real-world deployments showing that participants were able to independently complete slide reading and authoring tasks that were not possible without sighted assistance previously. We conclude with several design guidelines for making accessible digital content creation tools.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {47},
numpages = {15},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3544548.3580655,
author = {Zhang, Zhuohao (Jerry) and Wobbrock, Jacob O.},
title = {A11yBoard: Making Digital Artboards Accessible to Blind and Low-Vision Users},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580655},
doi = {10.1145/3544548.3580655},
abstract = {Digital artboards, which hold objects rather than pixels (e.g., Microsoft PowerPoint and Google Slides), remain largely inaccessible for blind and low-vision (BLV) users. Building on prior findings about the experiences of BLV users with digital artboards, we present a novel tool called A11yBoard, an interactive multimodal system that makes interpreting and authoring digital artboards accessible. A11yBoard combines a web-based drawing canvas paired with a mobile touch screen device such as a tablet. The mobile device displays the same canvas and enables risk-free spatial exploration of the artboard via touch and gesture. Speech recognition, non-speech audio, and keyboard-based commands are also used for input and output. Through a series of pilot studies and formal task-based user studies with BLV participants, we show that A11yBoard provides (1) intuitive spatial reasoning about two-dimensional objects, (2) multimodal access to objects’ properties and relationships, and (3) eyes-free creating and editing of objects to establish their desired properties and positions.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {55},
numpages = {17},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3715336.3735685,
author = {Cheema, Maryam and Seifi, Hasti and Fazli, Pooyan},
title = {Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735685},
doi = {10.1145/3715336.3735685},
abstract = {Audio descriptions (AD) make videos accessible for blind and low vision (BLV) users by describing visual elements that cannot be understood from the main audio track. AD created by professionals or novice describers is time-consuming and offers little customization or control to BLV viewers on description length and content and when they receive it. To address this gap, we explore user-driven AI-generated descriptions, enabling BLV viewers to control both the timing and level of detail of the descriptions they receive. In a study, 20 BLV participants activated audio descriptions for seven different video genres with two levels of detail: concise and detailed. Our findings reveal differences in the preferred frequency and level of detail of ADs for different videos, participants’ sense of control with this style of AD delivery, and its limitations. We discuss the implications of these findings for the development of future AD tools for BLV users.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {458–474},
numpages = {17},
keywords = {audio description, online videos, accessibility, multimodal large language models, blind and low vision},
location = {
},
series = {DIS '25}
}

@inproceedings{10.1145/3491102.3502092,
author = {Zhang, Mingrui Ray and Zhong, Mingyuan and Wobbrock, Jacob O.},
title = {Ga11y: An Automated GIF Annotation System for Visually Impaired Users},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502092},
doi = {10.1145/3491102.3502092},
abstract = {Animated GIF images have become prevalent in internet culture, often used to express richer and more nuanced meanings than static images. But animated GIFs often lack adequate alternative text descriptions, and it is challenging to generate such descriptions automatically, resulting in inaccessible GIFs for blind or low-vision (BLV) users. To improve the accessibility of animated GIFs for BLV users, we provide a system called Ga11y (pronounced “galley”), for creating GIF annotations. Ga11y combines the power of machine intelligence and crowdsourcing and has three components: an Android client for submitting annotation requests, a backend server and database, and a web interface where volunteers can respond to annotation requests. We evaluated three human annotation interfaces and employ the one that yielded the best annotation quality. We also conducted a multi-stage evaluation with 12 BLV participants from the United States and China, receiving positive feedback.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {197},
numpages = {16},
keywords = {GIF, accessibility., blind, crowdsourcing, human annotation, images, low vision, text description},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@INPROCEEDINGS{11236096,
  author={Dang, Khang and Lee, Sooyeon},
  booktitle={2025 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Personalized Conversational Audio Descriptions in 360° Virtual Reality for Blind and Low-Vision Users}, 
  year={2025},
  volume={},
  number={},
  pages={961-962},
  keywords={Headphones;Visualization;Pipelines;Speech recognition;Glass;Real-time systems;Museums;Encoding;Text to speech;Delays;Conversational AI;virtual reality;accessibility;audio description},
  doi={10.1109/ISMAR-Adjunct68609.2025.00268}}

@inproceedings{10.1145/3587281.3587293,
author = {Knaeble, Merlin and Sailer, Gabriel and Chen, Zihan and Schwarz, Thorsten and Yang, Kailun and Nadj, Mario and Stiefelhagen, Rainer and Maedche, Alexander},
title = {AutoChemplete - Making Chemical Structural Formulas Accessible},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587281.3587293},
doi = {10.1145/3587281.3587293},
abstract = {Despite their interests, blind and low vision students avoid STEM subjects. Research attributes this to a lack of accessible material. Annotating STEM content such as chemical structural formulas requires expert domain knowledge, is time consuming, and frustrating. We conduct interviews with blind and low vision chemists and accessibility professionals to derive requirements for tool support. On this basis, we develop AutoChemplete, an interactive labeling tool for chemical structural formulas. It ingests images and uses machine learning to predict the molecule. With a similarity search in the solution space, we enable even novices to simply pick from options. From this we are able to generate accessible representations. We conduct fifteen think-aloud sessions with participants of varying domain expertise and find support of different annotation styles simultaneously. Not only does AutoChemplete strike a balance in skill-support, participants even find it entertaining.},
booktitle = {Proceedings of the 20th International Web for All Conference},
pages = {104–115},
numpages = {12},
keywords = {SMILES, STEM, accessibility, autocomplete, chemistry, interactive labeling, structural formula},
location = {Austin, TX, USA},
series = {W4A '23}
}

@inproceedings{10.1145/3493612.3520470,
author = {Silva, Jorge Sassaki Resende and Freire, Andr\'{e} Pimenta and Cardoso, Paula Christina Figueira},
title = {When headers are not there: design and user evaluation of an automatic topicalisation and labelling tool to aid the exploration of web documents by blind users},
year = {2022},
isbn = {9781450391702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493612.3520470},
doi = {10.1145/3493612.3520470},
abstract = {Using headers to grasp a document's structure has been one of the main strategies employed by blind users on web documents when using screen readers. However, when headers are not available or not appropriately marked up, they can cause serious difficulties. This paper presents the design and evaluation of a tool for automatically generating headers for screen readers with topicalisation and labelling algorithms. The proposed tool uses Natural Language Processing techniques to divide a web document into topic segments and label each segment based on its content. We conducted an initial user study with eight blind and partially-sighted screen reader users. The evaluation involved tasks with questions answered by participants with information from texts with and without automatically generated headers. Results provided preliminary indicators of improvement in performance and reduction of cognitive load. The findings contribute to knowledge to improve tools to aid in text exploration. It also provides initial empirical evidence to be further explored to analyze the impact of automatically-generated headings in improving performance and reducing cognitive load for blind users.},
booktitle = {Proceedings of the 19th International Web for All Conference},
articleno = {18},
numpages = {11},
keywords = {screen readers, natural language processing, automatic topicalisation and labelling, accessibility},
location = {Lyon, France},
series = {W4A '22}
}

@InProceedings{10.1007/978-3-032-04999-5_3,
author="Oliveira, Alberto Dumont Alves
and Aljedaani, Wajdi
and Eler, Marcelo Medeiros",
editor="Ardito, Carmelo
and Diniz Junqueira Barbosa, Simone
and Conte, Tayana
and Freire, Andr{\'e}
and Gasparini, Isabela
and Palanque, Philippe
and Prates, Raquel",
title="Assessing Visual Impairment Feedback in Mobile Applications",
booktitle="Human-Computer Interaction -- INTERACT 2025",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="36--59",
abstract="Mobile software development emphasizes frequent deliveries by incorporating improvements based on user and stakeholder feedback. In this context, user reviews posted on platforms such as the Google Play Store contain important information about various quality aspects, including accessibility. Although prior studies have investigated accessibility issues through user reviews, few have explicitly mapped this feedback to established accessibility standards or guidelines, which could reinforce the relevance of such standards and illustrate the real impact of accessibility shortcomings on users with disabilities. This study addresses this gap by examining reviews related to visual impairments and ocular conditions and linking them to accessibility guidelines. We used a public dataset of 4,999 accessibility-related reviews of Android apps, applying manual content analysis to associate feedback with the BBC Mobile Accessibility Guidelines. Findings highlight frequent issues with ``Adjustability'', ``Colour Contrast'', and ``Content Resizing''.",
isbn="978-3-032-04999-5"
}

@article{10.1016/j.infsof.2025.107821,
author = {Vera-Amaro, Guillermo and Rojano-C\'{a}ceres, Jos\'{e} Rafael},
title = {Towards accessible website design through artificial intelligence: A systematic literature review},
year = {2025},
issue_date = {Oct 2025},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {186},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2025.107821},
doi = {10.1016/j.infsof.2025.107821},
journal = {Inf. Softw. Technol.},
month = oct,
numpages = {31},
keywords = {Web accessibility, Systematic literature review, Artificial intelligence, Wcag, Machine learning, Large language models}
}

@InProceedings{10.1007/978-3-032-04999-5_6,
author="Penna, Giuseppe Della
and Buzzi, Marina
and Leporini, Barbara",
editor="Ardito, Carmelo
and Diniz Junqueira Barbosa, Simone
and Conte, Tayana
and Freire, Andr{\'e}
and Gasparini, Isabela
and Palanque, Philippe
and Prates, Raquel",
title="Generative AI as a New Assistive Technology for Web Interaction",
booktitle="Human-Computer Interaction -- INTERACT 2025",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="91--107",
abstract="For users who are unfamiliar with technology or rely on assistive tools such as screen readers, interacting with a web page can be challenging. Ensuring a seamless experience requires a well-designed user interface (UI) that prioritizes accessibility and usability. However, achieving this target demands specialized expertise from developers and can involve significant effort. In this context, Generative Artificial Intelligence (GAI) has become a valuable aid for improving access to information and facilitating interaction with web interfaces. To effectively enhance user interaction---such as accessing services or specific functionalities---AI-driven tools must first be capable of understanding the structure and content of a web page. This study investigates if GAIs can be exploited to assist the user when navigating through a website, describing the site contents, explaining the interface structure and interactive elements, and suggesting actions or procedures to follow to perform a certain task or accomplish a specific goal. This kind of assistive technology can benefit not only visually impaired people but also persons with cognitive impairment and, more generally, people that are not ``skilled'' with modern web applications, like seniors. Specifically, thirteen popular websites were analyzed by asking Copilot one hundred questions. Results suggest that GAIs have the potential to assist people in web tasks. However, limitations have still been detected, with 20{\%} of completely erroneous answers received from the navigation and interaction questions and 15{\%} for those related to structure, mainly detected in pages having scarce accessibility and sites having a complex HTML structure, respectively.",
isbn="978-3-032-04999-5"
}

@article{Nino06062025,
author = {Juan Nino and Jocelyne Kiss and Frédérique Poncet and Walter Wittich and Geoffreyjen Edwards and Ernesto Morales},
title = {Toward improving internet navigation for visually impaired screen Reader users: Co-designing an open-source assistive technology system},
journal = {Assistive Technology},
volume = {0},
number = {0},
pages = {1--12},
year = {2025},
publisher = {Taylor \& Francis},
doi = {10.1080/10400435.2025.2509699},
note ={PMID: 40478980},
URL = {https://doi.org/10.1080/10400435.2025.2509699},
eprint = {https://doi.org/10.1080/10400435.2025.2509699},
abstract = { Visually impaired individuals, estimated at 285 million globally, rely heavily on-screen readers for internet access. However, much of the visually available information, such as the relationship between webpage elements, does not translate well to its textual representation and must be always kept in memory, limiting contextual interactions. To address this challenge, we developed Touch Matrix Assistive Technology Navigator (TOMAT), an open-source system that works alongside screen readers to provide an interactive, audio-tactile representation of webpage structure and enable contextual interactions. Our study employed a participatory design approach, involving visually impaired users, healthcare professionals, engineers, and community organizations in co-design sessions, prototype demonstrations, and focus groups. The resulting system extracts and presents non-linear web information at multiple levels of detail, allowing users to dynamically adjust granularity and efficiently navigate and interact with web content. Participants reported that TOMAT enhanced their understanding of webpage structure and provided an intuitive complement to screen reader software. The findings suggest TOMAT has the potential to improve the internet navigation experience for visually impaired users, fostering greater independence and digital participation. To support further development and collaboration, TOMAT’s source files have been released under an open-source license. }
}

@article{10.1007/s10209-020-00777-w,
author = {Aqle, Aboubakr and Al-Thani, Dena and Jaoua, Ali},
title = {Can search result summaries enhance the web search efficiency and experiences of the visually impaired users?},
year = {2022},
issue_date = {Mar 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {1},
issn = {1615-5289},
url = {https://doi.org/10.1007/s10209-020-00777-w},
doi = {10.1007/s10209-020-00777-w},
abstract = {There are limited studies that are addressing the challenges of visually impaired (VI) users when viewing search results on a search engine interface by using a screen reader. This study investigates the effect of providing an overview of search results to VI users. We present a novel interactive search engine interface called InteractSE to support VI users during the results exploration stage in order to improve their interactive experience and web search efficiency. An overview of the search results is generated using an unsupervised machine learning approach to present the discovered concepts via a formal concept analysis that is domain-independent. These concepts are arranged in a multi-level tree following a hierarchical order and covering all retrieved documents that share maximal features. The InteractSE interface was evaluated by 16 legally blind users and compared with the Google search engine interface for complex search tasks. The evaluation results were obtained based on both quantitative (as task completion time) and qualitative (as participants’ feedback) measures. These results are promising and indicate that InteractSE enhances the search efficiency and consequently advances user experience. Our observations and analysis of the user interactions and feedback yielded design suggestions to support VI users when exploring and interacting with search results.},
journal = {Univers. Access Inf. Soc.},
month = mar,
pages = {171–192},
numpages = {22},
keywords = {Web accessibility, User interaction, Usability, Interface evaluation, Visually impaired users, Search result summarization, Search engine interface}
}

@inproceedings{10.1145/3491102.3517695,
author = {Zhang, Lotus and Shao, Jingyao and Liu, Augustina Ao and Jiang, Lucy and Stangl, Abigale and Fourney, Adam and Morris, Meredith Ringel and Findlater, Leah},
title = {Exploring Interactive Sound Design for Auditory Websites},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517695},
doi = {10.1145/3491102.3517695},
abstract = {Auditory interfaces increasingly support access to website content, through recent advances in voice interaction. Typically, however, these interfaces provide only limited audio styling, collapsing rich visual design into a static audio output style with a single synthesized voice. To explore the potential for more aesthetic and intuitive sound design for websites, we prompted 14 professional sound designers to create auditory website mockups and interviewed them about their designs and rationale. Our findings reveal their prioritized design considerations (aesthetics and emotion, user engagement, audio clarity, information dynamics, and interactivity), specific sound design ideas to support each consideration (e.g., replacing spoken labels with short, memorable audio expressions), and challenges with applying sound design practices to auditory websites. These findings provide promising direction for how to support designers in creating richer auditory website experiences.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {222},
numpages = {16},
keywords = {audio display, interaction design, voice interaction},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@InProceedings{10.1007/978-3-032-05005-2_17,
author="Usabaev, Bela
and B{\"u}hrich, Nicola
and Ta{\c{s}}, Tutku Can
and Weber, Gerhard
and Ondrusek, Petra",
editor="Ardito, Carmelo
and Diniz Junqueira Barbosa, Simone
and Conte, Tayana
and Freire, Andr{\'e}
and Gasparini, Isabela
and Palanque, Philippe
and Prates, Raquel",
title="Integrating Touch, Gestures and Speech for Multi-modal Conversations with an Audio-Tactile Graphics Reader",
booktitle="Human-Computer Interaction -- INTERACT 2025",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="323--332",
abstract="Screen readers present one cell of a spreadsheet at a time and provide only a limited overview on a single table such as its dimensions. Screen reader users struggle, for example, in recognizing labels to explain another cell's purpose. In a Wizard-of-Oz study with two wizards as voice agents generating speech feedback we explore a novel audio-tactile graphics reader with tactile grid-based overlays and spoken feedback for touch to enable screen reader users to engage in a conversation about the spreadsheet with a voice assistant and utilize a screen reader to solve spreadsheet calculation tasks. In a pilot with 3 and a main study with 8 BLV students, we identify multi-modal interaction patterns and confirm the importance of two separate roles of speakers: a voice assistant and a screen reader. The conversation is driven by user's multi-modal speech input and hand gestures provided sequentially or in parallel. Verbal references to cells by spoken addresses, values, and formulas can be embodied as tangible objects to unify tactile and verbal representations.",
isbn="978-3-032-05005-2"
}

@inproceedings{10.1145/3746059.3747797,
author = {Peng, Yi-Hao and Li, Dingzeyu and Bigham, Jeffrey P and Pavel, Amy},
title = {Morae: Proactively Pausing UI Agents for User Choices},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747797},
doi = {10.1145/3746059.3747797},
abstract = {User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {198},
numpages = {14},
keywords = {Agents; User Interface Agents; Proactive Agents; Human-Agent Interaction; Accessibility; Generative UI},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3706598.3714125,
author = {Chheda-Kothary, Arnavi and Sharif, Ather and Rios, David Angel and Smith, Brian A.},
title = {"It Brought Me Joy": Opportunities for Spatial Browsing in Desktop Screen Readers},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714125},
doi = {10.1145/3706598.3714125},
abstract = {Blind or low-vision (BLV) screen-reader users have a significantly limited experience interacting with desktop websites compared to non-BLV, i.e., sighted users. This digital divide is exacerbated by the incapability to browse the web spatially—an affordance that leverages spatial reasoning, which sighted users often rely on. In this work, we investigate the value of and opportunities for BLV screen-reader users to browse websites spatially (e.g., understanding page layouts). We additionally explore at-scale website layout understanding as a feature of desktop screen readers. We created a technology probe, WebNExt, to facilitate our investigation. Specifically, we conducted a lab study with eight participants and a five-day field study with four participants to evaluate spatial browsing using WebNExt. Our findings show that participants found spatial browsing intuitive and fulfilling, strengthening their connection to the design of web pages. Furthermore, participants envisioned spatial browsing as a step toward reducing the digital divide.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {974},
numpages = {18},
keywords = {Blind or low-vision users; accessibility; desktop web applications; spatial awareness},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3597638.3614548,
author = {Glazko, Kate S and Yamagami, Momona and Desai, Aashaka and Mack, Kelly Avery and Potluri, Venkatesh and Xu, Xuhai and Mankoff, Jennifer},
title = {An Autoethnographic Case Study of Generative Artificial Intelligence's Utility for Accessibility},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614548},
doi = {10.1145/3597638.3614548},
abstract = {With the recent rapid rise in Generative Artificial Intelligence (GAI) tools, it is imperative that we understand their impact on people with disabilities, both positive and negative. However, although we know that AI in general poses both risks and opportunities for people with disabilities, little is known specifically about GAI in particular. To address this, we conducted a three-month autoethnography of our use of GAI to meet personal and professional needs as a team of researchers with and without disabilities. Our findings demonstrate a wide variety of potential accessibility-related uses for GAI while also highlighting concerns around verifiability, training data, ableism, and false promises.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {99},
numpages = {8},
keywords = {ableism, accessibility, auto-ethnography, generative artificial intelligence},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3663547.3759727,
author = {Raman, Gayatri},
title = {“Somewhere Between Images and Art”: How Blind Creators Engage with AI Image Generators},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3759727},
doi = {10.1145/3663547.3759727},
abstract = {Blind and low-vision (BLV) artists working in visual media navigate systemic barriers shaped by ableist norms that question their creative legitimacy. While AI image generators like Midjourney are gaining prominence in visual art practice, little is known about how BLV artists engage with these tools. This study explores how six BLV artists across diverse media interacted with Midjourney through guided prompting and interviews. Participants approached the tool with curiosity, investigating its potential for inspiration, evaluation, and fragment generation for multisensory works, while drawing clear boundaries around authorship and creative agency. Digital artists expressed particular vulnerability to assumptions of overreliance on AI, while others saw limited opportunities to embed embodied, adaptive, and intersectional elements central to their practice. We contribute to HCI by foregrounding BLV artists’ perspectives and identifying design opportunities for generative tools that prioritize accessibility, support dialogic meaning-making, and validate BLV artistic authorship in visual fields.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {148},
numpages = {5},
keywords = {Blind art, disability, generative AI, visual impairment},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3663547.3759725,
author = {Cao, Xinyun and Ju, Kexin Phyllis and Li, Chenglin and Potluri, Venkatesh and Jain, Dhruv},
title = {Demo of RAVEN: Realtime Accessibility in Virtual ENvironments for Blind and Low-Vision People},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3759725},
doi = {10.1145/3663547.3759725},
abstract = {As virtual 3D environments become prevalent, enabling presence and spatial exploration, equitable access is crucial for blind and low-vision (BLV) users who face challenges with spatial awareness, navigation, and interaction. To address these, previous work explored enhancing visual information or supplementing it with auditory and haptic modalities. However, these methods are static and might risk steep learning curves. In this work, we present RAVEN, a system that responds to query or modification prompts from BLV users to improve the accessibility of a 3D virtual scene at runtime. The system integrates LLMs with semantic scene data and runtime code generation to support iterative, dialogue-based user interactions. We evaluated the system with eight BLV people, uncovering key insights into the strengths and shortcomings of generative AI-driven accessibility in virtual 3D environments.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {168},
numpages = {5},
keywords = {Accessibility, blind and low-vision, virtual 3D environment, generative AI.},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3746059.3747597,
author = {Tang, Yilin and Fang, Yuyang and Wang, Tianle and Sun, Lingyun and Chen, Liuqing},
title = {"This is My Fault", Really? Understanding Blind and Low-Vision People’s Perception of Hallucination in Large Vision Language Models},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747597},
doi = {10.1145/3746059.3747597},
abstract = {Visual question-answering (VQA) tools powered by large visual language models (LVLMs) are used to assist blind and low-vision (BLV) individuals in overcoming visual challenges, raising concerns about hallucinations and associated risks. Existing literature overlooks the variations of hallucinations across distinct usage scenarios and types in the context of VQA for BLV people, resulting in limited understanding of their perceptions and insufficient guidance for targeted mitigation strategies. By analyzing 3,467 real-world VQA cases from BLV users, we developed a manifestation-scenario-based dual-dimensional hallucination typology, uncovering eight scenarios and five types of hallucinations. Through interviews with 16 BLV users, we examined their awareness levels, detection strategies, mental models of hallucinations, and their tolerance of associated risks, identifying key gaps between their perceptions and real situations. By designing with 12 BLV users, we uncovered their expectations for hallucination-mitigating solutions, including enhanced information provision, transparency in processing, verification strategies, and feedback mechanisms.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {44},
numpages = {20},
keywords = {Large visual language models (LVLMs), Artificial intelligence (AI), Hallucination, Blind and low vision, Human-centered AI},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3733155.3737910,
author = {Leporini, Barbara and Buzzi, Marina and Della Penna, Giuseppe},
title = {A Preliminary Evaluation of Generative AI Tools for Blind Users: Usability and Screen Reader Interaction},
year = {2025},
isbn = {9798400714023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733155.3737910},
doi = {10.1145/3733155.3737910},
abstract = {The increasing use of Generative Artificial Intelligence (GAI) tools such as ChatGPT, Copilot, Perplexity and Gemini opens up new possible scenarios for supporting work and everyday activities. For people who are blind, the usability of such tools through screen readers is crucial to ensure their use of such AI-based technologies. In this study, we explore the accessibility and usability of the interfaces of four popular AI-based tools via screen readers through a combination of semi-automated evaluations and inspections conducted by both sighted and blind accessibility experts and screen readers with more than 20 years of experience. Navigation, labeling of control elements, feedback mechanisms, and prompt handling were considered in the study. The results point to usability difficulties in all tools, particularly in navigation structure, clarity of feedback and interactive elements. Although this work empirically explores the accessibility of AI-based tools it brings out the first critical issues that deserve further investigation. However, they are based on a small group of experts and thus should be considered preliminary and useful for future studies.},
booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {562–568},
numpages = {7},
keywords = {Accessibility, Blind users, ChatGPT, Copilot, Gemini, Generative AI, Perplexity, screen reader interaction},
location = {
},
series = {PETRA '25}
}

@inproceedings{10.1145/3706598.3714121,
author = {Atcheson, Alex and Khan, Omar and Siemann, Brian and Jain, Anika and Karahalios, Karrie},
title = {"I'd Never Actually Realized How Big An Impact It Had Until Now": Perspectives of University Students with Disabilities on Generative Artificial Intelligence},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714121},
doi = {10.1145/3706598.3714121},
abstract = {Prior research on the experiences of students with disabilities in higher education has surfaced a number of barriers that prevent full inclusion. Generative artificial intelligence (GenAI) has begun to attract interest for its potential to address longstanding barriers to access. However, little is known about the impact of these tools on the living and learning experiences of post-secondary students with disabilities. As a mixed-abilities team, we investigated student experiences with GenAI tools by collecting survey and interview responses from 62 and 21 students with disabilities, respectively, across two universities to measure students’ use of GenAI tools and their perspectives on the impact of these tools in ways related to disability, university support, and sense of belonging. Despite concerns over potential risks of GenAI and unclear university policies, students described GenAI tools as a useful resource for personalizing learning, promoting self-care, and assisting with important self-advocacy work. Guidance demonstrating safe, acceptable uses of GenAI tools, along with clear policies and resources that acknowledge diverse student needs, were desired. We discuss implications of these tools for accessibility and inclusion in higher education.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {42},
numpages = {22},
keywords = {Students with Disabilities, Higher Education, Generative Artificial Intelligence, Student Perspectives},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3490099.3511126,
author = {Ferdous, Javedul and Lee, Hae-Na and Jayarathna, Sampath and Ashok, Vikas},
title = {InSupport: Proxy Interface for Enabling Efficient Non-Visual Interaction with Web Data Records},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511126},
doi = {10.1145/3490099.3511126},
abstract = {Interaction with web data records typically involves accessing auxiliary webpage segments such as filters, sort options, search form, and multi-page links. As these segments are usually scattered all across the screen, it is arduous and tedious for blind users who rely on screen readers to access the segments, given that content navigation with screen readers is predominantly one-dimensional, despite the available support for skipping content via either special keyboard shortcuts or selective navigation. The extant techniques to overcome inefficient web screen reader interaction have mostly focused on general web content navigation, and as such they provide little to no support for data record-specific interaction activities such as filtering and sorting – activities that are equally important for enabling quick and easy access to the desired data records. To fill this void, we present InSupport, a browser extension that: (i) employs custom-built machine learning models to automatically extract auxiliary segments on any webpage containing data records, and (ii) provides an instantly accessible proxy one-stop interface for easily navigating the extracted segments using basic screen reader shortcuts. An evaluation study with 14 blind participants showed significant improvement in usability with InSupport, driven by increased reduction in interaction time and the number of key presses, compared to state-of-the-art solutions.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {49–62},
numpages = {14},
keywords = {Blind, Data records, Screen reader, Visual impairment, Web accessibility},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.1145/3664639,
author = {Prakash, Yash and Nayak, Akshay Kolgar and Sunkara, Mohan and Jayarathna, Sampath and Lee, Hae-Na and Ashok, Vikas},
title = {All in One Place: Ensuring Usable Access to Online Shopping Items for Blind Users},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {EICS},
url = {https://doi.org/10.1145/3664639},
doi = {10.1145/3664639},
abstract = {Perusing web data items such as shopping products is a core online user activity. To prevent information overload, the content associated with data items is typically dispersed across multiple webpage sections over multiple web pages. However, such content distribution manifests an unintended side effect of significantly increasing the interaction burden for blind users, since navigating to-and-fro between different sections in different pages is tedious and cumbersome with their screen readers. While existing works have proposed methods for the context of a single webpage, solutions enabling usable access to content distributed across multiple webpages are few and far between. In this paper, we present InstaFetch, a browser extension that dynamically generates an alternative screen reader-friendly user interface in real-time, which blind users can leverage to almost instantly access different item-related information such as description, full specification, and user reviews, all in one place, without having to tediously navigate to different sections in different webpages. Moreover, InstaFetch also supports natural language queries about any item, a feature blind users can exploit to quickly obtain desired information, thereby avoiding manually trudging through reams of text. In a study with 14 blind users, we observed that the participants needed significantly lesser time to peruse data items with InstaFetch, than with a state-of-the-art solution.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {257},
numpages = {25},
keywords = {Blind, Online shopping, Screen reader, Visual impairment, Web usability}
}

@article{10.1145/3664639,
author = {Prakash, Yash and Nayak, Akshay Kolgar and Sunkara, Mohan and Jayarathna, Sampath and Lee, Hae-Na and Ashok, Vikas},
title = {All in One Place: Ensuring Usable Access to Online Shopping Items for Blind Users},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {EICS},
url = {https://doi.org/10.1145/3664639},
doi = {10.1145/3664639},
abstract = {Perusing web data items such as shopping products is a core online user activity. To prevent information overload, the content associated with data items is typically dispersed across multiple webpage sections over multiple web pages. However, such content distribution manifests an unintended side effect of significantly increasing the interaction burden for blind users, since navigating to-and-fro between different sections in different pages is tedious and cumbersome with their screen readers. While existing works have proposed methods for the context of a single webpage, solutions enabling usable access to content distributed across multiple webpages are few and far between. In this paper, we present InstaFetch, a browser extension that dynamically generates an alternative screen reader-friendly user interface in real-time, which blind users can leverage to almost instantly access different item-related information such as description, full specification, and user reviews, all in one place, without having to tediously navigate to different sections in different webpages. Moreover, InstaFetch also supports natural language queries about any item, a feature blind users can exploit to quickly obtain desired information, thereby avoiding manually trudging through reams of text. In a study with 14 blind users, we observed that the participants needed significantly lesser time to peruse data items with InstaFetch, than with a state-of-the-art solution.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {257},
numpages = {25},
keywords = {Blind, Online shopping, Screen reader, Visual impairment, Web usability}
}

@article{10.1145/3519032,
author = {Barbosa, Nat\~{a} M. and Hayes, Jordan and Kaushik, Smirity and Wang, Yang},
title = {“Every Website Is a Puzzle!”: Facilitating Access to Common Website Features for People with Visual Impairments},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7228},
url = {https://doi.org/10.1145/3519032},
doi = {10.1145/3519032},
abstract = {Navigating unfamiliar websites is challenging for users with visual impairments. Although many websites offer visual cues to facilitate access to pages/features most websites are expected to have (e.g., log in at the top right), such visual shortcuts are not accessible to users with visual impairments. Moreover, although such pages serve the same functionality across websites (e.g., to log in, to sign up), the location, wording, and navigation path of links to these pages vary from one website to another. Such inconsistencies are challenging for users with visual impairments, especially for users of screen readers, who often need to linearly listen to content of pages to figure out how to access certain website features. To study how to improve access to main website features, we iteratively designed and tested a command-based approach for main features of websites via a browser extension powered by machine learning and human input. The browser extension gives users a way to access high-level website features (e.g., log in, find stores, contact) via keyboard commands. We tested the browser extension in a lab setting with 15 Internet users, including 9 users with visual impairments and 6 without. Our study showed that commands for main website features can greatly improve the experience of users with visual impairments. People without visual impairments also found command-based access helpful when visiting unfamiliar, cluttered, or infrequently visited websites, suggesting that this approach can support users with visual impairments while also benefiting other user groups (i.e., universal design). Our study reveals concerns about the handling of unsupported commands and the availability and trustworthiness of human input. We discuss how websites, browsers, and assistive technologies could incorporate a command-based paradigm to enhance web accessibility and provide more consistency on the web to benefit users with varied abilities when navigating unfamiliar or complex websites.},
journal = {ACM Trans. Access. Comput.},
month = jul,
articleno = {19},
numpages = {35},
keywords = {website commands, intelligent personal assistants, Website accessibility}
}

@inproceedings{10.1145/3532106.3533522,
author = {Jung, Ju Yeon and Steinberger, Tom and Kim, Junbeom and Ackerman, Mark S.},
title = {“So What? What's That to Do With Me?” Expectations of People With Visual Impairments for Image Descriptions in Their Personal Photo Activities},
year = {2022},
isbn = {9781450393584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532106.3533522},
doi = {10.1145/3532106.3533522},
abstract = {People with visual impairments (PVI) access photos through image descriptions. Thus far, research has studied what PVI expect in these descriptions mostly regarding functional purposes (e.g., identifying an object) and when engaging with online, publicly available images. Extending this research, we interviewed 30 PVI to understand their expectations for image descriptions when viewing, taking, searching, and reminiscing with personal photos on their own devices. We show how their expectations varied across photo activities and often went well beyond identifying objects in photos. Based on our findings, we propose design opportunities for generating and providing image descriptions for personal photo use by PVI. The design opportunities for PVI also point to novel support for the sighted for using image descriptions to enrich their experience of photos.&nbsp;},
booktitle = {Proceedings of the 2022 ACM Designing Interactive Systems Conference},
pages = {1893–1906},
numpages = {14},
location = {Virtual Event, Australia},
series = {DIS '22}
}

@inproceedings{10.1145/3544548.3581532,
author = {Kim, Jiho and Srinivasan, Arjun and Kim, Nam Wook and Kim, Yea-Seul},
title = {Exploring Chart Question Answering for Blind and Low Vision Users},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581532},
doi = {10.1145/3544548.3581532},
abstract = {Data visualizations can be complex or involve numerous data points, making them impractical to navigate using screen readers alone. Question answering (QA) systems have the potential to support visualization interpretation and exploration without overwhelming blind and low vision (BLV) users. To investigate if and how QA systems can help BLV users in working with visualizations, we conducted a Wizard of Oz study with 24 BLV people where participants freely posed queries about four visualizations. We collected 979 queries and mapped them to popular analytic task taxonomies. We found that retrieving value and finding extremum were the most common tasks, participants often made complex queries and used visual references, and the data topic notably influenced the queries. We compile a list of design considerations for accessible chart QA systems and make our question corpus publicly available to guide future research and development.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {828},
numpages = {15},–
keywords = {Accessibility, Design Considerations, Human-Subjects Qualitative Studies, Question Answering, Visualization},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1007/978-3-031-97207-2_29,
author = {Piro, Ludovica and Di Fede, Giulia and Pucci, Emanuele and Tolomeo, Stefano and Matera, Maristella},
title = {Leveraging LLMs for Voice-Based form Filling on the Web: The ConWeb Approach},
year = {2025},
isbn = {978-3-031-97206-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-97207-2_29},
doi = {10.1007/978-3-031-97207-2_29},
abstract = {Web forms are among the most challenging components for individuals using assistive technologies. Improper coding practices can significantly impede screen readers, preventing them from correctly interpreting or even accessing input fields. Conversational interaction presents an opportunity to increase form accessibility. This demonstration presents ConWeb, a conversational platform with which users can interact with web forms using natural language interaction. Our work aims at introducing an alternative and inclusive paradigm to visual web browsing.},
booktitle = {Web Engineering: 25th International Conference, ICWE 2025, Delft, The Netherlands, June 30 – July 3, 2025, Proceedings},
pages = {370–373},
numpages = {4},
keywords = {Voice interaction, Conversational form filling, Accessibility},
location = {Delft, The Netherlands}
}

@inproceedings{10.1145/3764687.3764694,
author = {Palani, Hari and Adnin, Rudaiba and Nagar, Shivangee},
title = {Kanak: Automating the Generation of Accessible STEM Materials for Blind and Low-vision Students},
year = {2025},
isbn = {9798400720161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3764687.3764694},
doi = {10.1145/3764687.3764694},
abstract = {Blind and low-vision (BLV) students face significant barriers to timely access of visual STEM materials. Accessibility practitioners, such as braille transcribers and teachers of the visually impaired (TVIs), rely on manual and labor-intensive processes to produce accessible formats (e.g., braille, tactile graphics) of visual educational materials. To complement this effort and address ongoing accessibility gaps, we developed Kanak, an AI system that leverages generative intelligence to automate the generation of accessible STEM content, including text, math equations, and graphics. Through a comparative study with seven practitioners, we examined how Kanak supports traditional transcribing workflows and augments expert judgment. Our findings reveal how generative tools could complement practitioner expertise by providing context-aware formatting, proofreading support, and on-demand graphics generation. We conclude with design considerations for human-AI collaboration in transcribing work, positioning generative intelligence not as a replacement for expert insights, but as a catalyst for expanding equitable access.},
booktitle = {Proceedings of the 37th Australian Conference on Human-Computer Interaction},
pages = {287–299},
numpages = {13},
keywords = {Accessibility, STEM Education, Blind and Low-Vision (BLV), Human-AI Collaboration, Generative AI, Tactile Graphics, Inclusive Design},
location = {
},
series = {OzCHI '25}
}

@inproceedings{10.1145/3544549.3583909,
author = {Balasubramanian, Harshadha and Morrison, Cecily and Grayson, Martin and Makhataeva, Zhanat and Marques, Rita Faia and Gable, Thomas and Perez, Dalya and Cutrell, Edward},
title = {Enable Blind Users’ Experience in 3D Virtual Environments: The Scene Weaver Prototype},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583909},
doi = {10.1145/3544549.3583909},
abstract = {Three-dimensional virtual environments are currently inaccessible to people who are blind, as current screen-reading solutions for 2D content are not fully extensible to achieve the needed embodied spatial presence. Forefronting perceptual agency as key to any access approach for users who are blind, we offer Scene Weaving as an interactional metaphor that allows users to choose how and when they perceive the environment and the people in it. We illustrate how this metaphor can be implemented in an example prototype system. In this interactivity, users can control how and when they perceive a virtual museum environment and people within it through a range of interaction mechanisms.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {447},
numpages = {4},
keywords = {Accessibility, Interaction Metaphor, Perceptual Agency, Virtual Environments, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3586183.3606830,
author = {Jain, Gaurav and Hindi, Basel and Courtien, Connor and Xu, Xin Yi Therese and Wyrick, Conrad and Malcolm, Michael and Smith, Brian A.},
title = {Front Row: Automatically Generating Immersive Audio Representations of Tennis Broadcasts for Blind Viewers},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606830},
doi = {10.1145/3586183.3606830},
abstract = {Blind and low-vision (BLV) people face challenges watching sports due to the lack of accessibility of sports broadcasts. Currently, BLV people rely on descriptions from TV commentators, radio announcers, or their friends to understand the game. These descriptions, however, do not allow BLV viewers to visualize the action by themselves. We present Front Row, a system that automatically generates an immersive audio representation of sports broadcasts, specifically tennis, allowing BLV viewers to more directly perceive what is happening in the game. Front Row first recognizes gameplay from the video feed using computer vision, then renders players’ positions and shots via spatialized (3D) audio cues. User evaluations with 12 BLV participants show that Front Row gives BLV viewers a more accurate understanding of the game compared to TV and radio, enabling viewers to form their own opinions on players’ moods and strategies. We discuss future implications of Front Row and illustrate several applications, including a Front Row plug-in for video streaming platforms to enable BLV people to visualize the action in sports videos across the Web.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {39},
numpages = {17},
keywords = {Visual impairments, accessibility, computer vision, sports},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3746059.3747791,
author = {Xu, Shuchang and Jin, Xiaofu and Zhang, Wenshuo and Qu, Huamin and Yan, Yukang},
title = {Branch Explorer: Leveraging Branching Narratives to Support Interactive 360° Video Viewing for Blind and Low Vision Users},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747791},
doi = {10.1145/3746059.3747791},
abstract = {360° videos enable users to freely choose their viewing paths, but blind and low vision (BLV) users are often excluded from this interactive experience. To bridge this gap, we present Branch Explorer, a system that transforms 360° videos into branching narratives—stories that dynamically unfold based on viewer choices—to support interactive viewing for BLV audiences. Our formative study identified three key considerations for accessible branching narratives: providing diverse branch options, ensuring coherent story progression, and enabling immersive navigation among branches. To address these needs, Branch Explorer employs a multi-modal machine learning pipeline to generate diverse narrative paths, allowing users to flexibly make choices at detected branching points and seamlessly engage with each storyline through immersive audio guidance. Evaluation with 12 BLV viewers showed that Branch Explorer significantly enhanced user agency and engagement in 360° video viewing. Users also developed personalized strategies for exploring 360° content. We further highlight implications for supporting accessible exploration of videos and virtual environments.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {48},
numpages = {18},
keywords = {Blind, Low Vision, Visual Impairment, 360° Video, Panorama Video, Audio Description, Interactive Storytelling, Virtual Reality},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3663547.3759702,
author = {Yee, Diana and Kobenova, Amina and Jhangiani, Rohan and Stickler, Piper and Kurniawan, Sri},
title = {“Better Than Nothing” or Not Enough? User-Centered Reflections on AI-Generated Audio Descriptions Across Media Formats},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3759702},
doi = {10.1145/3663547.3759702},
abstract = {AI-generated audio descriptions (ADs) offer scalable solutions for making visual media accessible to blind and low-vision (BLV) audiences. Nevertheless, little is known about how BLV users experience and evaluate these descriptions across emerging platforms. In this qualitative study, we conducted semi-structured interviews with ten (N=10) BLV participants, recruited based on divergences in prior survey ratings, to explore their perceptions of both human- and AI-generated ADs in contexts ranging from traditional film to short-form video and livestreams. Thematic analysis revealed five key themes: (1) information prioritization and genre-sensitive details, (2) the “better-than-nothing” consensus tempered by emotional and contextual gaps in AI-generated ADs, (3) the social dynamics of shared viewing, (4) accessibility deserts on new media platforms, and (5) the artistry-precision dilemma. Our findings highlight the need for adaptive, transparent, and user-informed AD systems that balance narrative resonance with efficiency. We conclude with design recommendations for co-designing AI-assisted accessibility tools in partnership with BLV communities.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {121},
numpages = {4},
keywords = {Audio Description, Blind and Low-Vision, Generative AI, Video Accessibility, New Media},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3706599.3719849,
author = {Gupta, Chitralekha and Ram, Ashwin and Sridhar, Shreyas and Jouffrais, Christophe and Nanayakkara, Suranga},
title = {Scene-to-Audio: Distant Scene Sonification for Blind and Low Vision People},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719849},
doi = {10.1145/3706599.3719849},
abstract = {Awareness of distant environmental scenes, or Vista scenes, is necessary for comprehending one’s surroundings and enjoying their beauty in leisurely settings. Current tools for Blind and Low Vision people (BLV) typically use spoken descriptions but lack support for an enjoyable experience of visually pleasing scenes. We propose a Scene-to-Audio framework that generates comprehensible and enjoyable non-verbal sounds representing distant scenes using generative models informed by psycho-acoustics, audio scene analysis, and Foley sound synthesis. We conducted a user study with eleven BLV participants evaluating strategies to utilize our scene-to-audio framework. We found that these sounds, in combination with speech, provide a significantly more memorable and immersive experience of the scenes, compared to only spoken descriptions. Our work is a step towards bridging the gap between a visual and an auditory experience of a scene, addressing the aesthetic needs of BLVs.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {471},
numpages = {9},
keywords = {Blind and Low Vision, People with Visual Impairments, Vista Scenes, Spatial Cognition, Scene Experience, Cognitive and Aesthetic Needs, Sonification, Generative Models for Audio},
location = {
},
series = {CHI EA '25}
}
@inproceedings{10.1145/3663548.3675618,
author = {Dang, Khang and Burke, Grace and Korreshi, Hamdi and Lee, Sooyeon},
title = {Towards Accessible Musical Performances in Virtual Reality: Designing a Conceptual Framework for Omnidirectional Audio Descriptions},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675618},
doi = {10.1145/3663548.3675618},
abstract = {Our research focuses on making musical performance experience in virtual reality (VR) settings non-visually accessible for Blind and Low Vision (BLV) individuals by designing a conceptual framework for omnidirectional audio descriptions (AD). We address BLV users’ prevalent challenges in accessing effective AD during VR musical performances. Employing a two-phased interview methodology, we initially collected qualitative data about BLV AD users’ experiences, followed by gathering insights from BLV professionals who specialize in AD. This approach ensures that the developed solutions are both user-centric and practically feasible. The study devises strategies for three design concepts of omnidirectional AD (Spatial AD, View-dependent AD, and Explorative AD) tailored to different types of musical performances, which vary in their visual and auditory components. Each design concept offers unique benefits; collectively, they enhance accessibility and enjoyment for BLV audiences by addressing specific user needs. Key insights highlight the crucial role of flexibility and user control in AD implementation. Based on these insights, we propose a comprehensive conceptual framework to enhance musical experiences for BLV users within VR environments.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {6},
numpages = {17},
keywords = {Accessibility, Audio Description, Blind and Low Vision Users, Musical Performances, Virtual Reality},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3544548.3580921,
author = {Peng, Yi-Hao and Chi, Peggy and Kannan, Anjuli and Morris, Meredith Ringel and Essa, Irfan},
title = {Slide Gestalt: Automatic Structure Extraction in Slide Decks for Non-Visual Access},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580921},
doi = {10.1145/3544548.3580921},
abstract = {Presentation slides commonly use visual patterns for structural navigation, such as titles, dividers, and build slides. However, screen readers do not capture such intention, making it time-consuming and less accessible for blind and visually impaired (BVI) users to linearly consume slides with repeated content. We present Slide Gestalt, an automatic approach that identifies the hierarchical structure in a slide deck. Slide Gestalt computes the visual and textual correspondences between slides to generate hierarchical groupings. Readers can navigate the slide deck from the higher-level section overview to the lower-level description of a slide group or individual elements interactively with our UI. We derived side consumption and authoring practices from interviews with BVI readers and sighted creators and an analysis of 100 decks. We performed our pipeline with 50 real-world slide decks and a large dataset. Feedback from eight BVI participants showed that Slide Gestalt helped navigate a slide deck by anchoring content more efficiently, compared to using accessible slides.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {829},
numpages = {14},
keywords = {Accessibility, Multimodal correspondence and alignment, Presentation, Screen reader, Slide deck},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3587281.3587284,
author = {Sharif, Ather and Zhang, Andrew M. and Reinecke, Katharina and Wobbrock, Jacob O.},
title = {Understanding and Improving Drilled-Down Information Extraction from Online Data Visualizations for Screen-Reader Users},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587281.3587284},
doi = {10.1145/3587281.3587284},
abstract = {Inaccessible online data visualizations can significantly disenfranchise screen-reader users from accessing critical online information. Current accessibility measures, such as adding alternative text to visualizations, only provide a high-level overview of data, limiting screen-reader users from exploring data visualizations in depth. In this work, we build on prior research to develop taxonomies of information sought by screen-reader users to interact with online data visualizations granularly through role-based and longitudinal studies with screen-reader users. Utilizing these taxonomies, we extended the functionality of VoxLens—an open-source multi-modal system that improves the accessibility of data visualizations—by supporting drilled-down information extraction. We assessed the performance of our VoxLens enhancements through task-based user studies with 10 screen-reader and 10 non-screen-reader users. Our enhancements “closed the gap” between the two groups by enabling screen-reader users to extract information with approximately the same accuracy as non-screen-reader users, reducing interaction time by 22\% in the process.},
booktitle = {Proceedings of the 20th International Web for All Conference},
pages = {18–31},
numpages = {14},
keywords = {Data visualization, accessibility, blind, screen reader, voice assistant.},
location = {Austin, TX, USA},
series = {W4A '23}
}

@inproceedings{10.1145/3587281.3587284,
author = {Sharif, Ather and Zhang, Andrew M. and Reinecke, Katharina and Wobbrock, Jacob O.},
title = {Understanding and Improving Drilled-Down Information Extraction from Online Data Visualizations for Screen-Reader Users},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587281.3587284},
doi = {10.1145/3587281.3587284},
abstract = {Inaccessible online data visualizations can significantly disenfranchise screen-reader users from accessing critical online information. Current accessibility measures, such as adding alternative text to visualizations, only provide a high-level overview of data, limiting screen-reader users from exploring data visualizations in depth. In this work, we build on prior research to develop taxonomies of information sought by screen-reader users to interact with online data visualizations granularly through role-based and longitudinal studies with screen-reader users. Utilizing these taxonomies, we extended the functionality of VoxLens—an open-source multi-modal system that improves the accessibility of data visualizations—by supporting drilled-down information extraction. We assessed the performance of our VoxLens enhancements through task-based user studies with 10 screen-reader and 10 non-screen-reader users. Our enhancements “closed the gap” between the two groups by enabling screen-reader users to extract information with approximately the same accuracy as non-screen-reader users, reducing interaction time by 22\% in the process.},
booktitle = {Proceedings of the 20th International Web for All Conference},
pages = {18–31},
numpages = {14},
keywords = {Data visualization, accessibility, blind, screen reader, voice assistant.},
location = {Austin, TX, USA},
series = {W4A '23}
}

@inproceedings{10.1145/3656650.3656739,
author = {Pucci, Emanuele and Piro, Ludovica and Andolina, Salvatore and Matera, Maristella},
title = {From Conversational Web to Inclusive Conversations with LLMs},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656739},
doi = {10.1145/3656650.3656739},
abstract = {This paper explores how a library of conversational patterns defined for Web browsing can be adopted to improve the accessibility of prompt interfaces for Large Language Models. It reports preliminary findings from a formative evaluation with blind and visually impaired users. It also discusses how these findings support transferring the benefits of voice-based Web browsing to the inclusivity of prompt-based interfaces.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {87},
numpages = {3},
keywords = {Accessibility, Information Retrieval, Prompt-based Interaction},
location = {Arenzano, Genoa, Italy},
series = {AVI '24}
}

@INPROCEEDINGS{10463430,
  author={Kuzdeuov, Askat and Mukayev, Olzhas and Nurgaliyev, Shakhizat and Kunbolsyn, Alisher and Varol, Huseyin Atakan},
  booktitle={2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={ChatGPT for Visually Impaired and Blind}, 
  year={2024},
  volume={},
  number={},
  pages={722-727},
  keywords={Voice activity detection;Visualization;Source coding;Oral communication;Blindness;User interfaces;Chatbots},
  doi={10.1109/ICAIIC60209.2024.10463430}}

@misc{chen2025envisionvrsceneinterpretationtool,
      title={EnVisionVR: A Scene Interpretation Tool for Visual Accessibility in Virtual Reality}, 
      author={Junlong Chen and Rosella P. Galindo Esparza and Vanja Garaj and Per Ola Kristensson and John Dudley},
      year={2025},
      eprint={2502.03564},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2502.03564}, 
}

@inproceedings{10.1145/3663548.3688498,
author = {Collins, Jazmin and Nicholson, Kaylah Myranda and Khadir, Yusuf and Stevenson Won, Andrea and Azenkot, Shiri},
title = {An AI Guide to Enhance Accessibility of Social Virtual Reality for Blind People},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3688498},
doi = {10.1145/3663548.3688498},
abstract = {The rapid growth of virtual reality (VR) has led to increased use of social VR platforms for interaction. However, these platforms lack adequate features to support blind and low vision (BLV) users, posing significant challenges in navigation, visual interpretation, and social interaction. One promising approach to these challenges is employing human guides in VR. However, this approach faces limitations with a lack of availability of humans to serve as guides, or the inability to customize the guidance a user receives from the human guide. We introduce an AI-powered guide to address these limitations. The AI guide features six personas, each offering unique behaviors and appearances to meet diverse user needs, along with visual interpretation and navigation assistance. We aim to use this AI guide in the future to help us understand BLV users’ preferences for guide forms and functionalities.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {130},
numpages = {5},
keywords = {VR, accessibility, blind, low vision},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@article{10.1109/TVCG.2024.3456358,
author = {Reinders, Samuel and Butler, Matthew and Zukerman, Ingrid and Lee, Bongshin and Qu, Lizhen and Marriott, Kim},
title = {When Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis with Touch and Speech},
year = {2025},
issue_date = {Jan. 2025},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {31},
number = {1},
issn = {1077-2626},
url = {https://doi.org/10.1109/TVCG.2024.3456358},
doi = {10.1109/TVCG.2024.3456358},
abstract = {Despite the recent surge of research efforts to make data visualizations accessible to people who are blind or have low vision (BLV), how to support BLV people's data analysis remains an important and challenging question. As refreshable tactile displays (RTDs) become cheaper and conversational agents continue to improve, their combination provides a promising approach to support BLV people's interactive data exploration and analysis. To understand how BLV people would use and react to a system combining an RTD with a conversational agent, we conducted a Wizard-of-Oz study with 11 BLV participants, where they interacted with line charts, bar charts, and isarithmic maps. Our analysis of participants' interactions led to the identification of nine distinct patterns. We also learned that the choice of modalities depended on the type of task and prior experience with tactile graphics, and that participants strongly preferred the combination of RTD and speech to a single modality. In addition, participants with more tactile experience described how tactile images facilitated a deeper engagement with the data and supported independent interpretation. Our findings will inform the design of interfaces for such interactive mixed-modality systems.},
journal = {IEEE Transactions on Visualization and Computer Graphics},
month = jan,
pages = {864–874},
numpages = {11}
}

@article{10.1145/3678537,
author = {Zhao, Kaixing and Lai, Rui and Guo, Bin and Liu, Le and He, Liang and Zhao, Yuhang},
title = {AI-Vision: A Three-Layer Accessible Image Exploration System for People with Visual Impairments in China},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678537},
doi = {10.1145/3678537},
abstract = {Accessible image exploration systems are able to help people with visual impairments (PVI) to understand image content by providing different types of interactions. With the development of computer vision technologies, image exploration systems are supporting more fine-grained image content processing, including image segmentation, description and object recognition. However, in developing countries like China, it is still rare for PVI to widely rely on such accessible system. To better understand the usage situation of accessible image exploration system in China and improve the image understanding of PVI in China, we developed AI-Vision, an Android based hierarchical accessible image exploration system supporting the generations of image general description, local object description and metadata information. Our 7-day diary study with 10 PVI verified the usability of AI-Vision and also revealed a series of design implications for improving accessible image exploration systems similar to AI-Vision.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {145},
numpages = {27},
keywords = {Accessible, Image Exploration, Visual Impairments}
}

@ARTICLE{11146787,
  author={Salous, Mazen and Lange, Daniel and von Reeken, Timo and Wolters, Maria K. and Heuten, Wilko and Boll, Susanne and Abdenebaoui, Larbi},
  journal={IEEE Access}, 
  title={Semi-Automatic BVI Human-Centered Image Conversational Descriptions: Leveraging LLMs and Expert Refinements for Inclusive Visual Accessibility}, 
  year={2025},
  volume={13},
  number={},
  pages={156072-156090},
  keywords={Visualization;Oral communication;Videos;Large language models;Iterative methods;Guidelines;Training;Standards;Annotations;Training data;Accessibility;digital image description;human–AI image conversation},
  doi={10.1109/ACCESS.2025.3605490}}

@INPROCEEDINGS{10322247,
  author={Collins, Jazmin and Jung, Crescentia and Azenkot, Shiri},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Making Avatar Gaze Accessible for Blind and Low Vision People in Virtual Reality: Preliminary Insights}, 
  year={2023},
  volume={},
  number={},
  pages={701-705},
  keywords={Human computer interaction;Avatars;Oral communication;Haptic interfaces;Augmented reality;social virtual reality;blind and low vision;nonverbal communication},
  doi={10.1109/ISMAR-Adjunct60411.2023.00150}}

@inproceedings{10.1145/3663547.3746364,
author = {Kneitmix, Melanie Jo and Wobbrock, Jacob O.},
title = {From Screen Reading to “Scene Reading” in SceneVR: Touch-Based Interaction Techniques for Use in Virtual Reality by Blind and Low-Vision Users},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3746364},
doi = {10.1145/3663547.3746364},
abstract = {To improve the accessibility of virtual reality (VR) for blind and low-vision (BLV) users, we introduce “scene reading,” a technique inspired by touch-based screen reading for use in virtual environments. Scene reading provides semantic information about virtual objects and their on-screen positions, organizing details into hierarchies that users can navigate for more granular exploration; it also uses spatial audio for nonvisual feedback. To design and evaluate our scene reading technique, we developed a system called SceneVR, which supports touch and gesture input, and spatial audio output. SceneVR streams the live view from a VR headset onto a phone or tablet, letting BLV users drag their finger across the touchscreen to identify objects and avatars, navigate, and gain a spatial understanding of the scene. We conducted a task-based usability study to evaluate our SceneVR controller, collecting data on task performance, user experience, interaction patterns, and subjective feedback. Our findings indicate that scene reading with the SceneVR controller effectively supports BLV users in exploring virtual environments, enabling them to discover objects, navigate object hierarchies, and build an understanding of their surroundings while also providing a sense of enjoyment and agency. However, our findings also reveal initial design implications, including minimizing cognitive load and effectively integrating scene reading labels and descriptions with other sensory feedback to create a cohesive experience.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {3},
numpages = {18},
keywords = {Accessibility, blind and low-vision, virtual reality (VR), touchscreen-based interfaces, spatial audio.},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3706598.3713496,
author = {Xu, Shuchang and Jin, Xiaofu and Qu, Huamin and Yan, Yukang},
title = {DanmuA11y: Making Time-Synced On-Screen Video Comments (Danmu) Accessible to Blind and Low Vision Users via Multi-Viewer Audio Discussions},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713496},
doi = {10.1145/3706598.3713496},
abstract = {By overlaying time-synced user comments on videos, Danmu creates a co-watching experience for online viewers. However, its visual-centric design poses significant challenges for blind and low vision (BLV) viewers. Our formative study identified three primary challenges that hinder BLV viewers’ engagement with Danmu: the lack of visual context, the speech interference between comments and videos, and the disorganization of comments. To address these challenges, we present DanmuA11y, a system that makes Danmu accessible by transforming it into multi-viewer audio discussions. DanmuA11y incorporates three core features: (1) Augmenting Danmu with visual context, (2) Seamlessly integrating Danmu into videos, and (3) Presenting Danmu via multi-viewer discussions. Evaluation with twelve BLV viewers demonstrated that DanmuA11y significantly improved Danmu comprehension, provided smooth viewing experiences, and fostered social connections among viewers. We further highlight implications for enhancing commentary accessibility in video-based social media and live-streaming platforms.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {293},
numpages = {22},
keywords = {Visual Impairment, Blind, Low Vision, Video, Social Media, Danmaku, Danmu, Bullet Comment},
location = {
},
series = {CHI '25}
}

@INPROCEEDINGS{11228963,
  author={Qi, Xingyu and Li, He and Li, Linjie and Wu, Zhenyu},
  booktitle={2025 International Joint Conference on Neural Networks (IJCNN)}, 
  title={EmoAssist: Emotional Assistant for Visual Impairment Community}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  keywords={Measurement;Visualization;Emotion recognition;Navigation;Visual impairment;Neural networks;Benchmark testing;Propulsion;Question answering (information retrieval);Optimization;Visual Impairments (VI);Large Multi-modality Models (LMMs);Visual Question Answering (VQA);Emotional Intelligence},
  doi={10.1109/IJCNN64981.2025.11228963}}

@Article{Bousnitra2025,
author={Bousnitra, Walid
and Herv{\'a}s, Raquel
and D{\'i}az, Alberto
and Leo, Daniel
and Mart{\'i}n, Carlos
and Sosa, Sirma},
title={Self-guided interpretation of images for blind users based on multi-layered interaction},
journal={Universal Access in the Information Society},
year={2025},
month={Nov},
day={01},
volume={24},
number={4},
pages={3165-3183},
abstract={Image spatial composition, information about the number of objects and their characteristics, and additional details such as the colors and attributes of the people in the scene can provide additional information in an image or photograph. However, the usual approaches to make images accessible to blind persons focus mainly on describing the image's content, without inquiring into other aspects such as spatial composition, colors, background, known faces, etc. In doing so, much information that is present in the image but not included in the description is missing for a blind user. This work explores the combination of image captioning and object detection techniques with the final goal of making images more accessible to blind users. The approach is twofold: (1) state-of-the-art algorithms of image captioning and object detection will be combined so blind users can extract different kinds of information from a given image; and (2) blind users will guide the exploration of the images, so they can gather all the information in a personalized manner and make their own interpretation. This novel interaction is designed as a self-guided multi-layered approach, enabling blind users to personalize their exploration of image content by both guiding the process and accessing the different layers of information as desired. We implemented a mobile application based on requirements obtained from blind users and performed an evaluation that provided promising results. The participants were reasonably satisfied with the usability of the prototype, and in general they were able to gather additional information through their tactile exploration that was not present in the initial image descriptions. However, some issues that were detected in the evaluation, and functionalities that could not be implemented, will be addressed in future work.},
issn={1615-5297},
doi={10.1007/s10209-025-01256-w},
url={https://doi.org/10.1007/s10209-025-01256-w}
}

@InProceedings{10.1007/978-3-032-01632-4_25,
author="Salous, Mazen
and Riecken, John-Uwe
and Heuten, Wilko
and Abdenaboui, Larbi",
editor="Mavrou, Katerina
and Encarna{\c{c}}{\~a}o, Pedro",
title="Enhancing Image Accessibility in Educational Contexts for Blind and Visually Impaired Learners Through Integrated Computer Vision Techniques",
booktitle="Technology for Inclusion and Participation for All: Recent Achievements and Future Directions",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="193--200",
abstract="Blind and visually impaired (BVI) learners face challenges accessing image-based educational content. This paper presents an AI-driven system that generates multimodal representations---raised tactile graphics and descriptive text---by integrating open-vocabulary object detection (YOLO-World), image segmentation (SAM), edge detection (Canny) and natural language generation (GPT-4o). The goal is to improve image accessibility for BVI students. A user study with four BVI participants in two educational scenarios (animal anatomy and biology scales) showed that the multimodal outputs enhanced understanding and were not seen as overly complex. Participants appreciated the dual-modality approach (average rating: 4.25/5), though one participant noted a need for practice or assistance. These results, although preliminary, highlight the system's potential to make educational images more accessible and underscore the need for personalization and user training.",
isbn="978-3-032-01632-4"
}

@inproceedings{10.1145/3663548.3675658,
author = {Gubbi Mohanbabu, Ananya and Pavel, Amy},
title = {Context-Aware Image Descriptions for Web Accessibility},
year = {2024},https://shop.cineplex.de/kinogutscheine/lidl-plus.html
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675658},
doi = {10.1145/3663548.3675658},
abstract = {Blind and low vision (BLV) internet users access images on the web via text descriptions. New vision-to-language models such as GPT-V, Gemini, and LLaVa can now provide detailed image descriptions on-demand. While prior research and guidelines state that BLV audiences’ information preferences depend on the context of the image, existing tools for accessing vision-to-language models provide only context-free image descriptions by generating descriptions for the image alone without considering the surrounding webpage context. To explore how to integrate image context into image descriptions, we designed a Chrome Extension that automatically extracts webpage context to inform GPT-4V-generated image descriptions. We gained feedback from 12 BLV participants in a user study comparing typical context-free image descriptions to context-aware image descriptions. We then further evaluated our context-informed image descriptions with a technical evaluation. Our user evaluation demonstrates that BLV participants frequently prefer context-aware descriptions to context-free descriptions. BLV participants also rate context-aware descriptions significantly higher in quality, imaginability, relevance, and plausibility. All participants shared that they wanted to use context-aware descriptions in the future and highlighted the potential for use in online shopping, social media, news, and personal interest blogs.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {62},
numpages = {17},
keywords = {Accessibility, Context Awareness, Image Descriptions, Text;},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3677846.3677861,
author = {Xu, Andi and Cai, Minyu and Hou, Dier and Chang, Ruei-Che and Guo, Anhong},
title = {ImageExplorer Deployment: Understanding Text-Based and Touch-Based Image Exploration in the Wild},
year = {2024},
isbn = {9798400710308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677846.3677861},
doi = {10.1145/3677846.3677861},
abstract = {Blind and visually-impaired (BVI) users often rely on alt-texts to understand images. AI-generated alt-texts can be scalable and efficient but may lack details and are prone to errors. Multi-layered touch interfaces, on the other hand, can provide rich details and spatial information, but may take longer to explore and cause higher mental load. To understand how BVI users leverage these two methods, we deployed ImageExplorer, an iOS app on the Apple App Store that provides multi-layered image information via both text-based and touch-based interfaces with customizable levels of granularity. Across 12 months, 371 users uploaded 651 images and explored 694 times. Their activities were logged to help us understand how BVI users consume image captions in the wild. This work informs a holistic understanding of BVI users’ image exploration behavior and influential factors. We provide design implications for future models of image captioning and visual access tools.},
booktitle = {Proceedings of the 21st International Web for All Conference},
pages = {59–69},
numpages = {11},
keywords = {Accessibility, Alt Text, Blind, Deployment, Image Caption, ImageExplorer, Screen Reader, Touch Exploration, Visual Impairment},
location = {Singapore, Singapore},
series = {W4A '24}
}

@inproceedings{10.1145/3613904.3642839,
author = {Van Daele, Tess and Iyer, Akhil and Zhang, Yuning and Derry, Jalyn C and Huh, Mina and Pavel, Amy},
title = {Making Short-Form Videos Accessible with Hierarchical Video Summaries},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642839},
doi = {10.1145/3613904.3642839},
abstract = {Short videos on platforms such as TikTok, Instagram Reels, and YouTube Shorts (i.e. short-form videos) have become a primary source of information and entertainment. Many short-form videos are inaccessible to blind and low vision (BLV) viewers due to their rapid visual changes, on-screen text, and music or meme-audio overlays. In our formative study, 7 BLV viewers who regularly watched short-form videos reported frequently skipping such inaccessible content. We present &nbsp;ShortScribe, a system that provides hierarchical visual summaries of short-form videos at three levels of detail to support BLV viewers in selecting and understanding short-form videos. ShortScribe allows BLV users to navigate between video descriptions based on their level of interest. To evaluate &nbsp;ShortScribe, we assessed description accuracy and conducted a user study with 10 BLV participants comparing &nbsp;ShortScribe to a baseline interface. When using ShortScribe, participants reported higher comprehension and provided more accurate summaries of video content.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {895},
numpages = {17},
keywords = {Accessibility, Short-Form Video, Summaries, Video Description},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613904.3642713,
author = {Zhang, Lotus and Stangl, Abigale and Sharma, Tanusree and Tseng, Yu-Yun and Xu, Inan and Gurari, Danna and Wang, Yang and Findlater, Leah},
title = {Designing Accessible Obfuscation Support for Blind Individuals’ Visual Privacy Management},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642713},
doi = {10.1145/3613904.3642713},
abstract = {Blind individuals commonly share photos in everyday life. Despite substantial interest from the blind community in being able to independently obfuscate private information in photos, existing tools are designed without their inputs. In this study, we prototyped a preliminary screen reader-accessible obfuscation interface to probe for feedback and design insights. We implemented a version of the prototype through off-the-shelf AI models (e.g., SAM, BLIP2, ChatGPT) and a Wizard-of-Oz version that provides human-authored guidance. Through a user study with 12 blind participants who obfuscated diverse private photos using the prototype, we uncovered how they understood and approached visual private content manipulation, how they reacted to frictions such as inaccuracy with existing AI models and cognitive load, and how they envisioned such tools to be better designed to support their needs (e.g., guidelines for describing visual obfuscation effects, co-creative interaction design that respects blind users’ agency).},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {235},
numpages = {19},
keywords = {accessibility, blind photography, privacy-preservation technology},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3597638.3608426,
author = {Wang, Xiyue and Kayukawa, Seita and Takagi, Hironobu and Asakawa, Chieko},
title = {TouchPilot: Designing a Guidance System that Assists Blind People in Learning Complex 3D Structures},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608426},
doi = {10.1145/3597638.3608426},
abstract = {Making complex structures accessible to blind people is challenging due to the need for skilled explainers. Interactive 3D printed models (I3Ms) have been developed to enable independent learning of 3D models through activating audio labels. However, they present single-layered information and require users to identify interactive elements through a pinpointing action, which might be insufficient for learning complex and unfamiliar subjects. In this paper, we investigate I3Ms for complex structures. We propose TouchPilot, a guidance system designed based on a study that observed learner-explainer interaction styles. TouchPilot guides users step by step through navigation, exploration of hierarchical elements, and confirmation of their entire areas. A follow-up study found that the guidance system led to better learning outcomes and higher independence compared to a pinpointing system. Feedback suggests that being primed by the guidance system systematically, followed by pinpointing freely for review, is preferred for learning complex structures.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {5},
numpages = {18},
keywords = {computer vision, guidance, interactive 3D printed models, visual impairments},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3613905.3648648,
author = {Zhang, Zhe-Xin and Ochiai, Yoichi},
title = {A Design of Interface for Visual-Impaired People to Access Visual Information from Images Featuring Large Language Models and Visual Language Models},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3648648},
doi = {10.1145/3613905.3648648},
abstract = {We propose a design of interface for visual-impaired People to access visual information from images utilizing Large Language Models(LLMs), Visual Language Models (VLMs), and Segment-Anything. We use Semantic-Segment-Anything to generate the segmentation of semantic objects in images. The segmentation includes two parts: a term set describing the semantic object, and segmented mask which represents the shape of the semantic object. We provide two methods for the visual-impaired user to access the information of the semantic object and its peripheral information in image. In one method, the LLM summarize the term set to create an description. In the other method, the image with the object masked is provided to Visual Language Models which is prompted to respond with a description. In both methods, the mask can be accessed with dot display after processed for the visual-impaired people to access, and the description is prompted to the user in synthesized voice.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {390},
numpages = {4},
keywords = {Human-Computer Interaction, Large Language Models, Segment-Anything, Visual Language Models},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3491102.3501918,
author = {Das, Maitraye and McHugh, Thomas Barlow and Piper, Anne Marie and Gergle, Darren},
title = {Co11ab: Augmenting Accessibility in Synchronous Collaborative Writing for People with Vision Impairments},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501918},
doi = {10.1145/3491102.3501918},
abstract = {Collaborative writing is an integral part of academic and professional work. Although some prior research has focused on accessibility in collaborative writing, we know little about how visually impaired writers work in real-time with sighted collaborators or how online editing tools could better support their work. Grounded in formative interviews and observations with eight screen reader users, we built Co11ab, a Google Docs extension that provides configurable audio cues to facilitate understanding who is editing (or edited) what and where in a shared document. Results from a design exploration with fifteen screen reader users, including three naturalistic sessions of use with sighted colleagues, reveal how screen reader users understand various auditory representations and use them to coordinate real-time collaborative writing. We revisit what collaboration awareness means for screen reader users and discuss design considerations for future systems.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {196},
numpages = {18},
keywords = {Collaborative writing, ability-diverse collaboration, accessibility, collaboration awareness, screen readers, vision impairments},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3517635,
author = {Lee, Cheuk Yin Phipson and Zhang, Zhuohao and Herskovitz, Jaylin and Seo, JooYoung and Guo, Anhong},
title = {CollabAlly: Accessible Collaboration Awareness in Document Editing},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517635},
doi = {10.1145/3491102.3517635},
abstract = {Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified the current practices and challenges in collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {596},
numpages = {17},
keywords = {Blind, Collaborative writing, accessibility, collaboration awareness, earcon, screen reader, spatial audio, visual impairment, voice font},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3502081,
author = {Huh, Mina and Lee, YunJung and Choi, Dasom and Kim, Haesoo and Oh, Uran and Kim, Juho},
title = {Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502081},
doi = {10.1145/3491102.3502081},
abstract = {Webtoon is a type of digital comics read online where readers can leave comments to share their thoughts on the story. While it has experienced a surge in popularity internationally, people with visual impairments cannot enjoy webtoon with the lack of an accessible format. While traditional image description practices can be adopted, resulting descriptions cannot preserve webtoons’ unique values such as control over the reading pace and social engagement through comments. To improve the webtoon reading experience for BLV users, we propose Cocomix, an interactive webtoon reader that leverages comments into the design of novel webtoon interactions. Since comments can identify story highlights and provide additional context, we designed a system that provides 1) comments-based adaptive descriptions with selective access to details and 2) panel-anchored comments for easy access to relevant descriptive comments. Our evaluation (N=12) showed that Cocomix users could adapt the description for various needs and better utilize comments.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {607},
numpages = {18},
keywords = {Accessibility, Comments, Digital comics, Image description, Webtoons},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1145/3555720,
author = {Lee, Yun Jung and Joh, Hwayeon and Yoo, Suhyeon and Oh, Uran},
title = {AccessComics2: Understanding the User Experience of an Accessible Comic Book Reader for Blind People with Textual Sound Effects},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/3555720},
doi = {10.1145/3555720},
abstract = {For people with visual impairments, many studies have been conducted to improve the accessibility of various types of images on the web. However, the majority of the work focused on photos or graphs. In this study, we propose AccessComics, an accessible digital comic book reader for people with visual impairments. To understand the accessibility of existing platforms, we first conducted a formative online survey with 68 participants who are blind or have low vision asking about their prior experiences with audiobooks and eBooks. Then, to learn the implications of designing an accessible comic book reader for people with visual impairments, we conducted an interview study with eight participants and collected feedback about our system. Considering our findings that a brief description of the scene and sound effects are desired when listening to comic books, we conducted a follow-up study with 16 participants (8 blind, 8 sighted) to explore how to effectively provide scene descriptions and sound effects, generated based on the onomatopoeia and mimetic words that appear in comics. Then we assessed the impact of the overall reading experience and if it differs depending on the user group. The results show that the presence of scene descriptions was perceived to be useful for concentration and understanding the situation, while the sound effects were perceived to make the book-reading experience more immersive and realistic. Based on the findings, we suggest design implications specifying features that future accessible comic book readers should support.},
journal = {ACM Trans. Access. Comput.},
month = mar,
articleno = {2},
numpages = {25},
keywords = {onomatopoeia/mimetic words, textual sound effects, sound effects, scene descriptions, people with visual impairments, blind, screen readers, eBooks, audiobooks, Comics}
}

@inproceedings{Salous2023TowardsAA,
  title={Towards AI-based Accessible Digital Media: Image Analysis Pipelines and Blind User Studies*},
  author={Mazen Salous and Timo Daniel Lange and Von Reeken and Wilko Heuten and Susanne CJ Boll and Larbi Abdenebaoui},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:262232480}
}

@ARTICLE{9555469,
  author={Lundgard, Alan and Satyanarayan, Arvind},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content}, 
  year={2022},
  volume={28},
  number={1},
  pages={1073-1083},
  abstract={Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.},
  keywords={Semantics;Natural languages;Visualization;Guidelines;Data visualization;Media;Graphics;Visualization;natural language;description;caption;semantic;model;theory;alt text;blind;disability;accessibility},
  doi={10.1109/TVCG.2021.3114770},
  ISSN={1941-0506},
  month={Jan},}

  @article{10.1093/bioinformatics/btae670,
    author = {Smits, Thomas C and L’Yi, Sehi and Mar, Andrew P and Gehlenborg, Nils},
    title = {AltGosling: automatic generation of text descriptions for accessible genomics data visualization},
    journal = {Bioinformatics},
    volume = {40},
    number = {12},
    pages = {btae670},
    year = {2024},
    month = {11},
    abstract = {Biomedical visualizations are key to accessing biomedical knowledge and detecting new patterns in large datasets. Interactive visualizations are essential for biomedical data scientists and are omnipresent in data analysis software and data portals. Without appropriate descriptions, these visualizations are not accessible to all people with blindness and low vision, who often rely on screen reader accessibility technologies to access visual information on digital devices. Screen readers require descriptions to convey image content. However, many images lack informative descriptions due to unawareness and difficulty writing such descriptions. Describing complex and interactive visualizations, like genomics data visualizations, is even more challenging. Automatic generation of descriptions could be beneficial, yet current alt text generating models are limited to basic visualizations and cannot be used for genomics.We present AltGosling, an automated description generation tool focused on interactive data visualizations of genome-mapped data, created with the grammar-based genomics toolkit Gosling. The logic-based algorithm of AltGosling creates various descriptions including a tree-structured navigable panel. We co-designed AltGosling with a blind screen reader user (co-author). We show that AltGosling outperforms state-of-the-art large language models and common image-based neural networks for alt text generation of genomics data visualizations. As a first of its kind in genomic research, we lay the groundwork to increase accessibility in the field.The source code, examples, and interactive demo are accessible under the MIT License at https://github.com/gosling-lang/altgosling. The package is available at https://www.npmjs.com/package/altgosling.},
    issn = {1367-4811},
    doi = {10.1093/bioinformatics/btae670},
    url = {https://doi.org/10.1093/bioinformatics/btae670},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/40/12/btae670/60964495/btae670.pdf},
}

@InProceedings{10.1007/978-3-031-08645-8_2,
author="Egger, Niklas
and Zimmermann, Gottfried
and Strobbe, Christophe",
editor="Miesenberger, Klaus
and Kouroupetroglou, Georgios
and Mavrou, Katerina
and Manduchi, Roberto
and Covarrubias Rodriguez, Mario
and Pen{\'a}z, Petr",
title="Overlay Tools as a Support for Accessible Websites -- Possibilities and Limitations",
booktitle="Computers Helping People with Special Needs",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="6--17",
abstract="Current laws and directives such as the Americans with Disabilities Act and the European Union's Directive 2016/2102 aim to support people with disabilities. At the same time, they also represent a challenge for many website owners who have to comply with these legal requirements. Providers of so-called overlay tools offer, among other things, fully automated solutions which, according to their own statements, can be installed within a few minutes and subsequently improve the website in such a way that it is accessible and compliant with the laws and standards. This raises two questions. First, to what extent overlay tools provide real accessibility improvements? And second, is it possible to identify possibilities and limitations and therefore suggest potential improvements. Based on a comparison of the features of nine existing tools, three were selected for closer examination (accessiBe, EqualWeb and UserWay). In addition, a comparative study of these providers was conducted. For this purpose, a total of seven metrics were defined, which are based on the information and promises of the providers as well as on the requirements of the Web Content Accessibility Guidelines 2.1. To validate the changes made by the overlay tools, the adaptations were evaluated using 29 of the 92 test steps of the BIK BITV-Test. Moreover, the site owner's perspective was taken into account and the corresponding features and functions were analyzed and evaluated within four metrics. However, user tests are not part of this work. Significant differences between the overlay tools can be seen primarily in the adjustments to the actual website. In comparison, accessiBe corrects more failures and barriers for blind users and users who use the keyboard navigation and thus also achieves the best result in the overall score. Despite these improvements, full compliance is clearly not possible. However, both theoretical and concrete opportunities for improvement were identified.",
isbn="978-3-031-08645-8"
}

@inproceedings{10.1145/3613904.3641970,
author = {Jones, Shuli and Pedraza Pineros, Isabella and Hajas, Daniel and Zong, Jonathan and Satyanarayan, Arvind},
title = {“Customization is Key”: Reconfigurable Textual Tokens for Accessible Data Visualizations},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641970},
doi = {10.1145/3613904.3641970},
abstract = {Customization is crucial for making visualizations accessible to blind and low-vision (BLV) people with widely-varying needs. But what makes for usable or useful customization? We identify four design goals for how BLV people should be able to customize screen-reader-accessible visualizations: presence, or what content is included; verbosity, or how concisely content is presented; ordering, or how content is sequenced; and, duration, or how long customizations are active. To meet these goals, we model a customization as a sequence of content tokens, each with a set of adjustable properties. We instantiate our model by extending Olli, an open-source accessible visualization toolkit, with a settings menu and command box for persistent and ephemeral customization respectively. Through a study with 13 BLV participants, we find that customization increases the ease of identifying and remembering information. However, customization also introduces additional complexity, making it more helpful for users familiar with similar tools.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {47},
numpages = {14},
keywords = {accessible visualization, customization, hierarchical text structures, screen reader, text description},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.5555/3632186.3632206,
author = {Kaushik, Smirity and Barbosa, Nat\~{a} M. and Yu, Yaman and Sharma, Tanusree and Kilhoffer, Zachary and Seo, JooYoung and Das, Sauvik and Wang, Yang},
title = {GuardLens: supporting safer online browsing for people with visual impairments},
year = {2023},
isbn = {978-1-939133-36-6},
publisher = {USENIX Association},
address = {USA},
abstract = {Visual cues play a key role in how users assess the privacy/ security of a website, but often remain inaccessible to people with visual impairments (PVIs), disproportionately exposing them to privacy and security risks. We employed an iterative, user-centered design process with 25 PVIs to design and evaluate GuardLens, a browser extension that improves the accessibility of privacy/security cues and helps PVIs assess a website's legitimacy (i.e., if it is a spoof/phish). We started with a formative study to understand what privacy/ security cues PVIs find helpful, and then improved GuardLens based on the results. Next, we further refined Guardlens based on a pilot study, and lastly, conducted our main study to evaluate GuardLens' efficacy. The results suggest that GuardLens, by extracting and listing pertinent privacy/ security cues in one place for faster and easier access, helps PVIs quickly and accurately determine if websites are legitimate or spoofs. PVIs found cues such as domain age, search result ranking, and the presence/absence of HTTPS encryption especially helpful. We conclude with design implications for tools to support PVIs with safe web browsing.},
booktitle = {Proceedings of the Nineteenth USENIX Conference on Usable Privacy and Security},
articleno = {20},
numpages = {20},
location = {Anaheim, CA, USA},
series = {SOUPS '23}
}

@inproceedings{10.1145/3663548.3675650,
author = {Makati, Tlamelo and Tigwell, Garreth W. and Shinohara, Kristen},
title = {The Promise and Pitfalls of Web Accessibility Overlays for Blind and Low Vision Users},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675650},
doi = {10.1145/3663548.3675650},
abstract = {Web accessibility is essential for ensuring that all individuals, regardless of their physical or cognitive abilities, can access and effectively use the internet. This principle is fundamental as digital platforms increasingly become primary channels for education, communication, commerce, and entertainment. Our study critically evaluates the effectiveness of accessibility overlays, which are third-party tools that claim to enhance website usability for people with disabilities. Specifically, we focused on the experiences of blind and low-vision users, who are disproportionately impacted by poor web accessibility. Through a combination of online surveys and interviews, we engaged with participants who employ a variety of assistive technologies to navigate the web. The empirical evidence gathered paints a troubling picture: despite their intended purpose, accessibility overlays often fail to deliver on their promises and, in many cases, increase existing challenges. Participants frequently reported that these overlays conflicted with their assistive technologies, leading to reduced functionality and increased frustration. This points to a significant misalignment between the design of these tools and the real-world needs of users. The study highlights the pressing need to move away from superficial technological fixes and towards deeper, more meaningful engagement with the needs of disabled users. This involves embracing user-centered design practices that integrate accessibility considerations from the ground up, ensuring that digital environments are truly inclusive. By prioritizing comprehensive, well-integrated solutions over patches like overlays, we can foster a more accessible and equitable digital landscape.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {38},
numpages = {12},
keywords = {Accessibility overlays, web accessibility},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3604571.3604586,
author = {Ryskeldiev, Bektur and Kobayashi, Mariko and Teramoto, Kentaro and Kusano, Koki},
title = {Towards Immersive Inclusivity for C2C: How Immersive Multimodal Interactions Can Make Online Customer-to-Customer Shopping More Inclusive},
year = {2023},
isbn = {9798400707612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604571.3604586},
doi = {10.1145/3604571.3604586},
abstract = {Online shopping is becoming ubiquitous in everyday life, with shopping experiences primarily delivered through either websites or specialized applications. This format may create inclusivity problems for users with different needs, as not all websites or applications are optimized for accessibility or have multiple alternative forms of interaction with the content. This can be especially challenging in customer-to-customer (C2C) platforms. While in online shopping a single platform can make merchants provide a unified display for description, standardized packaging, and ensure the item quality, in C2C platforms this responsibility falls mostly on users, thus creating a need for a more versatile way of processing, display, and interaction with C2C content. We are interested in how this issue can be alleviated through addition of online assistants that support immersive and multimodal forms of interactions. To gain a better understanding of the problem and needs of users, we have built a prototype application that uses a voice-activated assistant for item search and purchase and evaluated it in a pilot study. The findings suggest that participants value efficiency in C2C shopping and find current platforms problematic, while showing interest in voice search, immersive and multimodal interfaces to improve the overall shopping experience, despite some issues with voice recognition and response time.},
booktitle = {Proceedings of the Asian HCI Symposium 2023},
pages = {84–88},
numpages = {5},
keywords = {artificial intelligence, human-computer interaction, immersive media, inclusive design},
location = {Online, Indonesia},
series = {Asian CHI '23}
}

@inproceedings{10.1145/3491102.3517469,
author = {Wang, Yanan and Wang, Ruobin and Jung, Crescentia and Kim, Yea-Seul},
title = {What makes web data tables accessible? Insights and a tool for rendering accessible tables for people with visual impairments},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517469},
doi = {10.1145/3491102.3517469},
abstract = {The data table is a basic but versatile representation to communicate data. From government reports to bank statements, tables effectively carry essential data-driven information by visually organizing data using rows, columns, and other arrangements (e.g., merged cells). However, many tables online neglect the accessibility requirements for people who rely on screen readers, such as people who are blind or have low vision (BLV). First, we consolidated guidelines to understand what makes a table inaccessible for BLV people. We conducted an interview study to understand the importance of tables and identify further design requirements for an accessible table. We built a tool that automatically detects HTML formatted tables online and transforms them into accessible tables. Our evaluative study demonstrates how our tool can help participants understand the table’s structure and layout and support smooth navigation when the table is large and complex.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {595},
numpages = {20},
keywords = {Accessibility, Artifact or System, Data Tables, Empirical study that tells us about how people use a system, Individuals with Disabilities \& Assistive Technologies},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3663547.3746401,
author = {Gubbi Mohanbabu, Ananya and Sechayk, Yotam and Pavel, Amy},
title = {Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3746401},
doi = {10.1145/3663547.3746401},
abstract = {Modern web interfaces are unnecessarily complex to use as they overwhelm users with excess text and visuals unrelated to their current goals. Such interfaces can particularly impact screen reader users (SRUs), who may need to navigate content sequentially and thus spend minutes traversing irrelevant elements compared to vision users (VUs) who visually skim in seconds. We present Task Mode, a system that dynamically filters web content based on user-specified goals using large language models to identify and prioritize relevant elements while minimizing distractions. Our approach preserves page structure while offering multiple viewing modes tailored to different access needs. Our user study with 12 participants (6 VUs, 6 SRUs) demonstrates that our approach halved task completion time for SRUs while maintaining performance for VUs, decreasing the completion time gap between groups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the future, reporting that Task Mode supported completing tasks with less effort and fewer distractions. This work demonstrates how designing new interactions simultaneously for visual and non-visual access can reduce rather than reinforce accessibility disparities in future technology created by researchers and practitioners.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {89},
numpages = {18},
keywords = {Task-Specific Interaction; Web Navigation; Accessibility},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3663548.3688491,
author = {Ghosh, Anujay and Padma Reddy, Monalika and Kodandaram, Satwik Ram and Uckun, Utku and Ashok, Vikas and Bi, Xiaojun and Ramakrishnan, IV},
title = {Screen Reading Enabled by Large Language Models},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3688491},
doi = {10.1145/3663548.3688491},
abstract = {Large language models (LLMs), such as the pioneering GPT technology by OpenAI, have undeniably become one of the most significant innovations in recent history. They have achieved phenomenal success across a broad spectrum of applications in numerous industries, transforming how we interact with the digital world. Notwithstanding these remarkable successes, applying LLMs within the realm of accessibility has largely been unexplored. We introduce&nbsp;Savant, as a demonstration of the potential of LLMs for accessibility. Specifically,&nbsp;Savant leverages the impressive text comprehension abilities of LLMs to provide uniform interaction for screen reader users across various applications, mitigating the significant interaction burden imposed by the heterogeneity in user interfaces for blind screen reader users.&nbsp;Savant automates screen reader actions on control elements like buttons, text fields, and drop-down menus via spoken natural language commands (NLCs). Interpreting the NLC, identifying the correct control element, and formulating the action sequence are facilitated by LLMs. Few-shot prompts supply context and guidance for the LLMs to produce appropriate responses, specifically converting the NLC into a correct series of actions on the user interface elements, which are then performed automatically. The demonstration will exhibit&nbsp;Savant’s capability across a variety of exemplar applications, emphasizing its versatility.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {125},
numpages = {5},
keywords = {Accessibility, Assistive technology, Blind users, Computer Interaction, Large language models (LLMs), Uniform interaction},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}





